{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import ciso8601\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Python-all-4b2efe4a27feed92\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"codeparrot/github-code\", streaming=True, split='train', languages=[\"Python\"])\n",
    "iterable_ds = iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_set(code_entry) -> dict[str:int]:\n",
    "    features = dict();\n",
    "    user_repo_name = code_entry['repo_name'].split('/')\n",
    "    owner = user_repo_name[0]\n",
    "    repo = \"\".join(user_repo_name[1:])\n",
    "    repo_response = requests.get(f\"https://api.github.com/repos/{owner}/{repo}\")\n",
    "    content = json.loads(repo_response.text)\n",
    "    # features[\"num_stars\"] = content[\"stargazers_count\"]\n",
    "    features[\"num_forks\"] = content[\"forks_count\"]\n",
    "    features[\"num_watchers\"] = content[\"watchers_count\"]\n",
    "    features[\"num_open_issues\"] = content[\"open_issues_count\"]\n",
    "    parsed_datetime = ciso8601.parse_datetime(content[\"created_at\"])\n",
    "    timestamp = time.mktime(parsed_datetime.timetuple())\n",
    "    features[\"created_at\"] = timestamp\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BELOW IS FOR PYTHON FILES (LEVERAGES INDENTATION RULES)\n",
    "def construct_list_of_functions(raw_code: str) -> list[str]:\n",
    "    lines = raw_code.split('\\n')\n",
    "    start = -1\n",
    "    functions = []\n",
    "    amnt_tabs = 0\n",
    "    for i in range(len(lines)):\n",
    "        # disregard empty lines (prune trailing whitespace later)\n",
    "        if (start != -1 and len(lines[i]) > 0):\n",
    "            amnt_tabs_new = len(lines[i].rstrip()) - len(lines[i].strip())\n",
    "            if amnt_tabs_new <= amnt_tabs:\n",
    "                functions.append((\"\\n\".join(lines[start:i])).strip())\n",
    "                start = -1\n",
    "        elif lines[i].lstrip().startswith(\"def \"):\n",
    "            start = i\n",
    "            amnt_tabs = len(lines[i].rstrip()) - len(lines[i].strip())\n",
    "    return functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_AMNT_CODE_ENTRIES = 1000\n",
    "augmented_code_entry_data = []\n",
    "for i in range(MAX_AMNT_CODE_ENTRIES):\n",
    "    code_entry = next(iterable_ds)\n",
    "    # features_set = construct_feature_set(code_entry)\n",
    "    functions = construct_list_of_functions(code_entry[\"code\"])\n",
    "    data = dict()\n",
    "    data[\"features\"] = []\n",
    "    data[\"snippets\"] = functions\n",
    "    augmented_code_entry_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features': [], 'snippets': ['def __init__(\\n        self,\\n        credential: \"AsyncTokenCredential\",\\n        subscription_id: str,\\n        base_url: str = \"https://management.azure.com\",\\n        **kwargs: Any', 'def _send_request(\\n        self,\\n        request: HttpRequest,\\n        **kwargs: Any']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, bot, settings):\\r\\n\\t\\tself.bot = bot\\r\\n\\t\\tself.settings = settings', 'def getTimeFromOffset(self, offset):\\r\\n\\t\\toffset = offset.replace(\\'+\\', \\'\\')\\r\\n\\t\\t# Split time string by : and get hour/minute values\\r\\n\\t\\ttry:\\r\\n\\t\\t\\thours, minutes = map(int, offset.split(\\':\\'))\\r\\n\\t\\texcept Exception:\\r\\n\\t\\t\\ttry:\\r\\n\\t\\t\\t\\thours = int(offset)\\r\\n\\t\\t\\t\\tminutes = 0\\r\\n\\t\\t\\texcept Exception:\\r\\n\\t\\t\\t\\treturn None\\r\\n\\t\\t\\t\\t# await ctx.channel.send(\\'Offset has to be in +-H:M!\\')\\r\\n\\t\\t\\t\\t# return\\r\\n\\t\\tmsg = \\'UTC\\'\\r\\n\\t\\t# Get the time\\r\\n\\t\\tt = datetime.datetime.utcnow()\\r\\n\\t\\t# Apply offset\\r\\n\\t\\tif hours > 0:\\r\\n\\t\\t\\t# Apply positive offset\\r\\n\\t\\t\\tmsg += \\'+{}\\'.format(offset)\\r\\n\\t\\t\\ttd = datetime.timedelta(hours=hours, minutes=minutes)\\r\\n\\t\\t\\tnewTime = t + td\\r\\n\\t\\telif hours < 0:\\r\\n\\t\\t\\t# Apply negative offset\\r\\n\\t\\t\\tmsg += \\'{}\\'.format(offset)\\r\\n\\t\\t\\ttd = datetime.timedelta(hours=(-1*hours), minutes=(-1*minutes))\\r\\n\\t\\t\\tnewTime = t - td\\r\\n\\t\\telse:\\r\\n\\t\\t\\t# No offset\\r\\n\\t\\t\\tnewTime = t\\r\\n\\t\\treturn { \"zone\" : msg, \"time\" : newTime.strftime(\"%I:%M %p\") }']}, {'features': [], 'snippets': [\"def test_equal_1(self):\\n        self.assertEqual(string_color('Jack'), '79CAE5')\", \"def test_equal_3(self):\\n        self.assertEqual(string_color('Joshua Smith'), '8F00FB')\", \"def test_equal_5(self):\\n        self.assertEqual(string_color('Mathew Smith'), '8B00F1')\"]}, {'features': [], 'snippets': [\"def setup_class(cls):\\n        global users, users_autoinc, metadata\\n        metadata = MetaData(testing.db)\\n        users = Table('users', metadata,\\n            Column('user_id', INT, primary_key=True, autoincrement=False),\\n            Column('user_name', VARCHAR(20)),\\n        )\\n        users_autoinc = Table('users_autoinc', metadata,\\n            Column('user_id', INT, primary_key=True,\\n                                    test_needs_autoincrement=True),\\n            Column('user_name', VARCHAR(20)),\\n        )\\n        metadata.create_all()\", 'def teardown(self):\\n        testing.db.execute(users.delete())', 'def teardown_class(cls):\\n        metadata.drop_all()', 'def test_no_params_option(self):\\n        stmt = \"SELECT \\'%\\'\" + testing.db.dialect.statement_compiler(\\n                                    testing.db.dialect, None).default_from()\\n\\n        conn = testing.db.connect()\\n        result = conn.\\\\\\n                execution_options(no_parameters=True).\\\\\\n                scalar(stmt)\\n        eq_(result, \\'%\\')', 'def test_raw_qmark(self):\\n        def go(conn):\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (?, ?)\\', (1, \\'jack\\'))\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (?, ?)\\', [2, \\'fred\\'])\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (?, ?)\\', [3, \\'ed\\'], [4, \\'horse\\'])\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (?, ?)\\', (5, \\'barney\\'), (6, \\'donkey\\'))\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (?, ?)\\', 7, \\'sally\\')\\n            res = conn.execute(\\'select * from users order by user_id\\')\\n            assert res.fetchall() == [\\n                (1, \\'jack\\'),\\n                (2, \\'fred\\'),\\n                (3, \\'ed\\'),\\n                (4, \\'horse\\'),\\n                (5, \\'barney\\'),\\n                (6, \\'donkey\\'),\\n                (7, \\'sally\\'),\\n                ]\\n            for multiparam, param in [\\n                ((\"jack\", \"fred\"), {}),\\n                (([\"jack\", \"fred\"],), {})\\n            ]:\\n                res = conn.execute(\\n                    \"select * from users where user_name=? or \"\\n                    \"user_name=? order by user_id\",\\n                    *multiparam, **param)\\n                assert res.fetchall() == [\\n                    (1, \\'jack\\'),\\n                    (2, \\'fred\\')\\n                ]\\n            res = conn.execute(\"select * from users where user_name=?\",\\n                \"jack\"\\n            )\\n            assert res.fetchall() == [(1, \\'jack\\')]\\n            conn.execute(\\'delete from users\\')\\n\\n        go(testing.db)\\n        conn = testing.db.connect()\\n        try:\\n            go(conn)\\n        finally:\\n            conn.close()', 'def test_raw_sprintf(self):\\n        def go(conn):\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (%s, %s)\\', [1, \\'jack\\'])\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (%s, %s)\\', [2, \\'ed\\'], [3, \\'horse\\'])\\n            conn.execute(\\'insert into users (user_id, user_name) \\'\\n                         \\'values (%s, %s)\\', 4, \\'sally\\')\\n            conn.execute(\\'insert into users (user_id) values (%s)\\', 5)\\n            res = conn.execute(\\'select * from users order by user_id\\')\\n            assert res.fetchall() == [(1, \\'jack\\'), (2, \\'ed\\'), (3,\\n                    \\'horse\\'), (4, \\'sally\\'), (5, None)]\\n            for multiparam, param in [\\n                ((\"jack\", \"ed\"), {}),\\n                (([\"jack\", \"ed\"],), {})\\n            ]:\\n                res = conn.execute(\\n                    \"select * from users where user_name=%s or \"\\n                    \"user_name=%s order by user_id\",\\n                    *multiparam, **param)\\n                assert res.fetchall() == [\\n                    (1, \\'jack\\'),\\n                    (2, \\'ed\\')\\n                ]\\n            res = conn.execute(\"select * from users where user_name=%s\",\\n                \"jack\"\\n            )\\n            assert res.fetchall() == [(1, \\'jack\\')]\\n\\n            conn.execute(\\'delete from users\\')\\n        go(testing.db)\\n        conn = testing.db.connect()\\n        try:\\n            go(conn)\\n        finally:\\n            conn.close()', \"def test_raw_python(self):\\n        def go(conn):\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (%(id)s, %(name)s)', {'id': 1, 'name'\\n                         : 'jack'})\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (%(id)s, %(name)s)', {'id': 2, 'name'\\n                         : 'ed'}, {'id': 3, 'name': 'horse'})\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (%(id)s, %(name)s)', id=4, name='sally'\\n                         )\\n            res = conn.execute('select * from users order by user_id')\\n            assert res.fetchall() == [(1, 'jack'), (2, 'ed'), (3,\\n                    'horse'), (4, 'sally')]\\n            conn.execute('delete from users')\\n        go(testing.db)\\n        conn = testing.db.connect()\\n        try:\\n            go(conn)\\n        finally:\\n            conn.close()\", \"def test_raw_named(self):\\n        def go(conn):\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (:id, :name)', {'id': 1, 'name': 'jack'\\n                         })\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (:id, :name)', {'id': 2, 'name': 'ed'\\n                         }, {'id': 3, 'name': 'horse'})\\n            conn.execute('insert into users (user_id, user_name) '\\n                         'values (:id, :name)', id=4, name='sally')\\n            res = conn.execute('select * from users order by user_id')\\n            assert res.fetchall() == [(1, 'jack'), (2, 'ed'), (3,\\n                    'horse'), (4, 'sally')]\\n            conn.execute('delete from users')\\n        go(testing.db)\\n        conn= testing.db.connect()\\n        try:\\n            go(conn)\\n        finally:\\n            conn.close()\", 'def test_exception_wrapping_dbapi(self):\\n        conn = testing.db.connect()\\n        for _c in testing.db, conn:\\n            assert_raises_message(\\n                tsa.exc.DBAPIError,\\n                r\"not_a_valid_statement\",\\n                _c.execute, \\'not_a_valid_statement\\'\\n            )', 'def test_exception_wrapping_non_dbapi_error(self):\\n        e = create_engine(\\'sqlite://\\')\\n        e.dialect.is_disconnect = is_disconnect = Mock()\\n\\n        with e.connect() as c:\\n            c.connection.cursor = Mock(\\n                    return_value=Mock(\\n                        execute=Mock(\\n                                side_effect=TypeError(\"I\\'m not a DBAPI error\")\\n                        ))\\n                    )\\n\\n            assert_raises_message(\\n                TypeError,\\n                \"I\\'m not a DBAPI error\",\\n                c.execute, \"select \"\\n            )\\n            eq_(is_disconnect.call_count, 0)', 'def process_bind_param(self, value, dialect):\\n                raise Exception(\"nope\")', 'def test_stmt_exception_non_ascii(self):\\n        name = util.u(\\'méil\\')\\n        with testing.db.connect() as conn:\\n            assert_raises_message(\\n                tsa.exc.StatementError,\\n                util.u(\\n                    \"A value is required for bind parameter \\'uname\\'\"\\n                    r\\'.*SELECT users.user_name AS .m\\\\\\\\xe9il.\\') if util.py2k\\n                else\\n                    util.u(\\n                        \"A value is required for bind parameter \\'uname\\'\"\\n                        \\'.*SELECT users.user_name AS .méil.\\')\\n                    ,\\n                conn.execute,\\n                select([users.c.user_name.label(name)]).where(\\n                                users.c.user_name == bindparam(\"uname\")),\\n                {\\'uname_incorrect\\': \\'foo\\'}\\n            )', 'def test_stmt_exception_pickleable_plus_dbapi(self):\\n        raw = testing.db.raw_connection()\\n        the_orig = None\\n        try:\\n            try:\\n                cursor = raw.cursor()\\n                cursor.execute(\"SELECTINCORRECT\")\\n            except testing.db.dialect.dbapi.DatabaseError as orig:\\n                # py3k has \"orig\" in local scope...\\n                the_orig = orig\\n        finally:\\n            raw.close()\\n        self._test_stmt_exception_pickleable(the_orig)', 'def test_dont_wrap_mixin(self):\\n        class MyException(Exception, tsa.exc.DontWrapMixin):\\n            pass\\n\\n        class MyType(TypeDecorator):\\n            impl = Integer\\n            def process_bind_param(self, value, dialect):\\n                raise MyException(\"nope\")\\n\\n        def _go(conn):\\n            assert_raises_message(\\n                MyException,\\n                \"nope\",\\n                conn.execute,\\n                    select([1]).\\\\\\n                        where(\\n                            column(\\'foo\\') == literal(\\'bar\\', MyType())\\n                        )\\n            )\\n        _go(testing.db)\\n        conn = testing.db.connect()\\n        try:\\n            _go(conn)\\n        finally:\\n            conn.close()', \"def test_engine_level_options(self):\\n        eng = engines.testing_engine(options={'execution_options':\\n                                            {'foo': 'bar'}})\\n        with eng.contextual_connect() as conn:\\n            eq_(conn._execution_options['foo'], 'bar')\\n            eq_(conn.execution_options(bat='hoho')._execution_options['foo'\\n                ], 'bar')\\n            eq_(conn.execution_options(bat='hoho')._execution_options['bat'\\n                ], 'hoho')\\n            eq_(conn.execution_options(foo='hoho')._execution_options['foo'\\n                ], 'hoho')\\n            eng.update_execution_options(foo='hoho')\\n            conn = eng.contextual_connect()\\n            eq_(conn._execution_options['foo'], 'hoho')\", 'def test_generative_engine_execution_options(self):\\n        eng = engines.testing_engine(options={\\'execution_options\\':\\n                                            {\\'base\\': \\'x1\\'}})\\n\\n        eng1 = eng.execution_options(foo=\"b1\")\\n        eng2 = eng.execution_options(foo=\"b2\")\\n        eng1a = eng1.execution_options(bar=\"a1\")\\n        eng2a = eng2.execution_options(foo=\"b3\", bar=\"a2\")\\n\\n        eq_(eng._execution_options,\\n                {\\'base\\': \\'x1\\'})\\n        eq_(eng1._execution_options,\\n                {\\'base\\': \\'x1\\', \\'foo\\': \\'b1\\'})\\n        eq_(eng2._execution_options,\\n                {\\'base\\': \\'x1\\', \\'foo\\': \\'b2\\'})\\n        eq_(eng1a._execution_options,\\n                {\\'base\\': \\'x1\\', \\'foo\\': \\'b1\\', \\'bar\\': \\'a1\\'})\\n        eq_(eng2a._execution_options,\\n                {\\'base\\': \\'x1\\', \\'foo\\': \\'b3\\', \\'bar\\': \\'a2\\'})\\n        is_(eng1a.pool, eng.pool)\\n\\n        # test pool is shared\\n        eng2.dispose()\\n        is_(eng1a.pool, eng2.pool)\\n        is_(eng.pool, eng2.pool)', 'def test_generative_engine_event_dispatch(self):\\n        canary = []\\n        def l1(*arg, **kw):\\n            canary.append(\"l1\")\\n        def l2(*arg, **kw):\\n            canary.append(\"l2\")\\n        def l3(*arg, **kw):\\n            canary.append(\"l3\")\\n\\n        eng = engines.testing_engine(options={\\'execution_options\\':\\n                                            {\\'base\\': \\'x1\\'}})\\n        event.listen(eng, \"before_execute\", l1)\\n\\n        eng1 = eng.execution_options(foo=\"b1\")\\n        event.listen(eng, \"before_execute\", l2)\\n        event.listen(eng1, \"before_execute\", l3)\\n\\n        eng.execute(select([1])).close()\\n        eng1.execute(select([1])).close()\\n\\n        eq_(canary, [\"l1\", \"l2\", \"l3\", \"l1\", \"l2\"])', 'def test_generative_engine_event_dispatch_hasevents(self):\\n        def l1(*arg, **kw):\\n            pass\\n        eng = create_engine(testing.db.url)\\n        assert not eng._has_events\\n        event.listen(eng, \"before_execute\", l1)\\n        eng2 = eng.execution_options(foo=\\'bar\\')\\n        assert eng2._has_events', 'def execute(self, stmt, params=None, **kw):\\n                if \"test unicode returns\" in stmt:\\n                    raise self.engine.dialect.dbapi.DatabaseError(\"boom\")\\n                else:\\n                    return super(MockCursor, self).execute(stmt, params, **kw)', 'def test_works_after_dispose(self):\\n        eng = create_engine(testing.db.url)\\n        for i in range(3):\\n            eq_(eng.scalar(select([1])), 1)\\n            eng.dispose()', \"def define_tables(cls, metadata):\\n        cls.table = Table('exec_test', metadata,\\n            Column('a', Integer),\\n            Column('b', Integer),\\n            test_needs_acid=True\\n        )\", 'def go(conn, x, value=None):\\n            if is_transaction:\\n                conn = conn.connection\\n            conn.execute(self.table.insert().values(a=x, b=value))', 'def _trans_rollback_fn(self, is_transaction=False):\\n        def go(conn, x, value=None):\\n            if is_transaction:\\n                conn = conn.connection\\n            conn.execute(self.table.insert().values(a=x, b=value))\\n            raise Exception(\"breakage\")\\n        return go', 'def _assert_fn(self, x, value=None):\\n        eq_(\\n            testing.db.execute(self.table.select()).fetchall(),\\n            [(x, value)]\\n        )', 'def test_transaction_engine_ctx_begin_fails(self):\\n        engine = engines.testing_engine()\\n\\n        mock_connection = Mock(\\n            return_value=Mock(\\n                        begin=Mock(side_effect=Exception(\"boom\"))\\n                    )\\n        )\\n        engine._connection_cls = mock_connection\\n        assert_raises(\\n            Exception,\\n            engine.begin\\n        )\\n\\n        eq_(\\n            mock_connection.return_value.close.mock_calls,\\n            [call()]\\n        )', \"def test_transaction_tlocal_engine_ctx_commit(self):\\n        fn = self._trans_fn()\\n        engine = engines.testing_engine(options=dict(\\n                                strategy='threadlocal',\\n                                pool=testing.db.pool))\\n        ctx = engine.begin()\\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\\n        self._assert_fn(5, value=8)\", 'def test_transaction_connection_ctx_commit(self):\\n        fn = self._trans_fn(True)\\n        conn = testing.db.connect()\\n        ctx = conn.begin()\\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\\n        self._assert_fn(5, value=8)', 'def test_connection_as_ctx(self):\\n        fn = self._trans_fn()\\n        ctx = testing.db.connect()\\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\\n        # autocommit is on\\n        self._assert_fn(5, value=8)', 'def test_connect_as_ctx_noautocommit(self):\\n        fn = self._trans_fn()\\n        self._assert_no_data()\\n        ctx = testing.db.connect().execution_options(autocommit=False)\\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\\n        # autocommit is off\\n        self._assert_no_data()', 'def test_transaction_engine_fn_rollback(self):\\n        fn = self._trans_rollback_fn()\\n        assert_raises_message(\\n            Exception,\\n            \"breakage\",\\n            testing.db.transaction, fn, 5, value=8\\n        )\\n        self._assert_no_data()', 'def test_transaction_connection_fn_rollback(self):\\n        fn = self._trans_rollback_fn()\\n        conn = testing.db.connect()\\n        assert_raises(\\n            Exception,\\n            conn.transaction, fn, 5, value=8\\n        )\\n        self._assert_no_data()', \"def setup_class(cls):\\n        global users, metadata\\n        metadata = MetaData(testing.db)\\n        users = Table('users', metadata,\\n            Column('user_id', INT, primary_key=True,\\n                            test_needs_autoincrement=True),\\n            Column('user_name', VARCHAR(20)),\\n        )\\n        metadata.create_all()\", 'def teardown(self):\\n        testing.db.execute(users.delete())', 'def teardown_class(cls):\\n        metadata.drop_all()', \"def _engine_fixture(self):\\n        buf = util.StringIO()\\n        def dump(sql, *multiparams, **params):\\n            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))\\n        engine = create_engine('postgresql://', strategy='mock', executor=dump)\\n        return engine, buf\", 'def test_nontuple_row(self):\\n        \"\"\"ensure the C version of BaseRowProxy handles\\n        duck-type-dependent rows.\"\"\"\\n\\n        from sqlalchemy.engine import RowProxy\\n\\n        class MyList(object):\\n            def __init__(self, l):\\n                self.l = l\\n\\n            def __len__(self):\\n                return len(self.l)\\n\\n            def __getitem__(self, i):\\n                return list.__getitem__(self.l, i)\\n\\n        proxy = RowProxy(object(), MyList([\\'value\\']), [None], {\\'key\\'\\n                         : (None, None, 0), 0: (None, None, 0)})\\n        eq_(list(proxy), [\\'value\\'])\\n        eq_(proxy[0], \\'value\\')\\n        eq_(proxy[\\'key\\'], \\'value\\')', 'def test_no_rowcount_on_selects_inserts(self):\\n        \"\"\"assert that rowcount is only called on deletes and updates.\\n\\n        This because cursor.rowcount may can be expensive on some dialects\\n        such as Firebird, however many dialects require it be called\\n        before the cursor is closed.\\n\\n        \"\"\"\\n\\n        metadata = self.metadata\\n\\n        engine = engines.testing_engine()\\n\\n        t = Table(\\'t1\\', metadata,\\n            Column(\\'data\\', String(10))\\n        )\\n        metadata.create_all(engine)\\n\\n        with patch.object(engine.dialect.execution_ctx_cls, \"rowcount\") as mock_rowcount:\\n            mock_rowcount.__get__ = Mock()\\n            engine.execute(t.insert(),\\n                                {\\'data\\': \\'d1\\'},\\n                                {\\'data\\': \\'d2\\'},\\n                                {\\'data\\': \\'d3\\'})\\n\\n            eq_(len(mock_rowcount.__get__.mock_calls), 0)\\n\\n            eq_(\\n                    engine.execute(t.select()).fetchall(),\\n                    [(\\'d1\\', ), (\\'d2\\', ), (\\'d3\\', )]\\n            )\\n            eq_(len(mock_rowcount.__get__.mock_calls), 0)\\n\\n            engine.execute(t.update(), {\\'data\\': \\'d4\\'})\\n\\n            eq_(len(mock_rowcount.__get__.mock_calls), 1)\\n\\n            engine.execute(t.delete())\\n            eq_(len(mock_rowcount.__get__.mock_calls), 2)', \"def test_row_c_sequence_check(self):\\n        import csv\\n        import collections\\n\\n        metadata = MetaData()\\n        metadata.bind = 'sqlite://'\\n        users = Table('users', metadata,\\n            Column('id', Integer, primary_key=True),\\n            Column('name', String(40)),\\n        )\\n        users.create()\\n\\n        users.insert().execute(name='Test')\\n        row = users.select().execute().fetchone()\\n\\n        s = util.StringIO()\\n        writer = csv.writer(s)\\n        # csv performs PySequenceCheck call\\n        writer.writerow(row)\\n        assert s.getvalue().strip() == '1,Test'\", 'def test_empty_accessors(self):\\n        statements = [\\n            (\\n                \"select 1\",\\n                [\\n                    lambda r: r.last_inserted_params(),\\n                    lambda r: r.last_updated_params(),\\n                    lambda r: r.prefetch_cols(),\\n                    lambda r: r.postfetch_cols(),\\n                    lambda r : r.inserted_primary_key\\n                ],\\n                \"Statement is not a compiled expression construct.\"\\n            ),\\n            (\\n                select([1]),\\n                [\\n                    lambda r: r.last_inserted_params(),\\n                    lambda r : r.inserted_primary_key\\n                ],\\n                r\"Statement is not an insert\\\\(\\\\) expression construct.\"\\n            ),\\n            (\\n                select([1]),\\n                [\\n                    lambda r: r.last_updated_params(),\\n                ],\\n                r\"Statement is not an update\\\\(\\\\) expression construct.\"\\n            ),\\n            (\\n                select([1]),\\n                [\\n                    lambda r: r.prefetch_cols(),\\n                    lambda r : r.postfetch_cols()\\n                ],\\n                r\"Statement is not an insert\\\\(\\\\) \"\\n                r\"or update\\\\(\\\\) expression construct.\"\\n            ),\\n        ]\\n\\n        for stmt, meths, msg in statements:\\n            r = testing.db.execute(stmt)\\n            try:\\n                for meth in meths:\\n                    assert_raises_message(\\n                        tsa.exc.InvalidRequestError,\\n                        msg,\\n                        meth, r\\n                    )\\n\\n            finally:\\n                r.close()', 'def test_dialect_conn_options(self):\\n        engine = testing_engine(\"sqlite://\", options=dict(_initialize=False))\\n        engine.dialect = Mock()\\n        conn = engine.connect()\\n        c2 = conn.execution_options(foo=\"bar\")\\n        eq_(\\n            engine.dialect.set_connection_execution_options.mock_calls,\\n            [call(c2, {\"foo\": \"bar\"})]\\n        )', 'def test_dialect_engine_construction_options(self):\\n        dialect = Mock()\\n        engine = Engine(Mock(), dialect, Mock(),\\n                                execution_options={\"foo\": \"bar\"})\\n        eq_(\\n            dialect.set_engine_execution_options.mock_calls,\\n            [call(engine, {\"foo\": \"bar\"})]\\n        )', 'def test_propagate_option_engine_to_connection(self):\\n        e1 = testing_engine(\"sqlite://\",\\n                        options=dict(execution_options={\"foo\": \"bar\"}))\\n        e2 = e1.execution_options(bat=\"hoho\")\\n        c1 = e1.connect()\\n        c2 = e2.connect()\\n        eq_(c1._execution_options, {\"foo\": \"bar\"})\\n        eq_(c2._execution_options, {\"foo\": \"bar\", \"bat\": \"hoho\"})', 'def setup_class(cls):\\n        from sqlalchemy.engine import base, default\\n        cls.engine = engine = testing_engine(\\'sqlite://\\')\\n        m = MetaData()\\n        cls.table = t = Table(\\'test\\', m,\\n            Column(\\'x\\', Integer, primary_key=True),\\n            Column(\\'y\\', String(50, convert_unicode=\\'force\\'))\\n        )\\n        m.create_all(engine)\\n        engine.execute(t.insert(), [\\n            {\\'x\\':i, \\'y\\':\"t_%d\" % i} for i in range(1, 12)\\n        ])', 'def get_result_proxy(self):\\n                return cls(self)', 'def test_plain(self):\\n        self._test_proxy(_result.ResultProxy)', 'def test_fully_buffered_result_proxy(self):\\n        self._test_proxy(_result.FullyBufferedResultProxy)', 'def tearDown(self):\\n        Engine.dispatch._clear()\\n        Engine._has_events = False', 'def test_per_engine_independence(self):\\n        e1 = testing_engine(config.db_url)\\n        e2 = testing_engine(config.db_url)\\n\\n        canary = Mock()\\n        event.listen(e1, \"before_execute\", canary)\\n        s1 = select([1])\\n        s2 = select([2])\\n        e1.execute(s1)\\n        e2.execute(s2)\\n        eq_(\\n            [arg[1][1] for arg in canary.mock_calls], [s1]\\n        )\\n        event.listen(e2, \"before_execute\", canary)\\n        e1.execute(s1)\\n        e2.execute(s2)\\n        eq_([arg[1][1] for arg in canary.mock_calls], [s1, s1, s2])', 'def test_per_connection_plus_engine(self):\\n        canary = Mock()\\n        e1 = testing_engine(config.db_url)\\n\\n        event.listen(e1, \"before_execute\", canary.be1)\\n\\n        conn = e1.connect()\\n        event.listen(conn, \"before_execute\", canary.be2)\\n        conn.execute(select([1]))\\n\\n        eq_(canary.be1.call_count, 1)\\n        eq_(canary.be2.call_count, 1)\\n\\n        conn._branch().execute(select([1]))\\n        eq_(canary.be1.call_count, 2)\\n        eq_(canary.be2.call_count, 2)', 'def test_force_conn_events_false(self):\\n        canary = Mock()\\n        e1 = create_engine(config.db_url)\\n        assert not e1._has_events\\n\\n        event.listen(e1, \"before_execute\", canary.be1)\\n\\n        conn = e1._connection_cls(e1, connection=e1.raw_connection(),\\n                            _has_events=False)\\n\\n        conn.execute(select([1]))\\n\\n        eq_(canary.be1.call_count, 0)\\n\\n        conn._branch().execute(select([1]))\\n        eq_(canary.be1.call_count, 0)', 'def test_cursor_events_execute(self):\\n        canary = Mock()\\n        e1 = testing_engine(config.db_url)\\n\\n        event.listen(e1, \"before_cursor_execute\", canary.bce)\\n        event.listen(e1, \"after_cursor_execute\", canary.ace)\\n\\n        stmt = str(select([1]).compile(dialect=e1.dialect))\\n\\n        with e1.connect() as conn:\\n\\n            result = conn.execute(stmt)\\n\\n        ctx = result.context\\n        eq_(canary.bce.mock_calls,\\n                [call(conn, ctx.cursor, stmt, ctx.parameters[0], ctx, False)])\\n        eq_(canary.ace.mock_calls,\\n                [call(conn, ctx.cursor, stmt, ctx.parameters[0], ctx, False)])', 'def before_execute(conn, clauseelement, multiparams, params):\\n            assert isinstance(multiparams, (list, tuple))\\n            assert isinstance(params, dict)', \"def test_execute_events(self):\\n\\n        stmts = []\\n        cursor_stmts = []\\n\\n        def execute(conn, clauseelement, multiparams,\\n                                                    params ):\\n            stmts.append((str(clauseelement), params, multiparams))\\n\\n        def cursor_execute(conn, cursor, statement, parameters,\\n                                context, executemany):\\n            cursor_stmts.append((str(statement), parameters, None))\\n\\n\\n        for engine in [\\n            engines.testing_engine(options=dict(implicit_returning=False)),\\n            engines.testing_engine(options=dict(implicit_returning=False,\\n                                   strategy='threadlocal')),\\n            engines.testing_engine(options=dict(implicit_returning=False)).\\\\\\n                connect()\\n            ]:\\n            event.listen(engine, 'before_execute', execute)\\n            event.listen(engine, 'before_cursor_execute', cursor_execute)\\n            m = MetaData(engine)\\n            t1 = Table('t1', m,\\n                Column('c1', Integer, primary_key=True),\\n                Column('c2', String(50), default=func.lower('Foo'),\\n                                            primary_key=True)\\n            )\\n            m.create_all()\\n            try:\\n                t1.insert().execute(c1=5, c2='some data')\\n                t1.insert().execute(c1=6)\\n                eq_(engine.execute('select * from t1').fetchall(), [(5,\\n                    'some data'), (6, 'foo')])\\n            finally:\\n                m.drop_all()\\n\\n            compiled = [('CREATE TABLE t1', {}, None),\\n                        ('INSERT INTO t1 (c1, c2)',\\n                                {'c2': 'some data', 'c1': 5}, None),\\n                        ('INSERT INTO t1 (c1, c2)',\\n                        {'c1': 6}, None),\\n                        ('select * from t1', {}, None),\\n                        ('DROP TABLE t1', {}, None)]\\n\\n            # or engine.dialect.preexecute_pk_sequences:\\n            if not testing.against('oracle+zxjdbc'):\\n                cursor = [\\n                    ('CREATE TABLE t1', {}, ()),\\n                    ('INSERT INTO t1 (c1, c2)', {\\n                        'c2': 'some data', 'c1': 5},\\n                        (5, 'some data')),\\n                    ('SELECT lower', {'lower_2': 'Foo'},\\n                        ('Foo', )),\\n                    ('INSERT INTO t1 (c1, c2)',\\n                     {'c2': 'foo', 'c1': 6},\\n                     (6, 'foo')),\\n                    ('select * from t1', {}, ()),\\n                    ('DROP TABLE t1', {}, ()),\\n                    ]\\n            else:\\n                insert2_params = 6, 'Foo'\\n                if testing.against('oracle+zxjdbc'):\\n                    insert2_params += (ReturningParam(12), )\\n                cursor = [('CREATE TABLE t1', {}, ()),\\n                          ('INSERT INTO t1 (c1, c2)',\\n                            {'c2': 'some data', 'c1': 5}, (5, 'some data')),\\n                          ('INSERT INTO t1 (c1, c2)', {'c1': 6,\\n                          'lower_2': 'Foo'}, insert2_params),\\n                          ('select * from t1', {}, ()),\\n                          ('DROP TABLE t1', {}, ())]\\n                                # bind param name 'lower_2' might\\n                                # be incorrect\\n            self._assert_stmts(compiled, stmts)\\n            self._assert_stmts(cursor, cursor_stmts)\", \"def execute(conn, *args, **kw):\\n            canary.append('execute')\", 'def test_retval_flag(self):\\n        canary = []\\n        def tracker(name):\\n            def go(conn, *args, **kw):\\n                canary.append(name)\\n            return go\\n\\n        def execute(conn, clauseelement, multiparams, params):\\n            canary.append(\\'execute\\')\\n            return clauseelement, multiparams, params\\n\\n        def cursor_execute(conn, cursor, statement,\\n                        parameters, context, executemany):\\n            canary.append(\\'cursor_execute\\')\\n            return statement, parameters\\n\\n        engine = engines.testing_engine()\\n\\n        assert_raises(\\n            tsa.exc.ArgumentError,\\n            event.listen, engine, \"begin\", tracker(\"begin\"), retval=True\\n        )\\n\\n        event.listen(engine, \"before_execute\", execute, retval=True)\\n        event.listen(engine, \"before_cursor_execute\", cursor_execute, retval=True)\\n        engine.execute(select([1]))\\n        eq_(\\n            canary, [\\'execute\\', \\'cursor_execute\\']\\n        )', 'def test_execution_options(self):\\n        engine = engines.testing_engine()\\n\\n        engine_tracker = Mock()\\n        conn_tracker = Mock()\\n\\n        event.listen(engine, \"set_engine_execution_options\", engine_tracker)\\n        event.listen(engine, \"set_connection_execution_options\", conn_tracker)\\n\\n        e2 = engine.execution_options(e1=\\'opt_e1\\')\\n        c1 = engine.connect()\\n        c2 = c1.execution_options(c1=\\'opt_c1\\')\\n        c3 = e2.connect()\\n        c4 = c3.execution_options(c3=\\'opt_c3\\')\\n        eq_(\\n            engine_tracker.mock_calls,\\n            [call(e2, {\\'e1\\': \\'opt_e1\\'})]\\n        )\\n        eq_(\\n            conn_tracker.mock_calls,\\n            [call(c2, {\"c1\": \"opt_c1\"}), call(c4, {\"c3\": \"opt_c3\"})]\\n        )', 'def test_cursor_execute(self):\\n        canary = []\\n        def tracker(name):\\n            def go(conn, cursor, statement, parameters, context, executemany):\\n                canary.append((statement, context))\\n            return go\\n        engine = engines.testing_engine()\\n\\n\\n        t = Table(\\'t\\', self.metadata,\\n                    Column(\\'x\\', Integer, Sequence(\\'t_id_seq\\'), primary_key=True),\\n                    implicit_returning=False\\n                    )\\n        self.metadata.create_all(engine)\\n        with engine.begin() as conn:\\n            event.listen(conn, \\'before_cursor_execute\\', tracker(\\'cursor_execute\\'))\\n            conn.execute(t.insert())\\n        # we see the sequence pre-executed in the first call\\n        assert \"t_id_seq\" in canary[0][0]\\n        assert \"INSERT\" in canary[1][0]\\n        # same context\\n        is_(\\n            canary[0][1], canary[1][1]\\n        )', 'def tracker(name):\\n            def go(conn, *args, **kw):\\n                canary.append(name)\\n            return go', \"def test_transactional_advanced(self):\\n        canary1 = []\\n        def tracker1(name):\\n            def go(*args, **kw):\\n                canary1.append(name)\\n            return go\\n        canary2 = []\\n        def tracker2(name):\\n            def go(*args, **kw):\\n                canary2.append(name)\\n            return go\\n\\n        engine = engines.testing_engine()\\n        for name in ['begin', 'savepoint',\\n                    'rollback_savepoint', 'release_savepoint',\\n                    'rollback', 'begin_twophase',\\n                       'prepare_twophase', 'commit_twophase']:\\n            event.listen(engine, '%s' % name, tracker1(name))\\n\\n        conn = engine.connect()\\n        for name in ['begin', 'savepoint',\\n                    'rollback_savepoint', 'release_savepoint',\\n                    'rollback', 'begin_twophase',\\n                       'prepare_twophase', 'commit_twophase']:\\n            event.listen(conn, '%s' % name, tracker2(name))\\n\\n        trans = conn.begin()\\n        trans2 = conn.begin_nested()\\n        conn.execute(select([1]))\\n        trans2.rollback()\\n        trans2 = conn.begin_nested()\\n        conn.execute(select([1]))\\n        trans2.commit()\\n        trans.rollback()\\n\\n        trans = conn.begin_twophase()\\n        conn.execute(select([1]))\\n        trans.prepare()\\n        trans.commit()\\n\\n        eq_(canary1, ['begin', 'savepoint',\\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\\n                    'rollback', 'begin_twophase',\\n                       'prepare_twophase', 'commit_twophase']\\n        )\\n        eq_(canary2, ['begin', 'savepoint',\\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\\n                    'rollback', 'begin_twophase',\\n                       'prepare_twophase', 'commit_twophase']\\n        )\", 'def tearDown(self):\\n        Engine.dispatch._clear()\\n        Engine._has_events = False', 'def test_legacy_dbapi_error_no_ad_hoc_context(self):\\n        engine = engines.testing_engine()\\n\\n        listener = Mock(return_value=None)\\n        event.listen(engine, \\'dbapi_error\\', listener)\\n\\n        nope = Exception(\"nope\")\\n        class MyType(TypeDecorator):\\n            impl = Integer\\n            def process_bind_param(self, value, dialect):\\n                raise nope\\n\\n        with engine.connect() as conn:\\n            assert_raises_message(\\n                tsa.exc.StatementError,\\n                r\"nope \\\\(original cause: Exception: nope\\\\) u?\\'SELECT 1 \",\\n                conn.execute,\\n                    select([1]).where(\\n                            column(\\'foo\\') == literal(\\'bar\\', MyType()))\\n            )\\n        # no legacy event\\n        eq_(listener.mock_calls, [])', 'def test_handle_error(self):\\n        engine = engines.testing_engine()\\n        canary = Mock(return_value=None)\\n\\n        event.listen(engine, \"handle_error\", canary)\\n\\n        with engine.connect() as conn:\\n            try:\\n                conn.execute(\"SELECT FOO FROM I_DONT_EXIST\")\\n                assert False\\n            except tsa.exc.DBAPIError as e:\\n                ctx = canary.mock_calls[0][1][0]\\n\\n                eq_(ctx.original_exception, e.orig)\\n                is_(ctx.sqlalchemy_exception, e)\\n                eq_(ctx.statement, \"SELECT FOO FROM I_DONT_EXIST\")', 'def err(context):\\n            stmt = context.statement\\n            exception = context.original_exception\\n            if \"ERROR ONE\" in str(stmt):\\n                return MyException(\"my exception\")\\n            elif \"ERROR TWO\" in str(stmt):\\n                return exception\\n            else:\\n                return None', 'def test_exception_event_reraise_chaining(self):\\n        engine = engines.testing_engine()\\n\\n        class MyException1(Exception):\\n            pass\\n\\n        class MyException2(Exception):\\n            pass\\n\\n        class MyException3(Exception):\\n            pass\\n\\n        @event.listens_for(engine, \\'handle_error\\', retval=True)\\n        def err1(context):\\n            stmt = context.statement\\n\\n            if \"ERROR ONE\" in str(stmt) or \"ERROR TWO\" in str(stmt) \\\\\\n                    or \"ERROR THREE\" in str(stmt):\\n                return MyException1(\"my exception\")\\n            elif \"ERROR FOUR\" in str(stmt):\\n                raise MyException3(\"my exception short circuit\")\\n\\n        @event.listens_for(engine, \\'handle_error\\', retval=True)\\n        def err2(context):\\n            stmt = context.statement\\n            if (\"ERROR ONE\" in str(stmt) or \"ERROR FOUR\" in str(stmt)) \\\\\\n                    and isinstance(context.chained_exception, MyException1):\\n                raise MyException2(\"my exception chained\")\\n            elif \"ERROR TWO\" in str(stmt):\\n                return context.chained_exception\\n            else:\\n                return None\\n\\n        conn = engine.connect()\\n\\n        with patch.object(engine.\\n                dialect.execution_ctx_cls,\\n                \"handle_dbapi_exception\") as patched:\\n            assert_raises_message(\\n                MyException2,\\n                \"my exception chained\",\\n                conn.execute, \"SELECT \\'ERROR ONE\\' FROM I_DONT_EXIST\"\\n            )\\n            eq_(patched.call_count, 1)\\n\\n        with patch.object(engine.\\n                dialect.execution_ctx_cls,\\n                \"handle_dbapi_exception\") as patched:\\n            assert_raises(\\n                MyException1,\\n                conn.execute, \"SELECT \\'ERROR TWO\\' FROM I_DONT_EXIST\"\\n            )\\n            eq_(patched.call_count, 1)\\n\\n        with patch.object(engine.\\n                dialect.execution_ctx_cls,\\n                \"handle_dbapi_exception\") as patched:\\n            # test that non None from err1 isn\\'t cancelled out\\n            # by err2\\n            assert_raises(\\n                MyException1,\\n                conn.execute, \"SELECT \\'ERROR THREE\\' FROM I_DONT_EXIST\"\\n            )\\n            eq_(patched.call_count, 1)\\n\\n        with patch.object(engine.\\n                dialect.execution_ctx_cls,\\n                \"handle_dbapi_exception\") as patched:\\n            assert_raises(\\n                tsa.exc.DBAPIError,\\n                conn.execute, \"SELECT \\'ERROR FIVE\\' FROM I_DONT_EXIST\"\\n            )\\n            eq_(patched.call_count, 1)\\n\\n        with patch.object(engine.\\n                dialect.execution_ctx_cls,\\n                \"handle_dbapi_exception\") as patched:\\n            assert_raises_message(\\n                MyException3,\\n                \"my exception short circuit\",\\n                conn.execute, \"SELECT \\'ERROR FOUR\\' FROM I_DONT_EXIST\"\\n            )\\n            eq_(patched.call_count, 1)', 'def process_bind_param(self, value, dialect):\\n                raise nope', 'def test_exception_event_non_dbapi_error(self):\\n        \"\"\"test that dbapi_error is called with a context in\\n        cases where DBAPI raises an exception that is not a DBAPI\\n        exception, e.g. internal errors or encoding problems.\\n\\n        \"\"\"\\n        engine = engines.testing_engine()\\n\\n        listener = Mock(return_value=None)\\n        event.listen(engine, \\'handle_error\\', listener)\\n\\n        nope = TypeError(\"I\\'m not a DBAPI error\")\\n        with engine.connect() as c:\\n            c.connection.cursor = Mock(\\n                    return_value=Mock(\\n                        execute=Mock(\\n                                side_effect=nope\\n                        ))\\n                    )\\n\\n            assert_raises_message(\\n                TypeError,\\n                \"I\\'m not a DBAPI error\",\\n                c.execute, \"select \"\\n            )\\n        ctx = listener.mock_calls[0][1][0]\\n        eq_(ctx.statement, \"select \")\\n        is_(ctx.is_disconnect, False)\\n        is_(ctx.original_exception, nope)', 'def evt(ctx):\\n            ctx.is_disconnect = evt_value', 'def test_alter_disconnect_to_true(self):\\n        self._test_alter_disconnect(False, True)\\n        self._test_alter_disconnect(True, True)', 'def test_proxy(self):\\n\\n        stmts = []\\n        cursor_stmts = []\\n\\n        class MyProxy(ConnectionProxy):\\n            def execute(\\n                self,\\n                conn,\\n                execute,\\n                clauseelement,\\n                *multiparams,\\n                **params\\n                ):\\n                stmts.append((str(clauseelement), params, multiparams))\\n                return execute(clauseelement, *multiparams, **params)\\n\\n            def cursor_execute(\\n                self,\\n                execute,\\n                cursor,\\n                statement,\\n                parameters,\\n                context,\\n                executemany,\\n                ):\\n                cursor_stmts.append((str(statement), parameters, None))\\n                return execute(cursor, statement, parameters, context)\\n\\n        def assert_stmts(expected, received):\\n            for stmt, params, posn in expected:\\n                if not received:\\n                    assert False, \"Nothing available for stmt: %s\" % stmt\\n                while received:\\n                    teststmt, testparams, testmultiparams = \\\\\\n                        received.pop(0)\\n                    teststmt = re.compile(r\\'[\\\\n\\\\t ]+\\', re.M).sub(\\' \\',\\n                            teststmt).strip()\\n                    if teststmt.startswith(stmt) and (testparams\\n                            == params or testparams == posn):\\n                        break\\n\\n        for engine in \\\\\\n            engines.testing_engine(options=dict(implicit_returning=False,\\n                                   proxy=MyProxy())), \\\\\\n            engines.testing_engine(options=dict(implicit_returning=False,\\n                                   proxy=MyProxy(),\\n                                   strategy=\\'threadlocal\\')):\\n            m = MetaData(engine)\\n            t1 = Table(\\'t1\\', m,\\n                Column(\\'c1\\', Integer, primary_key=True),\\n                Column(\\'c2\\', String(50), default=func.lower(\\'Foo\\'),\\n                                            primary_key=True)\\n            )\\n            m.create_all()\\n            try:\\n                t1.insert().execute(c1=5, c2=\\'some data\\')\\n                t1.insert().execute(c1=6)\\n                eq_(engine.execute(\\'select * from t1\\').fetchall(), [(5,\\n                    \\'some data\\'), (6, \\'foo\\')])\\n            finally:\\n                m.drop_all()\\n            engine.dispose()\\n            compiled = [(\\'CREATE TABLE t1\\', {}, None),\\n                        (\\'INSERT INTO t1 (c1, c2)\\', {\\'c2\\': \\'some data\\',\\n                        \\'c1\\': 5}, None), (\\'INSERT INTO t1 (c1, c2)\\',\\n                        {\\'c1\\': 6}, None), (\\'select * from t1\\', {},\\n                        None), (\\'DROP TABLE t1\\', {}, None)]\\n            if not testing.against(\\'oracle+zxjdbc\\'):  # or engine.dialect.pr\\n                                                      # eexecute_pk_sequence\\n                                                      # s:\\n                cursor = [\\n                    (\\'CREATE TABLE t1\\', {}, ()),\\n                    (\\'INSERT INTO t1 (c1, c2)\\', {\\'c2\\': \\'some data\\', \\'c1\\'\\n                     : 5}, (5, \\'some data\\')),\\n                    (\\'SELECT lower\\', {\\'lower_2\\': \\'Foo\\'},\\n                        (\\'Foo\\', )),\\n                    (\\'INSERT INTO t1 (c1, c2)\\', {\\'c2\\': \\'foo\\', \\'c1\\': 6},\\n                     (6, \\'foo\\')),\\n                    (\\'select * from t1\\', {}, ()),\\n                    (\\'DROP TABLE t1\\', {}, ()),\\n                    ]\\n            else:\\n                insert2_params = 6, \\'Foo\\'\\n                if testing.against(\\'oracle+zxjdbc\\'):\\n                    insert2_params += (ReturningParam(12), )\\n                cursor = [(\\'CREATE TABLE t1\\', {}, ()),\\n                          (\\'INSERT INTO t1 (c1, c2)\\', {\\'c2\\': \\'some data\\'\\n                          , \\'c1\\': 5}, (5, \\'some data\\')),\\n                          (\\'INSERT INTO t1 (c1, c2)\\', {\\'c1\\': 6,\\n                          \\'lower_2\\': \\'Foo\\'}, insert2_params),\\n                          (\\'select * from t1\\', {}, ()), (\\'DROP TABLE t1\\'\\n                          , {}, ())]  # bind param name \\'lower_2\\' might\\n                                      # be incorrect\\n            assert_stmts(compiled, stmts)\\n            assert_stmts(cursor, cursor_stmts)', \"def test_options(self):\\n        canary = []\\n        class TrackProxy(ConnectionProxy):\\n            def __getattribute__(self, key):\\n                fn = object.__getattribute__(self, key)\\n                def go(*arg, **kw):\\n                    canary.append(fn.__name__)\\n                    return fn(*arg, **kw)\\n                return go\\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\\n        conn = engine.connect()\\n        c2 = conn.execution_options(foo='bar')\\n        eq_(c2._execution_options, {'foo':'bar'})\\n        c2.execute(select([1]))\\n        c3 = c2.execution_options(bar='bat')\\n        eq_(c3._execution_options, {'foo':'bar', 'bar':'bat'})\\n        eq_(canary, ['execute', 'cursor_execute'])\", \"def test_transactional(self):\\n        canary = []\\n        class TrackProxy(ConnectionProxy):\\n            def __getattribute__(self, key):\\n                fn = object.__getattribute__(self, key)\\n                def go(*arg, **kw):\\n                    canary.append(fn.__name__)\\n                    return fn(*arg, **kw)\\n                return go\\n\\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\\n        conn = engine.connect()\\n        trans = conn.begin()\\n        conn.execute(select([1]))\\n        trans.rollback()\\n        trans = conn.begin()\\n        conn.execute(select([1]))\\n        trans.commit()\\n\\n        eq_(canary, [\\n            'begin', 'execute', 'cursor_execute', 'rollback',\\n            'begin', 'execute', 'cursor_execute', 'commit',\\n            ])\", \"def test_transactional_advanced(self):\\n        canary = []\\n        class TrackProxy(ConnectionProxy):\\n            def __getattribute__(self, key):\\n                fn = object.__getattribute__(self, key)\\n                def go(*arg, **kw):\\n                    canary.append(fn.__name__)\\n                    return fn(*arg, **kw)\\n                return go\\n\\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\\n        conn = engine.connect()\\n\\n        trans = conn.begin()\\n        trans2 = conn.begin_nested()\\n        conn.execute(select([1]))\\n        trans2.rollback()\\n        trans2 = conn.begin_nested()\\n        conn.execute(select([1]))\\n        trans2.commit()\\n        trans.rollback()\\n\\n        trans = conn.begin_twophase()\\n        conn.execute(select([1]))\\n        trans.prepare()\\n        trans.commit()\\n\\n        canary = [t for t in canary if t not in ('cursor_execute', 'execute')]\\n        eq_(canary, ['begin', 'savepoint',\\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\\n                    'rollback', 'begin_twophase',\\n                       'prepare_twophase', 'commit_twophase']\\n        )\", 'def _run_test(self, retval):\\n        m1 = Mock()\\n\\n        m1.do_execute.return_value = retval\\n        m1.do_executemany.return_value = retval\\n        m1.do_execute_no_params.return_value = retval\\n        e = engines.testing_engine(options={\"_initialize\": False})\\n\\n        event.listen(e, \"do_execute\", m1.do_execute)\\n        event.listen(e, \"do_executemany\", m1.do_executemany)\\n        event.listen(e, \"do_execute_no_params\", m1.do_execute_no_params)\\n\\n        e.dialect.do_execute = m1.real_do_execute\\n        e.dialect.do_executemany = m1.real_do_executemany\\n        e.dialect.do_execute_no_params = m1.real_do_execute_no_params\\n\\n        def mock_the_cursor(cursor, *arg):\\n            arg[-1].get_result_proxy = Mock(return_value=Mock(context=arg[-1]))\\n            return retval\\n\\n        m1.real_do_execute.side_effect = m1.do_execute.side_effect = mock_the_cursor\\n        m1.real_do_executemany.side_effect = m1.do_executemany.side_effect = mock_the_cursor\\n        m1.real_do_execute_no_params.side_effect = m1.do_execute_no_params.side_effect = mock_the_cursor\\n\\n        with e.connect() as conn:\\n            yield conn, m1', 'def _test_do_execute(self, retval):\\n        with self._run_test(retval) as (conn, m1):\\n            result = conn.execute(\"insert into table foo\", {\"foo\": \"bar\"})\\n        self._assert(\\n            retval,\\n            m1.do_execute, m1.real_do_execute,\\n            [call(\\n                    result.context.cursor,\\n                    \"insert into table foo\",\\n                    {\"foo\": \"bar\"}, result.context)]\\n        )', 'def _test_do_execute_no_params(self, retval):\\n        with self._run_test(retval) as (conn, m1):\\n            result = conn.execution_options(no_parameters=True).\\\\\\n                execute(\"insert into table foo\")\\n        self._assert(\\n            retval,\\n            m1.do_execute_no_params, m1.real_do_execute_no_params,\\n            [call(\\n                    result.context.cursor,\\n                    \"insert into table foo\", result.context)]\\n        )', 'def test_do_execute_w_replace(self):\\n        self._test_do_execute(True)', 'def test_do_executemany_w_replace(self):\\n        self._test_do_executemany(True)', 'def test_do_execute_no_params_w_replace(self):\\n        self._test_do_execute_no_params(True)', 'def test_cursor_execute_w_replace(self):\\n        self._test_cursor_execute(True)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def t_ID(self, t):\\n        r'\\\\d+\\\\.([uU]|[lL]|[uU][lL]|[lL][uU])?'\\n        t.value = int(t.value[:-1])\\n        return t\", \"def t_TASK(self, t):\\n        r'((?!\\\\(x\\\\))).+'\\n        return t\", 'def t_error(self, t):\\n        raise SyntaxError(\\n            \"Illegal character: \\'%s\\' at Line %d\" % (t.value[0], t.lineno)\\n        )', 'def p_error(self, p):\\n        if p:\\n            raise SyntaxError(\\n                \"Character \\'%s\\' at line %d\" % (p.value[0], p.lineno)\\n            )\\n        else:\\n            raise SyntaxError(\"SyntaxError at EOF\")', 'def p_translation_unit(self, p):\\n        \"\"\"\\n        translation_unit : translate_task\\n                         | translation_unit translate_task\\n                         |\\n        \"\"\"\\n        pass', 'def __init__(self):\\n        self.parser = yacc.yacc(module=self, debug=0, write_tables=0)']}, {'features': [], 'snippets': ['def setup(self, memcached, memcached_cluster):\\n        self.storage_url = \"memcached://localhost:22122\"', 'def test_fixed_window(self):\\n        storage = MemcachedStorage(\"memcached://localhost:22122\")\\n        limiter = FixedWindowRateLimiter(storage)\\n        per_min = RateLimitItemPerSecond(10)\\n        start = time.time()\\n        count = 0\\n\\n        while time.time() - start < 0.5 and count < 10:\\n            assert limiter.hit(per_min)\\n            count += 1\\n        assert not limiter.hit(per_min)\\n\\n        while time.time() - start <= 1:\\n            time.sleep(0.1)\\n        assert limiter.hit(per_min)', 'def test_fixed_window_cluster(self):\\n        storage = MemcachedStorage(\"memcached://localhost:22122,localhost:22123\")\\n        limiter = FixedWindowRateLimiter(storage)\\n        per_min = RateLimitItemPerSecond(10)\\n        start = time.time()\\n        count = 0\\n\\n        while time.time() - start < 0.5 and count < 10:\\n            assert limiter.hit(per_min)\\n            count += 1\\n        assert not limiter.hit(per_min)\\n\\n        while time.time() - start <= 1:\\n            time.sleep(0.1)\\n        assert limiter.hit(per_min)', 'def test_fixed_window_with_elastic_expiry(self):\\n        storage = MemcachedStorage(\"memcached://localhost:22122\")\\n        limiter = FixedWindowElasticExpiryRateLimiter(storage)\\n        per_sec = RateLimitItemPerSecond(2, 2)\\n\\n        assert limiter.hit(per_sec)\\n        time.sleep(1)\\n        assert limiter.hit(per_sec)\\n        assert not limiter.test(per_sec)\\n        time.sleep(1)\\n        assert not limiter.test(per_sec)\\n        time.sleep(1)\\n        assert limiter.test(per_sec)', 'def test_fixed_window_with_elastic_expiry_cluster(self):\\n        storage = MemcachedStorage(\"memcached://localhost:22122,localhost:22123\")\\n        limiter = FixedWindowElasticExpiryRateLimiter(storage)\\n        per_sec = RateLimitItemPerSecond(2, 2)\\n\\n        assert limiter.hit(per_sec)\\n        time.sleep(1)\\n        assert limiter.hit(per_sec)\\n        assert not limiter.test(per_sec)\\n        time.sleep(1)\\n        assert not limiter.test(per_sec)\\n        time.sleep(1)\\n        assert limiter.test(per_sec)']}, {'features': [], 'snippets': [\"def _get_data_dir(self, db_version):\\n        # Try to get from svc first\\n        output = run('svcprop -p config/data postgresql')\\n        if output.stdout and exists(output.stdout, use_sudo=True):\\n            return output.stdout\\n        return base_postgres.PostgresInstall._get_data_dir(self, db_version)\", \"def _restart_db_server(self, db_version):\\n        sudo('svcadm restart postgresql')\", \"def _start_db_server(self, db_version):\\n        sudo('svcadm enable postgresql')\", \"def install_package(self):\\n        sudo('pkg_add libevent')\\n        with cd('/tmp'):\\n            run('wget %s' %self.pgbouncer_src)\\n            sudo('pkg_add %s' %self.pkg_name)\", 'def _get_passwd(self, username):\\n        with hide(\\'output\\'):\\n            string = run(\\'echo \"select usename, passwd from pg_shadow where \\'\\n                         \\'usename=\\\\\\'%s\\\\\\' order by 1\" | sudo su postgres -c \\'\\n                         \\'\"psql\"\\' %username)\\n\\n        user, passwd = string.split(\\'\\\\n\\')[2].split(\\'|\\')\\n        user = user.strip()\\n        passwd = passwd.strip()\\n\\n        __, tmp_name = tempfile.mkstemp()\\n        fn = open(tmp_name, \\'w\\')\\n        fn.write(\\'\"%s\" \"%s\" \"\"\\\\n\\' %(user, passwd))\\n        fn.close()\\n        put(tmp_name, \\'%s/pgbouncer.userlist\\'%self.config_dir, use_sudo=True)\\n        local(\\'rm %s\\' %tmp_name)', 'def run(self, section=None):\\n        \"\"\"\\n        \"\"\"\\n\\n        sudo(\\'mkdir -p /opt/pkg/bin\\')\\n        sudo(\"ln -sf /opt/local/bin/awk /opt/pkg/bin/nawk\")\\n        sudo(\"ln -sf /opt/local/bin/sed /opt/pkg/bin/nbsed\")\\n\\n        self.install_package()\\n\\n        svc_method = os.path.join(env.configs_dir, \\'pgbouncer.xml\\')\\n        put(svc_method, self.config_dir, use_sudo=True)\\n\\n        home = run(\\'bash -c \"echo ~postgres\"\\')\\n        bounce_home = os.path.join(home, \\'pgbouncer\\')\\n\\n        pidfile = os.path.join(bounce_home, \\'pgbouncer.pid\\')\\n        self._setup_parameter(\\'%s/pgbouncer.ini\\' %self.config_dir,\\n                              pidfile=pidfile, **self.config)\\n\\n        if not section:\\n            section = \\'db-server\\'\\n        username = self._get_username(section)\\n        self._get_passwd(username)\\n        # postgres should be the owner of these config files\\n        sudo(\\'chown -R postgres:postgres %s\\' %self.config_dir)\\n\\n        sudo(\\'mkdir -p %s\\' % bounce_home)\\n        sudo(\\'chown postgres:postgres %s\\' % bounce_home)\\n\\n        sudo(\\'mkdir -p /var/log/pgbouncer\\')\\n        sudo(\\'chown postgres:postgres /var/log/pgbouncer\\')\\n\\n        # set up log\\n        sudo(\\'logadm -C 3 -p1d -c -w /var/log/pgbouncer/pgbouncer.log -z 1\\')\\n        run(\\'svccfg import %s/pgbouncer.xml\\' %self.config_dir)\\n\\n        # start pgbouncer\\n        sudo(\\'svcadm enable pgbouncer\\')']}, {'features': [], 'snippets': ['def parse_dsn(dsn_string):\\n    \"\"\"Parse a connection string and return the associated driver\"\"\"\\n    dsn = urlparse(dsn_string)\\n    scheme = dsn.scheme.split(\\'+\\')[0]\\n    username = password = host = port = None\\n    host = dsn.netloc\\n    if \\'@\\' in host:\\n        username, host = host.split(\\'@\\')\\n        if \\':\\' in username:\\n            username, password = username.split(\\':\\')\\n            password = unquote(password)\\n        username = unquote(username)\\n    if \\':\\' in host:\\n        host, port = host.split(\\':\\')\\n        port = int(port)\\n    database = dsn.path.split(\\'?\\')[0][1:]\\n    query = dsn.path.split(\\'?\\')[1] if \\'?\\' in dsn.path else dsn.query\\n    kwargs = dict(parse_qsl(query, True))\\n    if scheme == \\'sqlite\\':\\n        return SQLiteDriver, [dsn.path], {}\\n    elif scheme == \\'mysql\\':\\n        kwargs[\\'user\\'] = username or \\'root\\'\\n        kwargs[\\'db\\'] = database\\n        if port:\\n            kwargs[\\'port\\'] = port\\n        if host:\\n            kwargs[\\'host\\'] = host\\n        if password:\\n            kwargs[\\'passwd\\'] = password\\n        return MySQLDriver, [], kwargs\\n    elif scheme == \\'postgresql\\':\\n        kwargs[\\'user\\'] = username or \\'postgres\\'\\n        kwargs[\\'database\\'] = database\\n        if port:\\n            kwargs[\\'port\\'] = port\\n        if \\'unix_socket\\' in kwargs:\\n            kwargs[\\'host\\'] = kwargs.pop(\\'unix_socket\\')\\n        elif host:\\n            kwargs[\\'host\\'] = host\\n        if password:\\n            kwargs[\\'password\\'] = password\\n        return PostgreSQLDriver, [], kwargs\\n    else:\\n        raise ValueError(\\'Unknown driver %s\\' % dsn_string)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_deploy(self, cav, ue):\\n        s3 = boto.connect_s3()\\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\\n\\n        with tempfile.NamedTemporaryFile() as tmp:\\n            result = CliRunner().invoke(rubberjack, [\\'deploy\\', tmp.name], catch_exceptions=False)\\n\\n            self.assertEquals(result.exit_code, 0, result.output)', \"def test_promote(self, ue, de):\\n        de.return_value = {\\n            'DescribeEnvironmentsResponse': {\\n                'DescribeEnvironmentsResult': {\\n                    'Environments': [\\n                        {\\n                            'EnvironmentName': 'laterpay-devnull-live',  # FIXME Remove hardcoded EnvName\\n                            'VersionLabel': 'old',\\n                        },\\n                        {\\n                            'EnvironmentName': 'laterpay-devnull-dev',  # FIXME Remove hardcoded EnvName\\n                            'VersionLabel': 'new',\\n                        },\\n                    ],\\n                },\\n            },\\n        }\\n\\n        CliRunner().invoke(rubberjack, ['promote'], catch_exceptions=False)\", \"def test_promoting_same_version(self, ue, de, se):\\n        de.return_value = {\\n            'DescribeEnvironmentsResponse': {\\n                'DescribeEnvironmentsResult': {\\n                    'Environments': [\\n                        {\\n                            'EnvironmentName': 'laterpay-devnull-live',  # FIXME Remove hardcoded EnvName\\n                            'VersionLabel': 'same',\\n                        },\\n                        {\\n                            'EnvironmentName': 'laterpay-devnull-dev',  # FIXME Remove hardcoded EnvName\\n                            'VersionLabel': 'same',\\n                        },\\n                    ],\\n                },\\n            },\\n        }\\n\\n        CliRunner().invoke(rubberjack, ['promote'], catch_exceptions=False)\\n\\n        self.assertTrue(se.called)\", \"def test_sigv4(self):\\n        CliRunner().invoke(rubberjack, ['--sigv4-host', 'foo', 'deploy'], catch_exceptions=False)\", 'def test_deploy_to_custom_environment(self, ue, cav):\\n        s3 = boto.connect_s3()\\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\\n\\n        with tempfile.NamedTemporaryFile() as tmp:\\n            result = CliRunner().invoke(rubberjack, [\\'deploy\\', \\'--environment\\', \\'wibble\\', tmp.name], catch_exceptions=False)\\n\\n            self.assertEquals(result.exit_code, 0, result.output)\\n\\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn\\'t called, but it should\")\\n        self.assertEqual(ue.call_count, 1, \"update_environment wasn\\'t called, but it should\")', 'def test_deploy_without_updating_the_environment(self, ue, cav):\\n        s3 = boto.connect_s3()\\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\\n\\n        with tempfile.NamedTemporaryFile() as tmp:\\n            result = CliRunner().invoke(rubberjack, [\\'deploy\\', \\'--no-update-environment\\', tmp.name], catch_exceptions=False)\\n\\n            self.assertEquals(result.exit_code, 0, result.output)\\n\\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn\\'t called, but it should\")\\n        self.assertEqual(ue.call_count, 0, \"update_environment was called, but it shouldn\\'t\")', 'def test_deploy_to_custom_bucket(self, ue, cav):\\n        bucket_name = \\'rbbrjck-test\\'\\n        s3 = boto.connect_s3()\\n        s3.create_bucket(bucket_name)\\n\\n        with tempfile.NamedTemporaryFile() as tmp:\\n            result = CliRunner().invoke(rubberjack, [\\'--bucket\\', bucket_name, \\'deploy\\', tmp.name], catch_exceptions=False)\\n\\n            self.assertEquals(result.exit_code, 0, result.output)\\n\\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn\\'t called, but it should\")\\n        self.assertEqual(ue.call_count, 1, \"update_environment wasn\\'t called, but it should\")\\n\\n        _, cav_kwargs = cav.call_args\\n        self.assertEqual(bucket_name, cav_kwargs[\\'s3_bucket\\'])']}, {'features': [], 'snippets': [\"def tobytes(obj):\\n        if isinstance(obj, str):\\n            obj = obj.encode('UTF-8')\\n        assert isinstance(obj, bytes)\\n        return obj\", \"def tobytes(obj):\\n        if isinstance(obj, unicode):\\n            obj = obj.encode('UTF-8')\\n        assert isinstance(obj, str)\\n        return obj\", 'def oswritebytes(fd, obj):\\n    os.write(fd, tobytes(obj))', 'def StdCapture(out=True, err=True, in_=True):\\n    return capture.MultiCapture(out, err, in_, Capture=capture.SysCapture)', 'def test_getmethod_default_no_fd(self, monkeypatch):\\n        from _pytest.capture import pytest_addoption\\n        from _pytest.config import Parser\\n        parser = Parser()\\n        pytest_addoption(parser)\\n        default = parser._groups[0].options[0].default\\n        assert default == \"fd\" if hasattr(os, \"dup\") else \"sys\"\\n        parser = Parser()\\n        monkeypatch.delattr(os, \\'dup\\', raising=False)\\n        pytest_addoption(parser)\\n        assert parser._groups[0].options[0].default == \"sys\"', 'def test_capturing_basic_api(self, method):\\n        capouter = StdCaptureFD()\\n        old = sys.stdout, sys.stderr, sys.stdin\\n        try:\\n            capman = CaptureManager(method)\\n            capman.start_global_capturing()\\n            outerr = capman.suspend_global_capture()\\n            assert outerr == (\"\", \"\")\\n            outerr = capman.suspend_global_capture()\\n            assert outerr == (\"\", \"\")\\n            print(\"hello\")\\n            out, err = capman.suspend_global_capture()\\n            if method == \"no\":\\n                assert old == (sys.stdout, sys.stderr, sys.stdin)\\n            else:\\n                assert not out\\n            capman.resume_global_capture()\\n            print(\"hello\")\\n            out, err = capman.suspend_global_capture()\\n            if method != \"no\":\\n                assert out == \"hello\\\\n\"\\n            capman.stop_global_capturing()\\n        finally:\\n            capouter.stop_capturing()', 'def test_init_capturing(self):\\n        capouter = StdCaptureFD()\\n        try:\\n            capman = CaptureManager(\"fd\")\\n            capman.start_global_capturing()\\n            pytest.raises(AssertionError, \"capman.start_global_capturing()\")\\n            capman.stop_global_capturing()\\n        finally:\\n            capouter.stop_capturing()', 'def test_capturing_unicode(testdir, method):\\n    if hasattr(sys, \"pypy_version_info\") and sys.pypy_version_info < (2, 2):\\n        pytest.xfail(\"does not work on pypy < 2.2\")\\n    if sys.version_info >= (3, 0):\\n        obj = \"\\'b\\\\u00f6y\\'\"\\n    else:\\n        obj = \"u\\'\\\\u00f6y\\'\"\\n    testdir.makepyfile(\"\"\"\\n        # coding=utf8\\n        # taken from issue 227 from nosetests\\n        def test_unicode():\\n            import sys\\n            print (sys.stdout)\\n            print (%s)\\n    \"\"\" % obj)\\n    result = testdir.runpytest(\"--capture=%s\" % method)\\n    result.stdout.fnmatch_lines([\\n        \"*1 passed*\"\\n    ])', 'def test_capturing_bytes_in_utf8_encoding(testdir, method):\\n    testdir.makepyfile(\"\"\"\\n        def test_unicode():\\n            print (\\'b\\\\\\\\u00f6y\\')\\n    \"\"\")\\n    result = testdir.runpytest(\"--capture=%s\" % method)\\n    result.stdout.fnmatch_lines([\\n        \"*1 passed*\"\\n    ])', 'def test_capture_and_fixtures(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            def setup_module(mod):\\n                print (\"setup module\")\\n            def setup_function(function):\\n                print (\"setup \" + function.__name__)\\n            def test_func1():\\n                print (\"in func1\")\\n                assert 0\\n            def test_func2():\\n                print (\"in func2\")\\n                assert 0\\n        \"\"\")\\n        result = testdir.runpytest(p)\\n        result.stdout.fnmatch_lines([\\n            \"setup module*\",\\n            \"setup test_func1*\",\\n            \"in func1*\",\\n            \"setup test_func2*\",\\n            \"in func2*\",\\n        ])', 'def test_capture_scope_cache(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            import sys\\n            def setup_module(func):\\n                print (\"module-setup\")\\n            def setup_function(func):\\n                print (\"function-setup\")\\n            def test_func():\\n                print (\"in function\")\\n                assert 0\\n            def teardown_function(func):\\n                print (\"in teardown\")\\n        \"\"\")\\n        result = testdir.runpytest(p)\\n        result.stdout.fnmatch_lines([\\n            \"*test_func():*\",\\n            \"*Captured stdout during setup*\",\\n            \"module-setup*\",\\n            \"function-setup*\",\\n            \"*Captured stdout*\",\\n            \"in teardown*\",\\n        ])', 'def test_func1():\\n                print (\"in func1\")', 'def test_teardown_capturing(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            def setup_function(function):\\n                print (\"setup func1\")\\n            def teardown_function(function):\\n                print (\"teardown func1\")\\n                assert 0\\n            def test_func1():\\n                print (\"in func1\")\\n                pass\\n        \"\"\")\\n        result = testdir.runpytest(p)\\n        result.stdout.fnmatch_lines([\\n            \\'*teardown_function*\\',\\n            \\'*Captured stdout*\\',\\n            \"setup func1*\",\\n            \"in func1*\",\\n            \"teardown func1*\",\\n            # \"*1 fixture failure*\"\\n        ])', 'def teardown_module(mod):\\n                print (\"teardown module\")\\n                assert 0', 'def test_capturing_outerr(self, testdir):\\n        p1 = testdir.makepyfile(\"\"\"\\n            import sys\\n            def test_capturing():\\n                print (42)\\n                sys.stderr.write(str(23))\\n            def test_capturing_error():\\n                print (1)\\n                sys.stderr.write(str(2))\\n                raise ValueError\\n        \"\"\")\\n        result = testdir.runpytest(p1)\\n        result.stdout.fnmatch_lines([\\n            \"*test_capturing_outerr.py .F*\",\\n            \"====* FAILURES *====\",\\n            \"____*____\",\\n            \"*test_capturing_outerr.py:8: ValueError\",\\n            \"*--- Captured stdout *call*\",\\n            \"1\",\\n            \"*--- Captured stderr *call*\",\\n            \"2\",\\n        ])', 'def test_logging_stream_ownership(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            def test_logging():\\n                import logging\\n                import pytest\\n                stream = capture.CaptureIO()\\n                logging.basicConfig(stream=stream)\\n                stream.close() # to free memory/release resources\\n        \"\"\")\\n        result = testdir.runpytest_subprocess(p)\\n        assert result.stderr.str().find(\"atexit\") == -1', 'def setup_function(function):\\n                logging.warn(\"hello1\")', 'def teardown_function(function):\\n                logging.warn(\"hello3\")\\n                assert 0', 'def test_logging_and_crossscope_fixtures(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            import logging\\n            def setup_module(function):\\n                logging.warn(\"hello1\")\\n\\n            def test_logging():\\n                logging.warn(\"hello2\")\\n                assert 0\\n\\n            def teardown_module(function):\\n                logging.warn(\"hello3\")\\n                assert 0\\n        \"\"\")\\n        for optargs in ((\\'--capture=sys\\',), (\\'--capture=fd\\',)):\\n            print(optargs)\\n            result = testdir.runpytest_subprocess(p, *optargs)\\n            s = result.stdout.str()\\n            result.stdout.fnmatch_lines([\\n                \"*WARN*hello3\",  # errors come first\\n                \"*WARN*hello1\",\\n                \"*WARN*hello2\",\\n            ])\\n            # verify proper termination\\n            assert \"closed\" not in s', 'def test_conftestlogging_and_test_logging(self, testdir):\\n        testdir.makeconftest(\"\"\"\\n                import logging\\n                logging.basicConfig()\\n        \"\"\")\\n        # make sure that logging is still captured in tests\\n        p = testdir.makepyfile(\"\"\"\\n            def test_hello():\\n                import logging\\n                logging.warn(\"hello433\")\\n                assert 0\\n        \"\"\")\\n        result = testdir.runpytest_subprocess(p, \"-p\", \"no:capturelog\")\\n        assert result.ret != 0\\n        result.stdout.fnmatch_lines([\\n            \"WARNING*hello433*\",\\n        ])\\n        assert \\'something\\' not in result.stderr.str()\\n        assert \\'operation on closed file\\' not in result.stderr.str()', 'def test_std_functional(self, testdir, opt):\\n        reprec = testdir.inline_runsource(\"\"\"\\n            def test_hello(capsys):\\n                print (42)\\n                out, err = capsys.readouterr()\\n                assert out.startswith(\"42\")\\n        \"\"\", *opt)\\n        reprec.assertoutcome(passed=1)', 'def test_one(capsys, capfd):\\n                pass', 'def test_capturing_getfixturevalue(self, testdir):\\n        \"\"\"Test that asking for \"capfd\" and \"capsys\" using request.getfixturevalue\\n        in the same test is an error.\\n        \"\"\"\\n        testdir.makepyfile(\"\"\"\\n            def test_one(capsys, request):\\n                request.getfixturevalue(\"capfd\")\\n            def test_two(capfd, request):\\n                request.getfixturevalue(\"capsys\")\\n        \"\"\")\\n        result = testdir.runpytest()\\n        result.stdout.fnmatch_lines([\\n            \"*test_one*\",\\n            \"*capsys*capfd*same*time*\",\\n            \"*test_two*\",\\n            \"*capfd*capsys*same*time*\",\\n            \"*2 failed in*\",\\n        ])', 'def test_one(capsys, capfdbinary):\\n                pass', 'def test_capture_is_represented_on_failure_issue128(self, testdir, method):\\n        p = testdir.makepyfile(\"\"\"\\n            def test_hello(cap%s):\\n                print (\"xxx42xxx\")\\n                assert 0\\n        \"\"\" % method)\\n        result = testdir.runpytest(p)\\n        result.stdout.fnmatch_lines([\\n            \"xxx42xxx\",\\n        ])', 'def test_stdfd_functional(self, testdir):\\n        reprec = testdir.inline_runsource(\"\"\"\\n            def test_hello(capfd):\\n                import os\\n                os.write(1, \"42\".encode(\\'ascii\\'))\\n                out, err = capfd.readouterr()\\n                assert out.startswith(\"42\")\\n                capfd.close()\\n        \"\"\")\\n        reprec.assertoutcome(passed=1)', 'def test_capfdbinary(self, testdir):\\n        reprec = testdir.inline_runsource(\"\"\"\\n            def test_hello(capfdbinary):\\n                import os\\n                # some likely un-decodable bytes\\n                os.write(1, b\\'\\\\\\\\xfe\\\\\\\\x98\\\\\\\\x20\\')\\n                out, err = capfdbinary.readouterr()\\n                assert out == b\\'\\\\\\\\xfe\\\\\\\\x98\\\\\\\\x20\\'\\n                assert err == b\\'\\'\\n        \"\"\")\\n        reprec.assertoutcome(passed=1)', 'def test_capsysbinary(self, testdir):\\n        reprec = testdir.inline_runsource(\"\"\"\\n            def test_hello(capsysbinary):\\n                import sys\\n                # some likely un-decodable bytes\\n                sys.stdout.buffer.write(b\\'\\\\\\\\xfe\\\\\\\\x98\\\\\\\\x20\\')\\n                out, err = capsysbinary.readouterr()\\n                assert out == b\\'\\\\\\\\xfe\\\\\\\\x98\\\\\\\\x20\\'\\n                assert err == b\\'\\'\\n        \"\"\")\\n        reprec.assertoutcome(passed=1)', 'def test_capsysbinary_forbidden_in_python2(self, testdir):\\n        testdir.makepyfile(\"\"\"\\n            def test_hello(capsysbinary):\\n                pass\\n        \"\"\")\\n        result = testdir.runpytest()\\n        result.stdout.fnmatch_lines([\\n            \"*test_hello*\",\\n            \"*capsysbinary is only supported on python 3*\",\\n            \"*1 error in*\",\\n        ])', 'def test_hello(capsys, missingarg):\\n                pass', 'def test_keyboardinterrupt_disables_capturing(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            def test_hello(capfd):\\n                import os\\n                os.write(1, str(42).encode(\\'ascii\\'))\\n                raise KeyboardInterrupt()\\n        \"\"\")\\n        result = testdir.runpytest_subprocess(p)\\n        result.stdout.fnmatch_lines([\\n            \"*KeyboardInterrupt*\"\\n        ])\\n        assert result.ret == 2', 'def test_capture_and_logging(self, testdir):\\n        p = testdir.makepyfile(\"\"\"\\n            import logging\\n            def test_log(capsys):\\n                logging.error(\\'x\\')\\n            \"\"\")\\n        result = testdir.runpytest_subprocess(p)\\n        assert \\'closed\\' not in result.stderr.str()', 'def test_disabled_capture_fixture(self, testdir, fixture, no_capture):\\n        testdir.makepyfile(\"\"\"\\n            def test_disabled({fixture}):\\n                print(\\'captured before\\')\\n                with {fixture}.disabled():\\n                    print(\\'while capture is disabled\\')\\n                print(\\'captured after\\')\\n                assert {fixture}.readouterr() == (\\'captured before\\\\\\\\ncaptured after\\\\\\\\n\\', \\'\\')\\n\\n            def test_normal():\\n                print(\\'test_normal executed\\')\\n        \"\"\".format(fixture=fixture))\\n        args = (\\'-s\\',) if no_capture else ()\\n        result = testdir.runpytest_subprocess(*args)\\n        result.stdout.fnmatch_lines(\"\"\"\\n            *while capture is disabled*\\n        \"\"\")\\n        assert \\'captured before\\' not in result.stdout.str()\\n        assert \\'captured after\\' not in result.stdout.str()\\n        if no_capture:\\n            assert \\'test_normal executed\\' in result.stdout.str()\\n        else:\\n            assert \\'test_normal executed\\' not in result.stdout.str()', 'def test_fixture_use_by_other_fixtures(self, testdir, fixture):\\n        \"\"\"\\n        Ensure that capsys and capfd can be used by other fixtures during setup and teardown.\\n        \"\"\"\\n        testdir.makepyfile(\"\"\"\\n            from __future__ import print_function\\n            import sys\\n            import pytest\\n\\n            @pytest.fixture\\n            def captured_print({fixture}):\\n                print(\\'stdout contents begin\\')\\n                print(\\'stderr contents begin\\', file=sys.stderr)\\n                out, err = {fixture}.readouterr()\\n\\n                yield out, err\\n\\n                print(\\'stdout contents end\\')\\n                print(\\'stderr contents end\\', file=sys.stderr)\\n                out, err = {fixture}.readouterr()\\n                assert out == \\'stdout contents end\\\\\\\\n\\'\\n                assert err == \\'stderr contents end\\\\\\\\n\\'\\n\\n            def test_captured_print(captured_print):\\n                out, err = captured_print\\n                assert out == \\'stdout contents begin\\\\\\\\n\\'\\n                assert err == \\'stderr contents begin\\\\\\\\n\\'\\n        \"\"\".format(fixture=fixture))\\n        result = testdir.runpytest_subprocess()\\n        result.stdout.fnmatch_lines(\"*1 passed*\")\\n        assert \\'stdout contents begin\\' not in result.stdout.str()\\n        assert \\'stderr contents begin\\' not in result.stdout.str()', 'def pytest_runtest_setup(item):\\n            raise ValueError(42)', 'def test_fdfuncarg_skips_on_no_osdup(testdir):\\n    testdir.makepyfile(\"\"\"\\n        import os\\n        if hasattr(os, \\'dup\\'):\\n            del os.dup\\n        def test_hello(capfd):\\n            pass\\n    \"\"\")\\n    result = testdir.runpytest_subprocess(\"--capture=no\")\\n    result.stdout.fnmatch_lines([\\n        \"*1 skipped*\"\\n    ])', 'def pytest_runtest_setup():\\n            print (\"hello19\")', 'def test_capture_badoutput_issue412(testdir):\\n    testdir.makepyfile(\"\"\"\\n        import os\\n\\n        def test_func():\\n            omg = bytearray([1,129,1])\\n            os.write(1, omg)\\n            assert 0\\n        \"\"\")\\n    result = testdir.runpytest(\\'--cap=fd\\')\\n    result.stdout.fnmatch_lines(\\'\\'\\'\\n        *def test_func*\\n        *assert 0*\\n        *Captured*\\n        *1 failed*\\n    \\'\\'\\')', 'def pytest_runtest_setup():\\n            print (\"hello19\")', 'def test_capture_binary_output(testdir):\\n    testdir.makepyfile(r\"\"\"\\n        import pytest\\n\\n        def test_a():\\n            import sys\\n            import subprocess\\n            subprocess.call([sys.executable, __file__])\\n\\n        def test_foo():\\n            import os;os.write(1, b\\'\\\\xc3\\')\\n\\n        if __name__ == \\'__main__\\':\\n            test_foo()\\n        \"\"\")\\n    result = testdir.runpytest(\\'--assert=plain\\')\\n    result.assert_outcomes(passed=2)', \"def bad_snap(self):\\n            raise Exception('boom')\", 'def test_text(self):\\n        f = capture.CaptureIO()\\n        f.write(\"hello\")\\n        s = f.getvalue()\\n        assert s == \"hello\"\\n        f.close()', 'def test_write_bytes_to_buffer(self):\\n        \"\"\"In python3, stdout / stderr are text io wrappers (exposing a buffer\\n        property of the underlying bytestream).  See issue #1407\\n        \"\"\"\\n        f = capture.CaptureIO()\\n        f.buffer.write(b\\'foo\\\\r\\\\n\\')\\n        assert f.getvalue() == \\'foo\\\\r\\\\n\\'', 'def test_dontreadfrominput():\\n    from _pytest.capture import DontReadFromInput\\n    f = DontReadFromInput()\\n    assert not f.isatty()\\n    pytest.raises(IOError, f.read)\\n    pytest.raises(IOError, f.readlines)\\n    pytest.raises(IOError, iter, f)\\n    pytest.raises(UnsupportedOperation, f.fileno)\\n    f.close()  # just for completeness', 'def test_dontreadfrominput_buffer_python3():\\n    from _pytest.capture import DontReadFromInput\\n    f = DontReadFromInput()\\n    fb = f.buffer\\n    assert not fb.isatty()\\n    pytest.raises(IOError, fb.read)\\n    pytest.raises(IOError, fb.readlines)\\n    pytest.raises(IOError, iter, fb)\\n    pytest.raises(ValueError, fb.fileno)\\n    f.close()  # just for completeness', 'def test_dontreadfrominput_buffer_python2():\\n    from _pytest.capture import DontReadFromInput\\n    f = DontReadFromInput()\\n    with pytest.raises(AttributeError):\\n        f.buffer\\n    f.close()  # just for completeness', 'def tmpfile(testdir):\\n    f = testdir.makepyfile(\"\").open(\\'wb+\\')\\n    yield f\\n    if not f.closed:\\n        f.close()', 'def test_dupfile(tmpfile):\\n    flist = []\\n    for i in range(5):\\n        nf = capture.safe_text_dupfile(tmpfile, \"wb\")\\n        assert nf != tmpfile\\n        assert nf.fileno() != tmpfile.fileno()\\n        assert nf not in flist\\n        print(i, end=\"\", file=nf)\\n        flist.append(nf)\\n\\n    fname_open = flist[0].name\\n    assert fname_open == repr(flist[0].buffer)\\n\\n    for i in range(5):\\n        f = flist[i]\\n        f.close()\\n    fname_closed = flist[0].name\\n    assert fname_closed == repr(flist[0].buffer)\\n    assert fname_closed != fname_open\\n    tmpfile.seek(0)\\n    s = tmpfile.read()\\n    assert \"01234\" in repr(s)\\n    tmpfile.close()\\n    assert fname_closed == repr(flist[0].buffer)', 'def test_dupfile_on_textio():\\n    io = py.io.TextIO()\\n    f = capture.safe_text_dupfile(io, \"wb\")\\n    f.write(\"hello\")\\n    assert io.getvalue() == \"hello\"\\n    assert not hasattr(f, \\'name\\')', 'def lsof_check():\\n    pid = os.getpid()\\n    try:\\n        out = py.process.cmdexec(\"lsof -p %d\" % pid)\\n    except (py.process.cmdexec.Error, UnicodeDecodeError):\\n        # about UnicodeDecodeError, see note on pytester\\n        pytest.skip(\"could not run \\'lsof\\'\")\\n    yield\\n    out2 = py.process.cmdexec(\"lsof -p %d\" % pid)\\n    len1 = len([x for x in out.split(\"\\\\n\") if \"REG\" in x])\\n    len2 = len([x for x in out2.split(\"\\\\n\") if \"REG\" in x])\\n    assert len2 < len1 + 3, out2', 'def test_simple(self, tmpfile):\\n        fd = tmpfile.fileno()\\n        cap = capture.FDCapture(fd)\\n        data = tobytes(\"hello\")\\n        os.write(fd, data)\\n        s = cap.snap()\\n        cap.done()\\n        assert not s\\n        cap = capture.FDCapture(fd)\\n        cap.start()\\n        os.write(fd, data)\\n        s = cap.snap()\\n        cap.done()\\n        assert s == \"hello\"', 'def test_simple_many_check_open_files(self, testdir):\\n        with lsof_check():\\n            with testdir.makepyfile(\"\").open(\\'wb+\\') as tmpfile:\\n                self.test_simple_many(tmpfile)', 'def test_stderr(self):\\n        cap = capture.FDCapture(2)\\n        cap.start()\\n        print(\"hello\", file=sys.stderr)\\n        s = cap.snap()\\n        cap.done()\\n        assert s == \"hello\\\\n\"', 'def test_writeorg(self, tmpfile):\\n        data1, data2 = tobytes(\"foo\"), tobytes(\"bar\")\\n        cap = capture.FDCapture(tmpfile.fileno())\\n        cap.start()\\n        tmpfile.write(data1)\\n        tmpfile.flush()\\n        cap.writeorg(data2)\\n        scap = cap.snap()\\n        cap.done()\\n        assert scap == totext(data1)\\n        with open(tmpfile.name, \\'rb\\') as stmp_file:\\n            stmp = stmp_file.read()\\n            assert stmp == data2', 'def saved_fd(fd):\\n    new_fd = os.dup(fd)\\n    try:\\n        yield\\n    finally:\\n        os.dup2(new_fd, fd)\\n        os.close(new_fd)', 'def getcapture(self, **kw):\\n        cap = self.__class__.captureclass(**kw)\\n        cap.start_capturing()\\n        try:\\n            yield cap\\n        finally:\\n            cap.stop_capturing()', 'def test_capturing_reset_simple(self):\\n        with self.getcapture() as cap:\\n            print(\"hello world\")\\n            sys.stderr.write(\"hello error\\\\n\")\\n            out, err = cap.readouterr()\\n        assert out == \"hello world\\\\n\"\\n        assert err == \"hello error\\\\n\"', 'def test_capture_results_accessible_by_attribute(self):\\n        with self.getcapture() as cap:\\n            sys.stdout.write(\"hello\")\\n            sys.stderr.write(\"world\")\\n            capture_result = cap.readouterr()\\n        assert capture_result.out == \"hello\"\\n        assert capture_result.err == \"world\"', \"def test_capturing_readouterr_decode_error_handling(self):\\n        with self.getcapture() as cap:\\n            # triggered a internal error in pytest\\n            print('\\\\xa6')\\n            out, err = cap.readouterr()\\n        assert out == py.builtin._totext('\\\\ufffd\\\\n', 'unicode-escape')\", 'def test_capturing_modify_sysouterr_in_between(self):\\n        oldout = sys.stdout\\n        olderr = sys.stderr\\n        with self.getcapture() as cap:\\n            sys.stdout.write(\"hello\")\\n            sys.stderr.write(\"world\")\\n            sys.stdout = capture.CaptureIO()\\n            sys.stderr = capture.CaptureIO()\\n            print(\"not seen\")\\n            sys.stderr.write(\"not seen\\\\n\")\\n            out, err = cap.readouterr()\\n        assert out == \"hello\"\\n        assert err == \"world\"\\n        assert sys.stdout == oldout\\n        assert sys.stderr == olderr', 'def test_just_out_capture(self):\\n        with self.getcapture(out=True, err=False) as cap:\\n            sys.stdout.write(\"hello\")\\n            sys.stderr.write(\"world\")\\n            out, err = cap.readouterr()\\n        assert out == \"hello\"\\n        assert not err', 'def test_stdin_restored(self):\\n        old = sys.stdin\\n        with self.getcapture(in_=True):\\n            newstdin = sys.stdin\\n        assert newstdin != sys.stdin\\n        assert sys.stdin is old', 'def test_simple_only_fd(self, testdir):\\n        testdir.makepyfile(\"\"\"\\n            import os\\n            def test_x():\\n                os.write(1, \"hello\\\\\\\\n\".encode(\"ascii\"))\\n                assert 0\\n        \"\"\")\\n        result = testdir.runpytest_subprocess()\\n        result.stdout.fnmatch_lines(\"\"\"\\n            *test_x*\\n            *assert 0*\\n            *Captured stdout*\\n        \"\"\")', 'def test_many(self, capfd):\\n        with lsof_check():\\n            for i in range(10):\\n                cap = StdCaptureFD()\\n                cap.stop_capturing()', 'def test_stdcapture_fd_invalid_fd(self, testdir):\\n        testdir.makepyfile(\"\"\"\\n            import os\\n            from _pytest import capture\\n            def StdCaptureFD(out=True, err=True, in_=True):\\n                return capture.MultiCapture(out, err, in_,\\n                                              Capture=capture.FDCapture)\\n            def test_stdout():\\n                os.close(1)\\n                cap = StdCaptureFD(out=True, err=False, in_=False)\\n                cap.stop_capturing()\\n            def test_stderr():\\n                os.close(2)\\n                cap = StdCaptureFD(out=False, err=True, in_=False)\\n                cap.stop_capturing()\\n            def test_stdin():\\n                os.close(0)\\n                cap = StdCaptureFD(out=False, err=False, in_=True)\\n                cap.stop_capturing()\\n        \"\"\")\\n        result = testdir.runpytest_subprocess(\"--capture=fd\")\\n        assert result.ret == 0\\n        assert result.parseoutcomes()[\\'passed\\'] == 3', \"def test_using_capsys_fixture_works_with_sys_stdout_encoding(capsys):\\n    test_text = 'test text'\\n\\n    print(test_text.encode(sys.stdout.encoding, 'replace'))\\n    (out, err) = capsys.readouterr()\\n    assert out\\n    assert err == ''\", 'def test_fdcapture_tmpfile_remains_the_same(tmpfile, use):\\n    if not use:\\n        tmpfile = True\\n    cap = StdCaptureFD(out=False, err=tmpfile)\\n    try:\\n        cap.start_capturing()\\n        capfile = cap.err.tmpfile\\n        cap.readouterr()\\n    finally:\\n        cap.stop_capturing()\\n    capfile2 = cap.err.tmpfile\\n    assert capfile2 == capfile', 'def test_close_and_capture_again(testdir):\\n    testdir.makepyfile(\"\"\"\\n        import os\\n        def test_close():\\n            os.close(1)\\n        def test_capture_again():\\n            os.write(1, b\"hello\\\\\\\\n\")\\n            assert 0\\n    \"\"\")\\n    result = testdir.runpytest_subprocess()\\n    result.stdout.fnmatch_lines(\"\"\"\\n        *test_capture_again*\\n        *assert 0*\\n        *stdout*\\n        *hello*\\n    \"\"\")', 'def test_capturing_and_logging_fundamentals(testdir, method):\\n    if method == \"StdCaptureFD\" and not hasattr(os, \\'dup\\'):\\n        pytest.skip(\"need os.dup\")\\n    # here we check a fundamental feature\\n    p = testdir.makepyfile(\"\"\"\\n        import sys, os\\n        import py, logging\\n        from _pytest import capture\\n        cap = capture.MultiCapture(out=False, in_=False,\\n                                     Capture=capture.%s)\\n        cap.start_capturing()\\n\\n        logging.warn(\"hello1\")\\n        outerr = cap.readouterr()\\n        print (\"suspend, captured %%s\" %%(outerr,))\\n        logging.warn(\"hello2\")\\n\\n        cap.pop_outerr_to_orig()\\n        logging.warn(\"hello3\")\\n\\n        outerr = cap.readouterr()\\n        print (\"suspend2, captured %%s\" %% (outerr,))\\n    \"\"\" % (method,))\\n    result = testdir.runpython(p)\\n    result.stdout.fnmatch_lines(\"\"\"\\n        suspend, captured*hello1*\\n        suspend2, captured*WARNING:root:hello3*\\n    \"\"\")\\n    result.stderr.fnmatch_lines(\"\"\"\\n        WARNING:root:hello2\\n    \"\"\")\\n    assert \"atexit\" not in result.stderr.str()', 'def test_capattr():\\n            assert sys.stdout.errors == \"strict\"\\n            assert sys.stderr.errors == \"strict\"', 'def test_py36_windowsconsoleio_workaround_non_standard_streams():\\n    \"\"\"\\n    Ensure _py36_windowsconsoleio_workaround function works with objects that\\n    do not implement the full ``io``-based stream protocol, for example execnet channels (#2666).\\n    \"\"\"\\n    from _pytest.capture import _py36_windowsconsoleio_workaround\\n\\n    class DummyStream(object):\\n        def write(self, s):\\n            pass\\n\\n    stream = DummyStream()\\n    _py36_windowsconsoleio_workaround(stream)', 'def test_capattr():\\n            # should not raise AttributeError\\n            assert sys.stdout.encoding\\n            assert sys.stderr.encoding', \"def test_crash_on_closing_tmpfile_py27(testdir):\\n    testdir.makepyfile('''\\n        from __future__ import print_function\\n        import time\\n        import threading\\n        import sys\\n\\n        def spam():\\n            f = sys.stderr\\n            while True:\\n                print('.', end='', file=f)\\n\\n        def test_silly():\\n            t = threading.Thread(target=spam)\\n            t.daemon = True\\n            t.start()\\n            time.sleep(0.5)\\n\\n    ''')\\n    result = testdir.runpytest_subprocess()\\n    assert result.ret == 0\\n    assert 'IOError' not in result.stdout.str()\"]}, {'features': [], 'snippets': ['def log(self, message):\\n        f = open(settings.TASK_LOG_PATH, \\'a\\')\\n        now = datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\\n        log_message = \"%s\\\\t%s\\\\n\" % (now, message)\\n        self.stdout.write(log_message)\\n        f.write(log_message)\\n        f.close()']}, {'features': [], 'snippets': ['def main():']}, {'features': [], 'snippets': ['def __init__(self, names, pronouns, mc):\\n        self.speakers = [{\"name\": n, \"pronoun\": p} for n, p in list(zip(names, pronouns))]\\n        self._transitions = self.make_transition_probs()\\n        self._speech_acts = [\"said\", \"whispered\", \"shouted\", \"cried\"]\\n        self._acts_transitions = [25, 2, 2, 2]\\n        self.mc = mc\\n        # self.seeds = seeds\\n        self.target_len = np.random.randint(5, 50, size=len(names))  # rough words per sentence', 'def after(self, speaker_id):\\n        \"\"\"Pick next person to speak\"\"\"\\n        row = self._transitions[speaker_id]\\n        sucessor = searchsorted(cumsum(row), rand() * sum(row))\\n        return sucessor', 'def speech_sequence(self, n):\\n        speech_acts_seq = []\\n        next_speech_id = 0\\n        for i in range(n):\\n            next_speech_id = searchsorted(cumsum(self._acts_transitions), rand() * sum(self._acts_transitions))\\n            speech_acts_seq.append(self._speech_acts[next_speech_id])\\n        return speech_acts_seq', 'def make_speech_bits(self, seeds):\\n        n = len(seeds)\\n        speaker_id = self.speaker_sequence(0, n)\\n        speech_acts_seq = self.speech_sequence(n)\\n        bits = []\\n        ss = sentence.SentenceMaker(self.mc)\\n        for i in range(n):\\n            sent_toks = ss.generate_sentence_tokens([seeds[i]], self.target_len[speaker_id[i]])\\n            sent_toks = ss.polish_sentence(sent_toks)\\n            bits.append({\\'speaker_name\\': self.speakers[speaker_id[i]][\"name\"],\\n                         \\'speech_act\\': speech_acts_seq[speaker_id[i]],\\n                         \\'seq_id\\': speaker_id[i],\\n                         \\'speech\\': sent_toks,\\n                         \\'paragraph\\': True})\\n        return(bits)', 'def report_seq(self, seq_map):\\n        \"\"\"Convert sequence of speeches to a tokens.\"\"\"\\n        sents = []\\n        for i in range(0, len(seq_map)):\\n\\n            if seq_map[i][\\'paragraph\\']:\\n                # text += \"\\\\n    \"\\n                quote_start = \\'\"\\'\\n            else:\\n                quote_start = \"\"\\n            if i > len(seq_map) - 2 or seq_map[i + 1][\\'paragraph\\']:\\n                quote_end = \\'\"\\'\\n            else:\\n                quote_end = \" \"\\n            if len(seq_map[i][\\'speech_act\\']) > 0:\\n                speech_act = seq_map[i][\\'speech_act\\'] + \",\"\\n            else:\\n                speech_act = seq_map[i][\\'speech_act\\']\\n            tokens = [utils.START_TOKEN]\\n            tokens.append(seq_map[i][\\'speaker_str\\'])\\n            tokens.append(speech_act)\\n            tokens.append(quote_start)\\n            tokens.extend(seq_map[i][\\'speech\\'][1:-1])\\n            tokens.append(quote_end)\\n            tokens.append(utils.END_TOKEN)\\n            sents.append(tokens)\\n        return sents']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def make_render_children(separator: str) -> Render:\\n    def render_children(\\n        node: RenderTreeNode,\\n        context: RenderContext,\\n    ) -> str:\\n        return separator.join(child.render(context) for child in node.children)\\n\\n    return render_children', 'def code_inline(node: RenderTreeNode, context: RenderContext) -> str:\\n    code = node.content\\n    all_chars_are_whitespace = not code.strip()\\n    longest_backtick_seq = longest_consecutive_sequence(code, \"`\")\\n    if longest_backtick_seq:\\n        separator = \"`\" * (longest_backtick_seq + 1)\\n        return f\"{separator} {code} {separator}\"\\n    if code.startswith(\" \") and code.endswith(\" \") and not all_chars_are_whitespace:\\n        return f\"` {code} `\"\\n    return f\"`{code}`\"', 'def html_inline(node: RenderTreeNode, context: RenderContext) -> str:\\n    return node.content', 'def hardbreak(node: RenderTreeNode, context: RenderContext) -> str:\\n    if _in_block(\"heading\", node):\\n        return \"<br /> \"\\n    return \"\\\\\\\\\" + \"\\\\n\"', 'def text(node: RenderTreeNode, context: RenderContext) -> str:\\n    \"\"\"Process a text token.\\n\\n    Text should always be a child of an inline token. An inline token\\n    should always be enclosed by a heading or a paragraph.\\n    \"\"\"\\n    text = node.content\\n\\n    # Escape backslash to prevent it from making unintended escapes.\\n    # This escape has to be first, else we start multiplying backslashes.\\n    text = text.replace(\"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\")\\n\\n    text = escape_asterisk_emphasis(text)  # Escape emphasis/strong marker.\\n    text = escape_underscore_emphasis(text)  # Escape emphasis/strong marker.\\n    text = text.replace(\"[\", \"\\\\\\\\[\")  # Escape link label enclosure\\n    text = text.replace(\"]\", \"\\\\\\\\]\")  # Escape link label enclosure\\n    text = text.replace(\"<\", \"\\\\\\\\<\")  # Escape URI enclosure\\n    text = text.replace(\"`\", \"\\\\\\\\`\")  # Escape code span marker\\n\\n    # Escape \"&\" if it starts a sequence that can be interpreted as\\n    # a character reference.\\n    text = RE_CHAR_REFERENCE.sub(r\"\\\\\\\\\\\\g<0>\", text)\\n\\n    # The parser can give us consecutive newlines which can break\\n    # the markdown structure. Replace two or more consecutive newlines\\n    # with newline character\\'s decimal reference.\\n    text = text.replace(\"\\\\n\\\\n\", \"&#10;&#10;\")\\n\\n    # If the last character is a \"!\" and the token next up is a link, we\\n    # have to escape the \"!\" or else the link will be interpreted as image.\\n    next_sibling = node.next_sibling\\n    if text.endswith(\"!\") and next_sibling and next_sibling.type == \"link\":\\n        text = text[:-1] + \"\\\\\\\\!\"\\n\\n    if context.do_wrap and _in_block(\"paragraph\", node):\\n        text = re.sub(r\"\\\\s+\", WRAP_POINT, text)\\n\\n    return text', 'def code_block(node: RenderTreeNode, context: RenderContext) -> str:\\n    return fence(node, context)', 'def _render_inline_as_text(node: RenderTreeNode, context: RenderContext) -> str:\\n    \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.\\n\\n    Don\\'t try to use it! Spec requires to show `alt` content with\\n    stripped markup, instead of simple escaping.\\n    \"\"\"\\n\\n    def text_renderer(node: RenderTreeNode, context: RenderContext) -> str:\\n        return node.content\\n\\n    def image_renderer(node: RenderTreeNode, context: RenderContext) -> str:\\n        return _render_inline_as_text(node, context)\\n\\n    inline_renderers: Mapping[str, Render] = defaultdict(\\n        lambda: make_render_children(\"\"),\\n        {\\n            \"text\": text_renderer,\\n            \"image\": image_renderer,\\n            \"link\": link,\\n            \"softbreak\": softbreak,\\n        },\\n    )\\n    inline_context = RenderContext(\\n        inline_renderers, context.postprocessors, context.options, context.env\\n    )\\n    return make_render_children(\"\")(node, inline_context)', 'def em(node: RenderTreeNode, context: RenderContext) -> str:\\n    text = make_render_children(separator=\"\")(node, context)\\n    indicator = node.markup\\n    return indicator + text + indicator', 'def heading(node: RenderTreeNode, context: RenderContext) -> str:\\n    text = make_render_children(separator=\"\")(node, context)\\n\\n    if node.markup == \"=\":\\n        prefix = \"# \"\\n    elif node.markup == \"-\":\\n        prefix = \"## \"\\n    else:  # ATX heading\\n        prefix = node.markup + \" \"\\n\\n    # There can be newlines in setext headers, but we make an ATX\\n    # header always. Convert newlines to spaces.\\n    text = text.replace(\"\\\\n\", \" \")\\n\\n    # If the text ends in a sequence of hashes (#), the hashes will be\\n    # interpreted as an optional closing sequence of the heading, and\\n    # will not be rendered. Escape a line ending hash to prevent this.\\n    if text.endswith(\"#\"):\\n        text = text[:-1] + \"\\\\\\\\#\"\\n\\n    return prefix + text', 'def _wrap(text: str, *, width: int | Literal[\"no\"]) -> str:\\n    \"\"\"Wrap text at locations pointed by `WRAP_POINT`s.\\n\\n    Converts `WRAP_POINT`s to either a space or newline character, thus\\n    wrapping the text. Already existing whitespace will be preserved as\\n    is.\\n    \"\"\"\\n    text, replacements = _prepare_wrap(text)\\n    if width == \"no\":\\n        return _recover_preserve_chars(text, replacements)\\n\\n    wrapper = textwrap.TextWrapper(\\n        break_long_words=False,\\n        break_on_hyphens=False,\\n        width=width,\\n        expand_tabs=False,\\n        replace_whitespace=False,\\n    )\\n    wrapped = wrapper.fill(text)\\n    wrapped = _recover_preserve_chars(wrapped, replacements)\\n    return \" \" + wrapped if text.startswith(\" \") else wrapped', 'def _recover_preserve_chars(text: str, replacements: str) -> str:\\n    replacement_iterator = iter(replacements)\\n    return \"\".join(\\n        next(replacement_iterator) if c == PRESERVE_CHAR else c for c in text\\n    )', 'def list_item(node: RenderTreeNode, context: RenderContext) -> str:\\n    \"\"\"Return one list item as string.\\n\\n    This returns just the content. List item markers and indentation are\\n    added in `bullet_list` and `ordered_list` renderers.\\n    \"\"\"\\n    block_separator = \"\\\\n\" if is_tight_list_item(node) else \"\\\\n\\\\n\"\\n    text = make_render_children(block_separator)(node, context)\\n\\n    if not text.strip():\\n        return \"\"\\n    return text', 'def ordered_list(node: RenderTreeNode, context: RenderContext) -> str:\\n    consecutive_numbering = context.options.get(\"mdformat\", {}).get(\\n        \"number\", DEFAULT_OPTS[\"number\"]\\n    )\\n    marker_type = get_list_marker_type(node)\\n    first_line_indent = \" \"\\n    block_separator = \"\\\\n\" if is_tight_list(node) else \"\\\\n\\\\n\"\\n    list_len = len(node.children)\\n\\n    starting_number = node.attrs.get(\"start\")\\n    if starting_number is None:\\n        starting_number = 1\\n    assert isinstance(starting_number, int)\\n\\n    if consecutive_numbering:\\n        indent_width = len(\\n            f\"{list_len + starting_number - 1}{marker_type}{first_line_indent}\"\\n        )\\n    else:\\n        indent_width = len(f\"{starting_number}{marker_type}{first_line_indent}\")\\n\\n    text = \"\"\\n    with context.indented(indent_width):\\n        for list_item_index, list_item in enumerate(node.children):\\n            list_item_text = list_item.render(context)\\n            formatted_lines = []\\n            line_iterator = iter(list_item_text.split(\"\\\\n\"))\\n            first_line = next(line_iterator)\\n            if consecutive_numbering:\\n                # Prefix first line of the list item with consecutive numbering,\\n                # padded with zeros to make all markers of even length.\\n                # E.g.\\n                #   002. This is the first list item\\n                #   003. Second item\\n                #   ...\\n                #   112. Last item\\n                number = starting_number + list_item_index\\n                pad = len(str(list_len + starting_number - 1))\\n                number_str = str(number).rjust(pad, \"0\")\\n                formatted_lines.append(\\n                    f\"{number_str}{marker_type}{first_line_indent}{first_line}\"\\n                    if first_line\\n                    else f\"{number_str}{marker_type}\"\\n                )\\n            else:\\n                # Prefix first line of first item with the starting number of the\\n                # list. Prefix following list items with the number one\\n                # prefixed by zeros to make the list item marker of even length\\n                # with the first one.\\n                # E.g.\\n                #   5321. This is the first list item\\n                #   0001. Second item\\n                #   0001. Third item\\n                first_item_marker = f\"{starting_number}{marker_type}\"\\n                other_item_marker = (\\n                    \"0\" * (len(str(starting_number)) - 1) + \"1\" + marker_type\\n                )\\n                if list_item_index == 0:\\n                    formatted_lines.append(\\n                        f\"{first_item_marker}{first_line_indent}{first_line}\"\\n                        if first_line\\n                        else first_item_marker\\n                    )\\n                else:\\n                    formatted_lines.append(\\n                        f\"{other_item_marker}{first_line_indent}{first_line}\"\\n                        if first_line\\n                        else other_item_marker\\n                    )\\n            for line in line_iterator:\\n                formatted_lines.append(\" \" * indent_width + line if line else \"\")\\n\\n            text += \"\\\\n\".join(formatted_lines)\\n            if list_item_index != len(node.children) - 1:\\n                text += block_separator\\n\\n        return text', 'def indented(self, width: int) -> Generator[None, None, None]:\\n        self.env[\"indent_width\"] += width\\n        try:\\n            yield\\n        finally:\\n            self.env[\"indent_width\"] -= width', 'def do_wrap(self) -> bool:\\n        wrap_mode = self.options.get(\"mdformat\", {}).get(\"wrap\", DEFAULT_OPTS[\"wrap\"])\\n        return isinstance(wrap_mode, int) or wrap_mode == \"no\"']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.conf = tecaconf.ConfigHandler(\\n            \"tests/test_data/configuration.json\",\\n            {\"starting_path\": \"tests/test_data/images\"}\\n        )\\n        self.files_list = [\\n          \"foo.doc\",\\n          \"yukinon.jpg\",\\n          \"cuteflushadoingflushathings.webm\"\\n        ]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def main():\\n    pass']}, {'features': [], 'snippets': [\"def get_parent_id(self, name, attrs):\\n        final_attrs = self.build_attrs(attrs, type=self.input_type, name=name)\\n        return final_attrs['id']\", 'def get_values(self, min_value, max_value, step=1):\\n        decimal_step = Decimal(str(step))\\n        value = Decimal(str(min_value))\\n        while value <= max_value:\\n            yield value\\n            value += decimal_step', 'def __init__(self, min_value, max_value, step, instance=None,\\n        can_delete_vote=True, key=\\'\\', read_only=False, default=\\'\\',\\n        template=\\'ratings/slider_widget.html\\', attrs=None):\\n        \"\"\"\\n        The argument *default* is used when the initial value is None.\\n        \"\"\"\\n        super(SliderWidget, self).__init__(attrs)\\n        self.min_value = min_value\\n        self.max_value = max_value\\n        self.step = step\\n        self.instance = instance\\n        self.can_delete_vote = can_delete_vote\\n        self.read_only = read_only\\n        self.default = default\\n        self.template = template\\n        self.key = key', 'def render(self, name, value, attrs=None):\\n        context = self.get_context(name, value, attrs or {})\\n        return render_to_string(self.template, context)', \"def __init__(self, min_value, max_value, step, instance=None,\\n        can_delete_vote=True, key='', read_only=False,\\n        template='ratings/star_widget.html', attrs=None):\\n        super(StarWidget, self).__init__(attrs)\\n        self.min_value = min_value\\n        self.max_value = max_value\\n        self.step = step\\n        self.instance = instance\\n        self.can_delete_vote = can_delete_vote\\n        self.read_only = read_only\\n        self.template = template\\n        self.key = key\", 'def _get_value(self, original, split):\\n        if original:\\n            value = round(original * split) / split\\n            return Decimal(str(value))', \"def __init__(self, min_value, max_value, instance=None,\\n        can_delete_vote=True, template='ratings/like_widget.html', attrs=None):\\n        super(LikeWidget, self).__init__(attrs)\\n        self.min_value = min_value\\n        self.max_value = max_value\\n        self.instance = instance\\n        self.can_delete_vote = can_delete_vote\\n        self.template = template\", \"def get_context(self, name, value, attrs=None):\\n        # here we convert *min_value*, *max_value* and *step*\\n        # to string to avoid odd behaviours of Django localization\\n        # in the template (and, for backward compatibility we do not\\n        # want to use the *unlocalize* filter)\\n        attrs['type'] = 'hidden'\\n        return {\\n            'min_value': str(self.min_value),\\n            'max_value': str(self.max_value),\\n            'can_delete_vote': self.can_delete_vote,\\n            'parent': super(LikeWidget, self).render(name, value, attrs),\\n            'parent_id': self.get_parent_id(name, attrs),\\n            'value': str(value),\\n            'like_id': self.get_widget_id('like', name),\\n        }\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_stats_result(self, request):\\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\\n\\n        pub_state = self.get_pub_state(request)\\n\\n        if pub_state == self.PUB_STATE_ALL:\\n            stats_result = stats_datasets.get_dataverse_counts_by_month()\\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\\n            stats_result = stats_datasets.get_dataverse_counts_by_month_unpublished()\\n        else:\\n            stats_result = stats_datasets.get_dataverse_counts_by_month_published()\\n\\n        return stats_result', 'def get_stats_result(self, request):\\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\\n\\n        pub_state = self.get_pub_state(request)\\n\\n        if pub_state == self.PUB_STATE_ALL:\\n            stats_result = stats_datasets.get_dataverse_count()\\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\\n            stats_result = stats_datasets.get_dataverse_count_unpublished()\\n        else:\\n            stats_result = stats_datasets.get_dataverse_count_published()\\n\\n        return stats_result', 'def get_stats_result(self, request):\\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\\n\\n        pub_state = self.get_pub_state(request)\\n\\n        if pub_state == self.PUB_STATE_ALL:\\n            stats_result = stats_datasets.get_dataverse_affiliation_counts()\\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\\n            stats_result = stats_datasets.get_dataverse_affiliation_counts_unpublished()\\n        else:\\n            stats_result = stats_datasets.get_dataverse_affiliation_counts_published()\\n\\n        return stats_result', 'def is_show_uncategorized(self, request):\\n        \"\"\"Return the result of the \"?show_uncategorized\" query string param\"\"\"\\n\\n        show_uncategorized = request.GET.get(\\'show_uncategorized\\', False)\\n        if show_uncategorized is True or show_uncategorized == \\'true\\':\\n            return True\\n        return False']}, {'features': [], 'snippets': ['def _create_user(self, email, password,\\n                     is_staff, is_superuser, **extra_fields):\\n        \"\"\"\\n        Creates and saves an User with the given email and password.\\n        \"\"\"\\n        now = timezone.now()\\n        if not email:\\n            raise ValueError(\\'An email address must be provided.\\')\\n        email = self.normalize_email(email)\\n        if \"is_active\" not in extra_fields:\\n            extra_fields[\"is_active\"] = True\\n        if \"username\" not in extra_fields:\\n            # For now we need to have a unique id that is at\\n            # most 30 characters long.  Using uuid and truncating.\\n            # Ideally username goes away entirely at some point\\n            # since we\\'re really using email.  If we have to keep\\n            # username for some reason then we could switch over\\n            # to a string version of the pk which is guaranteed\\n            # be unique.\\n            extra_fields[\"username\"] = str(uuid.uuid4())[:MAX_USERNAME_LENGTH]\\n        user = self.model(email=email,\\n                          is_staff=is_staff,\\n                          is_superuser=is_superuser,\\n                          last_login=None,\\n                          date_joined=now,\\n                          **extra_fields)\\n        user.set_password(password)\\n        user.save(using=self._db)\\n        return user', 'def create_superuser(self, email, password, **extra_fields):\\n        return self._create_user(email, password, True, True,\\n                                 **extra_fields)', 'def __init__(self, *args, **kwargs):\\n        super(User, self).__init__(*args, **kwargs)\\n        self.startup = None\\n        self.team_member = None\\n        self.profile = None\\n        self.user_finalist_roles = None', 'def __str__(self):\\n        return self.email', 'def user_phone(self):\\n        return self._get_profile().phone', \"def team_member_id(self):\\n        return self.team_member.id if self._get_member() else ''\", 'def user_twitter_handle(self):\\n        return self._get_profile().twitter_handle', 'def user_facebook_url(self):\\n        return self._get_profile().facebook_url', 'def type(self):\\n        return self._get_profile().user_type', 'def _get_title_and_company(self):\\n        if self._is_expert() and self._has_expert_details():\\n            profile = self._get_profile()\\n            title = profile.title\\n            company = profile.company\\n            return {\\n                \"title\": title,\\n                \"company\": company\\n            }\\n        self._get_member()\\n        title = self.team_member.title if self.team_member else \"\"\\n        company = self.startup.name if self._get_startup() else None\\n        return {\\n            \"title\": title,\\n            \"company\": company\\n        }', 'def startup_industry(self):\\n        return self.startup.primary_industry if self._get_startup() else None', 'def startup_status_names(self):\\n        if self._get_startup():\\n            return [startup_status.program_startup_status.startup_status\\n                    for startup_status in self.startup.startupstatus_set.all()]', 'def program(self):\\n        return self.startup.current_program() if self._get_startup() else None', 'def year(self):\\n        program = self.program()\\n        return program.start_date.year if program else None', 'def _get_startup(self):\\n        if not self.startup:\\n            self._get_member()\\n            if self.team_member:\\n                self.startup = self.team_member.startup\\n        return self.startup', 'def _get_profile(self):\\n        if self.profile:\\n            return self.profile\\n        self.profile = self.get_profile()\\n        return self.profile']}, {'features': [], 'snippets': ['def read(*paths):\\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"\\n    with open(os.path.join(*paths), \\'r\\') as f:\\n        return f.read()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def calc():\\n\\th, l = input().split(' ')\\n\\n\\tmapa = []\\n\\n\\tfor i_row in range(int(h)):\\n\\t\\tmapa.append(input().split(' '))\\n\\n\\tmaior_num = 0\"]}, {'features': [], 'snippets': ['def __str__(self):\\n\\t\\t\"\"\"\\n\\t\\tSummarize information about the experiment.\\n\\n\\t\\t@rtype: string\\n\\t\\t@return: summary of the experiment\\n\\t\\t\"\"\"\\n\\n\\t\\tstrl = []\\n\\n\\t\\t# date and duration of experiment\\n\\t\\tstrl.append(strftime(\\'date \\\\t\\\\t %a, %d %b %Y %H:%M:%S\\', localtime(self.time)))\\n\\t\\tstrl.append(\\'duration \\\\t \\' + str(int(self.duration)) + \\'s\\')\\n\\t\\tstrl.append(\\'hostname \\\\t \\' + self.hostname)\\n\\n\\t\\t# commit hash\\n\\t\\tif self.commit:\\n\\t\\t\\tif self.modified:\\n\\t\\t\\t\\tstrl.append(\\'commit \\\\t\\\\t \\' + self.commit + \\' (modified)\\')\\n\\t\\t\\telse:\\n\\t\\t\\t\\tstrl.append(\\'commit \\\\t\\\\t \\' + self.commit)\\n\\n\\t\\t# results\\n\\t\\tstrl.append(\\'results \\\\t {\\' + \\', \\'.join(map(str, self.results.keys())) + \\'}\\')\\n\\n\\t\\t# comment\\n\\t\\tif self.comment:\\n\\t\\t\\tstrl.append(\\'\\\\n\\' + self.comment)\\n\\n\\t\\treturn \\'\\\\n\\'.join(strl)', 'def __init__(self, filename=\\'\\', comment=\\'\\', seed=None, server=None, port=8000):\\n\\t\\t\"\"\"\\n\\t\\tIf the filename is given and points to an existing experiment, load it.\\n\\t\\tOtherwise store the current timestamp and try to get commit information\\n\\t\\tfrom the repository in the current directory.\\n\\n\\t\\t@type  filename: string\\n\\t\\t@param filename: path to where the experiment will be stored', \"def status(self, status, **kwargs):\\n\\t\\tif self.server:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tconn = HTTPConnection(self.server, self.port)\\n\\t\\t\\t\\tconn.request('GET', '/version/')\\n\\t\\t\\t\\tresp = conn.getresponse()\\n\\n\\t\\t\\t\\tif not resp.read().startswith('Experiment'):\\n\\t\\t\\t\\t\\traise RuntimeError()\\n\\n\\t\\t\\t\\tHTTPConnection(self.server, self.port).request('POST', '', str(dict({\\n\\t\\t\\t\\t\\t\\t'id': self.id,\\n\\t\\t\\t\\t\\t\\t'version': __version__,\\n\\t\\t\\t\\t\\t\\t'status': status,\\n\\t\\t\\t\\t\\t\\t'hostname': self.hostname,\\n\\t\\t\\t\\t\\t\\t'cwd': self.cwd,\\n\\t\\t\\t\\t\\t\\t'script_path': self.script_path,\\n\\t\\t\\t\\t\\t\\t'script': self.script,\\n\\t\\t\\t\\t\\t\\t'comment': self.comment,\\n\\t\\t\\t\\t\\t\\t'time': self.time,\\n\\t\\t\\t\\t\\t}, **kwargs)))\\n\\t\\t\\texcept:\\n\\t\\t\\t\\twarn('Unable to connect to \\\\'{0}:{1}\\\\'.'.format(self.server, self.port))\", 'def save(self, filename=None, overwrite=False):\\n\\t\\t\"\"\"\\n\\t\\tStore results. If a filename is given, the default is overwritten.\\n\\n\\t\\t@type  filename: string\\n\\t\\t@param filename: path to where the experiment will be stored\\n\\n\\t\\t@type  overwrite: boolean\\n\\t\\t@param overwrite: overwrite existing files\\n\\t\\t\"\"\"\\n\\n\\t\\tself.duration = time() - self.time\\n\\n\\t\\tif filename is None:\\n\\t\\t\\tfilename = self.filename\\n\\t\\telse:\\n\\t\\t\\t# replace {0} and {1} by date and time\\n\\t\\t\\ttmp1 = strftime(\\'%d%m%Y\\', localtime(time()))\\n\\t\\t\\ttmp2 = strftime(\\'%H%M%S\\', localtime(time()))\\n\\t\\t\\tfilename = filename.format(tmp1, tmp2)\\n\\n\\t\\t\\tself.filename = filename\\n\\n\\t\\t# make sure directory exists\\n\\t\\ttry:\\n\\t\\t\\tos.makedirs(path.dirname(filename))\\n\\t\\texcept OSError:\\n\\t\\t\\tpass\\n\\n\\t\\t# make sure filename is unique\\n\\t\\tcounter = 0\\n\\t\\tpieces = path.splitext(filename)\\n\\n\\t\\tif not overwrite:\\n\\t\\t\\twhile path.exists(filename):\\n\\t\\t\\t\\tcounter += 1\\n\\t\\t\\t\\tfilename = pieces[0] + \\'.\\' + str(counter) + pieces[1]\\n\\n\\t\\t\\tif counter:\\n\\t\\t\\t\\twarn(\\'\\'.join(pieces) + \\' already exists. Saving to \\' + filename + \\'.\\')\\n\\n\\t\\t# store experiment\\n\\t\\twith open(filename, \\'wb\\') as handle:\\n\\t\\t\\tdump({\\n\\t\\t\\t\\t\\'version\\': __version__,\\n\\t\\t\\t\\t\\'id\\': self.id,\\n\\t\\t\\t\\t\\'time\\': self.time,\\n\\t\\t\\t\\t\\'seed\\': self.seed,\\n\\t\\t\\t\\t\\'duration\\': self.duration,\\n\\t\\t\\t\\t\\'environ\\': self.environ,\\n\\t\\t\\t\\t\\'hostname\\': self.hostname,\\n\\t\\t\\t\\t\\'cwd\\': self.cwd,\\n\\t\\t\\t\\t\\'argv\\': self.argv,\\n\\t\\t\\t\\t\\'script\\': self.script,\\n\\t\\t\\t\\t\\'script_path\\': self.script_path,\\n\\t\\t\\t\\t\\'processors\\': self.processors,\\n\\t\\t\\t\\t\\'platform\\': self.platform,\\n\\t\\t\\t\\t\\'comment\\': self.comment,\\n\\t\\t\\t\\t\\'commit\\': self.commit,\\n\\t\\t\\t\\t\\'modified\\': self.modified,\\n\\t\\t\\t\\t\\'versions\\': self.versions,\\n\\t\\t\\t\\t\\'results\\': self.results}, handle, 1)\\n\\n\\t\\tself.status(\\'SAVE\\', filename=filename, duration=self.duration)', 'def __getitem__(self, key):\\n\\t\\treturn self.results[key]', 'def __delitem__(self, key):\\n\\t\\tdel self.results[key]', 'def do_GET(self):\\n\\t\\t\"\"\"\\n\\t\\tRenders HTML displaying running and saved experiments.\\n\\t\\t\"\"\"\\n\\n\\t\\t# number of bars representing progress\\n\\t\\tmax_bars = 20\\n\\n\\t\\tif self.path == \\'/version/\\':\\n\\t\\t\\tself.send_response(200)\\n\\t\\t\\tself.send_header(\\'Content-type\\', \\'text/plain\\')\\n\\t\\t\\tself.end_headers()\\n\\n\\t\\t\\tself.wfile.write(\\'Experiment {0}\\'.format(__version__))\\n\\n\\t\\telif self.path.startswith(\\'/running/\\'):\\n\\t\\t\\tid = int([s for s in self.path.split(\\'/\\') if s != \\'\\'][-1])\\n\\n\\t\\t\\t# display running experiment\\n\\t\\t\\tif id in ExperimentRequestHandler.running:\\n\\t\\t\\t\\tself.send_response(200)\\n\\t\\t\\t\\tself.send_header(\\'Content-type\\', \\'text/html\\')\\n\\t\\t\\t\\tself.end_headers()\\n\\n\\t\\t\\t\\tself.wfile.write(HTML_HEADER)\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>Experiment</h2>\\')\\n\\n\\t\\t\\t\\tinstance = ExperimentRequestHandler.running[id]\\n\\n\\t\\t\\t\\tnum_bars = int(instance[\\'progress\\']) * max_bars / 100\\n\\n\\t\\t\\t\\tself.wfile.write(\\'<table>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Experiment:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tos.path.join(instance[\\'cwd\\'], instance[\\'script_path\\'])))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Hostname:</th><td>{0}</td></tr>\\'.format(instance[\\'hostname\\']))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Status:</th><td class=\"running\">{0}</td></tr>\\'.format(instance[\\'status\\']))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Progress:</th><td class=\"progress\"><span class=\"bars\">{0}</span>{1}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\t\\'|\\' * num_bars, \\'|\\' * (max_bars - num_bars)))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Start:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tstrftime(\\'%a, %d %b %Y %H:%M:%S\\', localtime(instance[\\'time\\']))))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Comment:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tinstance[\\'comment\\']  if instance[\\'comment\\'] else \\'-\\'))\\n\\t\\t\\t\\tself.wfile.write(\\'</table>\\')\\n\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>Script</h2>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'<pre>{0}</pre>\\'.format(instance[\\'script\\']))\\n\\t\\t\\t\\tself.wfile.write(HTML_FOOTER)\\n\\n\\t\\t\\telif id in ExperimentRequestHandler.finished:\\n\\t\\t\\t\\tself.send_response(302)\\n\\t\\t\\t\\tself.send_header(\\'Location\\', \\'/finished/{0}/\\'.format(id))\\n\\t\\t\\t\\tself.end_headers()\\n\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.send_response(200)\\n\\t\\t\\t\\tself.send_header(\\'Content-type\\', \\'text/html\\')\\n\\t\\t\\t\\tself.end_headers()\\n\\n\\t\\t\\t\\tself.wfile.write(HTML_HEADER)\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>404</h2>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'Requested experiment not found.\\')\\n\\t\\t\\t\\tself.wfile.write(HTML_FOOTER)\\n\\n\\t\\telif self.path.startswith(\\'/finished/\\'):\\n\\t\\t\\tself.send_response(200)\\n\\t\\t\\tself.send_header(\\'Content-type\\', \\'text/html\\')\\n\\t\\t\\tself.end_headers()\\n\\n\\t\\t\\tself.wfile.write(HTML_HEADER)\\n\\n\\t\\t\\tid = int([s for s in self.path.split(\\'/\\') if s != \\'\\'][-1])\\n\\n\\t\\t\\t# display finished experiment\\n\\t\\t\\tif id in ExperimentRequestHandler.finished:\\n\\t\\t\\t\\tinstance = ExperimentRequestHandler.finished[id]\\n\\n\\t\\t\\t\\tif id in ExperimentRequestHandler.running:\\n\\t\\t\\t\\t\\tprogress = ExperimentRequestHandler.running[id][\\'progress\\']\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprogress = 100\\n\\n\\t\\t\\t\\tnum_bars = int(progress) * max_bars / 100\\n\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>Experiment</h2>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'<table>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Experiment:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tos.path.join(instance[\\'cwd\\'], instance[\\'script_path\\'])))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Results:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tos.path.join(instance[\\'cwd\\'], instance[\\'filename\\'])))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Status:</th><td class=\"finished\">{0}</td></tr>\\'.format(instance[\\'status\\']))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Progress:</th><td class=\"progress\"><span class=\"bars\">{0}</span>{1}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\t\\'|\\' * num_bars, \\'|\\' * (max_bars - num_bars)))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Start:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tstrftime(\\'%a, %d %b %Y %H:%M:%S\\', localtime(instance[\\'time\\']))))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>End:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tstrftime(\\'%a, %d %b %Y %H:%M:%S\\', localtime(instance[\\'duration\\']))))\\n\\t\\t\\t\\tself.wfile.write(\\'<tr><th>Comment:</th><td>{0}</td></tr>\\'.format(\\n\\t\\t\\t\\t\\tinstance[\\'comment\\']  if instance[\\'comment\\'] else \\'-\\'))\\n\\t\\t\\t\\tself.wfile.write(\\'</table>\\')\\n\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>Results</h2>\\')\\n\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\texperiment = Experiment(os.path.join(instance[\\'cwd\\'], instance[\\'filename\\']))\\n\\t\\t\\t\\texcept:\\n\\t\\t\\t\\t\\tself.wfile.write(\\'Could not open file.\\')\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.wfile.write(\\'<table>\\')\\n\\t\\t\\t\\t\\tfor key, value in experiment.results.items():\\n\\t\\t\\t\\t\\t\\tself.wfile.write(\\'<tr><th>{0}</th><td>{1}</td></tr>\\'.format(key, value))\\n\\t\\t\\t\\t\\tself.wfile.write(\\'</table>\\')\\n\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>Script</h2>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'<pre>{0}</pre>\\'.format(instance[\\'script\\']))\\n\\n\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.wfile.write(\\'<h2>404</h2>\\')\\n\\t\\t\\t\\tself.wfile.write(\\'Requested experiment not found.\\')\\n\\n\\t\\t\\tself.wfile.write(HTML_FOOTER)\\n\\n\\t\\telse:\\n\\t\\t\\tfiles = []\\n\\n\\t\\t\\tif \\'xpck_path\\' in ExperimentRequestHandler.__dict__:\\n\\t\\t\\t\\tif ExperimentRequestHandler.xpck_path != \\'\\':\\n\\t\\t\\t\\t\\tfor path in ExperimentRequestHandler.xpck_path.split(\\':\\'):\\n\\t\\t\\t\\t\\t\\tfiles += [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\\'.xpck\\')]', \"def do_POST(self):\\n\\t\\tinstances = ExperimentRequestHandler.running\\n\\t\\tinstance = eval(self.rfile.read(int(self.headers['Content-Length'])))\", 'def find_class(self, module, name):\\n\\t\\t\"\"\"\\n\\t\\tHelps Unpickler to find certain Numpy modules.\\n\\t\\t\"\"\"\\n\\n\\t\\ttry:\\n\\t\\t\\tnumpy_version = StrictVersion(numpy.__version__)\\n\\n\\t\\t\\tif numpy_version >= \\'1.5.0\\':\\n\\t\\t\\t\\tif module == \\'numpy.core.defmatrix\\':\\n\\t\\t\\t\\t\\tmodule = \\'numpy.matrixlib.defmatrix\\'\\n\\n\\t\\texcept ValueError:\\n\\t\\t\\tpass\\n\\n\\t\\treturn Unpickler.find_class(self, module, name)', 'def main(argv):\\n\\t\"\"\"\\n\\tLoad and display experiment information.\\n\\t\"\"\"\\n\\n\\tif len(argv) < 2:\\n\\t\\tprint \\'Usage:\\', argv[0], \\'[--server] [--port=<port>] [--path=<path>] [filename]\\'\\n\\t\\treturn 0\\n\\n\\toptlist, argv = getopt(argv[1:], \\'\\', [\\'server\\', \\'port=\\', \\'path=\\'])\\n\\toptlist = dict(optlist)\\n\\n\\tif \\'--server\\' in optlist:\\n\\t\\ttry:\\n\\t\\t\\tExperimentRequestHandler.xpck_path = optlist.get(\\'--path\\', \\'\\')\\n\\t\\t\\tport = optlist.get(\\'--port\\', 8000)\\n\\n\\t\\t\\t# start server\\n\\t\\t\\tserver = HTTPServer((\\'\\', port), ExperimentRequestHandler)\\n\\t\\t\\tserver.serve_forever()\\n\\n\\t\\texcept KeyboardInterrupt:\\n\\t\\t\\tserver.socket.close()\\n\\n\\t\\treturn 0\\n\\n\\t# load experiment\\n\\texperiment = Experiment(sys.argv[1])\\n\\n\\tif len(argv) > 1:\\n\\t\\t# print arguments\\n\\t\\tfor arg in argv[1:]:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tprint experiment[arg]\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tprint experiment[int(arg)]\\n\\t\\treturn 0\\n\\n\\t# print summary of experiment\\n\\tprint experiment\\n\\n\\treturn 0']}, {'features': [], 'snippets': ['def __init__(self, exc, handler):\\n        self.exc = exc\\n        self.hndl = handler\\n        self.cls = type(exc)\\n        self.tb = None', 'def clear(self):\\n        self.exc = None\\n        self.tb = None']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def LogPABotMessage(message):\\n    _pabotlog.info(message)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def send_simple_message():\\n    return requests.post(\\n        \"https://api.mailgun.net/v3/sandbox049ff464a4d54974bb0143935f9577ef.mailgun.org/messages\",\\n        auth=(\"api\", \"key-679dc79b890e700f11f001a6bf86f4a1\"),\\n        data={\"from\": \"Mailgun Sandbox <postmaster@sandbox049ff464a4d54974bb0143935f9577ef.mailgun.org>\",\\n              \"to\": \"nick <nicorellius@gmail.com>\",\\n              \"subject\": \"Hello nick\",\\n              \"text\": \"Congratulations nick, you just sent an email with Mailgun!  You are truly awesome!  You can see a record of this email in your logs: https://mailgun.com/cp/log .  You can send up to 300 emails/day from this sandbox server.  Next, you should add your own domain so you can send 10,000 emails/month for free.\"})']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def snfpsf(wave, psfparams, header, psftype):\\n    \"\"\"Create a 3-d PSF based on SNFactory-specific parameterization of\\n    Gaussian + Moffat PSF parameters and ADR.\"\"\"\\n\\n    # Get Gaussian+Moffat parameters at each wavelength.\\n    relwave = wave / REFWAVE - 1.0\\n    ellipticity = abs(psfparams[0]) * np.ones_like(wave)\\n    alpha = np.abs(psfparams[1] +\\n                   psfparams[2] * relwave +\\n                   psfparams[3] * relwave**2)\\n\\n    # correlated parameters (coefficients determined externally)\\n    sigma = 0.545 + 0.215 * alpha  # Gaussian parameter\\n    beta  = 1.685 + 0.345 * alpha  # Moffat parameter\\n    eta   = 1.040 + 0.0   * alpha  # gaussian ampl. / moffat ampl.\\n\\n    # Atmospheric differential refraction (ADR): Because of ADR,\\n    # the center of the PSF will be different at each wavelength,\\n    # by an amount that we can determine (pretty well) from the\\n    # atmospheric conditions and the pointing and angle of the\\n    # instrument. We calculate the offsets here as a function of\\n    # observation and wavelength and input these to the model.\\n\\n    # Correction to parallactic angle and airmass for 2nd-order effects\\n    # such as MLA rotation, mechanical flexures or finite-exposure\\n    # corrections. These values have been trained on faint-std star\\n    # exposures.\\n    #\\n    # `predict_adr_params` uses \\'AIRMASS\\', \\'PARANG\\' and \\'CHANNEL\\' keys\\n    # in input dictionary.\\n    delta, theta = Hyper_PSF3D_PL.predict_adr_params(header)\\n\\n    # check for crazy values of pressure and temperature, and assign default\\n    # values.\\n    pressure = header.get(\\'PRESSURE\\', 617.)\\n    if not 550. < pressure < 650.:\\n        pressure = 617.\\n    temp = header.get(\\'TEMP\\', 2.)\\n    if not -20. < temp < 20.:\\n        temp = 2.\\n\\n    adr = ADR(pressure, temp, lref=REFWAVE, delta=delta, theta=theta)\\n    adr_refract = adr.refract(0, 0, wave, unit=SPAXEL_SIZE)', 'def setup_logging(loglevel, logfname=None):\\n\\n    # if loglevel isn\\'t an integer, parse it as \"debug\", \"info\", etc:\\n    if not isinstance(loglevel, int):\\n        loglevel = getattr(logging, loglevel.upper(), None)\\n    if not isinstance(loglevel, int):\\n        print(\\'Invalid log level: %s\\' % loglevel)\\n        exit(1)\\n\\n    # remove logfile if it already exists\\n    if logfname is not None and os.path.exists(logfname):\\n        os.remove(logfname)\\n\\n    logging.basicConfig(filename=logfname, format=\"%(levelname)s %(message)s\",\\n                        level=loglevel)', 'def cubefit_subtract(argv=None):\\n    DESCRIPTION = \\\\']}, {'features': [], 'snippets': [\"def calc_note(count, value):\\n    qnt = 0\\n    if count >= value:\\n        qnt = int(count) / value\\n    print '%d nota(s) de R$ %d.00' % (qnt, value)\\n    return count - qnt * value\"]}, {'features': [], 'snippets': ['def index(request, **kwargs):\\n    if request.method == \"GET\" and request.subdomain and request.subdomain not in [\\'dev\\', \\'www\\', \\'debug\\']:\\n        username = request.subdomain\\n        try:\\n            if \\'.\\' in username:\\n                username = username.split(\\'.\\')[0]\\n            user = User.objects.get(username__iexact=username)\\n        except User.DoesNotExist:\\n            return HttpResponseRedirect(\\'http://%s%s\\' % (\\n                Site.objects.get_current().domain,\\n                reverse(\\'index\\')))\\n        return load_social_page(request, user_id=user.pk, username=request.subdomain, **kwargs)\\n\\n    if request.user.is_anonymous():\\n        return welcome(request, **kwargs)\\n    else:\\n        return dashboard(request, **kwargs)', 'def welcome(request, **kwargs):\\n    user              = get_user(request)\\n    statistics        = MStatistics.all()\\n    social_profile    = MSocialProfile.get_user(user.pk)', 'def login(request):\\n    code = -1\\n    message = \"\"\\n    if request.method == \"POST\":\\n        form = LoginForm(request.POST, prefix=\\'login\\')\\n        if form.is_valid():\\n            login_user(request, form.get_user())\\n            if request.POST.get(\\'api\\'):\\n                logging.user(form.get_user(), \"~FG~BB~SKiPhone Login~FW\")\\n                code = 1\\n            else:\\n                logging.user(form.get_user(), \"~FG~BBLogin~FW\")\\n                return HttpResponseRedirect(reverse(\\'index\\'))\\n        else:\\n            message = form.errors.items()[0][1][0]\\n\\n    if request.POST.get(\\'api\\'):\\n        return HttpResponse(json.encode(dict(code=code, message=message)), mimetype=\\'application/json\\')\\n    else:\\n        return index(request)', 'def signup(request):\\n    if request.method == \"POST\":\\n        form = SignupForm(prefix=\\'signup\\', data=request.POST)\\n        if form.is_valid():\\n            new_user = form.save()\\n            login_user(request, new_user)\\n            logging.user(new_user, \"~FG~SB~BBNEW SIGNUP: ~FW%s\" % new_user.email)\\n            if not new_user.is_active:\\n                url = \"https://%s%s\" % (Site.objects.get_current().domain,\\n                                         reverse(\\'stripe-form\\'))\\n                return HttpResponseRedirect(url)', 'def logout(request):\\n    logging.user(request, \"~FG~BBLogout~FW\")\\n    logout_user(request)', \"def autologin(request, username, secret):\\n    next = request.GET.get('next', '')\", \"def load_feeds(request):\\n    user             = get_user(request)\\n    feeds            = {}\\n    include_favicons = request.REQUEST.get('include_favicons', False)\\n    flat             = request.REQUEST.get('flat', False)\\n    update_counts    = request.REQUEST.get('update_counts', False)\\n    version          = int(request.REQUEST.get('v', 1))\", \"def load_feed_favicons(request):\\n    user = get_user(request)\\n    feed_ids = request.REQUEST.getlist('feed_ids')\", \"def load_feeds_flat(request):\\n    user = request.user\\n    include_favicons = is_true(request.REQUEST.get('include_favicons', False))\\n    update_counts    = is_true(request.REQUEST.get('update_counts', True))\", \"def refresh_feeds(request):\\n    user = get_user(request)\\n    feed_ids = request.REQUEST.getlist('feed_id')\\n    check_fetch_status = request.REQUEST.get('check_fetch_status')\\n    favicons_fetching = request.REQUEST.getlist('favicons_fetching')\\n    social_feed_ids = [feed_id for feed_id in feed_ids if 'social:' in feed_id]\\n    feed_ids = list(set(feed_ids) - set(social_feed_ids))\", \"def interactions_count(request):\\n    user = get_user(request)\\n\\n    interactions_count = MInteraction.user_unread_count(user.pk)\\n\\n    return {\\n        'interactions_count': interactions_count,\\n    }\", \"def feed_unread_count(request):\\n    user = request.user\\n    feed_ids = request.REQUEST.getlist('feed_id')\\n    force = request.REQUEST.get('force', False)\\n    social_feed_ids = [feed_id for feed_id in feed_ids if 'social:' in feed_id]\\n    feed_ids = list(set(feed_ids) - set(social_feed_ids))\", 'def refresh_feed(request, feed_id):\\n    user = get_user(request)\\n    feed = get_object_or_404(Feed, pk=feed_id)', \"def load_single_feed(request, feed_id):\\n    start                   = time.time()\\n    user                    = get_user(request)\\n    # offset                  = int(request.REQUEST.get('offset', 0))\\n    # limit                   = int(request.REQUEST.get('limit', 6))\\n    limit                   = 6\\n    page                    = int(request.REQUEST.get('page', 1))\\n    offset                  = limit * (page-1)\\n    order                   = request.REQUEST.get('order', 'newest')\\n    read_filter             = request.REQUEST.get('read_filter', 'all')\\n    query                   = request.REQUEST.get('query')\\n    include_story_content   = is_true(request.REQUEST.get('include_story_content', True))\\n    include_hidden          = is_true(request.REQUEST.get('include_hidden', False))\\n    message                 = None\\n    user_search             = None\\n\\n    dupe_feed_id = None\\n    user_profiles = []\\n    now = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\\n    if not feed_id: raise Http404\\n\\n    feed_address = request.REQUEST.get('feed_address')\\n    feed = Feed.get_by_id(feed_id, feed_address=feed_address)\\n    if not feed:\\n        raise Http404\", 'def load_feed_page(request, feed_id):\\n    if not feed_id:\\n        raise Http404', 'def load_starred_stories(request):\\n    user         = get_user(request)\\n    offset       = int(request.REQUEST.get(\\'offset\\', 0))\\n    limit        = int(request.REQUEST.get(\\'limit\\', 10))\\n    page         = int(request.REQUEST.get(\\'page\\', 0))\\n    query        = request.REQUEST.get(\\'query\\')\\n    order        = request.REQUEST.get(\\'order\\', \\'newest\\')\\n    tag          = request.REQUEST.get(\\'tag\\')\\n    story_hashes = request.REQUEST.getlist(\\'h\\')[:100]\\n    version      = int(request.REQUEST.get(\\'v\\', 1))\\n    now          = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\\n    message      = None\\n    order_by     = \\'-\\' if order == \"newest\" else \"\"\\n    if page: offset = limit * (page - 1)', \"def starred_story_hashes(request):\\n    user               = get_user(request)\\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))\", 'def starred_stories_rss_feed(request, user_id, secret_token, tag_slug):\\n    try:\\n        user = User.objects.get(pk=user_id)\\n    except User.DoesNotExist:\\n        raise Http404', \"def load_read_stories(request):\\n    user   = get_user(request)\\n    offset = int(request.REQUEST.get('offset', 0))\\n    limit  = int(request.REQUEST.get('limit', 10))\\n    page   = int(request.REQUEST.get('page', 0))\\n    order  = request.REQUEST.get('order', 'newest')\\n    query  = request.REQUEST.get('query')\\n    now    = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\\n    message = None\\n    if page: offset = limit * (page - 1)\", 'def load_river_stories__redis(request):\\n    limit             = 12\\n    start             = time.time()\\n    user              = get_user(request)\\n    message           = None\\n    feed_ids          = [int(feed_id) for feed_id in request.REQUEST.getlist(\\'feeds\\') if feed_id]\\n    if not feed_ids:\\n        feed_ids      = [int(feed_id) for feed_id in request.REQUEST.getlist(\\'f\\') if feed_id]\\n    story_hashes      = request.REQUEST.getlist(\\'h\\')[:100]\\n    original_feed_ids = list(feed_ids)\\n    page              = int(request.REQUEST.get(\\'page\\', 1))\\n    order             = request.REQUEST.get(\\'order\\', \\'newest\\')\\n    read_filter       = request.REQUEST.get(\\'read_filter\\', \\'unread\\')\\n    query             = request.REQUEST.get(\\'query\\')\\n    include_hidden    = is_true(request.REQUEST.get(\\'include_hidden\\', False))\\n    now               = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\\n    usersubs          = []\\n    code              = 1\\n    user_search       = None\\n    offset = (page-1) * limit\\n    limit = page * limit\\n    story_date_order = \"%sstory_date\" % (\\'\\' if order == \\'oldest\\' else \\'-\\')', \"def unread_story_hashes__old(request):\\n    user              = get_user(request)\\n    feed_ids          = [int(feed_id) for feed_id in request.REQUEST.getlist('feed_id') if feed_id]\\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))\\n    usersubs = {}\", \"def unread_story_hashes(request):\\n    user               = get_user(request)\\n    feed_ids           = [int(feed_id) for feed_id in request.REQUEST.getlist('feed_id') if feed_id]\\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))\\n    order              = request.REQUEST.get('order', 'newest')\\n    read_filter        = request.REQUEST.get('read_filter', 'unread')\", 'def mark_all_as_read(request):\\n    code = 1\\n    try:\\n        days = int(request.REQUEST.get(\\'days\\', 0))\\n    except ValueError:\\n        return dict(code=-1, message=\"Days parameter must be an integer, not: %s\" %\\n                    request.REQUEST.get(\\'days\\'))\\n    read_date = datetime.datetime.utcnow() - datetime.timedelta(days=days)', 'def mark_story_as_read(request):\\n    story_ids = request.REQUEST.getlist(\\'story_id\\')\\n    try:\\n        feed_id = int(get_argument_or_404(request, \\'feed_id\\'))\\n    except ValueError:\\n        return dict(code=-1, errors=[\"You must pass a valid feed_id: %s\" %\\n                                     request.REQUEST.get(\\'feed_id\\')])', \"def mark_story_hashes_as_read(request):\\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\\n    story_hashes = request.REQUEST.getlist('story_hash')\", 'def mark_feed_stories_as_read(request):\\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\\n    feeds_stories = request.REQUEST.get(\\'feeds_stories\\', \"{}\")\\n    feeds_stories = json.decode(feeds_stories)\\n    data = {\\n        \\'code\\': -1,\\n        \\'message\\': \\'Nothing was marked as read\\'\\n    }', 'def mark_social_stories_as_read(request):\\n    code = 1\\n    errors = []\\n    data = {}\\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\\n    users_feeds_stories = request.REQUEST.get(\\'users_feeds_stories\\', \"{}\")\\n    users_feeds_stories = json.decode(users_feeds_stories)\\n\\n    for social_user_id, feeds in users_feeds_stories.items():\\n        for feed_id, story_ids in feeds.items():\\n            feed_id = int(feed_id)\\n            try:\\n                socialsub = MSocialSubscription.objects.get(user_id=request.user.pk, \\n                                                            subscription_user_id=social_user_id)\\n                data = socialsub.mark_story_ids_as_read(story_ids, feed_id, request=request)\\n            except OperationError, e:\\n                code = -1\\n                errors.append(\"Already read story: %s\" % e)\\n            except MSocialSubscription.DoesNotExist:\\n                MSocialSubscription.mark_unsub_story_ids_as_read(request.user.pk, social_user_id,\\n                                                                 story_ids, feed_id,\\n                                                                 request=request)\\n            except Feed.DoesNotExist:\\n                duplicate_feed = DuplicateFeed.objects.filter(duplicate_feed_id=feed_id)\\n                if duplicate_feed:\\n                    try:\\n                        socialsub = MSocialSubscription.objects.get(user_id=request.user.pk,\\n                                                                    subscription_user_id=social_user_id)\\n                        data = socialsub.mark_story_ids_as_read(story_ids, duplicate_feed[0].feed.pk, request=request)\\n                    except (UserSubscription.DoesNotExist, Feed.DoesNotExist):\\n                        code = -1\\n                        errors.append(\"No feed exists for feed_id %d.\" % feed_id)\\n                else:\\n                    continue\\n            r.publish(request.user.username, \\'feed:%s\\' % feed_id)\\n        r.publish(request.user.username, \\'social:%s\\' % social_user_id)\\n\\n    data.update(code=code, errors=errors)\\n    return data', \"def mark_story_as_unread(request):\\n    story_id = request.REQUEST.get('story_id', None)\\n    feed_id = int(request.REQUEST.get('feed_id', 0))\", 'def mark_story_hash_as_unread(request):\\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\\n    story_hash = request.REQUEST.get(\\'story_hash\\')\\n    feed_id, _ = MStory.split_story_hash(story_hash)\\n    story, _ = MStory.find_story(feed_id, story_hash)\\n    if not story:\\n        data = dict(code=-1, message=\"That story has been removed from the feed, no need to mark it unread.\")\\n        return data        \\n    message = RUserStory.story_can_be_marked_read_by_user(story, request.user)\\n    if message:\\n        data = dict(code=-1, message=message)\\n        return data', \"def mark_feed_as_read(request):\\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\\n    feed_ids = request.REQUEST.getlist('feed_id')\\n    cutoff_timestamp = int(request.REQUEST.get('cutoff_timestamp', 0))\\n    direction = request.REQUEST.get('direction', 'older')\\n    multiple = len(feed_ids) > 1\\n    code = 1\\n    errors = []\\n    cutoff_date = datetime.datetime.fromtimestamp(cutoff_timestamp) if cutoff_timestamp else None\", \"def _parse_user_info(user):\\n    return {\\n        'user_info': {\\n            'is_anonymous': json.encode(user.is_anonymous()),\\n            'is_authenticated': json.encode(user.is_authenticated()),\\n            'username': json.encode(user.username if user.is_authenticated() else 'Anonymous')\\n        }\\n    }\", \"def add_url(request):\\n    code = 0\\n    url = request.POST['url']\\n    folder = request.POST.get('folder', '')\\n    new_folder = request.POST.get('new_folder')\\n    auto_active = is_true(request.POST.get('auto_active', 1))\\n    skip_fetch = is_true(request.POST.get('skip_fetch', False))\\n    feed = None\", 'def add_folder(request):\\n    folder = request.POST[\\'folder\\']\\n    parent_folder = request.POST.get(\\'parent_folder\\', \\'\\')\\n    folders = None\\n    logging.user(request, \"~FRAdding Folder: ~SB%s (in %s)\" % (folder, parent_folder))', 'def delete_feed(request):\\n    feed_id = int(request.POST[\\'feed_id\\'])\\n    in_folder = request.POST.get(\\'in_folder\\', None)\\n    if not in_folder or in_folder == \\' \\':\\n        in_folder = \"\"', 'def delete_feed_by_url(request):\\n    message = \"\"\\n    code = 0\\n    url = request.POST[\\'url\\']\\n    in_folder = request.POST.get(\\'in_folder\\', \\'\\')\\n    if in_folder == \\' \\':\\n        in_folder = \"\"', 'def delete_folder(request):\\n    folder_to_delete = request.POST.get(\\'folder_name\\') or request.POST.get(\\'folder_to_delete\\')\\n    in_folder = request.POST.get(\\'in_folder\\', None)\\n    feed_ids_in_folder = [int(f) for f in request.REQUEST.getlist(\\'feed_id\\') if f]\\n\\n    request.user.profile.send_opml_export_email(reason=\"You have deleted an entire folder of feeds, so here\\'s a backup just in case.\")', 'def delete_feeds_by_folder(request):\\n    feeds_by_folder = json.decode(request.POST[\\'feeds_by_folder\\'])\\n\\n    request.user.profile.send_opml_export_email(reason=\"You have deleted a number of feeds at once, so here\\'s a backup just in case.\")', \"def rename_feed(request):\\n    feed = get_object_or_404(Feed, pk=int(request.POST['feed_id']))\\n    user_sub = UserSubscription.objects.get(user=request.user, feed=feed)\\n    feed_title = request.POST['feed_title']\", \"def rename_folder(request):\\n    folder_to_rename = request.POST.get('folder_name') or request.POST.get('folder_to_rename')\\n    new_folder_name = request.POST['new_folder_name']\\n    in_folder = request.POST.get('in_folder', '')\\n    code = 0\", \"def move_feed_to_folders(request):\\n    feed_id = int(request.POST['feed_id'])\\n    in_folders = request.POST.getlist('in_folders', '')\\n    to_folders = request.POST.getlist('to_folders', '')\\n\\n    user_sub_folders = get_object_or_404(UserSubscriptionFolders, user=request.user)\\n    user_sub_folders = user_sub_folders.move_feed_to_folders(feed_id, in_folders=in_folders,\\n                                                             to_folders=to_folders)\", \"def move_feed_to_folder(request):\\n    feed_id = int(request.POST['feed_id'])\\n    in_folder = request.POST.get('in_folder', '')\\n    to_folder = request.POST.get('to_folder', '')\\n\\n    user_sub_folders = get_object_or_404(UserSubscriptionFolders, user=request.user)\\n    user_sub_folders = user_sub_folders.move_feed_to_folder(feed_id, in_folder=in_folder,\\n                                                            to_folder=to_folder)\", \"def move_folder_to_folder(request):\\n    folder_name = request.POST['folder_name']\\n    in_folder = request.POST.get('in_folder', '')\\n    to_folder = request.POST.get('to_folder', '')\", 'def move_feeds_by_folder_to_folder(request):\\n    feeds_by_folder = json.decode(request.POST[\\'feeds_by_folder\\'])\\n    to_folder = request.POST[\\'to_folder\\']\\n    new_folder = request.POST.get(\\'new_folder\\', None)\\n\\n    request.user.profile.send_opml_export_email(reason=\"You have moved a number of feeds at once, so here\\'s a backup just in case.\")', 'def add_feature(request):\\n    if not request.user.is_staff:\\n        return HttpResponseForbidden()\\n\\n    code = -1    \\n    form = FeatureForm(request.POST)', 'def load_features(request):\\n    user = get_user(request)\\n    page = max(int(request.REQUEST.get(\\'page\\', 0)), 0)\\n    logging.user(request, \"~FBBrowse features: ~SBPage #%s\" % (page+1))\\n    features = Feature.objects.all()[page*3:(page+1)*3+1].values()\\n    features = [{\\n        \\'description\\': f[\\'description\\'], \\n        \\'date\\': localtime_for_timezone(f[\\'date\\'], user.profile.timezone).strftime(\"%b %d, %Y\")\\n    } for f in features]\\n    return features', 'def save_feed_order(request):\\n    folders = request.POST.get(\\'folders\\')\\n    if folders:\\n        # Test that folders can be JSON decoded\\n        folders_list = json.decode(folders)\\n        assert folders_list is not None\\n        logging.user(request, \"~FBFeed re-ordering: ~SB%s folders/feeds\" % (len(folders_list)))\\n        user_sub_folders = UserSubscriptionFolders.objects.get(user=request.user)\\n        user_sub_folders.folders = folders\\n        user_sub_folders.save()', \"def feeds_trainer(request):\\n    classifiers = []\\n    feed_id = request.REQUEST.get('feed_id')\\n    user = get_user(request)\\n    usersubs = UserSubscription.objects.filter(user=user, active=True)\", \"def save_feed_chooser(request):\\n    is_premium = request.user.profile.is_premium\\n    approved_feeds = [int(feed_id) for feed_id in request.POST.getlist('approved_feeds') if feed_id]\\n    if not is_premium:\\n        approved_feeds = approved_feeds[:64]\\n    activated = 0\\n    usersubs = UserSubscription.objects.filter(user=request.user)\", 'def retrain_all_sites(request):\\n    for sub in UserSubscription.objects.filter(user=request.user):\\n        sub.is_trained = False\\n        sub.save()', 'def activate_premium_account(request):\\n    try:\\n        usersubs = UserSubscription.objects.select_related(\\'feed\\').filter(user=request.user)\\n        for sub in usersubs:\\n            sub.active = True\\n            sub.save()\\n            if sub.feed.premium_subscribers <= 0:\\n                sub.feed.count_subscribers()\\n                sub.feed.schedule_feed_fetch_immediately()\\n    except Exception, e:\\n        subject = \"Premium activation failed\"\\n        message = \"%s -- %s\\\\n\\\\n%s\" % (request.user, usersubs, e)\\n        mail_admins(subject, message, fail_silently=True)', 'def login_as(request):\\n    if not request.user.is_staff:\\n        logging.user(request, \"~SKNON-STAFF LOGGING IN AS ANOTHER USER!\")\\n        assert False\\n        return HttpResponseForbidden()\\n    username = request.GET[\\'user\\']\\n    user = get_object_or_404(User, username__iexact=username)\\n    user.backend = settings.AUTHENTICATION_BACKENDS[0]\\n    login_user(request, user)\\n    return HttpResponseRedirect(reverse(\\'index\\'))', 'def iframe_buster(request):\\n    logging.user(request, \"~FB~SBiFrame bust!\")\\n    return HttpResponse(status=204)', 'def mark_story_as_starred(request):\\n    return _mark_story_as_starred(request)', 'def mark_story_hash_as_starred(request):\\n    return _mark_story_as_starred(request)', 'def _mark_story_as_starred(request):\\n    code       = 1\\n    feed_id    = int(request.REQUEST.get(\\'feed_id\\', 0))\\n    story_id   = request.REQUEST.get(\\'story_id\\', None)\\n    story_hash = request.REQUEST.get(\\'story_hash\\', None)\\n    user_tags  = request.REQUEST.getlist(\\'user_tags\\')\\n    message    = \"\"\\n    if story_hash:\\n        story, _   = MStory.find_story(story_hash=story_hash)\\n        feed_id = story and story.story_feed_id\\n    else:\\n        story, _   = MStory.find_story(story_feed_id=feed_id, story_id=story_id)', 'def mark_story_as_unstarred(request):\\n    return _mark_story_as_unstarred(request)', 'def mark_story_hash_as_unstarred(request):\\n    return _mark_story_as_unstarred(request)', 'def send_story_email(request):\\n    code       = 1\\n    message    = \\'OK\\'\\n    story_id   = request.POST[\\'story_id\\']\\n    feed_id    = request.POST[\\'feed_id\\']\\n    to_addresses = request.POST.get(\\'to\\', \\'\\').replace(\\',\\', \\' \\').replace(\\'  \\', \\' \\').strip().split(\\' \\')\\n    from_name  = request.POST[\\'from_name\\']\\n    from_email = request.POST[\\'from_email\\']\\n    email_cc   = is_true(request.POST.get(\\'email_cc\\', \\'true\\'))\\n    comments   = request.POST[\\'comments\\']\\n    comments   = comments[:2048] # Separated due to PyLint\\n    from_address = \\'share@newsblur.com\\'\\n    share_user_profile = MSocialProfile.get_user(request.user.pk)\\n\\n    if not to_addresses:\\n        code = -1\\n        message = \\'Please provide at least one email address.\\'\\n    elif not all(email_re.match(to_address) for to_address in to_addresses if to_addresses):\\n        code = -1\\n        message = \\'You need to send the email to a valid email address.\\'\\n    elif not email_re.match(from_email):\\n        code = -1\\n        message = \\'You need to provide your email address.\\'\\n    elif not from_name:\\n        code = -1\\n        message = \\'You need to provide your name.\\'\\n    else:\\n        story, _ = MStory.find_story(feed_id, story_id)\\n        story   = Feed.format_story(story, feed_id, text=True)\\n        feed    = Feed.get_by_id(story[\\'story_feed_id\\'])\\n        params  = {\\n            \"to_addresses\": to_addresses,\\n            \"from_name\": from_name,\\n            \"from_email\": from_email,\\n            \"email_cc\": email_cc,\\n            \"comments\": comments,\\n            \"from_address\": from_address,\\n            \"story\": story,\\n            \"feed\": feed,\\n            \"share_user_profile\": share_user_profile,\\n        }\\n        text    = render_to_string(\\'mail/email_story.txt\\', params)\\n        html    = render_to_string(\\'mail/email_story.xhtml\\', params)\\n        subject = \\'%s\\' % (story[\\'story_title\\'])\\n        cc      = None\\n        if email_cc:\\n            cc = [\\'%s <%s>\\' % (from_name, from_email)]\\n        subject = subject.replace(\\'\\\\n\\', \\' \\')\\n        msg     = EmailMultiAlternatives(subject, text, \\n                                         from_email=\\'NewsBlur <%s>\\' % from_address,\\n                                         to=to_addresses, \\n                                         cc=cc,\\n                                         headers={\\'Reply-To\\': \\'%s <%s>\\' % (from_name, from_email)})\\n        msg.attach_alternative(html, \"text/html\")\\n        try:\\n            msg.send()\\n        except boto.ses.connection.ResponseError, e:\\n            code = -1\\n            message = \"Email error: %s\" % str(e)\\n        logging.user(request, \\'~BMSharing story by email to %s recipient%s: ~FY~SB%s~SN~BM~FY/~SB%s\\' % \\n                              (len(to_addresses), \\'\\' if len(to_addresses) == 1 else \\'s\\', \\n                               story[\\'story_title\\'][:50], feed and feed.feed_title[:50]))']}, {'features': [], 'snippets': ['def __init__(self, inputfiles):\\n        \"\"\"\\n        :param inputfiles: list of pdb files needed for averaging\\n        \"\"\"\\n        self.inputs = inputfiles\\n        self.size = []\\n        self.nbknots = None\\n        self.radius = None\\n        self.coordknots = []', 'def spatial_extent(self):\\n        \"\"\"\\n        Calculate the maximal extent of input models', 'def calc_radius(self, nbknots=None):\\n        \"\"\"\\n        Calculate the radius of each point of a hexagonal close-packed grid, \\n        knowing the total volume and the number of knots in this grid.\\n\\n        :param nbknots: number of knots wanted for the grid\\n        :return radius: the radius of each knot of the grid\\n        \"\"\"\\n        if len(self.size)==0:\\n            self.spatial_extent()\\n        nbknots = nbknots if nbknots is not None else 5000\\n        size = self.size\\n        dx = size[0] - size[3]\\n        dy = size[1] - size[4]\\n        dz = size[2] - size[5]\\n        volume = dx * dy * dz\\n\\n        density = numpy.pi / (3*2**0.5)\\n        radius = ((3 /( 4 * numpy.pi)) * density * volume / nbknots)**(1.0/3)\\n        self.radius = radius\\n\\n        return radius', 'def __init__(self, inputfiles, grid):\\n        \"\"\"\\n        :param inputfiles: list of pdb files of aligned models\\n        :param grid: 2d-array coordinates of each point of a grid, fourth column full of zeros\\n        \"\"\"\\n        self.inputfiles = inputfiles\\n        self.models = []\\n        self.header = []\\n        self.radius = None\\n        self.atoms = []\\n        self.grid = grid', 'def read_files(self, reference=None):\\n        \"\"\"\\n        Read all the pdb file in the inputfiles list, creating SASModels.\\n        The SASModels created are save in a list, the reference model is the first model in the list.\\n\\n        :param reference: position of the reference model file in the inputfiles list\\n        \"\"\"\\n        ref = reference if reference is not None else 0\\n        inputfiles = self.inputfiles\\n\\n        models = []\\n        models.append(SASModel(inputfiles[ref]))\\n        for i in range(len(inputfiles)):\\n            if i==ref:\\n                continue\\n            else:\\n                models.append(SASModel(inputfiles[i]))\\n        self.models = models\\n\\n        return models', 'def assign_occupancy(self):\\n        \"\"\"\\n        For each point of the grid, total occupancy and contribution factor are computed and saved.\\n        The grid is then ordered with decreasing value of occupancy.\\n        The fourth column of the array correspond to the occupancy of the point and the fifth to \\n        the contribution for this point.\\n\\n        :return sortedgrid: 2d-array, coordinates of each point of the grid\\n        \"\"\"\\n        grid = self.grid\\n        nbknots = grid.shape[0]\\n        grid = numpy.append(grid, numpy.zeros((nbknots, 1), dtype=\"float\"), axis=1)\\n\\n        for i in range(nbknots):\\n            occ, contrib = self.calc_occupancy(grid[i, 0:3])\\n            grid[i, 3] = occ\\n            grid[i, 4] = contrib\\n\\n        order = numpy.argsort(grid, axis=0)[:, -2]\\n        sortedgrid = numpy.empty_like(grid)\\n        for i in range(nbknots):\\n            sortedgrid[nbknots - i - 1, :] = grid[order[i], :]\\n\\n        return sortedgrid']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def add_arguments(self, parser):\\n        parser.add_argument(\\n            \\'--send-out-for-real\\', action=\\'store_true\\', default=False,\\n            help=\\'Send information to the instructors.\\',\\n        )\\n        parser.add_argument(\\n            \\'--no-may-contact-only\\', action=\\'store_true\\', default=False,\\n            help=\\'Include instructors not willing to be contacted.\\',\\n        )\\n        parser.add_argument(\\n            \\'--django-mailing\\', action=\\'store_true\\', default=False,\\n            help=\\'Use Django mailing system. This requires some environmental \\'\\n                 \\'variables to be set, see `settings.py`.\\',\\n        )\\n        parser.add_argument(\\n            \\'-s\\', \\'--sender\\', action=\\'store\\',\\n            default=\\'workshops@carpentries.org\\',\\n            help=\\'E-mail used in \"from:\" field.\\',\\n        )', \"def fetch_activity(self, may_contact_only=True):\\n        roles = Role.objects.filter(name__in=['instructor', 'helper'])\\n        instructor_badges = Badge.objects.instructor_badges()\\n\\n        instructors = Person.objects.filter(badges__in=instructor_badges)\\n        instructors = instructors.exclude(email__isnull=True)\\n        if may_contact_only:\\n            instructors = instructors.exclude(may_contact=False)\\n\\n        # let's get some things faster\\n        instructors = instructors.select_related('airport') \\\\\\n                                 .prefetch_related('task_set', 'lessons',\\n                                                   'award_set', 'badges')\\n\\n        # don't repeat the records\\n        instructors = instructors.distinct()\\n\\n        result = []\\n        for person in instructors:\\n            tasks = person.task_set.filter(role__in=roles) \\\\\\n                                   .select_related('event', 'role')\\n            record = {\\n                'person': person,\\n                'lessons': person.lessons.all(),\\n                'instructor_awards': person.award_set.filter(\\n                    badge__in=person.badges.instructor_badges()\\n                ),\\n                'tasks': zip(tasks,\\n                             self.foreign_tasks(tasks, person, roles)),\\n            }\\n            result.append(record)\\n\\n        return result\", \"def subject(self, record):\\n        # in future we can vary the subject depending on the record details\\n        return 'Updating your Software Carpentry information'\", 'def send_message(self, subject, message, sender, recipient, for_real=False,\\n                     django_mailing=False):\\n        if for_real:\\n            if django_mailing:\\n                send_mail(subject, message, sender, [recipient])\\n\\n            else:\\n                command = \\'mail -s \"{subject}\" -r {sender} {recipient}\\'.format(\\n                    subject=subject,\\n                    sender=sender,\\n                    recipient=recipient,\\n                )\\n\\n                writer = os.popen(command, \\'w\\')\\n                writer.write(message)\\n                writer.close()\\n\\n        if self.verbosity >= 2:\\n            # write only a header\\n            self.stdout.write(\\'-\\' * 40 + \\'\\\\n\\')\\n            self.stdout.write(\\'To: {}\\\\n\\'.format(recipient))\\n            self.stdout.write(\\'Subject: {}\\\\n\\'.format(subject))\\n            self.stdout.write(\\'From: {}\\\\n\\'.format(sender))\\n        if self.verbosity >= 3:\\n            # write whole message out\\n            self.stdout.write(message + \\'\\\\n\\')']}, {'features': [], 'snippets': ['def build_delete_request(\\n    scope: str,\\n    policy_assignment_name: str,\\n    **kwargs: Any', 'def build_create_request(\\n    scope: str,\\n    policy_assignment_name: str,\\n    *,\\n    json: JSONType = None,\\n    content: Any = None,\\n    **kwargs: Any', 'def build_get_request(\\n    scope: str,\\n    policy_assignment_name: str,\\n    **kwargs: Any', 'def build_list_for_resource_group_request(\\n    resource_group_name: str,\\n    subscription_id: str,\\n    *,\\n    filter: Optional[str] = None,\\n    **kwargs: Any', 'def build_list_for_resource_request(\\n    resource_group_name: str,\\n    resource_provider_namespace: str,\\n    parent_resource_path: str,\\n    resource_type: str,\\n    resource_name: str,\\n    subscription_id: str,\\n    *,\\n    filter: Optional[str] = None,\\n    **kwargs: Any', 'def build_list_request(\\n    subscription_id: str,\\n    *,\\n    filter: Optional[str] = None,\\n    **kwargs: Any', 'def build_delete_by_id_request(\\n    policy_assignment_id: str,\\n    **kwargs: Any', 'def build_create_by_id_request(\\n    policy_assignment_id: str,\\n    *,\\n    json: JSONType = None,\\n    content: Any = None,\\n    **kwargs: Any', 'def build_get_by_id_request(\\n    policy_assignment_id: str,\\n    **kwargs: Any', 'def __init__(self, client, config, serializer, deserializer):\\n        self._client = client\\n        self._serialize = serializer\\n        self._deserialize = deserializer\\n        self._config = config', 'def delete(\\n        self,\\n        scope: str,\\n        policy_assignment_name: str,\\n        **kwargs: Any', 'def create(\\n        self,\\n        scope: str,\\n        policy_assignment_name: str,\\n        parameters: \"_models.PolicyAssignment\",\\n        **kwargs: Any', 'def get(\\n        self,\\n        scope: str,\\n        policy_assignment_name: str,\\n        **kwargs: Any', 'def list_for_resource_group(\\n        self,\\n        resource_group_name: str,\\n        filter: Optional[str] = None,\\n        **kwargs: Any', 'def prepare_request(next_link=None):\\n            if not next_link:', 'def extract_data(pipeline_response):\\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\\n            list_of_elem = deserialized.value\\n            if cls:\\n                list_of_elem = cls(list_of_elem)\\n            return deserialized.next_link or None, iter(list_of_elem)', 'def list_for_resource(\\n        self,\\n        resource_group_name: str,\\n        resource_provider_namespace: str,\\n        parent_resource_path: str,\\n        resource_type: str,\\n        resource_name: str,\\n        filter: Optional[str] = None,\\n        **kwargs: Any', 'def prepare_request(next_link=None):\\n            if not next_link:', 'def extract_data(pipeline_response):\\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\\n            list_of_elem = deserialized.value\\n            if cls:\\n                list_of_elem = cls(list_of_elem)\\n            return deserialized.next_link or None, iter(list_of_elem)', 'def list(\\n        self,\\n        filter: Optional[str] = None,\\n        **kwargs: Any', 'def prepare_request(next_link=None):\\n            if not next_link:', 'def extract_data(pipeline_response):\\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\\n            list_of_elem = deserialized.value\\n            if cls:\\n                list_of_elem = cls(list_of_elem)\\n            return deserialized.next_link or None, iter(list_of_elem)', 'def delete_by_id(\\n        self,\\n        policy_assignment_id: str,\\n        **kwargs: Any', 'def create_by_id(\\n        self,\\n        policy_assignment_id: str,\\n        parameters: \"_models.PolicyAssignment\",\\n        **kwargs: Any', 'def get_by_id(\\n        self,\\n        policy_assignment_id: str,\\n        **kwargs: Any']}, {'features': [], 'snippets': ['def __init__(self, filename=\"Default.log\"):\\n        self.terminal = sys.stdout\\n        self.log = open(filename, \"a\")', 'def flush(self):\\n        pass']}, {'features': [], 'snippets': [\"def __init__ (self, url=None, scheduler='default', session=None) :\\n\\n        Attributes.__init__ (self)\", 'def add_pilot (self, pid) :\\n        \"\"\"\\n        add (Compute or Data)-Pilot(s) to the pool\\n        \"\"\"\\n\\n        raise Exception (\"%s.add_pilot() is not implemented\" % self.__class__.__name__)', 'def list_pilots (self, ptype=ANY) :\\n        \"\"\"\\n        List IDs of data and/or compute pilots\\n        \"\"\"\\n\\n        raise Exception (\"%s.list_pilots() is not implemented\" % self.__class__.__name__)', 'def remove_pilot (self, pid, drain=False) :\\n        \"\"\"\\n        Remove pilot(s) (does not cancel the pilot(s), but removes all units\\n        from the pilot(s).\\n\\n        `drain` determines what happens to the units which are managed by the\\n        removed pilot(s).  If `True`, the pilot removal is delayed until all\\n        units reach a final state.  If `False` (the default), then `RUNNING`\\n        units will be canceled, and `PENDING` units will be re-assinged to the\\n        unit managers for re-scheduling to other pilots.\\n        \"\"\"\\n\\n        raise Exception (\"%s.remove_pilot() is not implemented\" % self.__class__.__name__)', 'def submit_unit (self, description) :\\n        \"\"\"\\n        Instantiate and return (Compute or Data)-Unit object(s)\\n        \"\"\"\\n\\n        raise Exception (\"%s.submit_unit() is not implemented\" % self.__class__.__name__)', 'def list_units (self, utype=ANY) :\\n        \"\"\"\\n        List IDs of data and/or compute units\\n        \"\"\"\\n\\n        raise Exception (\"%s.list_units() is not implemented\" % self.__class__.__name__)', 'def get_unit (self, uids) :\\n        \"\"\"\\n        Reconnect to and return (Compute or Data)-Unit object(s)\\n        \"\"\"\\n\\n        raise Exception (\"%s.get_unit() is not implemented\" % self.__class__.__name__)', 'def wait_unit (self, uids, state=[DONE, FAILED, CANCELED], timeout=-1.0) :\\n        \"\"\"\\n        Wait for given unit(s) to enter given state\\n        \"\"\"\\n\\n        raise Exception (\"%s.wait_unit() is not implemented\" % self.__class__.__name__)', 'def cancel_units (self, uids) :\\n        \"\"\"\\n        Cancel given unit(s)\\n        \"\"\"\\n\\n        raise Exception (\"%s.cancel_unit() is not implemented\" % self.__class__.__name__)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __missing__(self, key):\\n        try:\\n            return super().__missing__(key)\\n        except KeyError:\\n            return NotImplemented', \"def __init__(self, shortname, loader):\\n        # Not preloaded\\n        # loaders must produce dictionaries (or an appropriate iterable)\\n        # with the required keys.\\n        # The reason for this is that code for certain servers need not be loaded\\n        # if it's not going to be used at all\\n        # It also prevents import loop collisions.\\n        global __ServerImplementationDict\\n        self.__data = ServerImplementationDict(loader)\\n        self.__shortname = shortname\", 'def shortname(self):', 'def __str__(self):\\n        return str(self.__shortname)', \"def name(self):\\n        return self.__data['str_name']\", \"def internal_shortname(self):\\n        return self.__data['str_shortname']\", \"def beta(self):\\n        return self.__data['bool_tester']\", \"def Auth(self): # I really don't know how to call this.\\n        return self.__data['cls_auth']\", \"def auth_fields(self):\\n        return self.__data['list_authkeys']\", \"def Player(self):\\n        return self.__data['cls_player']\"]}, {'features': [], 'snippets': ['def __init__(self, name):\\n            self.name = name', 'def encode(value,\\n           unpicklable=True,\\n           make_refs=True,\\n           keys=False,\\n           max_depth=None,\\n           backend=None,\\n           warn=False,\\n           max_iter=None):\\n    \"\"\"Return a JSON formatted representation of value, a Python object.\\n\\n    :param unpicklable: If set to False then the output will not contain the\\n        information necessary to turn the JSON data back into Python objects,\\n        but a simpler JSON stream is produced.\\n    :param max_depth: If set to a non-negative integer then jsonpickle will\\n        not recurse deeper than \\'max_depth\\' steps into the object.  Anything\\n        deeper than \\'max_depth\\' is represented using a Python repr() of the\\n        object.\\n    :param make_refs: If set to False jsonpickle\\'s referencing support is\\n        disabled.  Objects that are id()-identical won\\'t be preserved across\\n        encode()/decode(), but the resulting JSON stream will be conceptually\\n        simpler.  jsonpickle detects cyclical objects and will break the cycle\\n        by calling repr() instead of recursing when make_refs is set False.\\n    :param keys: If set to True then jsonpickle will encode non-string\\n        dictionary keys instead of coercing them into strings via `repr()`.\\n    :param warn: If set to True then jsonpickle will warn when it\\n        returns None for an object which it cannot pickle\\n        (e.g. file descriptors).\\n    :param max_iter: If set to a non-negative integer then jsonpickle will\\n        consume at most `max_iter` items when pickling iterators.\\n\\n    >>> encode(\\'my string\\')\\n    \\'\"my string\"\\'\\n    >>> encode(36)\\n    \\'36\\'\\n\\n    >>> encode({\\'foo\\': True})\\n    \\'{\"foo\": true}\\'\\n\\n    >>> encode({\\'foo\\': True}, max_depth=0)\\n    \\'\"{\\\\\\\\\\'foo\\\\\\\\\\': True}\"\\'\\n\\n    >>> encode({\\'foo\\': True}, max_depth=1)\\n    \\'{\"foo\": \"True\"}\\'\\n\\n\\n    \"\"\"\\n    if backend is None:\\n        backend = json\\n    return pickler.encode(value,\\n                          backend=backend,\\n                          unpicklable=unpicklable,\\n                          make_refs=make_refs,\\n                          keys=keys,\\n                          max_depth=max_depth,\\n                          warn=warn)']}, {'features': [], 'snippets': ['def main():']}, {'features': [], 'snippets': ['def normalizeEnter(src):\\n\\t#Deletes all user defined for readability reason existing line breaks that are issues for the HTML output\\n\\tfor elem in blocklevel:\\n\\t\\twhile src.find(\"\\\\r<\" + elem) > -1:\\n\\t\\t\\tsrc = src.replace(\"\\\\r<\" + elem, \"<\" + elem)\\n\\t\\twhile src.find(\"</\" + elem + \">\\\\r\") > -1:\\n\\t\\t\\tsrc = src.replace(\"</\" + elem + \">\\\\r\", \"</\" + elem + \">\")\\n\\t\\twhile src.find(\">\\\\r\") > -1:\\n\\t\\t\\tsrc = src.replace(\">\\\\r\", \">\") #It is really needed, it created some other bugs?!\\n\\t\\twhile src.find(\"\\\\r</\") > -1:\\n\\t\\t\\tsrc = src.replace(\"\\\\r</\", \"</\") ##It is really needed, it created some other bugs?!\\n\\treturn src']}, {'features': [], 'snippets': [\"def __init__(self):\\n        super(Window, self).__init__(skip_pager_hint=True,\\n                                     skip_taskbar_hint=True,\\n                                    )\\n        self.set_title('Pylsner')\\n\\n        screen = self.get_screen()\\n        self.width = screen.get_width()\\n        self.height = screen.get_height()\\n        self.set_size_request(self.width, self.height)\\n        self.set_position(Gtk.WindowPosition.CENTER)\\n        rgba = screen.get_rgba_visual()\\n        self.set_visual(rgba)\\n        self.override_background_color(Gtk.StateFlags.NORMAL,\\n                                       Gdk.RGBA(0, 0, 0, 0),\\n                                      )\\n\\n        self.set_wmclass('pylsner', 'pylsner')\\n        self.set_type_hint(Gdk.WindowTypeHint.DOCK)\\n        self.stick()\\n        self.set_keep_below(True)\\n\\n        drawing_area = Gtk.DrawingArea()\\n        drawing_area.connect('draw', self.redraw)\\n        self.refresh_cnt = 0\\n        self.add(drawing_area)\\n\\n        self.connect('destroy', lambda q: Gtk.main_quit())\\n\\n        self.widgets = []\\n\\n        self.show_all()\", 'def redraw(self, _, ctx):\\n        ctx.set_antialias(cairo.ANTIALIAS_SUBPIXEL)\\n        for wid in self.widgets:\\n            wid.redraw(ctx)', \"def __init__(self,\\n                 name='default',\\n                 metric={'plugin': 'time'},\\n                 indicator={'plugin': 'arc'},\\n                 fill={'plugin': 'rgba_255'},\\n                ):\\n        self.name = name\\n        MetricPlugin = plugin.load_plugin('metrics', metric['plugin'])\\n        self.metric = MetricPlugin(**metric)\\n        IndicatorPlugin = plugin.load_plugin('indicators', indicator['plugin'])\\n        self.indicator = IndicatorPlugin(**indicator)\\n        FillPlugin = plugin.load_plugin('fills', fill['plugin'])\\n        self.fill = FillPlugin(**fill)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def errno_value():\\n    \"\"\"\\n    A particular errno.\\n    \"\"\"\\n    return errno.EINVAL', 'def strerror(errno_value):\\n    \"\"\"\\n    The string representation of a particular errno\\n    \"\"\"\\n    return \"[Errno {}] Invalid argument\".format(errno_value)', 'def apply_failing_clock_call(monkeypatch):\\n    \"\"\"\\n    Return a callable that patches in a failing system call fake that\\n    fails and return a list of calls to that fake.\\n    \"\"\"\\n\\n    def _apply_failing_clock_call(name, errno_value):\\n        calls = []\\n\\n        def _failing_clock_call(clock_id, timespec):\\n            calls.append((clock_id, timespec))\\n            monkeypatch.setattr(_api.ffi, \"errno\", errno.EINVAL)\\n            return -1\\n\\n        monkeypatch.setattr(_api, name, _failing_clock_call)\\n\\n        return calls\\n\\n    return _apply_failing_clock_call', 'def apply_timespec(monkeypatch):\\n    \"\"\"\\n    Return a callable that patches in a fake over the specified clock\\n    call that sets the specified resolution and returns a list of\\n    calls to that fake.\\n    \"\"\"\\n\\n    def _apply_timespec(name, goal_timespec):\\n        calls = []\\n\\n        def _fake_clock_call(clock_id, timespec):\\n            calls.append((clock_id, timespec))\\n            timespec[0] = goal_timespec[0]\\n            return 0\\n\\n        monkeypatch.setattr(_api, name, _fake_clock_call)\\n\\n        return calls\\n\\n    return _apply_timespec', 'def test_init(self):\\n        \"\"\"\\n        The initializer updates the instance\\'s C{__dict__} with its\\n        keyword arguments.\\n        \"\"\"\\n        namespace = _api._SimpleNamespace(x=1)\\n        assert namespace.x == 1', 'def test_eq(self):\\n        \"\"\"\\n        Two instances with equal C{__dict__}s are equal.\\n        \"\"\"\\n        assert _api._SimpleNamespace(a=1) == _api._SimpleNamespace(a=1)', 'def test_non_monotonic(self):\\n        \"\"\"\\n        L{get_clock_info} only knows about the monotonic clock.\\n        \"\"\"\\n        with pytest.raises(ValueError):\\n            get_clock_info(\"not monotonic\")', 'def test_info(self, clock_getres_spec, apply_timespec):\\n        \"\"\"\\n        The reported info always includes a nanosecond resolution when\\n        C{clock_getres} indicates nanosecond resolution.\\n        \"\"\"\\n        calls = apply_timespec(\\n            \"_clock_getres\",\\n            _bindings.ffi.new(\"struct timespec *\", clock_getres_spec),\\n        )\\n\\n        expected_info = _api._SimpleNamespace(\\n            adjustable=False,\\n            implementation=\"clock_gettime(MONOTONIC)\",\\n            monotonic=True,\\n            resolution=None,    # checked separately\\n        )\\n\\n        if clock_getres_spec[\\'tv_nsec\\']:\\n            expected_resolution = 1e-09\\n        else:\\n            expected_resolution = 1.0\\n\\n        info = get_clock_info(\"monotonic\")\\n        resolution, info.resolution = info.resolution, None\\n\\n        assert info == expected_info\\n        assert resolution - expected_resolution == pytest.approx(0.0)\\n\\n        assert len(calls) == 1\\n        assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC', 'def test_non_monotonic(self):\\n        \"\"\"\\n        L{get_clock_info} only knows about the monotonic clock.\\n        \"\"\"\\n        with pytest.raises(ValueError):\\n            get_clock_info(\"not monotonic\")', 'def test_monotonic_fails_posix(apply_failing_clock_call,\\n                               errno_value,\\n                               strerror):\\n    \"\"\"\\n    A failure in C{clock_gettime} results in an L{OSError} that\\n    presents the failure\\'s errno.\\n    \"\"\"\\n    calls = apply_failing_clock_call(\\'_clock_gettime\\', errno_value)\\n\\n    with pytest.raises(OSError) as exc:\\n        monotonic()\\n\\n    assert len(calls) == 1\\n    assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC\\n\\n    assert str(exc.value) == strerror', 'def test_clock(clock_gettime_spec, apply_timespec):\\n    \"\"\"\\n    For any given time resolution, the monotonic time equals the\\n    sum of the seconds and nanoseconds.\\n    \"\"\"\\n    clock_gettime_calls = apply_timespec(\\n        \\'_clock_gettime\\',\\n        _bindings.ffi.new(\"struct timespec *\", clock_gettime_spec),\\n    )\\n\\n    # we a float, representing the current seconds plus the\\n    # nanoseconds (offset by a billion) iff the resolution is accurate\\n    # to the nanosecond.\\n    expected = float(clock_gettime_spec[\\'tv_sec\\']) + (\\n        clock_gettime_spec[\\'tv_nsec\\'] * 1e-09)\\n\\n    result = monotonic()\\n\\n    assert result - expected == pytest.approx(0.0)\\n\\n    assert clock_gettime_calls[0][0] == _bindings.lib.CLOCK_MONOTONIC']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def format_fans(fans):\\n    return format_line(prefix='fans'.rjust(RJUST), values=fans)\", \"def format_pwms(pwms):\\n    return format_line(prefix='pwms'.rjust(RJUST), values=pwms)\", \"def format_names(names):\\n    return format_line(prefix='names'.rjust(RJUST), values=names)\", \"def format_temps(temps):\\n    return format_line(prefix='temps'.rjust(RJUST), values=temps)\", \"def format_limits(limits):\\n    return format_line(prefix='limits'.rjust(RJUST), values=limits)\", \"def format_headrooms(headrooms):\\n    return format_line(prefix='headrooms'.rjust(RJUST), values=headrooms)\", \"def format_differences(differences):\\n    return format_line(prefix='differences'.rjust(RJUST), values=differences)\"]}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Adding model 'Package'\\n        db.create_table(u'api_package', (\\n            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\\n            ('name', self.gf('django.db.models.fields.CharField')(unique=True, max_length=500, db_index=True)),\\n            ('url', self.gf('django.db.models.fields.CharField')(unique=True, max_length=500)),\\n            ('created_at', self.gf('django.db.models.fields.DateField')(auto_now_add=True, blank=True)),\\n        ))\\n        db.send_create_signal(u'api', ['Package'])\\n\\n        # Adding unique constraint on 'Package', fields ['name', 'url']\\n        db.create_unique(u'api_package', ['name', 'url'])\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, host='localhost', port=8125, enabled=True, prefix=''):\\n        self.addr = None\\n        self.enabled = enabled\\n        if enabled:\\n            self.set_address(host, port)\\n        self.prefix = prefix\\n        self.udp_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\", \"def timed(self, stat, sample_rate=1):\\n        log.debug('Entering timed context for %r' % (stat,))\\n        start = time.time()\\n        yield\\n        duration = int((time.time() - start) * 1000)\\n        log.debug('Exiting timed context for %r' % (stat,))\\n        self.timing(stat, duration, sample_rate)\", 'def increment(self, stats, sample_rate=1):\\n        \"\"\"\\n        Increments one or more stats counters\\n        \"\"\"\\n        self.update_stats(stats, 1, sample_rate)', 'def update_stats(self, stats, delta=1, sampleRate=1):\\n        \"\"\"\\n        Updates one or more stats counters by arbitrary amounts\\n        \"\"\"\\n        if not self.enabled or self.addr is None:\\n            return\\n\\n        if type(stats) is not list:\\n            stats = [stats]\\n        data = {}\\n        for stat in stats:\\n            data[\"%s%s\" % (self.prefix, stat)] = \"%s|c\" % delta\\n\\n        self.send(data, sampleRate)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def Key_Stats(gather=\"Total Debt/Equity (mrq)\"):\\n  statspath = path+\\'/_KeyStats\\'\\n  stock_list = [x[0] for x in os.walk(statspath)]\\n  df = pd.DataFrame(\\n    columns = [\\n      \\'Date\\',\\n      \\'Unix\\',\\n      \\'Ticker\\',\\n      \\'DE Ratio\\',\\n      \\'Price\\',\\n      \\'stock_p_change\\',\\n      \\'SP500\\',\\n      \\'sp500_p_change\\',\\n      \\'Difference\\',\\n      \\'Status\\'\\n    ]\\n  )\\n\\n  sp500_df = pd.DataFrame.from_csv(\"YAHOO-INDEX_GSPC.csv\")\\n\\n  ticker_list = []\\n\\n  for each_dir in stock_list[1:25]:\\n    each_file = os.listdir(each_dir)\\n\\n    # ticker = each_dir.split(\"\\\\\\\\\")[1] # Windows only\\n    # ticker = each_dir.split(\"/\")[1] # this didn\\'t work so do this:\\n    ticker = os.path.basename(os.path.normpath(each_dir))\\n    # print(ticker) # uncomment to verify\\n    ticker_list.append(ticker)\\n\\n    starting_stock_value = False\\n    starting_sp500_value = False']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, fichier, player):\\n        self.fichier = fichier\\n        self.grille = self.getFirstGrid()\\n        self.best_hit = 0\\n        self.players = player', 'def updateGrid(self):\\n        \"\"\"\\n        Implements function to update the grid to alter n-1\\n        round values\\n\\n        \"\"\"\\n        with open(self.fichier, \\'r\\') as fi:\\n            for line in fi.readlines():\\n                i = 0\\n                for car in line:\\n                    j = 0\\n                    if car != \\'\\\\n\\':\\n                        self.grille[i][j] = car\\n                        j += 1\\n                    i += 1', 'def checkLines(self, player, inARow):\\n        \"\"\"\\n        Implements function to check the current lines setup to evaluate best combinaison.\\n\\n        :param player: check for your numbers (your player number) or those of your opponent.\\n        :param inARow: how many tokens in a row (3 or 2).\\n        :return: true or false\\n\\n        \"\"\"\\n        count = 0\\n        flag = False\\n        for line_number, line in enumerate(self.grille):\\n            count = 0\\n            for car_pos, car in enumerate(line[:len(line) - 1]):\\n                if int(car) == player and not flag:\\n                    count = 1\\n                    flag = True\\n                elif int(car) == player and flag:\\n                    count += 1\\n                    if count == inARow:\\n                        if car_pos - inARow >= 0 and self.canPlayLine(line_number, car_pos - inARow):\\n                            return True, car_pos - inARow\\n                        if car_pos + 1 <= 6 and self.canPlayLine(line_number, car_pos + 1):\\n                            return True, car_pos + 1\\n                else:\\n                    count = 0\\n        return False, 0', 'def changeColumnInLines(self):\\n        \"\"\"\\n        Implements function to transform columns in lines to make tests eaiser.\\n        :return: a reverse matrice\\n        \"\"\"\\n        column = []\\n        for x in xrange(7):\\n            col = \\'\\'\\n            for y in xrange(6):\\n                col += self.grille[y][x]\\n            column.append(col)\\n        return column', 'def checkDiagonalLeftToRight(self, player, inARow):\\n        \"\"\"\\n        Implements function to check the current diagonal to evaluate best combinaison.\\n\\n        :param player: check for your numbers or opponent ones.\\n        :param inARow:  how many tokens in a row (3 or 2).\\n        :return:\\n        \"\"\"\\n\\n        x = 3\\n        flag = False\\n        while x < 6:\\n            count = 0\\n            x_int = x\\n            y_int = 0\\n            while x_int >= 0:\\n                if int(self.grille[x_int][y_int]) == player and not flag:\\n                    count = 1\\n                    flag = True\\n                elif int(self.grille[x_int][y_int]) == player and flag:\\n                    count += 1\\n                    if count == inARow and y_int + 1 <= 6 and x_int - 1 >= 0 and self.grille[x_int][y_int + 1] != \\'0\\':\\n                        return True, y_int + 1\\n                else:\\n                    count = 0\\n                    flag = False\\n                x_int -= 1\\n                y_int += 1\\n            x += 1\\n\\n        y = 1\\n        flag = False\\n        while y <= 3:\\n            count = 0\\n            x_int = 5\\n            y_int = y\\n            while y_int <= 6 and x_int >= 0:\\n                if int(self.grille[x_int][y_int]) == player and not flag:\\n                    count = 1\\n                    flag = True\\n                elif int(self.grille[x_int][y_int]) == player and flag:\\n                    count += 1\\n                    if count == inARow and y_int + 1 <= 6 and x_int - 1 >= 0 and self.grille[x_int][y + 1] != \\'0\\':\\n                        return True, y_int + 1\\n                else:\\n                    count = 0\\n                    flage = False\\n                x_int -= 1\\n                y_int += 1\\n            y += 1\\n\\n        return False, 0', 'def checkDiagonals(self, player, inARow):\\n        \"\"\"\\n        Calls two diagonal functional.\\n        :return: an int, representing the column where to play or 0 and False if there is no pattern search.\\n        \"\"\"\\n        check = self.checkDiagonalLeftToRight(player, inARow)\\n        if check[0]:\\n            return check\\n        else:\\n            return self.checkDiagonalRightToLeft(player, inARow)', 'def findFirstColumnEmpty(self):\\n        \"\"\"\\n        Implements function to get the first column where a slot remain.\\n        :return: the column\\n        \"\"\"\\n        for col in xrange(7):\\n            if self.grille[0][col] == \\'0\\':\\n                return col\\n        return -1']}, {'features': [], 'snippets': ['def __init__(self, raw_data):\\n        self._raw = raw_data', 'def display_name(self):\\n        \"\"\"\\n        Find the most appropriate display name for a user: look for a \"display_name\", then\\n        a \"real_name\", and finally fall back to the always-present \"name\".\\n        \"\"\"\\n        for k in self._NAME_KEYS:\\n            if self._raw.get(k):\\n                return self._raw[k]\\n            if \"profile\" in self._raw and self._raw[\"profile\"].get(k):\\n                return self._raw[\"profile\"][k]\\n        return self._raw[\"name\"]', 'def email(self):\\n        \"\"\"\\n        Shortcut property for finding the e-mail address or bot URL.\\n        \"\"\"\\n        if \"profile\" in self._raw:\\n            email = self._raw[\"profile\"].get(\"email\")\\n        elif \"bot_url\" in self._raw:\\n            email = self._raw[\"bot_url\"]\\n        else:\\n            email = None\\n        if not email:\\n            logging.debug(\"No email found for %s\", self._raw.get(\"name\"))\\n        return email']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def setUp(self):\\n    self.schedule = schedule_parser.Schedule()\\n    self.schedule.Parse(SCHEDULE_PATH)', 'def suite():\\n  loader = unittest.TestLoader()\\n  suite = unittest.TestSuite()\\n  suite.addTest(WeekdayRangeTest())\\n  suite.addTest(DepartmentCountTest())\\n  suite.addTest(DepartmentRangeTest())\\n  suite.addTest(DepartmentsRowTest())\\n  suite.addTest(HoursColumnTest())\\n  suite.addTest(HoursRangesTest())\\n  suite.addTest(GroupCountTest())\\n  suite.addTest(GroupListTest())\\n  suite.addTest(GroupRangeTest())\\n  suite.addTest(WeekdayByRowTest())\\n  suite.addTest(PairByRowTest())\\n  suite.addTest(DepartmentByColumnTest())\\n  suite.addTest(GroupByColumnTest())\\n  return suite']}, {'features': [], 'snippets': ['def get_num_lines_gz(filename):\\n    num_lines = 0\\n    with gzip.open(filename, \"r\") as fp:\\n        for line in fp:\\n            num_lines += 1', 'def main():\\n    \"\"\"get stats from PAS-seq']}, {'features': [], 'snippets': ['def Handler() :\\n\\twhile (1) :\\n\\t\\tchoice = eval(input(\"Enter :\\\\t 1 - to search student name \\\\n \\\\t 2 - to insert new student record \\\\n \\\\t 0 - to quit\\\\n\"))\\n\\t\\tprint(choice)\\n\\t\\tif (choice == 1) :\\n\\t\\t\\tif (student_phoneNumber_name) :\\n\\t\\t\\t\\tphone_number = input(\"Enter student\\'s phone number : \")\\n\\t\\t\\t\\tname = SearchRecord(phone_number)\\n\\t\\t\\t\\tif (name) :\\n\\t\\t\\t\\t\\tprint(\"name : \" + name )\\n\\t\\t\\t\\telse :\\n\\t\\t\\t\\t\\tprint(str(phone_number) + \"Does not exist in record\" + str(name))\\n\\t\\t\\telse :\\n\\t\\t\\t\\tprint(\"Record is empty \")\\n\\t\\telif (choice == 2) :\\n\\t\\t\\tphone_number = input(\"Enter student\\'s phone number : \")\\n\\t\\t\\tname = input(\"Enter student\\'s name : \") #best example to understand input() and raw_input()\\n\\t\\t\\tInsertRecord(phone_number, name)\\n\\t\\telif (choice == 0) :\\n\\t\\t\\tbreak\\n\\t\\telse:\\n\\t\\t\\tprint(\"Enter correct choice\")', 'def InsertRecord(x, y):\\n\\tstudent_phoneNumber_name[x] = y\\n\\treturn;', 'def SearchRecord(x):\\n\\tprint(x)\\n\\tif (x in student_phoneNumber_name) :\\n\\t\\treturn student_phoneNumber_name[x]']}, {'features': [], 'snippets': ['def setUp(self):\\n        super(BaseSystemTest, self).setUp()\\n        # Clear out any pre-existing tables\\n        for tablename in self.dynamo.list_tables():\\n            self.dynamo.delete_table(tablename)', 'def tearDown(self):\\n        super(TestMisc, self).tearDown()\\n        self.dynamo.default_return_capacity = False', 'def test_connection_region(self):\\n        \"\"\"Connection can access name of connected region\"\"\"\\n        self.assertTrue(isinstance(self.dynamo.region, str))', 'def test_connect_to_region_creds(self):\\n        \"\"\"Can connect to a dynamo region with credentials\"\"\"\\n        conn = DynamoDBConnection.connect(\\n            \"us-west-1\", access_key=\"abc\", secret_key=\"12345\"\\n        )\\n        self.assertIsNotNone(conn.host)', 'def test_retry_on_throughput_error(self, time):\\n        \"\"\"Throughput exceptions trigger a retry of the request\"\"\"\\n\\n        def call(*_, **__):\\n            \"\"\"Dummy service call\"\"\"\\n            response = {\\n                \"ResponseMetadata\": {\\n                    \"HTTPStatusCode\": 400,\\n                },\\n                \"Error\": {\\n                    \"Code\": \"ProvisionedThroughputExceededException\",\\n                    \"Message\": \"Does not matter\",\\n                },\\n            }\\n            raise ClientError(response, \"list_tables\")\\n\\n        with patch.object(self.dynamo, \"client\") as client:\\n            client.list_tables.side_effect = call\\n            with self.assertRaises(ThroughputException):\\n                self.dynamo.call(\"list_tables\")\\n        self.assertEqual(len(time.sleep.mock_calls), self.dynamo.request_retries - 1)\\n        self.assertTrue(time.sleep.called)', 'def test_magic_table_props(self):\\n        \"\"\"Table can look up properties on response object\"\"\"\\n        hash_key = DynamoKey(\"id\")\\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)\\n        ret = self.dynamo.describe_table(\"foobar\")\\n        assert ret is not None\\n        self.assertEqual(ret.item_count, ret[\"ItemCount\"])\\n        with self.assertRaises(KeyError):\\n            self.assertIsNotNone(ret[\"Missing\"])', 'def test_describe_during_delete(self):\\n        \"\"\"Describing a table during a delete operation should not crash\"\"\"\\n        response = {\\n            \"ItemCount\": 0,\\n            \"ProvisionedThroughput\": {\\n                \"NumberOfDecreasesToday\": 0,\\n                \"ReadCapacityUnits\": 5,\\n                \"WriteCapacityUnits\": 5,\\n            },\\n            \"TableName\": \"myTableName\",\\n            \"TableSizeBytes\": 0,\\n            \"TableStatus\": \"DELETING\",\\n        }\\n        table = Table.from_response(response)\\n        self.assertEqual(table.status, \"DELETING\")', 'def test_re_raise_passthrough(self):\\n        \"\"\"DynamoDBError can re-raise itself if missing original exception\"\"\"\\n        err = DynamoDBError(400, Code=\"ErrCode\", Message=\"Ouch\", args={})\\n        caught = False\\n        try:\\n            err.re_raise()\\n        except DynamoDBError as e:\\n            caught = True\\n            self.assertEqual(err, e)\\n        self.assertTrue(caught)', 'def test_default_return_capacity(self):\\n        \"\"\"When default_return_capacity=True, always return capacity\"\"\"\\n        self.dynamo.default_return_capacity = True\\n        with patch.object(self.dynamo, \"call\") as call:\\n            call().get.return_value = None\\n            rs = self.dynamo.scan(\"foobar\")\\n            list(rs)\\n        call.assert_called_with(\\n            \"scan\",\\n            TableName=\"foobar\",\\n            ReturnConsumedCapacity=\"INDEXES\",\\n            ConsistentRead=False,\\n        )', 'def test_limit_complete(self):\\n        \"\"\"A limit with item_capacity = 0 is \\'complete\\'\"\"\"\\n        limit = Limit(item_limit=0)\\n        self.assertTrue(limit.complete)', 'def test_wait_delete_table(self):\\n        \"\"\"Delete table shall wait for the table to go offline.\"\"\"\\n        tablename = \"foobar_wait\"\\n        hash_key = DynamoKey(\"id\")\\n        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)\\n        result = self.dynamo.delete_table(tablename, wait=True)\\n        self.assertTrue(result)', 'def make_table(self):\\n        \"\"\"Convenience method for making a table\"\"\"\\n        hash_key = DynamoKey(\"id\")\\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)', 'def test_int(self):\\n        \"\"\"Store and retrieve an int\"\"\"\\n        self.make_table()\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"num\": 1})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(item[\"num\"], 1)', 'def test_decimal(self):\\n        \"\"\"Store and retrieve a Decimal\"\"\"\\n        self.make_table()\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"num\": Decimal(\"1.1\")})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(item[\"num\"], Decimal(\"1.1\"))', 'def test_binary_bytes(self):\\n        \"\"\"Store and retrieve bytes as a binary\"\"\"\\n        self.make_table()\\n        data = {\"a\": 1, \"b\": 2}\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"data\": Binary(dumps(data))})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(loads(item[\"data\"].value), data)', 'def test_number_set(self):\\n        \"\"\"Store and retrieve a number set\"\"\"\\n        self.make_table()\\n        item = {\\n            \"id\": \"a\",\\n            \"datas\": set([1, 2, 3]),\\n        }\\n        self.dynamo.put_item(\"foobar\", item)\\n        ret = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(ret, item)', 'def test_binary_equal(self):\\n        \"\"\"Binary should eq other Binaries and also raw bytestrings\"\"\"\\n        self.assertEqual(Binary(\"a\"), Binary(\"a\"))\\n        self.assertEqual(Binary(\"a\"), b\"a\")\\n        self.assertFalse(Binary(\"a\") != Binary(\"a\"))', 'def test_binary_converts_unicode(self):\\n        \"\"\"Binary will convert unicode to bytes\"\"\"\\n        b = Binary(\"a\")\\n        self.assertTrue(isinstance(b.value, bytes))', 'def test_bool(self):\\n        \"\"\"Store and retrieve a boolean\"\"\"\\n        self.make_table()\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"b\": True})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(item[\"b\"], True)\\n        self.assertTrue(isinstance(item[\"b\"], bool))', 'def test_dict(self):\\n        \"\"\"Store and retrieve a dict\"\"\"\\n        self.make_table()\\n        data = {\\n            \"i\": 1,\\n            \"s\": \"abc\",\\n            \"n\": None,\\n            \"l\": [\"a\", 1, True],\\n            \"b\": False,\\n        }\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"d\": data})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(item[\"d\"], data)', 'def test_nested_list(self):\\n        \"\"\"Store and retrieve a nested list\"\"\"\\n        self.make_table()\\n        data = [\\n            1,\\n            [\\n                True,\\n                None,\\n                \"abc\",\\n            ],\\n        ]\\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"l\": data})\\n        item = list(self.dynamo.scan(\"foobar\"))[0]\\n        self.assertEqual(item[\"l\"], data)', 'def test_register_encoder(self):\\n        \"\"\"Can register a custom encoder\"\"\"\\n        from datetime import datetime\\n\\n        dynamizer = Dynamizer()\\n        dynamizer.register_encoder(datetime, lambda d, v: (STRING, v.isoformat()))\\n        now = datetime.utcnow()\\n        self.assertEqual(dynamizer.raw_encode(now), (STRING, now.isoformat()))', 'def test_add_dicts_base_case(self):\\n        \"\"\"add_dict where one argument is None returns the other\"\"\"\\n        f = object()\\n        self.assertEqual(add_dicts(f, None), f)\\n        self.assertEqual(add_dicts(None, f), f)', 'def test_count_repr(self):\\n        \"\"\"Count repr\"\"\"\\n        count = Count(0, 0)\\n        self.assertEqual(repr(count), \"Count(0)\")', 'def test_count_subtraction(self):\\n        \"\"\"Count subtraction\"\"\"\\n        count = Count(4, 2)\\n        self.assertEqual(count - 2, 2)', 'def test_count_division(self):\\n        \"\"\"Count division\"\"\"\\n        count = Count(4, 2)\\n        self.assertEqual(count / 2, 2)', 'def test_count_add_capacity(self):\\n        \"\"\"Count addition with consumed_capacity\"\"\"\\n        count = Count(4, 2, Capacity(3, 0))\\n        count2 = Count(5, 3, Capacity(2, 0))\\n        ret = count + count2\\n        self.assertEqual(ret, 9)\\n        self.assertEqual(ret.scanned_count, 5)\\n        self.assertEqual(ret.consumed_capacity.read, 5)', 'def test_capacity_format(self):\\n        \"\"\"String formatting for Capacity\"\"\"\\n        c = Capacity(1, 3)\\n        self.assertEqual(str(c), \"R:1.0 W:3.0\")\\n        c = Capacity(0, 0)\\n        self.assertEqual(str(c), \"0\")', 'def test_consumed_capacity_equality(self):\\n        \"\"\"ConsumedCapacity addition and equality\"\"\"\\n        cap = ConsumedCapacity(\\n            \"foobar\",\\n            Capacity(0, 10),\\n            Capacity(0, 2),\\n            {\\n                \"l-index\": Capacity(0, 4),\\n            },\\n            {\\n                \"g-index\": Capacity(0, 3),\\n            },\\n        )\\n        c2 = ConsumedCapacity(\\n            \"foobar\",\\n            Capacity(0, 10),\\n            Capacity(0, 2),\\n            {\\n                \"l-index\": Capacity(0, 4),\\n                \"l-index2\": Capacity(0, 7),\\n            },\\n        )\\n\\n        self.assertNotEqual(cap, c2)\\n        c3 = ConsumedCapacity(\\n            \"foobar\",\\n            Capacity(0, 10),\\n            Capacity(0, 2),\\n            {\\n                \"l-index\": Capacity(0, 4),\\n            },\\n            {\\n                \"g-index\": Capacity(0, 3),\\n            },\\n        )\\n        self.assertIn(cap, set([c3]))\\n        combined = cap + c2\\n        self.assertEqual(\\n            cap + c2,\\n            ConsumedCapacity(\\n                \"foobar\",\\n                Capacity(0, 20),\\n                Capacity(0, 4),\\n                {\\n                    \"l-index\": Capacity(0, 8),\\n                    \"l-index2\": Capacity(0, 7),\\n                },\\n                {\\n                    \"g-index\": Capacity(0, 3),\\n                },\\n            ),\\n        )\\n        self.assertIn(str(Capacity(0, 3)), str(combined))', 'def test_always_continue_query(self):\\n        \"\"\"Regression test.\\n        If result has no items but does have LastEvaluatedKey, keep querying.\\n        \"\"\"\\n        conn = MagicMock()\\n        conn.dynamizer.decode_keys.side_effect = lambda x: x\\n        items = [\"a\", \"b\"]\\n        results = [\\n            {\"Items\": [], \"LastEvaluatedKey\": {\"foo\": 1, \"bar\": 2}},\\n            {\"Items\": [], \"LastEvaluatedKey\": {\"foo\": 1, \"bar\": 2}},\\n            {\"Items\": items},\\n        ]\\n        conn.call.side_effect = lambda *_, **__: results.pop(0)\\n        rs = ResultSet(conn, Limit())\\n        results = list(rs)\\n        self.assertEqual(results, items)', 'def tearDown(self):\\n        super(TestHooks, self).tearDown()\\n        for hooks in self.dynamo._hooks.values():\\n            while hooks:\\n                hooks.pop()', 'def throw(**_):\\n            \"\"\"Throw an exception to terminate the request\"\"\"\\n            raise Exception()', 'def test_postcall(self):\\n        \"\"\"postcall hooks are called after API call\"\"\"\\n        hash_key = DynamoKey(\"id\")\\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)\\n        calls = []\\n\\n        def hook(*args):\\n            \"\"\"Log the call into a list\"\"\"\\n            calls.append(args)\\n\\n        self.dynamo.subscribe(\"postcall\", hook)\\n        self.dynamo.describe_table(\"foobar\")\\n        self.assertEqual(len(calls), 1)\\n        args = calls[0]\\n        self.assertEqual(len(args), 4)\\n        conn, command, kwargs, response = args\\n        self.assertEqual(conn, self.dynamo)\\n        self.assertEqual(command, \"describe_table\")\\n        self.assertEqual(kwargs[\"TableName\"], \"foobar\")\\n        self.assertEqual(response[\"Table\"][\"TableName\"], \"foobar\")']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, user_id):\\n        self.user_id = user_id\\n        self.from_ts = 0\\n        self.till_ts = 0\\n        self.get_requests = 0\\n        self.reget_requests = 0\\n        self.put_requests = 0\\n        self.get_bytes = 0\\n        self.put_bytes = 0\\n        self.rename_requests = 0\\n        self.del_requests = 0\\n        self.get_dirs = 0\\n        self.put_dirs = 0\\n        self.put_files_per_dir = 0.0\\n        self.get_files_per_dir = 0.0\\n        self.window_seconds = 0\\n\\n        self.file_cnt_gets = Counter()\\n        self.file_cnt_puts = Counter()\\n        self.dir_cnt_gets = Counter()\\n        self.dir_cnt_puts = Counter()\\n\\n        self.num_ops = 0\\n        self.last_ts = 0', 'def finish(self):\\n        self.get_dirs = len(self.dir_cnt_gets)\\n        if self.get_dirs > 0:\\n            self.get_files_per_dir = float(self.get_requests) / self.get_dirs\\n\\n        self.put_dirs = len(self.dir_cnt_puts)\\n        if self.put_dirs > 0:\\n            self.put_files_per_dir = float(self.put_requests) / self.put_dirs\\n\\n        \"\"\"\\n        set reget_counter\\n        :param counter: contains [ 1, 1, 5] counts of objects. value > 1 is a re-retrieval.\\n        :return:\\n        \"\"\"\\n        for c in self.file_cnt_gets.values():\\n            if c > 1:\\n                self.reget_requests += (c - 1)\\n\\n        # self.announce()\\n\\n        return \";\".join([str(x) for x in [\\n        self.user_id,\\n        self.from_ts,\\n        self.till_ts,\\n        self.till_ts - self.from_ts,\\n        self.get_requests,\\n        self.reget_requests,\\n        self.put_requests,\\n        self.get_bytes,\\n        self.put_bytes,\\n        self.rename_requests,\\n        self.del_requests,\\n        self.get_dirs,\\n        self.put_dirs,\\n        self.put_files_per_dir,\\n        self.get_files_per_dir,\\n        self.window_seconds\\n        ]]\\n        )', 'def find_clusters(atimes):\\n    foo = Counter()\\n    bar = dict()\\n    for i in xrange(120, 3660, 10):\\n        clusters = get_clusters(atimes, i)\\n        cs = len(clusters)\\n        foo[cs] += 1\\n\\n        # note first occurance of this cluster size.\\n        if cs not in bar:\\n            bar[cs] = i\\n        # print(len(atimes), i, cs)\\n\\n    return bar[foo.most_common()[0][0]]', 'def analyze_user_session(user_session_file, out_pipeline, target_file_name):\\n    with open(user_session_file, \\'r\\') as sf:\\n        ops = list()\\n        atimes = list()\\n\\n        for line in sf:\\n            op = Operation()\\n            op.init(line.strip())\\n            ops.append(op)\\n            atimes.append(op.ts)\\n\\n        ops.sort(key=operator.attrgetter(\\'ts\\'))\\n        atimes.sort()\\n        window_seconds = find_clusters(atimes)\\n\\n        session_counter = 1\\n\\n        uf = os.path.basename(user_session_file)\\n        user_id = uf[:uf.find(\".user_session.csv\")]\\n\\n        session = UserSession(user_id)\\n        session.window_seconds = window_seconds\\n\\n        for op in ops:\\n            if session.from_ts == 0:\\n                    session.from_ts = op.ts\\n                    session.till_ts = op.ts + op.execution_time\\n\\n            if (session.till_ts + window_seconds) < op.ts:\\n                # this session is over, so archive it.\\n                out_pipeline.write_to(target_file_name, session.finish())\\n                del session\\n                session = UserSession(user_id)\\n                session.window_seconds = window_seconds\\n                session_counter += 1']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _debug(m):\\n        print >> sys.stderr, m', 'def apply_method(self, r, **attr):\\n        \"\"\"\\n            Apply CRUD methods\\n\\n            @param r: the S3Request\\n            @param attr: dictionary of parameters for the method handler\\n\\n            @returns: output object to send to the view\\n\\n            Known means of communicating with this module:\\n\\n            It expects a URL of the form: /prefix/name/import\\n\\n            It will interpret the http requests as follows:\\n\\n            GET     will trigger the upload\\n            POST    will trigger either commits or display the import details\\n            DELETE  will trigger deletes\\n\\n            It will accept one of the following control vars:\\n            item:   to specify a single item in the import job\\n            job:    to specify a job\\n            It should not receive both so job takes precedent over item\\n\\n            For CSV imports, the calling controller can add extra fields\\n            to the upload form to add columns to each row in the CSV. To add\\n            the extra fields, pass a named parameter \"csv_extra_fields\" to the\\n            s3_rest_controller call (or the S3Request call, respectively):\\n\\n            s3_rest_controller(module, resourcename,\\n                               csv_extra_fields=[\\n                                    dict(label=\"ColumnLabelInTheCSV\",\\n                                         field=field_instance)\\n                               ])\\n\\n            The Field instance \"field\" will be added to the upload form, and\\n            the user input will be added to each row of the CSV under the\\n            label as specified. If the \"field\" validator has options, the\\n            input value will be translated into the option representation,\\n            otherwise the value will be used as-is.\\n\\n            Note that the \"label\" in the dict is the column label in the CSV,\\n            whereas the field label for the form is to be set in the Field\\n            instance passed as \"field\".\\n\\n            You can add any arbitrary number of csv_extra_fields to the list.\\n\\n            Additionally, you may want to allow the user to choose whether\\n            the import shall first remove all existing data in the target\\n            table. To do so, pass a label for the \"replace_option\" to the\\n            request:\\n\\n            s3_rest_controller(module, resourcename,\\n                               replace_option=T(\"Remove existing data before import\"))\\n\\n            This will add the respective checkbox to the upload form.\\n\\n            You may also want to provide a link to download a CSV template from\\n            the upload form. To do that, add the resource name to the request\\n            attributes:\\n\\n            s3_rest_controller(module, resourcename,\\n                               csv_template=\"<resourcename>\")\\n\\n            This will provide a link to:\\n                - static/formats/s3csv/<controller>/<resourcename>.csv\\n            at the top of the upload form.\\n\\n        \"\"\"\\n\\n        _debug(\"S3Importer.apply_method(%s)\" % r)\\n\\n        # Messages\\n        T = current.T\\n        messages = self.messages = Messages(T)\\n        messages.download_template = \"Download Template\"\\n        messages.invalid_file_format = \"Invalid File Format\"\\n        messages.unsupported_file_type = \"Unsupported file type of %s\"\\n        messages.stylesheet_not_found = \"No Stylesheet %s could be found to manage the import file.\"\\n        messages.no_file = \"No file submitted\"\\n        messages.file_open_error = \"Unable to open the file %s\"\\n        messages.file_not_found = \"The file to upload is missing\"\\n        messages.no_records_to_import = \"No records to import\"\\n        messages.no_job_to_delete = \"No job to delete, maybe it has already been deleted.\"\\n        messages.title_job_read = \"Details of the selected import job\"\\n        messages.title_job_list = \"List of import items\"\\n        messages.file_uploaded = \"Import file uploaded\"\\n        messages.upload_submit_btn = \"Upload Data File\"\\n        messages.open_btn = \"Open\"\\n        messages.view_btn = \"View\"\\n        messages.delete_btn = \"Delete\"\\n        messages.item_show_details = \"Display Details\"\\n        messages.job_total_records = \"Total records in the Import Job\"\\n        messages.job_records_selected = \"Records selected\"\\n        messages.job_deleted = \"Import job deleted\"\\n        messages.job_completed = \"Job run on %s. With result of (%s)\"\\n        messages.import_file = \"Import File\"\\n        messages.import_file_comment = \"Upload a file formatted according to the Template.\"\\n        messages.user_name = \"User Name\"\\n        messages.commit_total_records_imported = \"%s records imported\"\\n        messages.commit_total_records_ignored = \"%s records ignored\"\\n        messages.commit_total_errors = \"%s records in error\"\\n\\n        try:\\n            self.uploadTitle = current.response.s3.crud_strings[self.tablename].title_upload\\n        except:\\n            self.uploadTitle = T(\"Upload a %s import file\" % r.function)\\n\\n        # @todo: correct to switch this off for the whole session?\\n        current.session.s3.ocr_enabled = False\\n\\n        # Reset all errors/warnings\\n        self.error = None\\n        self.warning = None\\n\\n        # CSV upload configuration\\n        if \"csv_stylesheet\" in attr:\\n            self.csv_stylesheet = attr[\"csv_stylesheet\"]\\n        else:\\n            self.csv_stylesheet = None\\n        self.csv_extra_fields = None\\n        self.csv_extra_data = None\\n\\n        # Environment\\n        self.controller = r.controller\\n        self.function = r.function\\n\\n        # Target table for the data import\\n        self.controller_resource = self.resource\\n        self.controller_table = self.table\\n        self.controller_tablename = self.tablename\\n\\n        # Table for uploads\\n        self.__define_table()\\n        self.upload_resource = None\\n        self.item_resource = None\\n\\n        # XSLT Path\\n        self.xslt_path = os.path.join(r.folder, r.XSLT_PATH)\\n        self.xslt_extension = r.XSLT_EXTENSION\\n\\n        # Check authorization\\n        authorised = self.permit(\"create\", self.upload_tablename) and \\\\\\n                     self.permit(\"create\", self.controller_tablename)\\n        if not authorised:\\n            if r.method is not None:\\n                r.unauthorised()\\n            else:\\n                return dict(form=None)\\n\\n        # @todo: clean this up\\n        source = None\\n        transform = None\\n        upload_id = None\\n        items = None\\n        # @todo get the data from either get_vars or post_vars appropriately\\n        #       for post -> commit_items would need to add the uploadID\\n        if \"transform\" in r.get_vars:\\n            transform = r.get_vars[\"transform\"]\\n        if \"filename\" in r.get_vars:\\n            source = r.get_vars[\"filename\"]\\n        if \"job\" in r.post_vars:\\n            upload_id = r.post_vars[\"job\"]\\n        elif \"job\" in r.get_vars:\\n            upload_id = r.get_vars[\"job\"]\\n        items = self._process_item_list(upload_id, r.vars)\\n        if \"delete\" in r.get_vars:\\n            r.http = \"DELETE\"\\n\\n        # If we have an upload ID, then get upload and import job\\n        self.upload_id = upload_id\\n        query = (self.upload_table.id == upload_id)\\n        self.upload_job = current.db(query).select(limitby=(0, 1)).first()\\n        if self.upload_job:\\n            self.job_id = self.upload_job.job_id\\n        else:\\n            self.job_id = None\\n\\n        # Now branch off to the appropriate controller function\\n        if r.http == \"GET\":\\n            if source != None:\\n                self.commit(source, transform)\\n                output = self.upload(r, **attr)\\n            if upload_id != None:\\n                output = self.display_job(upload_id)\\n            else:\\n                output = self.upload(r, **attr)\\n        elif r.http == \"POST\":\\n            if items != None:\\n                output = self.commit_items(upload_id, items)\\n            else:\\n                output = self.generate_job(r, **attr)\\n        elif r.http == \"DELETE\":\\n            if upload_id != None:\\n                output = self.delete_job(upload_id)\\n        else:\\n            r.error(405, current.manager.ERROR.BAD_METHOD)\\n\\n        return output', 'def upload(self, r, **attr):\\n        \"\"\"\\n            This will display the upload form\\n            It will ask for a file to be uploaded or for a job to be selected.\\n\\n            If a file is uploaded then it will guess at the file type and\\n            ask for the transform file to be used. The transform files will\\n            be in a dataTable with the module specific files shown first and\\n            after those all other known transform files. Once the transform\\n            file is selected the import process can be started which will\\n            generate an importJob, and a \"POST\" method will occur\\n\\n            If a job is selected it will have two actions, open and delete.\\n            Open will mean that a \"GET\" method will occur, with the job details\\n            passed in.\\n            Whilst the delete action will trigger a \"DELETE\" method.\\n        \"\"\"\\n\\n        _debug(\"S3Importer.upload()\")\\n\\n        request = self.request\\n\\n        form = self._upload_form(r, **attr)\\n        output = self._create_upload_dataTable()\\n        if request.representation == \"aadata\":\\n            return output\\n\\n        output.update(form=form, title=self.uploadTitle)\\n        return output', 'def generate_job(self, r, **attr):\\n        \"\"\"\\n            Generate an ImportJob from the submitted upload form\\n        \"\"\"\\n\\n        _debug(\"S3Importer.display()\")\\n\\n        response = current.response\\n        s3 = response.s3\\n\\n        db = current.db\\n        table = self.upload_table\\n\\n        title=self.uploadTitle\\n        form = self._upload_form(r, **attr)\\n\\n        r = self.request\\n        r.read_body()\\n        sfilename = form.vars.file\\n        try:\\n            ofilename = r.post_vars[\"file\"].filename\\n        except:\\n            form.errors.file = self.messages.no_file\\n\\n        if form.errors:\\n            response.flash = \"\"\\n            output = self._create_upload_dataTable()\\n            output.update(form=form, title=title)\\n\\n        elif not sfilename or \\\\\\n             ofilename not in r.files or r.files[ofilename] is None:\\n            response.flash = \"\"\\n            response.error = self.messages.file_not_found\\n            output = self._create_upload_dataTable()\\n            output.update(form=form, title=title)\\n\\n        else:\\n            output = dict()\\n            query = (table.file == sfilename)\\n            db(query).update(controller=self.controller,\\n                             function=self.function,\\n                             filename=ofilename,\\n                             user_id=current.session.auth.user.id)\\n            # must commit here to separate this transaction from\\n            # the trial import phase which will be rolled back.\\n            db.commit()\\n\\n            extension = ofilename.rsplit(\".\", 1).pop()\\n            if extension not in (\"csv\", \"xls\"):\\n                response.flash = None\\n                response.error = self.messages.invalid_file_format\\n                return self.upload(r, **attr)\\n\\n            upload_file = r.files[ofilename]\\n            if extension == \"xls\":\\n                if \"xls_parser\" in s3:\\n                    upload_file.seek(0)\\n                    upload_file = s3.xls_parser(upload_file.read())\\n                    extension = \"csv\"\\n\\n            if upload_file is None:\\n                response.flash = None\\n                response.error = self.messages.file_not_found\\n                return self.upload(r, **attr)\\n            else:\\n                upload_file.seek(0)\\n\\n            row = db(query).select(table.id, limitby=(0, 1)).first()\\n            upload_id = row.id\\n            if \"single_pass\" in r.vars:\\n                single_pass = r.vars[\"single_pass\"]\\n            else:\\n                single_pass = None\\n            self._generate_import_job(upload_id,\\n                                      upload_file,\\n                                      extension,\\n                                      commit_job = single_pass)\\n            if upload_id is None:\\n                row = db(query).update(status = 2) # in error\\n                if self.error != None:\\n                    response.error = self.error\\n                if self.warning != None:\\n                    response.warning = self.warning\\n                response.flash = \"\"\\n                return self.upload(r, **attr)\\n            else:\\n                if single_pass:\\n                    current.session.flash = self.messages.file_uploaded\\n                    # For a single pass retain the vars from the original URL\\n                    next_URL = URL(r=self.request,\\n                                   f=self.function,\\n                                   args=[\"import\"],\\n                                   vars=current.request.get_vars\\n                                  )\\n                    redirect(next_URL)\\n                s3.dataTable_vars = {\"job\" : upload_id}\\n                return self.display_job(upload_id)\\n        return output', 'def display_job(self, upload_id):\\n        \"\"\"\\n            @todo: docstring?\\n        \"\"\"\\n\\n        _debug(\"S3Importer.display_job()\")\\n\\n        request = self.request\\n        response = current.response\\n\\n        db = current.db\\n        table = self.upload_table\\n        job_id = self.job_id\\n        output = dict()\\n        if job_id == None:\\n            # redirect to the start page (removes all vars)\\n            query = (table.id == upload_id)\\n            row = db(query).update(status = 2) # in error\\n            current.session.warning = self.messages.no_records_to_import\\n            redirect(URL(r=request, f=self.function, args=[\"import\"]))\\n\\n        # Get the status of the upload job\\n        query = (table.id == upload_id)\\n        row = db(query).select(table.status,\\n                               table.modified_on,\\n                               table.summary_added,\\n                               table.summary_error,\\n                               table.summary_ignored,\\n                               limitby=(0, 1)).first()\\n        status = row.status\\n        # completed display details\\n        if status == 3: # Completed\\n            # @todo currently this is an unnecessary server call,\\n            #       change for completed records to be a display details\\n            #       and thus avoid the round trip.\\n            #       but keep this code to protect against hand-crafted URLs\\n            #       (and the \\'go back\\' syndrome on the browser)\\n            result = (row.summary_added,\\n                      row.summary_error,\\n                      row.summary_ignored,\\n                     )\\n            self._display_completed_job(result, row.modified_on)\\n            redirect(URL(r=request, f=self.function, args=[\"import\"]))\\n        # otherwise display import items\\n        response.view = self._view(request, \"list.html\")\\n\\n        output = self._create_import_item_dataTable(upload_id, job_id)\\n        if request.representation == \"aadata\":\\n            return output\\n\\n        if response.s3.error_report:\\n            error_report = \"Errors|\" + \"|\".join(response.s3.error_report)\\n            error_tip = A(\"All Errors\",\\n                          _class=\"errortip\",\\n                          _title=error_report)\\n        else:\\n            # @todo: restore the error tree from all items?\\n            error_tip = \"\"\\n\\n        rowcount = len(self._get_all_items(upload_id))\\n        rheader = DIV(TABLE(\\n            TR(\\n                TH(\"%s: \" % self.messages.job_total_records),\\n                TD(rowcount, _id=\"totalAvaliable\"),\\n                TH(\"%s: \" % self.messages.job_records_selected),\\n                TD(0, _id=\"totalSelected\"),\\n                TH(error_tip)\\n              ),\\n        ))\\n\\n        output[\"title\"] = self.messages.title_job_read\\n        output[\"rheader\"] = rheader\\n        output[\"subtitle\"] = self.messages.title_job_list\\n\\n        return output', 'def commit(self, source, transform):\\n        \"\"\"\\n            @todo: docstring?\\n        \"\"\"\\n\\n        _debug(\"S3Importer.commit(%s, %s)\" % (source, transform))\\n\\n        db = current.db\\n        session = current.session\\n        request = self.request\\n\\n        try:\\n            openFile = open(source, \"r\")\\n        except:\\n            session.error = self.messages.file_open_error % source\\n            redirect(URL(r=request, f=self.function))\\n\\n        # @todo: manage different file formats\\n        # @todo: find file format from request.extension\\n        fileFormat = \"csv\"\\n\\n        # insert data in the table and get the ID\\n        try:\\n            user = session.auth.user.id\\n        except:\\n            user = None\\n\\n        upload_id = self.upload_table.insert(controller=self.controller,\\n                                             function=self.function,\\n                                             filename = source,\\n                                             user_id = user,\\n                                             status = 1)\\n        db.commit()\\n\\n        # create the import job\\n        result = self._generate_import_job(upload_id,\\n                                           openFile,\\n                                           fileFormat,\\n                                           stylesheet=transform\\n                                          )\\n        if result == None:\\n            if self.error != None:\\n                if session.error == None:\\n                    session.error = self.error\\n                else:\\n                    session.error += self.error\\n            if self.warning != None:\\n                if session.warning == None:\\n                    session.warning = self.warning\\n                else:\\n                    session.warning += self.warning\\n        else:\\n            items = self._get_all_items(upload_id, True)\\n            # commit the import job\\n            self._commit_import_job(upload_id, items)\\n            result = self._update_upload_job(upload_id)\\n\\n            # get the results and display\\n            msg = \"%s : %s %s %s\" % (source,\\n                                     self.messages.commit_total_records_imported,\\n                                     self.messages.commit_total_errors,\\n                                     self.messages.commit_total_records_ignored)\\n            msg = msg % result\\n\\n            if session.flash == None:\\n                session.flash = msg\\n            else:\\n                session.flash += msg\\n\\n        # @todo: return the upload_id?', 'def commit_items(self, upload_id, items):\\n        \"\"\"\\n            @todo: docstring?\\n        \"\"\"\\n\\n        _debug(\"S3Importer.commit_items(%s, %s)\" % (upload_id, items))\\n        # Save the import items\\n        self._commit_import_job(upload_id, items)\\n        # Update the upload table\\n        # change the status to completed\\n        # record the summary details\\n        # delete the upload file\\n        result = self._update_upload_job(upload_id)\\n        # redirect to the start page (removes all vars)\\n        self._display_completed_job(result)\\n        redirect(URL(r=self.request, f=self.function, args=[\"import\"]))', 'def delete_job(self, upload_id):\\n        \"\"\"\\n            Delete an uploaded file and the corresponding import job\\n\\n            @param upload_id: the upload ID\\n        \"\"\"\\n\\n        _debug(\"S3Importer.delete_job(%s)\" % (upload_id))\\n\\n        db = current.db\\n\\n        request = self.request\\n        resource = request.resource # use self.resource?\\n        response = current.response\\n\\n        # Get the import job ID\\n        job_id = self.job_id\\n\\n        # Delete the import job (if any)\\n        if job_id:\\n            result = resource.import_xml(None,\\n                                         id = None,\\n                                         tree = None,\\n                                         job_id = job_id,\\n                                         delete_job = True)\\n        # @todo: check result\\n\\n        # now delete the upload entry\\n        query = (self.upload_table.id == upload_id)\\n        count = db(query).delete()\\n        # @todo: check that the record has been deleted\\n\\n        # Now commit the changes\\n        db.commit()\\n\\n        result = count\\n\\n        # return to the main import screen\\n        # @todo: check result properly\\n        if result == False:\\n            response.warning = self.messages.no_job_to_delete\\n        else:\\n            response.flash = self.messages.job_deleted\\n\\n        # redirect to the start page (remove all vars)\\n        self.next = self.request.url(vars=dict())\\n        return', 'def _upload_form(self, r, **attr):\\n        \"\"\"\\n            Create and process the upload form, including csv_extra_fields\\n        \"\"\"\\n\\n        EXTRA_FIELDS = \"csv_extra_fields\"\\n        TEMPLATE = \"csv_template\"\\n        REPLACE_OPTION = \"replace_option\"\\n\\n        session = current.session\\n        response = current.response\\n        s3 = response.s3\\n        request = self.request\\n        table = self.upload_table\\n\\n        formstyle = s3.crud.formstyle\\n        response.view = self._view(request, \"list_create.html\")\\n\\n        if REPLACE_OPTION in attr:\\n            replace_option = attr[REPLACE_OPTION]\\n            if replace_option is not None:\\n                table.replace_option.readable = True\\n                table.replace_option.writable = True\\n                table.replace_option.label = replace_option\\n\\n        fields = [f for f in table if f.readable or f.writable and not f.compute]\\n        if EXTRA_FIELDS in attr:\\n            extra_fields = attr[EXTRA_FIELDS]\\n            if extra_fields is not None:\\n                fields.extend([f[\"field\"] for f in extra_fields if \"field\" in f])\\n            self.csv_extra_fields = extra_fields\\n        labels, required = s3_mark_required(fields)\\n        if required:\\n            s3.has_required = True\\n\\n        form = SQLFORM.factory(table_name=self.UPLOAD_TABLE_NAME,\\n                               labels=labels,\\n                               formstyle=formstyle,\\n                               upload = os.path.join(request.folder, \"uploads\", \"imports\"),\\n                               separator = \"\",\\n                               message=self.messages.file_uploaded,\\n                               *fields)\\n\\n        args = [\"s3csv\"]\\n        template = attr.get(TEMPLATE, True)\\n        if template is True:\\n            args.extend([self.controller, \"%s.csv\" % self.function])\\n        elif isinstance(template, basestring):\\n            args.extend([self.controller, \"%s.csv\" % template])\\n        elif isinstance(template, (tuple, list)):\\n            args.extend(template[:-1])\\n            args.append(\"%s.csv\" % template[-1])\\n        else:\\n            template = None\\n        if template is not None:\\n            url = URL(r=request, c=\"static\", f=\"formats\", args=args)\\n            try:\\n                # only add the download link if the template can be opened\\n                open(\"%s/../%s\" % (r.folder, url))\\n                form[0][0].insert(0, TR(TD(A(self.messages.download_template,\\n                                             _href=url)),\\n                                        _id=\"template__row\"))\\n            except:\\n                pass\\n\\n        if form.accepts(r.post_vars, session,\\n                        formname=\"upload_form\"):\\n            upload_id = table.insert(**table._filter_fields(form.vars))\\n            if self.csv_extra_fields:\\n                self.csv_extra_data = Storage()\\n                for f in self.csv_extra_fields:\\n                    label = f.get(\"label\", None)\\n                    if not label:\\n                        continue\\n                    field = f.get(\"field\", None)\\n                    value = f.get(\"value\", None)\\n                    if field:\\n                        if field.name in form.vars:\\n                            data = form.vars[field.name]\\n                        else:\\n                            data = field.default\\n                        value = data\\n                        requires = field.requires\\n                        if not isinstance(requires, (list, tuple)):\\n                            requires = [requires]\\n                        if requires:\\n                            requires = requires[0]\\n                            if isinstance(requires, IS_EMPTY_OR):\\n                                requires = requires.other\\n                            try:\\n                                options = requires.options()\\n                            except:\\n                                pass\\n                            else:\\n                                for k, v in options:\\n                                    if k == str(data):\\n                                        value = v\\n                    elif value is None:\\n                        continue\\n                    self.csv_extra_data[label] = value\\n        s3.no_formats = True\\n        return form', 'def _create_upload_dataTable(self):\\n        \"\"\"\\n            List of previous Import jobs\\n        \"\"\"\\n\\n        db = current.db\\n        request = self.request\\n        controller = self.controller\\n        function = self.function\\n        s3 = current.response.s3\\n\\n        table = self.upload_table\\n        s3.filter = (table.controller == controller) & \\\\\\n                    (table.function == function)\\n        fields = [\"id\",\\n                  \"filename\",\\n                  \"created_on\",\\n                  \"user_id\",\\n                  \"replace_option\",\\n                  \"status\"]\\n\\n        self._use_upload_table()\\n\\n        # Hide the list of prior uploads for now\\n        #output = self._dataTable(fields, sort_by = [[2,\"desc\"]])\\n        output = dict()\\n\\n        self._use_controller_table()\\n\\n        if request.representation == \"aadata\":\\n            return output\\n\\n        query = (table.status != 3) # Status of Pending or in-Error\\n        rows = db(query).select(table.id)\\n        restrictOpen = [str(row.id) for row in rows]\\n        query = (table.status == 3) # Status of Completed\\n        rows = db(query).select(table.id)\\n        restrictView = [str(row.id) for row in rows]\\n\\n        s3.actions = [\\n                    dict(label=str(self.messages.open_btn),\\n                         _class=\"action-btn\",\\n                         url=URL(r=request,\\n                                 c=controller,\\n                                 f=function,\\n                                 args=[\"import\"],\\n                                 vars={\"job\":\"[id]\"}),\\n                         restrict = restrictOpen\\n\\n                         ),\\n                    dict(label=str(self.messages.view_btn),\\n                         _class=\"action-btn\",\\n                         url=URL(r=request,\\n                                 c=controller,\\n                                 f=function,\\n                                 args=[\"import\"],\\n                                 vars={\"job\":\"[id]\"}),\\n                         restrict = restrictView\\n                         ),\\n                    dict(label=str(self.messages.delete_btn),\\n                         _class=\"delete-btn\",\\n                         url=URL(r=request,\\n                                 c=controller,\\n                                 f=function,\\n                                 args=[\"import\"],\\n                                 vars={\"job\":\"[id]\",\\n                                       \"delete\":\"True\"\\n                                      }\\n                                )\\n                         ),\\n                  ]\\n        # Display an Error if no job is attached with this record\\n        query = (table.status == 1) # Pending\\n        rows = db(query).select(table.id)\\n        s3.dataTableStyleAlert = [str(row.id) for row in rows]\\n        query = (table.status == 2) # in error\\n        rows = db(query).select(table.id)\\n        s3.dataTableStyleWarning = [str(row.id) for row in rows]\\n\\n        return output', 'def _create_import_item_dataTable(self, upload_id, job_id):\\n        \"\"\"\\n            @todo: docstring?\\n        \"\"\"\\n\\n        s3 = current.response.s3\\n\\n        represent = {\"element\" : self._item_element_represent}\\n        self._use_import_item_table(job_id)\\n\\n        # Add a filter to the dataTable query\\n        s3.filter = (self.table.job_id == job_id) & \\\\\\n                    (self.table.tablename == self.controller_tablename)\\n\\n        # Get a list of the records that have an error of None\\n        query =  (self.table.job_id == job_id) & \\\\\\n                 (self.table.tablename == self.controller_tablename)\\n        rows = current.db(query).select(self.table.id, self.table.error)\\n        select_list = []\\n        error_list = []\\n        for row in rows:\\n            if row.error:\\n                error_list.append(str(row.id))\\n            else:\\n                select_list.append(\"%s\" % row.id)\\n        select_id = \",\".join(select_list)\\n\\n        output = self._dataTable([\"id\", \"element\", \"error\"],\\n                                 sort_by = [[1, \"asc\"]],\\n                                 represent=represent)\\n\\n        self._use_controller_table()\\n\\n        if self.request.representation == \"aadata\":\\n            return output\\n\\n        # Highlight rows in error in red\\n        s3.dataTableStyleWarning = error_list\\n\\n        s3.dataTableSelectable = True\\n        s3.dataTablePostMethod = True\\n        table = output[\"items\"]\\n        job = INPUT(_type=\"hidden\", _id=\"importUploadID\", _name=\"job\",\\n                    _value=\"%s\" % upload_id)\\n        mode = INPUT(_type=\"hidden\", _id=\"importMode\", _name=\"mode\",\\n                     _value=\"Inclusive\")\\n        # only select the rows with no errors\\n        selected = INPUT(_type=\"hidden\", _id=\"importSelected\",\\n                         _name=\"selected\", _value=\"[%s]\" % select_id)\\n        form = FORM(table, job, mode, selected)\\n        output[\"items\"] = form\\n        s3.dataTableSelectSubmitURL = \"import?job=%s&\" % upload_id\\n        s3.actions = [\\n                        dict(label= str(self.messages.item_show_details),\\n                             _class=\"action-btn\",\\n                             _jqclick=\"$(\\'.importItem.\\'+id).toggle();\",\\n                             ),\\n                      ]\\n        return output', 'def _generate_import_job(self,\\n                             upload_id,\\n                             openFile,\\n                             fileFormat,\\n                             stylesheet=None,\\n                             commit_job=False):\\n        \"\"\"\\n            This will take a s3_import_upload record and\\n            generate the importJob\\n\\n            @param uploadFilename: The name of the uploaded file\\n\\n            @todo: complete parameter descriptions\\n        \"\"\"\\n\\n        _debug(\"S3Importer._generate_import_job(%s, %s, %s, %s)\" % (upload_id,\\n                                                                openFile,\\n                                                                fileFormat,\\n                                                                stylesheet\\n                                                               )\\n              )\\n\\n        db = current.db\\n        request = self.request\\n        resource = request.resource\\n\\n        # ---------------------------------------------------------------------\\n        # CSV\\n        if fileFormat == \"csv\" or fileFormat == \"comma-separated-values\":\\n\\n            fmt = \"csv\"\\n            src = openFile\\n\\n        # ---------------------------------------------------------------------\\n        # XML\\n        # @todo: implement\\n        #elif fileFormat == \"xml\":\\n\\n        # ---------------------------------------------------------------------\\n        # S3JSON\\n        # @todo: implement\\n        #elif fileFormat == \"s3json\":\\n\\n        # ---------------------------------------------------------------------\\n        # PDF\\n        # @todo: implement\\n        #elif fileFormat == \"pdf\":\\n\\n        # ---------------------------------------------------------------------\\n        # Unsupported Format\\n        else:\\n            msg = self.messages.unsupported_file_type % fileFormat\\n            self.error = msg\\n            _debug(msg)\\n            return None\\n\\n        # Get the stylesheet\\n        if stylesheet == None:\\n            stylesheet = self._get_stylesheet()\\n        if stylesheet == None:\\n            return None\\n\\n        # before calling import tree ensure the db.table is the controller_table\\n        self.table = self.controller_table\\n        self.tablename = self.controller_tablename\\n\\n        # Pass stylesheet arguments\\n        args = Storage()\\n        mode = request.get_vars.get(\"xsltmode\", None)\\n        if mode is not None:\\n            args.update(mode=mode)\\n\\n        # Generate the import job\\n        resource.import_xml(src,\\n                            format=fmt,\\n                            extra_data=self.csv_extra_data,\\n                            stylesheet=stylesheet,\\n                            ignore_errors = True,\\n                            commit_job = commit_job,\\n                            **args)\\n\\n        job = resource.job\\n        if job is None:\\n            if resource.error:\\n                # Error\\n                self.error = resource.error\\n                return None\\n            else:\\n                # Nothing to import\\n                self.warning = self.messages.no_records_to_import\\n                return None\\n        else:\\n            # Job created\\n            job_id = job.job_id\\n            errors = current.xml.collect_errors(job)\\n            if errors:\\n                current.response.s3.error_report = errors\\n            query = (self.upload_table.id == upload_id)\\n            result = db(query).update(job_id=job_id)\\n            # @todo: add check that result == 1, if not we are in error\\n            # Now commit the changes\\n            db.commit()\\n\\n        self.job_id = job_id\\n        return True', 'def _get_stylesheet(self, file_format=\"csv\"):\\n        \"\"\"\\n            Get the stylesheet for transformation of the import\\n\\n            @param file_format: the import source file format\\n        \"\"\"\\n\\n        if file_format == \"csv\":\\n            xslt_path = os.path.join(self.xslt_path, \"s3csv\")\\n        else:\\n            xslt_path = os.path.join(self.xslt_path, file_format, \"import.xsl\")\\n            return xslt_path\\n\\n        # Use the \"csv_stylesheet\" parameter to override the CSV stylesheet subpath\\n        # and filename, e.g.\\n        #       s3_rest_controller(module, resourcename,\\n        #                          csv_stylesheet=(\"inv\", \"inv_item.xsl\"))\\n        if self.csv_stylesheet:\\n            if isinstance(self.csv_stylesheet, (tuple, list)):\\n                stylesheet = os.path.join(xslt_path,\\n                                          *self.csv_stylesheet)\\n            else:\\n                stylesheet = os.path.join(xslt_path,\\n                                          self.controller,\\n                                          self.csv_stylesheet)\\n        else:\\n            xslt_filename = \"%s.%s\" % (self.function, self.xslt_extension)\\n            stylesheet = os.path.join(xslt_path,\\n                                      self.controller,\\n                                      xslt_filename)\\n\\n        if os.path.exists(stylesheet) is False:\\n            msg = self.messages.stylesheet_not_found % stylesheet\\n            self.error = msg\\n            _debug(msg)\\n            return None\\n\\n        return stylesheet', 'def _commit_import_job(self, upload_id, items):\\n        \"\"\"\\n            This will save all of the selected import items\\n\\n            @todo: parameter descriptions?\\n        \"\"\"\\n\\n        _debug(\"S3Importer._commit_import_job(%s, %s)\" % (upload_id, items))\\n\\n        db = current.db\\n        resource = self.request.resource\\n\\n        # Load the items from the s3_import_item table\\n        self.importDetails = dict()\\n\\n        table = self.upload_table\\n        query = (table.id == upload_id)\\n        row = db(query).select(table.job_id,\\n                               table.replace_option,\\n                               limitby=(0, 1)).first()\\n        if row is None:\\n            return False\\n        else:\\n            job_id = row.job_id\\n            current.response.s3.import_replace = row.replace_option\\n\\n        itemTable = S3ImportJob.define_item_table()\\n\\n        if itemTable != None:\\n            #****************************************************************\\n            # EXPERIMENTAL\\n            # This doesn\\'t delete related items\\n            # but import_tree will tidy it up later\\n            #****************************************************************\\n            # get all the items selected for import\\n            rows = self._get_all_items(upload_id, as_string=True)\\n\\n            # loop through each row and delete the items not required\\n            self._store_import_details(job_id, \"preDelete\")\\n            for id in rows:\\n                if str(id) not in items:\\n                    # @todo: replace with a helper method from the API\\n                    _debug(\"Deleting item.id = %s\" % id)\\n                    query = (itemTable.id == id)\\n                    db(query).delete()\\n\\n            #****************************************************************\\n            # EXPERIMENTAL\\n            #****************************************************************\\n\\n            # set up the table we will import data into\\n            self.table = self.controller_table\\n            self.tablename = self.controller_tablename\\n\\n            self._store_import_details(job_id, \"preImportTree\")\\n\\n            # Now commit the remaining items\\n            msg = resource.import_xml(None,\\n                                      job_id = job_id,\\n                                      ignore_errors = True)\\n            return resource.error is None', 'def _store_import_details(self, job_id, key):\\n        \"\"\"\\n            This will store the details from an importJob\\n\\n            @todo: parameter descriptions?\\n        \"\"\"\\n\\n        _debug(\"S3Importer._store_import_details(%s, %s)\" % (job_id, key))\\n\\n        itemTable = S3ImportJob.define_item_table()\\n\\n        query = (itemTable.job_id == job_id)  & \\\\\\n                (itemTable.tablename == self.controller_tablename)\\n        rows = current.db(query).select(itemTable.data, itemTable.error)\\n        items = [dict(data=row.data, error=row.error) for row in rows]\\n\\n        self.importDetails[key] = items', 'def _update_upload_job(self, upload_id):\\n        \"\"\"\\n            This will record the results from the import, and change the\\n            status of the upload job\\n\\n            @todo: parameter descriptions?\\n            @todo: report errors in referenced records, too\\n        \"\"\"\\n\\n        _debug(\"S3Importer._update_upload_job(%s)\" % (upload_id))\\n\\n        request = self.request\\n        resource = request.resource\\n        db = current.db\\n\\n        totalPreDelete = len(self.importDetails[\"preDelete\"])\\n        totalPreImport = len(self.importDetails[\"preImportTree\"])\\n        totalIgnored = totalPreDelete - totalPreImport\\n\\n        if resource.error_tree is None:\\n            totalErrors = 0\\n        else:\\n            totalErrors = len(resource.error_tree.findall(\\n                            \"resource[@name=\\'%s\\']\" % resource.tablename))\\n\\n        totalRecords = totalPreImport - totalErrors\\n        if totalRecords < 0:\\n            totalRecords = 0\\n\\n        query = (self.upload_table.id == upload_id)\\n        result = db(query).update(summary_added=totalRecords,\\n                                  summary_error=totalErrors,\\n                                  summary_ignored = totalIgnored,\\n                                  status = 3)\\n\\n        # Now commit the changes\\n        db.commit()\\n        return (totalRecords, totalErrors, totalIgnored)', 'def _display_completed_job(self, totals, timestmp=None):\\n        \"\"\"\\n            Generate a summary flash message for a completed import job\\n\\n            @param totals: the job totals as tuple\\n                           (total imported, total errors, total ignored)\\n            @param timestmp: the timestamp of the completion\\n        \"\"\"\\n\\n        session = current.session\\n\\n        msg = \"%s - %s - %s\" % \\\\\\n              (self.messages.commit_total_records_imported,\\n               self.messages.commit_total_errors,\\n               self.messages.commit_total_records_ignored)\\n        msg = msg % totals\\n\\n        if timestmp != None:\\n            session.flash = self.messages.job_completed % \\\\\\n                            (self.date_represent(timestmp), msg)\\n        elif totals[1] is not 0:\\n            session.error = msg\\n        elif totals[2] is not 0:\\n            session.warning = msg\\n        else:\\n            session.flash = msg', 'def _dataTable(self,\\n                   list_fields = [],\\n                   sort_by = [[1, \"asc\"]],\\n                   represent={},\\n                  ):\\n        \"\"\"\\n            Method to get the data for the dataTable\\n            This can be either a raw html representation or\\n            and ajax call update\\n            Additional data will be cached to limit calls back to the server\\n\\n            @param list_fields: list of field names\\n            @param sort_by: list of sort by columns\\n            @param represent: a dict of field callback functions used\\n                              to change how the data will be displayed\\n\\n            @return: a dict()\\n               In html representations this will be a table of the data\\n               plus the sortby instructions\\n               In ajax this will be a json response\\n\\n               In addition the following values will be made available:\\n               totalRecords         Number of records in the filtered data set\\n               totalDisplayRecords  Number of records to display\\n               start                Start point in the ordered data set\\n               limit                Number of records in the ordered set\\n               NOTE: limit - totalDisplayRecords = total cached\\n        \"\"\"\\n\\n        # ********************************************************************\\n        # Common tasks\\n        # ********************************************************************\\n        db = current.db\\n        session = current.session\\n        request = self.request\\n        response = current.response\\n        resource = self.resource\\n        s3 = response.s3\\n        representation = request.representation\\n        table = self.table\\n        tablename = self.tablename\\n        vars = request.get_vars\\n        output = dict()\\n\\n        # Check permission to read this table\\n        authorised = self.permit(\"read\", tablename)\\n        if not authorised:\\n            request.unauthorised()\\n\\n        # List of fields to select from\\n        # fields is a list of Field objects\\n        # list_field is a string list of field names\\n        if list_fields == []:\\n            fields = resource.readable_fields()\\n        else:\\n            fields = [table[f] for f in list_fields if f in table.fields]\\n        if not fields:\\n            fields = []\\n\\n        # attach any represent callbacks\\n        for f in fields:\\n            if f.name in represent:\\n                f.represent = represent[f.name]\\n\\n        # Make sure that we have the table id as the first column\\n        if fields[0].name != table.fields[0]:\\n            fields.insert(0, table[table.fields[0]])\\n\\n        list_fields = [f.name for f in fields]\\n\\n        # Filter\\n        if s3.filter is not None:\\n            self.resource.add_filter(s3.filter)\\n\\n        # ********************************************************************\\n        # ajax call\\n        # ********************************************************************\\n        if representation == \"aadata\":\\n            start = vars.get(\"iDisplayStart\", None)\\n            limit = vars.get(\"iDisplayLength\", None)\\n            if limit is not None:\\n                try:\\n                    start = int(start)\\n                    limit = int(limit)\\n                except ValueError:\\n                    start = None\\n                    limit = None # use default\\n            else:\\n                start = None # use default\\n            # Using the sort variables sent from dataTables\\n            if vars.iSortingCols:\\n                orderby = self.ssp_orderby(resource, list_fields)\\n\\n            # Echo\\n            sEcho = int(vars.sEcho or 0)\\n\\n            # Get the list\\n            items = resource.sqltable(fields=list_fields,\\n                                      start=start,\\n                                      limit=limit,\\n                                      orderby=orderby,\\n                                      download_url=self.download_url,\\n                                      as_page=True) or []\\n            # Ugly hack to change any occurrence of [id] with the true id\\n            # Needed because the represent doesn\\'t know the id\\n            for i in range(len(items)):\\n                id = items[i][0]\\n                for j in range(len(items[i])):\\n                    new = items[i][j].replace(\"[id]\",id)\\n                    items[i][j] = new\\n            totalrows = self.resource.count()\\n            result = dict(sEcho = sEcho,\\n                          iTotalRecords = totalrows,\\n                          iTotalDisplayRecords = totalrows,\\n                          aaData = items)\\n\\n            output = jsons(result)\\n\\n        # ********************************************************************\\n        # html \\'initial\\' call\\n        # ********************************************************************\\n        else: # catch all\\n            start = 0\\n            limit = 1\\n            # Sort by\\n            vars[\"iSortingCols\"] = len(sort_by)\\n\\n            # generate the dataTables.js variables for sorting\\n            index = 0\\n            for col in sort_by:\\n                colName = \"iSortCol_%s\" % str(index)\\n                colValue = col[0]\\n                dirnName = \"sSortDir_%s\" % str(index)\\n                if len(col) > 1:\\n                    dirnValue = col[1]\\n                else:\\n                    dirnValue = \"asc\"\\n                vars[colName] = colValue\\n                vars[dirnName] = dirnValue\\n            # Now using these sort variables generate the order by statement\\n            orderby = self.ssp_orderby(resource, list_fields)\\n\\n            del vars[\"iSortingCols\"]\\n            for col in sort_by:\\n                del vars[\"iSortCol_%s\" % str(index)]\\n                del vars[\"sSortDir_%s\" % str(index)]\\n\\n            # Get the first row for a quick up load\\n            items = resource.sqltable(fields=list_fields,\\n                                      start=start,\\n                                      limit=1,\\n                                      orderby=orderby,\\n                                      download_url=self.download_url)\\n            totalrows = resource.count()\\n            if items:\\n                if totalrows:\\n                    if s3.dataTable_iDisplayLength:\\n                        limit = 2 * s3.dataTable_iDisplayLength\\n                    else:\\n                        limit = 50\\n                # Add a test on the first call here:\\n                # Now get the limit rows for ajax style update of table\\n                sqltable = resource.sqltable(fields=list_fields,\\n                                             start=start,\\n                                             limit=limit,\\n                                             orderby=orderby,\\n                                             download_url=self.download_url,\\n                                             as_page=True)\\n                aadata = dict(aaData = sqltable or [])\\n                # Ugly hack to change any occurrence of [id] with the true id\\n                # Needed because the represent doesn\\'t know the id\\n                for i in range(len(aadata[\"aaData\"])):\\n                    id = aadata[\"aaData\"][i][0]\\n                    for j in range(len(aadata[\"aaData\"][i])):\\n                        new = aadata[\"aaData\"][i][j].replace(\"[id]\",id)\\n                        aadata[\"aaData\"][i][j] = new\\n\\n                aadata.update(iTotalRecords=totalrows,\\n                              iTotalDisplayRecords=totalrows)\\n                response.aadata = jsons(aadata)\\n                s3.start = 0\\n                s3.limit = limit\\n            else: # No items in database\\n                # s3import tables don\\'t have a delete field but kept for the record\\n                if \"deleted\" in table:\\n                    available_records = db(table.deleted == False)\\n                else:\\n                    available_records = db(table.id > 0)\\n                # check for any records on an unfiltered table\\n                if available_records.select(table.id,\\n                                            limitby=(0, 1)).first():\\n                    items = self.crud_string(tablename, \"msg_no_match\")\\n                else:\\n                    items = self.crud_string(tablename, \"msg_list_empty\")\\n\\n            output.update(items=items, sortby=sort_by)\\n            # Value to be added to the dataTable ajax call\\n            s3.dataTable_Method = \"import\"\\n\\n        return output', 'def _item_element_represent(self, value):\\n        \"\"\"\\n            Represent the element in an import item for dataTable display\\n\\n            @param value: the string containing the element\\n        \"\"\"\\n\\n        T = current.T\\n        db = current.db\\n\\n        value = S3XML.xml_decode(value)\\n        try:\\n            element = etree.fromstring(value)\\n        except:\\n            # XMLSyntaxError: return the element as-is\\n            return DIV(value)\\n\\n        tablename = element.get(\"name\")\\n        table = current.db[tablename]\\n\\n        output = DIV()\\n        details = TABLE(_class=\"importItem [id]\")\\n        header, rows = self._add_item_details(element.findall(\"data\"), table)\\n        if header is not None:\\n            output.append(header)\\n        # Add components, if present\\n        components = element.findall(\"resource\")\\n        for component in components:\\n            ctablename = component.get(\"name\")\\n            ctable = db[ctablename]\\n            self._add_item_details(component.findall(\"data\"), ctable,\\n                                   details=rows, prefix=True)\\n        if rows:\\n            details.append(TBODY(rows))\\n        # Add error messages, if present\\n        errors = current.xml.collect_errors(element)\\n        if errors:\\n            details.append(TFOOT(TR(TH(\"%s:\" % T(\"Errors\")),\\n                                   TD(UL([LI(e) for e in errors])))))\\n        if rows == [] and components == []:\\n            # At this stage we don\\'t have anything to display to see if we can\\n            # find something to show. This could be the case when a table being\\n            # imported is a resolver for a many to many relationship\\n            refdetail = TABLE(_class=\"importItem [id]\")\\n            references = element.findall(\"reference\")\\n            for reference in references:\\n                tuid = reference.get(\"tuid\")\\n                resource = reference.get(\"resource\")\\n                refdetail.append(TR(TD(resource), TD(tuid)))\\n            output.append(refdetail)\\n        else:\\n            output.append(details)\\n        return str(output)', 'def _add_item_details(data, table, details=None, prefix=False):\\n        \"\"\"\\n            Add details of the item element\\n\\n            @param data: the list of data elements in the item element\\n            @param table: the table for the data\\n            @param details: the existing details rows list (to append to)\\n        \"\"\"\\n\\n        tablename = table._tablename\\n        if details is None:\\n            details = []\\n        first = None\\n        firstString = None\\n        header = None\\n        for child in data:\\n            f = child.get(\"field\", None)\\n            if f not in table.fields:\\n                continue\\n            elif f == \"wkt\":\\n                # Skip bulky WKT fields\\n                continue\\n            field = table[f]\\n            ftype = str(field.type)\\n            value = child.get(\"value\", None)\\n            if not value:\\n                value = child.text\\n            try:\\n                value = S3Importer._decode_data(field, value)\\n            except:\\n                pass\\n            if value:\\n                value = S3XML.xml_encode(unicode(value))\\n            else:\\n                value = \"\"\\n            if f != None and value != None:\\n                headerText = P(B(\"%s: \" % f), value)\\n                if not first:\\n                    first = headerText\\n                if ftype == \"string\" and not firstString:\\n                    firstString = headerText\\n                if f == \"name\":\\n                    header = headerText\\n                if prefix:\\n                    details.append(TR(TH(\"%s.%s:\" % (tablename, f)), TD(value)))\\n                else:\\n                    details.append(TR(TH(\"%s:\" % f), TD(value)))\\n        if not header:\\n            if firstString:\\n                header = firstString\\n            else:\\n                header = first\\n        return (header, details)', 'def _decode_data(field, value):\\n        \"\"\"\\n            Try to decode string data into their original type\\n\\n            @param field: the Field instance\\n            @param value: the stringified value\\n\\n            @todo: replace this by ordinary decoder\\n        \"\"\"\\n\\n        if field.type == \"string\" or \\\\\\n            field.type == \"string\" or  \\\\\\n            field.type == \"password\" or \\\\\\n            field.type == \"upload\" or \\\\\\n            field.type == \"text\":\\n            return value\\n        elif field.type == \"integer\" or field.type == \"id\":\\n            return int(value)\\n        elif field.type == \"double\" or field.type == \"decimal\":\\n            return double(value)\\n        elif  field.type == \\'boolean\\':\\n            if value and not str(value)[:1].upper() in [\"F\", \"0\"]:\\n                return \"T\"\\n            else:\\n                return \"F\"\\n        elif field.type == \"date\":\\n            return value # @todo fix this to get a date\\n        elif field.type == \"time\":\\n            return value # @todo fix this to get a time\\n        elif field.type == \"datetime\":\\n            return value # @todo fix this to get a datetime\\n        else:\\n            return value', 'def date_represent(date_obj):\\n        \"\"\"\\n            Represent a datetime object as string\\n\\n            @param date_obj: the datetime object\\n\\n            @todo: replace by S3DateTime method?\\n        \"\"\"\\n        return date_obj.strftime(\"%d %B %Y, %I:%M%p\")', 'def _process_item_list(self, upload_id, vars):\\n        \"\"\"\\n            Get the list of IDs for the selected items from the \"mode\"\\n            and \"selected\" request variables\\n\\n            @param upload_id: the upload_id\\n            @param vars: the request variables\\n        \"\"\"\\n\\n        items = None\\n        if \"mode\" in vars:\\n            mode = vars[\"mode\"]\\n            if \"selected\" in vars:\\n                selected = vars[\"selected\"].split(\",\")\\n            else:\\n                selected = []\\n            if mode == \"Inclusive\":\\n                items = selected\\n            elif mode == \"Exclusive\":\\n                all_items = self._get_all_items(upload_id, as_string=True)\\n                items = [i for i in all_items if i not in selected]\\n        return items', 'def _get_all_items(self, upload_id, as_string=False):\\n        \"\"\" Get a list of the record IDs of all import items for\\n            the the given upload ID\\n\\n            @param upload_id: the upload ID\\n            @param as_string: represent each ID as string\\n        \"\"\"\\n\\n        item_table = S3ImportJob.define_item_table()\\n        upload_table = self.upload_table\\n\\n        query = (upload_table.id == upload_id) & \\\\\\n                (item_table.job_id == upload_table.job_id) & \\\\\\n                (item_table.tablename == self.controller_tablename)\\n\\n        rows = current.db(query).select(item_table.id)\\n        if as_string:\\n            items = [str(row.id) for row in rows]\\n        else:\\n            items = [row.id for row in rows]\\n\\n        return items', 'def _use_upload_table(self):\\n        \"\"\"\\n            Set the resource and the table to being s3_import_upload\\n        \"\"\"\\n\\n        if self.upload_resource == None:\\n            from s3resource import S3Resource\\n            (prefix, name) = self.UPLOAD_TABLE_NAME.split(\"_\",1)\\n            self.upload_resource = S3Resource(prefix, name)\\n        self.resource = self.upload_resource\\n        self.table = self.upload_table\\n        self.tablename = self.upload_tablename', 'def _use_controller_table(self):\\n        \"\"\"\\n            Set the resource and the table to be the imported resource\\n        \"\"\"\\n\\n        self.resource = self.controller_resource\\n        self.table = self.controller_table\\n        self.tablename = self.controller_tablename', 'def _use_import_item_table(self, job_id):\\n        \"\"\"\\n            Set the resource and the table to being s3_import_item \\n        \"\"\"\\n\\n        if self.item_resource == None:\\n            from s3resource import S3Resource\\n            (prefix, name) = S3ImportJob.ITEM_TABLE_NAME.split(\"_\",1)\\n            self.item_resource = S3Resource(prefix, name)\\n        self.resource = self.item_resource\\n        self.tablename = S3ImportJob.ITEM_TABLE_NAME\\n        self.table = S3ImportJob.define_item_table()', 'def __define_table(self):\\n        \"\"\" Configures the upload table \"\"\"\\n\\n        _debug(\"S3Importer.__define_table()\")\\n\\n        T = current.T\\n        db = current.db\\n        request = current.request\\n\\n        self.upload_tablename = self.UPLOAD_TABLE_NAME\\n\\n        import_upload_status = {\\n            1: T(\"Pending\"),\\n            2: T(\"In error\"),\\n            3: T(\"Completed\"),\\n        }\\n\\n        def user_name_represent(id):\\n            # @todo: use s3_present_user?\\n\\n            rep_str = \"-\"\\n            table = db.auth_user\\n            query = (table.id == id)\\n            row = db(query).select(table.first_name,\\n                                   table.last_name,\\n                                   limitby=(0, 1)).first()\\n            if row:\\n                rep_str = \"%s %s\" % (row.first_name, row.last_name)\\n            return rep_str\\n\\n        def status_represent(index):\\n            if index == None:\\n                return \"Unknown\" # @todo: use messages (internationalize)\\n            else:\\n                return import_upload_status[index]\\n\\n        now = request.utcnow\\n        table = self.define_upload_table()\\n        table.file.upload_folder = os.path.join(request.folder,\\n                                                \"uploads\",\\n                                                #\"imports\"\\n                                                )\\n        table.file.comment = DIV(_class=\"tooltip\",\\n                                 _title=\"%s|%s\" %\\n                                    (self.messages.import_file,\\n                                     self.messages.import_file_comment))\\n        table.file.label = self.messages.import_file\\n        table.status.requires = IS_IN_SET(import_upload_status, zero=None)\\n        table.status.represent = status_represent\\n        table.user_id.label = self.messages.user_name\\n        table.user_id.represent = user_name_represent\\n        table.created_on.default = now\\n        table.created_on.represent = self.date_represent\\n        table.modified_on.default = now\\n        table.modified_on.update = now\\n        table.modified_on.represent = self.date_represent\\n\\n        table.replace_option.label = T(\"Replace\")\\n\\n        self.upload_table = db[self.UPLOAD_TABLE_NAME]', 'def define_upload_table(cls):\\n        \"\"\" Defines the upload table \"\"\"\\n\\n        db = current.db\\n        uploadfolder = os.path.join(current.request.folder,\\n                                    \"uploads\",\\n                                    )\\n        if cls.UPLOAD_TABLE_NAME not in db:\\n            upload_table = db.define_table(cls.UPLOAD_TABLE_NAME,\\n                    Field(\"controller\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"function\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"file\", \"upload\",\\n                          uploadfolder=os.path.join(current.request.folder, \"uploads\", \"imports\"),\\n                          autodelete=True),\\n                    Field(\"filename\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"status\", \"integer\",\\n                          default=1,\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"extra_data\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"replace_option\", \"boolean\",\\n                          default=False,\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"job_id\",\\n                          length=128,\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"user_id\", \"integer\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"created_on\", \"datetime\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"modified_on\", \"datetime\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"summary_added\", \"integer\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"summary_error\", \"integer\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"summary_ignored\", \"integer\",\\n                          readable=False,\\n                          writable=False),\\n                    Field(\"completed_details\", \"text\",\\n                          readable=False,\\n                          writable=False))\\n        else:\\n            upload_table = db[cls.UPLOAD_TABLE_NAME]\\n\\n        return upload_table', 'def __init__(self, job):\\n        \"\"\"\\n            Constructor\\n\\n            @param job: the import job this item belongs to\\n        \"\"\"\\n\\n        self.job = job\\n        self.ERROR = current.manager.ERROR\\n\\n        # Locking and error handling\\n        self.lock = False\\n        self.error = None\\n\\n        # Identification\\n        import uuid\\n        self.item_id = uuid.uuid4() # unique ID for this item\\n        self.id = None\\n        self.uid = None\\n\\n        # Data elements\\n        self.table = None\\n        self.tablename = None\\n        self.element = None\\n        self.data = None\\n        self.original = None\\n        self.components = []\\n        self.references = []\\n        self.load_components = []\\n        self.load_references = []\\n        self.parent = None\\n        self.skip = False\\n\\n        # Conflict handling\\n        self.mci = 2\\n        self.mtime = datetime.utcnow()\\n        self.modified = True\\n        self.conflict = False\\n\\n        # Allowed import methods\\n        self.strategy = job.strategy\\n        # Update and conflict resolution policies\\n        self.update_policy = job.update_policy\\n        self.conflict_policy = job.conflict_policy\\n\\n        # Actual import method\\n        self.method = None\\n\\n        self.onvalidation = None\\n        self.onaccept = None\\n\\n        # Item import status flags\\n        self.accepted = None\\n        self.permitted = False\\n        self.committed = False\\n\\n        # Writeback hook for circular references:\\n        # Items which need a second write to update references\\n        self.update = []', 'def __repr__(self):\\n        \"\"\" Helper method for debugging \"\"\"\\n\\n        _str = \"<S3ImportItem %s {item_id=%s uid=%s id=%s error=%s data=%s}>\" % \\\\\\n               (self.table, self.item_id, self.uid, self.id, self.error, self.data)\\n        return _str', 'def parse(self,\\n              element,\\n              original=None,\\n              table=None,\\n              tree=None,\\n              files=None):\\n        \"\"\"\\n            Read data from a <resource> element\\n\\n            @param element: the element\\n            @param table: the DB table\\n            @param tree: the import tree\\n            @param files: uploaded files\\n\\n            @returns: True if successful, False if not (sets self.error)\\n        \"\"\"\\n\\n        db = current.db\\n        xml = current.xml\\n        manager = current.manager\\n        validate = manager.validate\\n        s3db = current.s3db\\n\\n        self.element = element\\n        if table is None:\\n            tablename = element.get(xml.ATTRIBUTE.name, None)\\n            try:\\n                table = s3db[tablename]\\n            except:\\n                self.error = self.ERROR.BAD_RESOURCE\\n                element.set(xml.ATTRIBUTE.error, self.error)\\n                return False\\n\\n        self.table = table\\n        self.tablename = table._tablename\\n\\n        if original is None:\\n            original = manager.original(table, element)\\n        data = xml.record(table, element,\\n                          files=files,\\n                          original=original,\\n                          validate=validate)\\n\\n        if data is None:\\n            self.error = self.ERROR.VALIDATION_ERROR\\n            self.accepted = False\\n            if not element.get(xml.ATTRIBUTE.error, False):\\n                element.set(xml.ATTRIBUTE.error, str(self.error))\\n            return False\\n\\n        self.data = data\\n\\n        if original is not None:\\n            self.original = original\\n            self.id = original[table._id.name]\\n            if xml.UID in original:\\n                self.uid = original[xml.UID]\\n                self.data.update({xml.UID:self.uid})\\n        elif xml.UID in data:\\n            self.uid = data[xml.UID]\\n        if xml.MTIME in data:\\n            self.mtime = data[xml.MTIME]\\n        if xml.MCI in data:\\n            self.mci = data[xml.MCI]\\n\\n        _debug(\"New item: %s\" % self)\\n        return True', 'def deduplicate(self):\\n\\n        RESOLVER = \"deduplicate\"\\n\\n        if self.id:\\n            return\\n\\n        table = self.table\\n\\n        if table is None:\\n            return\\n        if self.original is not None:\\n            original = self.original\\n        else:\\n            original = current.manager.original(table, self.data)\\n\\n        if original is not None:\\n            self.original = original\\n            self.id = original[table._id.name]\\n            UID = current.xml.UID\\n            if UID in original:\\n                self.uid = original[UID]\\n                self.data.update({UID:self.uid})\\n            self.method = self.METHOD.UPDATE\\n        else:\\n            resolve = current.s3db.get_config(self.tablename, RESOLVER)\\n            if self.data and resolve:\\n                resolve(self)\\n\\n        return', 'def authorize(self):\\n        \"\"\"\\n            Authorize the import of this item, sets self.permitted\\n        \"\"\"\\n\\n        db = current.db\\n        manager = current.manager\\n        authorize = manager.permit\\n\\n        self.permitted = False\\n\\n        if not self.table:\\n            return False\\n\\n        prefix = self.tablename.split(\"_\", 1)[0]\\n        if prefix in manager.PROTECTED:\\n            return False\\n\\n        if not authorize:\\n            self.permitted = True\\n\\n        self.method = self.METHOD.CREATE\\n        if self.id:\\n\\n            if self.data.deleted is True:\\n                self.method = self.METHOD.DELETE\\n                self.accepted = True\\n\\n            else:\\n                if not self.original:\\n                    query = (self.table.id == self.id)\\n                    self.original = db(query).select(limitby=(0, 1)).first()\\n                if self.original:\\n                    self.method = self.METHOD.UPDATE\\n\\n        if self.method == self.METHOD.CREATE:\\n            self.id = 0\\n\\n        if authorize:\\n            self.permitted = authorize(self.method,\\n                                       self.tablename,\\n                                       record_id=self.id)\\n\\n        return self.permitted', 'def validate(self):\\n        \"\"\"\\n            Validate this item (=record onvalidation), sets self.accepted\\n        \"\"\"\\n\\n        if self.accepted is not None:\\n            return self.accepted\\n        if self.data is None or not self.table:\\n            self.accepted = False\\n            return False\\n\\n        form = Storage()\\n        form.method = self.method\\n        form.vars = self.data\\n        if self.id:\\n            form.vars.id = self.id\\n        form.errors = Storage()\\n        tablename = self.tablename\\n        key = \"%s_onvalidation\" % self.method\\n        s3db = current.s3db\\n        onvalidation = s3db.get_config(tablename, key,\\n                       s3db.get_config(tablename, \"onvalidation\"))\\n        if onvalidation:\\n            try:\\n                callback(onvalidation, form, tablename=tablename)\\n            except:\\n                pass # @todo need a better handler here.\\n        self.accepted = True\\n        if form.errors:\\n            error = current.xml.ATTRIBUTE.error\\n            for k in form.errors:\\n                e = self.element.findall(\"data[@field=\\'%s\\']\" % k)\\n                if not e:\\n                    e = self.element.findall(\"reference[@field=\\'%s\\']\" % k)\\n                if not e:\\n                    e = self.element\\n                    form.errors[k] = \"[%s] %s\" % (k, form.errors[k])\\n                else:\\n                    e = e[0]\\n                e.set(error,\\n                      str(form.errors[k]).decode(\"utf-8\"))\\n            self.error = self.ERROR.VALIDATION_ERROR\\n            self.accepted = False\\n        return self.accepted', 'def commit(self, ignore_errors=False):\\n        \"\"\"\\n            Commit this item to the database\\n\\n            @param ignore_errors: skip invalid components\\n                                  (still reports errors)\\n        \"\"\"\\n\\n        db = current.db\\n        s3db = current.s3db\\n        xml = current.xml\\n        manager = current.manager\\n        table = self.table\\n\\n        # Check if already committed\\n        if self.committed:\\n            # already committed\\n            return True\\n\\n        # If the parent item gets skipped, then skip this item as well\\n        if self.parent is not None and self.parent.skip:\\n            return True\\n\\n        _debug(\"Committing item %s\" % self)\\n\\n        # Resolve references\\n        self._resolve_references()\\n\\n        # Validate\\n        if not self.validate():\\n            _debug(\"Validation error: %s (%s)\" % (self.error, xml.tostring(self.element, pretty_print=True)))\\n            self.skip = True\\n            return ignore_errors\\n\\n        elif self.components:\\n            for component in self.components:\\n                if not component.validate():\\n                    if hasattr(component, \"tablename\"):\\n                        tn = component.tablename\\n                    else:\\n                        tn = None\\n                    _debug(\"Validation error, component=%s\" % tn)\\n                    component.skip = True\\n                    # Skip this item on any component validation errors\\n                    # unless ignore_errors is True\\n                    if ignore_errors:\\n                        continue\\n                    else:\\n                        self.skip = True\\n                        return False\\n\\n        # De-duplicate\\n        self.deduplicate()\\n\\n        # Log this item\\n        if manager.log is not None:\\n            manager.log(self)\\n\\n        # Authorize item\\n        if not self.authorize():\\n            _debug(\"Not authorized - skip\")\\n            self.error = manager.ERROR.NOT_PERMITTED\\n            self.skip = True\\n            return ignore_errors\\n\\n        _debug(\"Method: %s\" % self.method)\\n\\n        # Check if import method is allowed in strategy\\n        if not isinstance(self.strategy, (list, tuple)):\\n            self.strategy = [self.strategy]\\n        if self.method not in self.strategy:\\n            _debug(\"Method not in strategy - skip\")\\n            self.error = manager.ERROR.NOT_PERMITTED\\n            self.skip = True\\n            return True\\n\\n        this = self.original\\n        if not this and self.id and \\\\\\n           self.method in (self.METHOD.UPDATE, self.METHOD.DELETE):\\n            query = (table.id == self.id)\\n            this = db(query).select(limitby=(0, 1)).first()\\n        this_mtime = None\\n        this_mci = 0\\n        if this:\\n            if xml.MTIME in table.fields:\\n                this_mtime = xml.as_utc(this[xml.MTIME])\\n            if xml.MCI in table.fields:\\n                this_mci = this[xml.MCI]\\n        self.mtime = xml.as_utc(self.mtime)\\n\\n        # Conflict detection\\n        this_modified = True\\n        self.modified = True\\n        self.conflict = False\\n        last_sync = xml.as_utc(self.job.last_sync)\\n        if last_sync:\\n            if this_mtime and this_mtime < last_sync:\\n                this_modified = False\\n            if self.mtime and self.mtime < last_sync:\\n                self.modified = False\\n            if self.modified and this_modified:\\n                self.conflict = True\\n\\n        if self.conflict and \\\\\\n           self.method in (self.METHOD.UPDATE, self.METHOD.DELETE):\\n            _debug(\"Conflict: %s\" % self)\\n            if self.job.onconflict:\\n                self.job.onconflict(self)\\n\\n        if self.data is not None:\\n            data = Storage(self.data)\\n        else:\\n            data = Storage()\\n\\n        # Update existing record\\n        if self.method == self.METHOD.UPDATE:\\n\\n            if this:\\n                if \"deleted\" in this and this.deleted:\\n                    policy = self._get_update_policy(None)\\n                    if policy == self.POLICY.NEWER and \\\\\\n                       this_mtime and this_mtime > self.mtime or \\\\\\n                       policy == self.POLICY.MASTER and \\\\\\n                       (this_mci == 0 or self.mci != 1):\\n                        self.skip = True\\n                        return True\\n                fields = data.keys()\\n                for f in fields:\\n                    if f not in this:\\n                        continue\\n                    if isinstance(this[f], datetime):\\n                        if xml.as_utc(data[f]) == xml.as_utc(this[f]):\\n                            del data[f]\\n                            continue\\n                    else:\\n                        if data[f] == this[f]:\\n                            del data[f]\\n                            continue\\n                    remove = False\\n                    policy = self._get_update_policy(f)\\n                    if policy == self.POLICY.THIS:\\n                        remove = True\\n                    elif policy == self.POLICY.NEWER:\\n                        if this_mtime and this_mtime > self.mtime:\\n                            remove = True\\n                    elif policy == self.POLICY.MASTER:\\n                        if this_mci == 0 or self.mci != 1:\\n                            remove = True\\n                    if remove:\\n                        del data[f]\\n                        self.data.update({f:this[f]})\\n                if \"deleted\" in this and this.deleted:\\n                    # Undelete re-imported records:\\n                    data.update(deleted=False)\\n                    if \"deleted_fk\" in table:\\n                        data.update(deleted_fk=\"\")\\n                    if \"created_by\" in table:\\n                        data.update(created_by=table.created_by.default)\\n                    if \"modified_by\" in table:\\n                        data.update(modified_by=table.modified_by.default)\\n\\n            if not self.skip and not self.conflict and \\\\\\n               (len(data) or self.components or self.references):\\n                if self.uid and xml.UID in table:\\n                    data.update({xml.UID:self.uid})\\n                if xml.MTIME in table:\\n                    data.update({xml.MTIME: self.mtime})\\n                if xml.MCI in data:\\n                    # retain local MCI on updates\\n                    del data[xml.MCI]\\n                query = (table._id == self.id)\\n                try:\\n                    success = db(query).update(**dict(data))\\n                except:\\n                    self.error = sys.exc_info()[1]\\n                    self.skip = True\\n                    return False\\n                if success:\\n                    self.committed = True\\n            else:\\n                # Nothing to update\\n                self.committed = True\\n\\n        # Create new record\\n        elif self.method == self.METHOD.CREATE:\\n\\n            # Do not apply field policy to UID and MCI\\n            UID = xml.UID\\n            if UID in data:\\n                del data[UID]\\n            MCI = xml.MCI\\n            if MCI in data:\\n                del data[MCI]\\n\\n            for f in data:\\n                policy = self._get_update_policy(f)\\n                if policy == self.POLICY.MASTER and self.mci != 1:\\n                    del data[f]\\n\\n            if len(data) or self.components or self.references:\\n\\n                # Restore UID and MCI\\n                if self.uid and UID in table.fields:\\n                    data.update({UID:self.uid})\\n                if MCI in table.fields:\\n                    data.update({MCI:self.mci})\\n\\n                # Insert the new record\\n                try:\\n                    success = table.insert(**dict(data))\\n                except:\\n                    self.error = sys.exc_info()[1]\\n                    self.skip = True\\n                    return False\\n                if success:\\n                    self.id = success\\n                    self.committed = True\\n\\n            else:\\n                # Nothing to create\\n                self.skip = True\\n                return True\\n\\n        # Delete local record\\n        elif self.method == self.METHOD.DELETE:\\n\\n            if this:\\n                if this.deleted:\\n                    self.skip = True\\n                policy = self._get_update_policy(None)\\n                if policy == self.POLICY.THIS:\\n                    self.skip = True\\n                elif policy == self.POLICY.NEWER and \\\\\\n                     (this_mtime and this_mtime > self.mtime):\\n                    self.skip = True\\n                elif policy == self.POLICY.MASTER and \\\\\\n                     (this_mci == 0 or self.mci != 1):\\n                    self.skip = True\\n            else:\\n                self.skip = True\\n\\n            if not self.skip and not self.conflict:\\n\\n                prefix, name = self.tablename.split(\"_\", 1)\\n                resource = manager.define_resource(prefix, name, id=self.id)\\n\\n                ondelete = s3db.get_config(self.tablename, \"ondelete\")\\n                success = resource.delete(ondelete=ondelete,\\n                                          cascade=True)\\n                if resource.error:\\n                    self.error = resource.error\\n                    self.skip = True\\n                    return ignore_errors\\n\\n            _debug(\"Success: %s, id=%s %sd\" % (self.tablename, self.id,\\n                                               self.skip and \"skippe\" or \\\\\\n                                               self.method))\\n            return True\\n\\n        # Audit + onaccept on successful commits\\n        if self.committed:\\n            form = Storage()\\n            form.method = self.method\\n            form.vars = self.data\\n            tablename = self.tablename\\n            prefix, name = tablename.split(\"_\", 1)\\n            if self.id:\\n                form.vars.id = self.id\\n            if manager.audit is not None:\\n                manager.audit(self.method, prefix, name,\\n                              form=form,\\n                              record=self.id,\\n                              representation=\"xml\")\\n            s3db.update_super(table, form.vars)\\n            if self.method == self.METHOD.CREATE:\\n                current.auth.s3_set_record_owner(table, self.id)\\n            key = \"%s_onaccept\" % self.method\\n            onaccept = s3db.get_config(tablename, key,\\n                       s3db.get_config(tablename, \"onaccept\"))\\n            if onaccept:\\n                callback(onaccept, form, tablename=self.tablename)\\n\\n        # Update referencing items\\n        if self.update and self.id:\\n            for u in self.update:\\n                item = u.get(\"item\", None)\\n                if not item:\\n                    continue\\n                field = u.get(\"field\", None)\\n                if isinstance(field, (list, tuple)):\\n                    pkey, fkey = field\\n                    query = table.id == self.id\\n                    row = db(query).select(table[pkey],\\n                                           limitby=(0, 1)).first()\\n                    if row:\\n                        item._update_reference(fkey, row[pkey])\\n                else:\\n                    item._update_reference(field, self.id)\\n\\n        _debug(\"Success: %s, id=%s %sd\" % (self.tablename, self.id,\\n                                           self.skip and \"skippe\" or \\\\\\n                                           self.method))\\n        return True', 'def _get_update_policy(self, field):\\n        \"\"\"\\n            Get the update policy for a field (if the item will\\n            update an existing record)\\n\\n            @param field: the name of the field\\n        \"\"\"\\n\\n        if isinstance(self.update_policy, dict):\\n            r = self.update_policy.get(field,\\n                self.update_policy.get(\"__default__\", self.POLICY.THIS))\\n        else:\\n            r = self.update_policy\\n        if not r in self.POLICY.values():\\n            r = self.POLICY.THIS\\n        return r', 'def _resolve_references(self):\\n        \"\"\"\\n            Resolve the references of this item (=look up all foreign\\n            keys from other items of the same job). If a foreign key\\n            is not yet available, it will be scheduled for later update.\\n        \"\"\"\\n\\n        if not self.table:\\n            return\\n\\n        items = self.job.items\\n        for reference in self.references:\\n\\n            item = None\\n            field = reference.field\\n            entry = reference.entry\\n            if not entry:\\n                continue\\n\\n            # Resolve key tuples\\n            if isinstance(field, (list,tuple)):\\n                pkey, fkey = field\\n            else:\\n                pkey, fkey = (\"id\", field)\\n\\n            # Resolve the key table name\\n            ktablename, key, multiple = s3_get_foreign_key(self.table[fkey])\\n            if not ktablename:\\n                if self.tablename == \"auth_user\" and \\\\\\n                   fkey == \"organisation_id\":\\n                    ktablename = \"org_organisation\"\\n                else:\\n                    continue\\n            if entry.tablename:\\n                ktablename = entry.tablename\\n            try:\\n                ktable = current.s3db[ktablename]\\n            except:\\n                continue\\n\\n            # Resolve the foreign key (value)\\n            fk = entry.id\\n            if entry.item_id:\\n                item = items[entry.item_id]\\n                if item:\\n                    fk = item.id\\n            if fk and pkey != \"id\":\\n                row = current.db(ktable._id == fk).select(ktable[pkey],\\n                                                          limitby=(0, 1)).first()\\n                if not row:\\n                    fk = None\\n                    continue\\n                else:\\n                    fk = row[pkey]\\n\\n            # Update record data\\n            if fk:\\n                if multiple:\\n                    val = self.data.get(fkey, [])\\n                    if fk not in val:\\n                        val.append(fk)\\n                    self.data[fkey] = val\\n                else:\\n                    self.data[fkey] = fk\\n            else:\\n                if fkey in self.data and not multiple:\\n                    del self.data[fkey]\\n                if item:\\n                    item.update.append(dict(item=self, field=fkey))', 'def _update_reference(self, field, value):\\n        \"\"\"\\n            Helper method to update a foreign key in an already written\\n            record. Will be called by the referenced item after (and only\\n            if) it has been committed. This is only needed if the reference\\n            could not be resolved before commit due to circular references.\\n\\n            @param field: the field name of the foreign key\\n            @param value: the value of the foreign key\\n        \"\"\"\\n\\n        if not value or not self.table:\\n            return\\n        db = current.db\\n        if self.id and self.permitted:\\n            fieldtype = str(self.table[field].type)\\n            if fieldtype.startswith(\"list:reference\"):\\n                query = (self.table.id == self.id)\\n                record = db(query).select(self.table[field],\\n                                          limitby=(0,1)).first()\\n                if record:\\n                    values = record[field]\\n                    if value not in values:\\n                        values.append(value)\\n                        db(self.table.id == self.id).update(**{field:values})\\n            else:\\n                db(self.table.id == self.id).update(**{field:value})', 'def store(self, item_table=None):\\n        \"\"\"\\n            Store this item in the DB\\n        \"\"\"\\n\\n        _debug(\"Storing item %s\" % self)\\n        if item_table is None:\\n            return None\\n        db = current.db\\n        query = item_table.item_id == self.item_id\\n        row = db(query).select(item_table.id, limitby=(0, 1)).first()\\n        if row:\\n            record_id = row.id\\n        else:\\n            record_id = None\\n        record = Storage(job_id = self.job.job_id,\\n                         item_id = self.item_id,\\n                         tablename = self.tablename,\\n                         record_uid = self.uid,\\n                         error = self.error)\\n        if self.element is not None:\\n            element_str = current.xml.tostring(self.element,\\n                                               xml_declaration=False)\\n            record.update(element=element_str)\\n        if self.data is not None:\\n            data = Storage()\\n            for f in self.data.keys():\\n                table = self.table\\n                if f not in table.fields:\\n                    continue\\n                fieldtype = str(self.table[f].type)\\n                if fieldtype == \"id\" or s3_has_foreign_key(self.table[f]):\\n                    continue\\n                data.update({f:self.data[f]})\\n            data_str = cPickle.dumps(data)\\n            record.update(data=data_str)\\n        ritems = []\\n        for reference in self.references:\\n            field = reference.field\\n            entry = reference.entry\\n            store_entry = None\\n            if entry:\\n                if entry.item_id is not None:\\n                    store_entry = dict(field=field,\\n                                       item_id=str(entry.item_id))\\n                elif entry.uid is not None:\\n                    store_entry = dict(field=field,\\n                                       tablename=entry.tablename,\\n                                       uid=str(entry.uid))\\n                if store_entry is not None:\\n                    ritems.append(json.dumps(store_entry))\\n        if ritems:\\n            record.update(ritems=ritems)\\n        citems = [c.item_id for c in self.components]\\n        if citems:\\n            record.update(citems=citems)\\n        if self.parent:\\n            record.update(parent=self.parent.item_id)\\n        if record_id:\\n            db(item_table.id == record_id).update(**record)\\n        else:\\n            record_id = item_table.insert(**record)\\n        _debug(\"Record ID=%s\" % record_id)\\n        return record_id', 'def restore(self, row):\\n        \"\"\"\\n            Restore an item from a item table row. This does not restore\\n            the references (since this can not be done before all items\\n            are restored), must call job.restore_references() to do that\\n\\n            @param row: the item table row\\n        \"\"\"\\n\\n        xml = current.xml\\n\\n        self.item_id = row.item_id\\n        self.accepted = None\\n        self.permitted = False\\n        self.committed = False\\n        tablename = row.tablename\\n        self.id = None\\n        self.uid = row.record_uid\\n        if row.data is not None:\\n            self.data = cPickle.loads(row.data)\\n        else:\\n            self.data = Storage()\\n        data = self.data\\n        if xml.MTIME in data:\\n            self.mtime = data[xml.MTIME]\\n        if xml.MCI in data:\\n            self.mci = data[xml.MCI]\\n        UID = xml.UID\\n        if UID in data:\\n            self.uid = data[UID]\\n        self.element = etree.fromstring(row.element)\\n        if row.citems:\\n            self.load_components = row.citems\\n        if row.ritems:\\n            self.load_references = [json.loads(ritem) for ritem in row.ritems]\\n        self.load_parent = row.parent\\n        try:\\n            table = current.s3db[tablename]\\n        except:\\n            self.error = self.ERROR.BAD_RESOURCE\\n            return False\\n        else:\\n            self.table = table\\n            self.tablename = tablename\\n        original = current.manager.original(table, self.data)\\n        if original is not None:\\n            self.original = original\\n            self.id = original[table._id.name]\\n            if UID in original:\\n                self.uid = original[UID]\\n                self.data.update({UID:self.uid})\\n        self.error = row.error\\n        if self.error and not self.data:\\n            # Validation error\\n            return False\\n        return True', 'def __init__(self, manager, table,\\n                 tree=None,\\n                 files=None,\\n                 job_id=None,\\n                 strategy=None,\\n                 update_policy=None,\\n                 conflict_policy=None,\\n                 last_sync=None,\\n                 onconflict=None):\\n        \"\"\"\\n            Constructor\\n\\n            @param manager: the S3RequestManager instance performing this job\\n            @param tree: the element tree to import\\n            @param files: files attached to the import (for upload fields)\\n            @param job_id: restore job from database (record ID or job_id)\\n            @param strategy: the import strategy\\n            @param update_policy: the update policy\\n            @param conflict_policy: the conflict resolution policy\\n            @param last_sync: the last synchronization time stamp (datetime)\\n            @param onconflict: custom conflict resolver function\\n        \"\"\"\\n\\n        self.error = None # the last error\\n        self.error_tree = etree.Element(current.xml.TAG.root)\\n\\n        self.table = table\\n        self.tree = tree\\n        self.files = files\\n        self.directory = Storage()\\n\\n        self.elements = Storage()\\n        self.items = Storage()\\n        self.references = []\\n\\n        self.job_table = None\\n        self.item_table = None\\n\\n        self.count = 0 # total number of records imported\\n        self.created = [] # IDs of created records\\n        self.updated = [] # IDs of updated records\\n        self.deleted = [] # IDs of deleted records\\n\\n        # Import strategy\\n        self.strategy = strategy\\n        if self.strategy is None:\\n            self.strategy = [S3ImportItem.METHOD.CREATE,\\n                             S3ImportItem.METHOD.UPDATE,\\n                             S3ImportItem.METHOD.DELETE]\\n        if not isinstance(self.strategy, (tuple, list)):\\n            self.strategy = [self.strategy]\\n\\n        # Update policy (default=always update)\\n        self.update_policy = update_policy\\n        if not self.update_policy:\\n            self.update_policy = S3ImportItem.POLICY.OTHER\\n        # Conflict resolution policy (default=always update)\\n        self.conflict_policy = conflict_policy\\n        if not self.conflict_policy:\\n            self.conflict_policy = S3ImportItem.POLICY.OTHER\\n\\n        # Synchronization settings\\n        self.mtime = None\\n        self.last_sync = last_sync\\n        self.onconflict = onconflict\\n\\n        if job_id:\\n            self.__define_tables()\\n            jobtable = self.job_table\\n            if str(job_id).isdigit():\\n                query = jobtable.id == job_id\\n            else:\\n                query = jobtable.job_id == job_id\\n            row = current.db(query).select(limitby=(0, 1)).first()\\n            if not row:\\n                raise SyntaxError(\"Job record not found\")\\n            self.job_id = row.job_id\\n            if not self.table:\\n                tablename = row.tablename\\n                try:\\n                    table = current.s3db[tablename]\\n                except:\\n                    pass\\n        else:\\n            import uuid\\n            self.job_id = uuid.uuid4() # unique ID for this job', 'def add_item(self,\\n                 element=None,\\n                 original=None,\\n                 components=None,\\n                 parent=None,\\n                 joinby=None):\\n        \"\"\"\\n            Parse and validate an XML element and add it as new item\\n            to the job.\\n\\n            @param element: the element\\n            @param original: the original DB record (if already available,\\n                             will otherwise be looked-up by this function)\\n            @param components: a dictionary of components (as in S3Resource)\\n                               to include in the job (defaults to all\\n                               defined components)\\n            @param parent: the parent item (if this is a component)\\n            @param joinby: the component join key(s) (if this is a component)\\n\\n            @returns: a unique identifier for the new item, or None if there\\n                      was an error. self.error contains the last error, and\\n                      self.error_tree an element tree with all failing elements\\n                      including error attributes.\\n        \"\"\"\\n\\n        if element in self.elements:\\n            # element has already been added to this job\\n            return self.elements[element]\\n\\n        # Parse the main element\\n        item = S3ImportItem(self)\\n\\n        # Update lookup lists\\n        item_id = item.item_id\\n        self.items[item_id] = item\\n        if element is not None:\\n            self.elements[element] = item_id\\n\\n        if not item.parse(element,\\n                          original=original,\\n                          files=self.files):\\n            self.error = item.error\\n            item.accepted = False\\n            if parent is None:\\n                self.error_tree.append(deepcopy(item.element))\\n\\n        else:\\n            # Now parse the components\\n            table = item.table\\n            components = current.s3db.get_components(table, names=components)\\n\\n            cnames = Storage()\\n            cinfos = Storage()\\n            for alias in components:\\n                component = components[alias]\\n                pkey = component.pkey\\n                if component.linktable:\\n                    ctable = component.linktable\\n                    fkey = component.lkey\\n                else:\\n                    ctable = component.table\\n                    fkey = component.fkey\\n                ctablename = ctable._tablename\\n                if ctablename in cnames:\\n                    cnames[ctablename].append(alias)\\n                else:\\n                    cnames[ctablename] = [alias]\\n                cinfos[(ctablename, alias)] = Storage(component = component,\\n                                                      ctable = ctable,\\n                                                      pkey = pkey,\\n                                                      fkey = fkey,\\n                                                      original = None,\\n                                                      uid = None)\\n            add_item = self.add_item\\n            xml = current.xml\\n            for celement in xml.components(element, names=cnames.keys()):\\n\\n                # Get the component tablename\\n                ctablename = celement.get(xml.ATTRIBUTE.name, None)\\n                if not ctablename:\\n                    continue\\n\\n                # Get the component alias (for disambiguation)\\n                calias = celement.get(xml.ATTRIBUTE.alias, None)\\n                if calias is None:\\n                    if ctablename not in cnames:\\n                        continue\\n                    aliases = cnames[ctablename]\\n                    if len(aliases) == 1:\\n                        calias = aliases[0]\\n                    else:\\n                        # ambiguous components *must* use alias\\n                        continue\\n                if (ctablename, calias) not in cinfos:\\n                    continue\\n                else:\\n                    cinfo = cinfos[(ctablename, calias)]\\n\\n                component = cinfo.component\\n                original = cinfo.original\\n                ctable = cinfo.ctable\\n                pkey = cinfo.pkey\\n                fkey = cinfo.fkey\\n                if not component.multiple:\\n                    if cinfo.uid is not None:\\n                        continue\\n                    if original is None and item.id:\\n                        query = (table.id == item.id) & \\\\\\n                                (table[pkey] == ctable[fkey])\\n                        original = current.db(query).select(ctable.ALL,\\n                                                            limitby=(0, 1)).first()\\n                    if original:\\n                        cinfo.uid = uid = original.get(xml.UID, None)\\n                        celement.set(xml.UID, uid)\\n                    cinfo.original = original\\n\\n                item_id = add_item(element=celement,\\n                                   original=original,\\n                                   parent=item,\\n                                   joinby=(pkey, fkey))\\n                if item_id is None:\\n                    item.error = self.error\\n                    self.error_tree.append(deepcopy(item.element))\\n                else:\\n                    citem = self.items[item_id]\\n                    citem.parent = item\\n                    item.components.append(citem)\\n\\n            # Handle references\\n            table = item.table\\n            tree = self.tree\\n            if tree is not None:\\n                fields = [table[f] for f in table.fields]\\n                rfields = filter(s3_has_foreign_key, fields)\\n                item.references = self.lookahead(element,\\n                                                 table=table,\\n                                                 fields=rfields,\\n                                                 tree=tree,\\n                                                 directory=self.directory)\\n                for reference in item.references:\\n                    entry = reference.entry\\n                    if entry and entry.element is not None:\\n                        item_id = add_item(element=entry.element)\\n                        if item_id:\\n                            entry.update(item_id=item_id)\\n\\n            # Parent reference\\n            if parent is not None:\\n                entry = Storage(item_id=parent.item_id,\\n                                element=parent.element,\\n                                tablename=parent.tablename)\\n                item.references.append(Storage(field=joinby,\\n                                               entry=entry))\\n\\n        return item.item_id', 'def lookahead(self,\\n                  element,\\n                  table=None,\\n                  fields=None,\\n                  tree=None,\\n                  directory=None):\\n        \"\"\"\\n            Find referenced elements in the tree\\n\\n            @param element: the element\\n            @param table: the DB table\\n            @param fields: the FK fields in the table\\n            @param tree: the import tree\\n            @param directory: a dictionary to lookup elements in the tree\\n                              (will be filled in by this function)\\n        \"\"\"\\n\\n        db = current.db\\n        s3db = current.s3db\\n        xml = current.xml\\n        import_uid = xml.import_uid\\n        ATTRIBUTE = xml.ATTRIBUTE\\n        TAG = xml.TAG\\n        UID = xml.UID\\n        reference_list = []\\n\\n        root = None\\n        if tree is not None:\\n            if isinstance(tree, etree._Element):\\n                root = tree\\n            else:\\n                root = tree.getroot()\\n        references = element.findall(\"reference\")\\n        for reference in references:\\n            field = reference.get(ATTRIBUTE.field, None)\\n            # Ignore references without valid field-attribute\\n            if not field or field not in fields:\\n                continue\\n            # Find the key table\\n            multiple = False\\n            fieldtype = str(table[field].type)\\n            if fieldtype.startswith(\"reference\"):\\n                ktablename = fieldtype[10:]\\n            elif fieldtype.startswith(\"list:reference\"):\\n                ktablename = fieldtype[15:]\\n                multiple = True\\n            else:\\n                # ignore if the field is not a reference type\\n                continue\\n            try:\\n                ktable = s3db[ktablename]\\n            except:\\n                # Invalid tablename - skip\\n                continue\\n            tablename = reference.get(ATTRIBUTE.resource, None)\\n            # Ignore references to tables without UID field:\\n            if UID not in ktable.fields:\\n                continue\\n            # Fall back to key table name if tablename is not specified:\\n            if not tablename:\\n                tablename = ktablename\\n            # Super-entity references must use the super-key:\\n            if tablename != ktablename:\\n                field = (ktable._id.name, field)\\n            # Ignore direct references to super-entities:\\n            if tablename == ktablename and ktable._id.name != \"id\":\\n                continue\\n            # Get the foreign key\\n            uids = reference.get(UID, None)\\n            attr = UID\\n            if not uids:\\n                uids = reference.get(ATTRIBUTE.tuid, None)\\n                attr = ATTRIBUTE.tuid\\n            if uids and multiple:\\n                uids = json.loads(uids)\\n            elif uids:\\n                uids = [uids]\\n\\n            # Find the elements and map to DB records\\n            relements = []\\n\\n            # Create a UID<->ID map\\n            id_map = Storage()\\n            if attr == UID and uids:\\n                _uids = map(import_uid, uids)\\n                query = ktable[UID].belongs(_uids)\\n                records = db(query).select(ktable.id,\\n                                           ktable[UID])\\n                id_map = dict([(r[UID], r.id) for r in records])\\n\\n            if not uids:\\n                # Anonymous reference: <resource> inside the element\\n                expr = \\'.//%s[@%s=\"%s\"]\\' % (TAG.resource,\\n                                            ATTRIBUTE.name,\\n                                            tablename)\\n                relements = reference.xpath(expr)\\n                if relements and not multiple:\\n                    relements = [relements[0]]\\n\\n            elif root is not None:\\n\\n                for uid in uids:\\n\\n                    entry = None\\n                    # Entry already in directory?\\n                    if directory is not None:\\n                        entry = directory.get((tablename, attr, uid), None)\\n                    if not entry:\\n                        expr = \".//%s[@%s=\\'%s\\' and @%s=\\'%s\\']\" % (\\n                                    TAG.resource,\\n                                    ATTRIBUTE.name,\\n                                    tablename,\\n                                    attr,\\n                                    uid)\\n                        e = root.xpath(expr)\\n                        if e:\\n                            # Element in the source => append to relements\\n                            relements.append(e[0])\\n                        else:\\n                            # No element found, see if original record exists\\n                            _uid = import_uid(uid)\\n                            if _uid and _uid in id_map:\\n                                _id = id_map[_uid]\\n                                entry = Storage(tablename=tablename,\\n                                                element=None,\\n                                                uid=uid,\\n                                                id=_id,\\n                                                item_id=None)\\n                                reference_list.append(Storage(field=field,\\n                                                              entry=entry))\\n                            else:\\n                                continue\\n                    else:\\n                        reference_list.append(Storage(field=field,\\n                                                      entry=entry))\\n\\n            # Create entries for all newly found elements\\n            for relement in relements:\\n                uid = relement.get(attr, None)\\n                if attr == UID:\\n                    _uid = import_uid(uid)\\n                    id = _uid and id_map and id_map.get(_uid, None) or None\\n                else:\\n                    _uid = None\\n                    id = None\\n                entry = Storage(tablename=tablename,\\n                                element=relement,\\n                                uid=uid,\\n                                id=id,\\n                                item_id=None)\\n                # Add entry to directory\\n                if uid and directory is not None:\\n                    directory[(tablename, attr, uid)] = entry\\n                # Append the entry to the reference list\\n                reference_list.append(Storage(field=field, entry=entry))\\n\\n        return reference_list', 'def load_item(self, row):\\n        \"\"\"\\n            Load an item from the item table (counterpart to add_item\\n            when restoring a job from the database)\\n        \"\"\"\\n\\n        item = S3ImportItem(self)\\n        if not item.restore(row):\\n            self.error = item.error\\n            if item.load_parent is None:\\n                self.error_tree.append(deepcopy(item.element))\\n        # Update lookup lists\\n        item_id = item.item_id\\n        self.items[item_id] = item\\n        return item_id', 'def resolve(self, item_id, import_list):\\n        \"\"\"\\n            Resolve the reference list of an item\\n\\n            @param item_id: the import item UID\\n            @param import_list: the ordered list of items (UIDs) to import\\n        \"\"\"\\n\\n        item = self.items[item_id]\\n        if item.lock or item.accepted is False:\\n            return False\\n        references = []\\n        for reference in item.references:\\n            ritem_id = reference.entry.item_id\\n            if ritem_id and ritem_id not in import_list:\\n                references.append(ritem_id)\\n        for ritem_id in references:\\n            item.lock = True\\n            if self.resolve(ritem_id, import_list):\\n                import_list.append(ritem_id)\\n            item.lock = False\\n        return True', 'def commit(self, ignore_errors=False):\\n        \"\"\"\\n            Commit the import job to the DB\\n\\n            @param ignore_errors: skip any items with errors\\n                                  (does still report the errors)\\n        \"\"\"\\n\\n        ATTRIBUTE = current.xml.ATTRIBUTE\\n\\n        # Resolve references\\n        import_list = []\\n        for item_id in self.items:\\n            self.resolve(item_id, import_list)\\n            if item_id not in import_list:\\n                import_list.append(item_id)\\n        # Commit the items\\n        items = self.items\\n        count = 0\\n        mtime = None\\n        created = []\\n        cappend = created.append\\n        updated = []\\n        deleted = []\\n        tablename = self.table._tablename\\n        for item_id in import_list:\\n            item = items[item_id]\\n            error = None\\n            success = item.commit(ignore_errors=ignore_errors)\\n            error = item.error\\n            if error:\\n                self.error = error\\n                element = item.element\\n                if element is not None:\\n                    if not element.get(ATTRIBUTE.error, False):\\n                        element.set(ATTRIBUTE.error, str(self.error))\\n                    self.error_tree.append(deepcopy(element))\\n                if not ignore_errors:\\n                    return False\\n            elif item.tablename == tablename:\\n                count += 1\\n                if mtime is None or item.mtime > mtime:\\n                    mtime = item.mtime\\n                if item.id:\\n                    if item.method == item.METHOD.CREATE:\\n                        cappend(item.id)\\n                    elif item.method == item.METHOD.UPDATE:\\n                        updated.append(item.id)\\n                    elif item.method == item.METHOD.DELETE:\\n                        deleted.append(item.id)\\n        self.count = count\\n        self.mtime = mtime\\n        self.created = created\\n        self.updated = updated\\n        self.deleted = deleted\\n        return True', 'def __define_tables(self):\\n        \"\"\"\\n            Define the database tables for jobs and items\\n        \"\"\"\\n\\n        self.job_table = self.define_job_table()\\n        self.item_table = self.define_item_table()', 'def define_job_table(cls):\\n\\n        db = current.db\\n        if cls.JOB_TABLE_NAME not in db:\\n            job_table = db.define_table(cls.JOB_TABLE_NAME,\\n                                        Field(\"job_id\", length=128,\\n                                              unique=True,\\n                                              notnull=True),\\n                                        Field(\"tablename\"),\\n                                        Field(\"timestmp\", \"datetime\",\\n                                              default=datetime.utcnow()))\\n        else:\\n            job_table = db[cls.JOB_TABLE_NAME]\\n        return job_table', 'def define_item_table(cls):\\n\\n        db = current.db\\n        if cls.ITEM_TABLE_NAME not in db:\\n            item_table = db.define_table(cls.ITEM_TABLE_NAME,\\n                                        Field(\"item_id\", length=128,\\n                                              unique=True,\\n                                              notnull=True),\\n                                        Field(\"job_id\", length=128),\\n                                        Field(\"tablename\", length=128),\\n                                        #Field(\"record_id\", \"integer\"),\\n                                        Field(\"record_uid\"),\\n                                        Field(\"error\", \"text\"),\\n                                        Field(\"data\", \"text\"),\\n                                        Field(\"element\", \"text\"),\\n                                        Field(\"ritems\", \"list:string\"),\\n                                        Field(\"citems\", \"list:string\"),\\n                                        Field(\"parent\", length=128))\\n        else:\\n            item_table = db[cls.ITEM_TABLE_NAME]\\n        return item_table', 'def store(self):\\n        \"\"\"\\n            Store this job and all its items in the job table\\n        \"\"\"\\n\\n        db = current.db\\n\\n        _debug(\"Storing Job ID=%s\" % self.job_id)\\n        self.__define_tables()\\n        jobtable = self.job_table\\n        query = jobtable.job_id == self.job_id\\n        row = db(query).select(jobtable.id, limitby=(0, 1)).first()\\n        if row:\\n            record_id = row.id\\n        else:\\n            record_id = None\\n        record = Storage(job_id=self.job_id)\\n        try:\\n            tablename = self.table._tablename\\n        except:\\n            pass\\n        else:\\n            record.update(tablename=tablename)\\n        for item in self.items.values():\\n            item.store(item_table=self.item_table)\\n        if record_id:\\n            db(jobtable.id == record_id).update(**record)\\n        else:\\n            record_id = jobtable.insert(**record)\\n        _debug(\"Job record ID=%s\" % record_id)\\n        return record_id', 'def get_tree(self):\\n        \"\"\"\\n            Reconstruct the element tree of this job\\n        \"\"\"\\n\\n        if self.tree is not None:\\n            return tree\\n        else:\\n            xml = current.xml\\n            root = etree.Element(xml.TAG.root)\\n            for item in self.items.values():\\n                if item.element is not None and not item.parent:\\n                    if item.tablename == self.table._tablename or \\\\\\n                       item.element.get(xml.UID, None) or \\\\\\n                       item.element.get(xml.ATTRIBUTE.tuid, None):\\n                        root.append(deepcopy(item.element))\\n            return etree.ElementTree(root)', 'def delete(self):\\n        \"\"\"\\n            Delete this job and all its items from the job table\\n        \"\"\"\\n\\n        db = current.db\\n\\n        _debug(\"Deleting job ID=%s\" % self.job_id)\\n        self.__define_tables()\\n        item_table = self.item_table\\n        query = item_table.job_id == self.job_id\\n        db(query).delete()\\n        job_table = self.job_table\\n        query = job_table.job_id == self.job_id\\n        db(query).delete()', 'def restore_references(self):\\n        \"\"\"\\n            Restore the job\\'s reference structure after loading items\\n            from the item table\\n        \"\"\"\\n\\n        db = current.db\\n        UID = current.xml.UID\\n\\n        for item in self.items.values():\\n            for citem_id in item.load_components:\\n                if citem_id in self.items:\\n                    item.components.append(self.items[citem_id])\\n            item.load_components = []\\n            for ritem in item.load_references:\\n                field = ritem[\"field\"]\\n                if \"item_id\" in ritem:\\n                    item_id = ritem[\"item_id\"]\\n                    if item_id in self.items:\\n                        _item = self.items[item_id]\\n                        entry = Storage(tablename=_item.tablename,\\n                                        element=_item.element,\\n                                        uid=_item.uid,\\n                                        id=_item.id,\\n                                        item_id=item_id)\\n                        item.references.append(Storage(field=field,\\n                                                       entry=entry))\\n                else:\\n                    _id = None\\n                    uid = ritem.get(\"uid\", None)\\n                    tablename = ritem.get(\"tablename\", None)\\n                    if tablename and uid:\\n                        try:\\n                            table = current.s3db[tablename]\\n                        except:\\n                            continue\\n                        if UID not in table.fields:\\n                            continue\\n                        query = table[UID] == uid\\n                        row = db(query).select(table._id,\\n                                               limitby=(0, 1)).first()\\n                        if row:\\n                            _id = row[table._id.name]\\n                        else:\\n                            continue\\n                        entry = Storage(tablename = ritem[\"tablename\"],\\n                                        element=None,\\n                                        uid = ritem[\"uid\"],\\n                                        id = _id,\\n                                        item_id = None)\\n                        item.references.append(Storage(field=field,\\n                                                       entry=entry))\\n            item.load_references = []\\n            if item.load_parent is not None:\\n                item.parent = self.items[item.load_parent]\\n                item.load_parent = None']}, {'features': [], 'snippets': [\"def random_ascii_string(length):\\n    random = SystemRandom()\\n    return ''.join([random.choice(UNICODE_ASCII_CHARACTERS) for x in range(length)])\", 'def url_dequery(url):\\n    \"\"\"Return a URL with the query component removed.\\n\\n    :param url: URL to dequery.\\n    :type url: str\\n    :rtype: str\\n    \"\"\"\\n    url = urlparse(url)\\n    return urlunparse((url.scheme,\\n                                url.netloc,\\n                                url.path,\\n                                url.params,\\n                                \\'\\',\\n                                url.fragment))']}, {'features': [], 'snippets': [\"def __init__(self, air):\\n        DistributedCCharBaseAI.DistributedCCharBaseAI.__init__(self, air, TTLocalizer.Goofy)\\n        self.fsm = ClassicFSM.ClassicFSM('DistributedGoofySpeedwayAI', [State.State('Off', self.enterOff, self.exitOff, ['Lonely', 'TransitionToCostume', 'Walk']),\\n         State.State('Lonely', self.enterLonely, self.exitLonely, ['Chatty', 'Walk', 'TransitionToCostume']),\\n         State.State('Chatty', self.enterChatty, self.exitChatty, ['Lonely', 'Walk', 'TransitionToCostume']),\\n         State.State('Walk', self.enterWalk, self.exitWalk, ['Lonely', 'Chatty', 'TransitionToCostume']),\\n         State.State('TransitionToCostume', self.enterTransitionToCostume, self.exitTransitionToCostume, ['Off'])], 'Off', 'Off')\\n        self.fsm.enterInitialState()\\n        self.handleHolidays()\", \"def generate(self):\\n        DistributedCCharBaseAI.DistributedCCharBaseAI.generate(self)\\n        name = self.getName()\\n        self.lonelyDoneEvent = self.taskName(name + '-lonely-done')\\n        self.lonely = CharStateDatasAI.CharLonelyStateAI(self.lonelyDoneEvent, self)\\n        self.chattyDoneEvent = self.taskName(name + '-chatty-done')\\n        self.chatty = CharStateDatasAI.CharChattyStateAI(self.chattyDoneEvent, self)\\n        self.walkDoneEvent = self.taskName(name + '-walk-done')\\n        if self.diffPath == None:\\n            self.walk = CharStateDatasAI.CharWalkStateAI(self.walkDoneEvent, self)\\n        else:\\n            self.walk = CharStateDatasAI.CharWalkStateAI(self.walkDoneEvent, self, self.diffPath)\\n        return\", \"def start(self):\\n        self.fsm.request('Lonely')\", 'def enterOff(self):\\n        pass', 'def enterLonely(self):\\n        self.lonely.enter()\\n        self.acceptOnce(self.lonelyDoneEvent, self.__decideNextState)', \"def __goForAWalk(self, task):\\n        self.notify.debug('going for a walk')\\n        self.fsm.request('Walk')\\n        return Task.done\", 'def exitChatty(self):\\n        self.ignore(self.chattyDoneEvent)\\n        self.chatty.exit()', 'def exitWalk(self):\\n        self.ignore(self.walkDoneEvent)\\n        self.walk.exit()', \"def avatarExitNextState(self):\\n        if len(self.nearbyAvatars) == 0:\\n            if self.fsm.getCurrentState().getName() != 'Walk':\\n                self.fsm.request('Lonely')\", 'def getCCLocation(self):\\n        if self.diffPath == None:\\n            return 1\\n        else:\\n            return 0\\n        return']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def _display_login_form(request, error_message=''):\\n    request.session.set_test_cookie()\\n    return render_to_response('admin/login.html', {\\n        'title': _('Log in'),\\n        'app_path': request.get_full_path(),\\n        'error_message': error_message\\n    }, context_instance=template.RequestContext(request))\", 'def _checklogin(request, *args, **kwargs):\\n        if request.user.is_authenticated() and request.user.is_staff:\\n            # The user is valid. Continue to the admin page.\\n            return view_func(request, *args, **kwargs)\\n\\n        assert hasattr(request, \\'session\\'), \"The Django admin requires session middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert \\'django.contrib.sessions.middleware.SessionMiddleware\\'.\"\\n\\n        # If this isn\\'t already the login page, display it.\\n        if LOGIN_FORM_KEY not in request.POST:\\n            if request.POST:\\n                message = _(\"Please log in again, because your session has expired.\")\\n            else:\\n                message = \"\"\\n            return _display_login_form(request, message)\\n\\n        # Check that the user accepts cookies.\\n        if not request.session.test_cookie_worked():\\n            message = _(\"Looks like your browser isn\\'t configured to accept cookies. Please enable cookies, reload this page, and try again.\")\\n            return _display_login_form(request, message)\\n        else:\\n            request.session.delete_test_cookie()\\n\\n        # Check the password.\\n        username = request.POST.get(\\'username\\', None)\\n        password = request.POST.get(\\'password\\', None)\\n        user = authenticate(username=username, password=password)\\n        if user is None:\\n            message = ERROR_MESSAGE\\n            if \\'@\\' in username:\\n                # Mistakenly entered e-mail address instead of username? Look it up.\\n                users = list(User.all().filter(\\'email =\\', username))\\n                if len(users) == 1 and users[0].check_password(password):\\n                    message = _(\"Your e-mail address is not your username. Try \\'%s\\' instead.\") % users[0].username\\n                else:\\n                    # Either we cannot find the user, or if more than 1\\n                    # we cannot guess which user is the correct one.\\n                    message = _(\"Usernames cannot contain the \\'@\\' character.\")\\n            return _display_login_form(request, message)\\n\\n        # The user data is correct; log in the user in and continue.\\n        else:\\n            if user.is_active and user.is_staff:\\n                login(request, user)\\n                return http.HttpResponseRedirect(request.get_full_path())\\n            else:\\n                return _display_login_form(request, ERROR_MESSAGE)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def send_info_handler(bot, update, args):\\n\\targs = list(parse_args(args))\\n\\tif len(args) == 0 or \"portfolio\" in [arg.lower() for arg in args] :\\n\\t\\tsend_portfolio_info(bot, update)\\n\\telse:\\n\\t\\tinfo_companies = get_companies(args)\\n\\t\\tsend_companies_info(bot, update, info_companies)', 'def send_portfolio_info(bot, update):\\n\\tprint \"Userid: %d requested portfolio information\" %(update.message.chat_id)\\n\\tcontext = {\\n\\t\\'positions\\': Portfolio.instance.positions,\\n    \\'wallet_value\\': Portfolio.instance.wallet_value,\\n\\t}\\n\\thtml_str = engine.render(\\'portfolio_info.pyhtml\\', context)\\n\\tbot.sendMessage(parse_mode=\"HTML\", chat_id=update.message.chat_id, text=html_str)', 'def send_companies_info(bot, update, companies):\\n\\tprint \"Userid: requested information for following companies %s\" %\\',\\'.join([c.name for c in companies])\\n\\n\\tfor company in companies:\\n\\t\\tcontext = {\\n\\t\\t\\'company\\': company,\\n\\t\\t\\'current_price\\': get_current_price(company),\\n\\t\\t\\'description\\': wikipedia.summary(company.name.split()[0], sentences=2)\\n\\t\\t}\\n\\n\\t\\twiki_page = wikipedia.page(company.name.split()[0])\\n\\t\\thtml_page = urllib2.urlopen(wiki_page.url)\\n\\t\\tsoup = bs.BeautifulSoup(html_page)\\n\\t\\timg_url = \\'http:\\' + soup.find(\\'td\\', { \"class\" : \"logo\" }).find(\\'img\\')[\\'src\\']\\n\\t\\tbot.sendPhoto(chat_id=update.message.chat_id, photo=img_url)\\n\\n\\t\\thtml_str = engine.render(\\'company_template.pyhtml\\', context)\\n\\t\\tbot.sendMessage(parse_mode=\"HTML\", chat_id=update.message.chat_id, text=html_str)\\n\\n\\tsymbols = [c.symbol for c in companies]\\n\\tif len(symbols) >= 2:\\n\\t\\tsymbol_string = \", \".join(symbols[:-1]) + \" and \" + symbols[-1]\\n\\telse:\\n\\t\\tsymbol_string = symbols[0]\\n\\n\\tlast_n_days = 10\\n\\n\\tif len(companies) < 4:\\n\\t\\tcreate_graph(companies, last_n_days)\\n\\t\\thistory_text = \\'\\'\\'\\n\\t\\t\\tHere\\'s the price history for {} for the last {} days\\n\\t\\t\\'\\'\\'.format(symbol_string, last_n_days)\\n\\n\\t\\tbot.sendMessage(chat_id=update.message.chat_id, text=history_text)\\n\\t\\tbot.sendPhoto(chat_id=update.message.chat_id, photo=open(\"plots/temp.png\",\\'rb\\'))']}, {'features': [], 'snippets': ['def bsearch(nums, left, right, res, i, j, target):\\n    while left <= right:\\n        middle = (left + right) // 2\\n        candidate = nums[i] + nums[j] + nums[middle]\\n        if res is None or abs(candidate - target) < abs(res - target):\\n            res = candidate\\n        if candidate == target:\\n            return res\\n        elif candidate > target:\\n            right = middle - 1\\n        else:\\n            left = middle + 1\\n    return res', 'def threeSumClosest(self, nums: List[int], target: int) -> Optional[int]:\\n        res = None\\n        nums = sorted(nums)\\n\\n        for i in range(len(nums)):\\n            for j in range(i + 1, len(nums)):\\n                res = bsearch(nums, j + 1, len(nums) - 1, res, i, j, target)\\n        return res']}, {'features': [], 'snippets': [\"def long_time_task(name):\\n    print 'Run task %s (%s)...' % (name, os.getpid())\\n    start = time.time()\\n    time.sleep(random.random() * 3)\\n    end = time.time()\\n    print 'Task %s runs %0.2f seconds.' % (name, (end - start))\"]}, {'features': [], 'snippets': ['def _setup(self, storage_account_name, key):\\n        # test chunking functionality by reducing the threshold\\n        # for chunking and the size of each chunk, otherwise\\n        # the tests would take too long to execute\\n        self.bsc = BlobServiceClient(\\n            self.account_url(storage_account_name, \"blob\"),\\n            credential=key,\\n            max_single_put_size=32 * 1024,\\n            max_block_size=2 * 1024 * 1024,\\n            min_large_block_upload_threshold=1 * 1024 * 1024)\\n        self.config = self.bsc._config\\n        self.container_name = self.get_resource_name(\\'utcontainer\\')\\n\\n        if self.is_live:\\n            try:\\n                self.bsc.create_container(self.container_name)\\n            except:\\n                pass', 'def _get_blob_reference(self):\\n        return self.get_resource_name(TEST_BLOB_PREFIX)', 'def assertBlobEqual(self, container_name, blob_name, expected_data):\\n        blob = self.bsc.get_blob_client(container_name, blob_name)\\n        actual_data = blob.download_blob()\\n        self.assertEqual(b\"\".join(list(actual_data.chunks())), expected_data)', \"def test_put_block_bytes_large(self, storage_account_name, storage_account_key):\\n        self._setup(storage_account_name, storage_account_key)\\n        blob = self._create_blob()\\n\\n        # Act\\n        for i in range(5):\\n            resp = blob.stage_block(\\n                'block {0}'.format(i).encode('utf-8'), urandom(LARGE_BLOCK_SIZE))\\n            self.assertIsNotNone(resp)\\n            assert 'content_md5' in resp\\n            assert 'content_crc64' in resp\\n            assert 'request_id' in resp\\n\\n            # Assert\", \"def test_put_block_bytes_large_with_md5(self, storage_account_name, storage_account_key):\\n        self._setup(storage_account_name, storage_account_key)\\n        blob = self._create_blob()\\n\\n        # Act\\n        for i in range(5):\\n            resp = blob.stage_block(\\n                'block {0}'.format(i).encode('utf-8'),\\n                urandom(LARGE_BLOCK_SIZE),\\n                validate_content=True)\\n            self.assertIsNotNone(resp)\\n            assert 'content_md5' in resp\\n            assert 'content_crc64' in resp\\n            assert 'request_id' in resp\", \"def test_put_block_stream_large(self, storage_account_name, storage_account_key):\\n        self._setup(storage_account_name, storage_account_key)\\n        blob = self._create_blob()\\n\\n        # Act\\n        for i in range(5):\\n            stream = BytesIO(bytearray(LARGE_BLOCK_SIZE))\\n            resp = resp = blob.stage_block(\\n                'block {0}'.format(i).encode('utf-8'),\\n                stream,\\n                length=LARGE_BLOCK_SIZE)\\n            self.assertIsNotNone(resp)\\n            assert 'content_md5' in resp\\n            assert 'content_crc64' in resp\\n            assert 'request_id' in resp\\n\\n            # Assert\", \"def test_put_block_stream_large_with_md5(self, storage_account_name, storage_account_key):\\n        self._setup(storage_account_name, storage_account_key)\\n        blob = self._create_blob()\\n\\n        # Act\\n        for i in range(5):\\n            stream = BytesIO(bytearray(LARGE_BLOCK_SIZE))\\n            resp = resp = blob.stage_block(\\n                'block {0}'.format(i).encode('utf-8'),\\n                stream,\\n                length=LARGE_BLOCK_SIZE,\\n                validate_content=True)\\n            self.assertIsNotNone(resp)\\n            assert 'content_md5' in resp\\n            assert 'content_crc64' in resp\\n            assert 'request_id' in resp\\n\\n        # Assert\", \"def test_create_large_blob_from_path(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'large_blob_from_path.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, max_concurrency=2, overwrite=True)\\n\\n        block_list = blob.get_block_list()\\n\\n        # Assert\\n        self.assertIsNot(len(block_list), 0)\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self._teardown(FILE_PATH)\", 'def test_create_large_blob_from_path_with_md5(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = \"blob_from_path_with_md5.temp.dat\"\\n        with open(FILE_PATH, \\'wb\\') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        with open(FILE_PATH, \\'rb\\') as stream:\\n            blob.upload_blob(stream, validate_content=True, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self._teardown(FILE_PATH)', 'def test_create_large_blob_from_path_non_parallel(self, storage_account_name, storage_account_key):\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(self.get_random_bytes(100))\\n        FILE_PATH = \"blob_from_path_non_parallel.temp.dat\"\\n        with open(FILE_PATH, \\'wb\\') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        with open(FILE_PATH, \\'rb\\') as stream:\\n            blob.upload_blob(stream, max_concurrency=1)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self._teardown(FILE_PATH)', 'def test_create_large_blob_from_path_with_progress(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = \"blob_from_path_with_progress.temp.dat\"\\n        with open(FILE_PATH, \\'wb\\') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        progress = []\\n        def callback(response):\\n            current = response.context[\\'upload_stream_current\\']\\n            total = response.context[\\'data_stream_total\\']\\n            if current is not None:\\n                progress.append((current, total))\\n\\n        with open(FILE_PATH, \\'rb\\') as stream:\\n            blob.upload_blob(stream, max_concurrency=2, raw_response_hook=callback)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self.assert_upload_progress(len(data), self.config.max_block_size, progress)\\n        self._teardown(FILE_PATH)', \"def test_create_large_blob_from_path_with_properties(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'blob_from_path_with_properties.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        content_settings = ContentSettings(\\n            content_type='image/png',\\n            content_language='spanish')\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, content_settings=content_settings, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        properties = blob.get_blob_properties()\\n        self.assertEqual(properties.content_settings.content_type, content_settings.content_type)\\n        self.assertEqual(properties.content_settings.content_language, content_settings.content_language)\\n        self._teardown(FILE_PATH)\", \"def test_create_large_blob_from_stream_chunked_upload(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'blob_from_stream_chunked_upload.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self._teardown(FILE_PATH)\", \"def test_creat_lrgblob_frm_stream_w_progress_chnkd_upload(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'stream_w_progress_chnkd_upload.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        progress = []\\n        def callback(response):\\n            current = response.context['upload_stream_current']\\n            total = response.context['data_stream_total']\\n            if current is not None:\\n                progress.append((current, total))\\n\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, max_concurrency=2, raw_response_hook=callback)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        self.assert_upload_progress(len(data), self.config.max_block_size, progress)\\n        self._teardown(FILE_PATH)\", \"def test_create_large_blob_from_stream_chunked_upload_with_count(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'chunked_upload_with_count.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        blob_size = len(data) - 301\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, length=blob_size, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data[:blob_size])\\n        self._teardown(FILE_PATH)\", \"def test_creat_lrgblob_frm_strm_chnkd_uplod_w_count_n_props(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'plod_w_count_n_props.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        content_settings = ContentSettings(\\n            content_type='image/png',\\n            content_language='spanish')\\n        blob_size = len(data) - 301\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(\\n                stream, length=blob_size, content_settings=content_settings, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data[:blob_size])\\n        properties = blob.get_blob_properties()\\n        self.assertEqual(properties.content_settings.content_type, content_settings.content_type)\\n        self.assertEqual(properties.content_settings.content_language, content_settings.content_language)\\n        self._teardown(FILE_PATH)\", \"def test_creat_lrg_blob_frm_stream_chnked_upload_w_props(self, storage_account_name, storage_account_key):\\n        # parallel tests introduce random order of requests, can only run live\\n\\n        self._setup(storage_account_name, storage_account_key)\\n        blob_name = self._get_blob_reference()\\n        blob = self.bsc.get_blob_client(self.container_name, blob_name)\\n        data = bytearray(urandom(LARGE_BLOB_SIZE))\\n        FILE_PATH = 'creat_lrg_blob.temp.{}.dat'.format(str(uuid.uuid4()))\\n        with open(FILE_PATH, 'wb') as stream:\\n            stream.write(data)\\n\\n        # Act\\n        content_settings = ContentSettings(\\n            content_type='image/png',\\n            content_language='spanish')\\n        with open(FILE_PATH, 'rb') as stream:\\n            blob.upload_blob(stream, content_settings=content_settings, max_concurrency=2)\\n\\n        # Assert\\n        self.assertBlobEqual(self.container_name, blob_name, data)\\n        properties = blob.get_blob_properties()\\n        self.assertEqual(properties.content_settings.content_type, content_settings.content_type)\\n        self.assertEqual(properties.content_settings.content_language, content_settings.content_language)\\n        self._teardown(FILE_PATH)\"]}, {'features': [], 'snippets': ['def _walk_to_root(path):\\n    \"\"\"Yield directories starting from the given directory up to the root.\"\"\"\\n    # Based on code from python-dotenv (BSD-licensed):\\n    # https://github.com/theskumar/python-dotenv/blob/e13d957b/src/dotenv/main.py#L245\\n\\n    if os.path.isfile(path):\\n        path = os.path.dirname(path)\\n\\n    last_dir = None\\n    current_dir = os.path.abspath(path)\\n    while last_dir != current_dir:\\n        yield current_dir\\n        parent_dir = os.path.abspath(os.path.join(current_dir, os.path.pardir))\\n        last_dir, current_dir = current_dir, parent_dir', \"def gen_header(data):\\n    if data['start_year'] == data['end_year']:\\n        data['dates'] = data['start_year']\\n    else:\\n        data['dates'] = '{} - {}'.format(data['start_year'], data['end_year'])\\n    return '\\\\n'.join(line.rstrip() for line in data['header'].format(**data).strip().splitlines())\", \"def update_header(file_path, year, ci):\\n    config = _get_config(file_path, year)\\n    ext = file_path.rsplit('.', 1)[-1]\\n    if ext not in SUPPORTED_FILES or not os.path.isfile(file_path):\\n        return False\\n    if os.path.basename(file_path)[0] == '.':\\n        return False\\n    return _update_header(file_path, config, SUBSTRING, SUPPORTED_FILES[ext]['regex'],\\n                          SUPPORTED_FILES[ext]['format'], ci)\", \"def main(ctx, ci, year, path):\\n    error = False\\n    if path and os.path.isdir(path):\\n        if not ci:\\n            print(cformat('Updating headers to the year %{yellow!}{year}%{reset} for all the files in '\\n                          '%{yellow!}{path}%{reset}...').format(year=year, path=path))\\n        for root, _, filenames in os.walk(path):\\n            for filename in filenames:\\n                if not blacklisted(path, root):\\n                    if update_header(os.path.join(root, filename), year, ci):\\n                        error = True\\n    elif path and os.path.isfile(path):\\n        if not ci:\\n            print(cformat('Updating headers to the year %{yellow!}{year}%{reset} for the file '\\n                          '%{yellow!}{file}%{reset}...').format(year=year, file=path))\\n        if update_header(path, year, ci):\\n            error = True\\n    else:\\n        if not ci:\\n            print(cformat('Updating headers to the year %{yellow!}{year}%{reset} for all '\\n                          'git-tracked files...').format(year=year))\\n        try:\\n            for filepath in subprocess.check_output(['git', 'ls-files'], text=True).splitlines():\\n                filepath = os.path.abspath(filepath)\\n                if not blacklisted(os.getcwd(), os.path.dirname(filepath)):\\n                    if update_header(filepath, year, ci):\\n                        error = True\\n        except subprocess.CalledProcessError:\\n            raise click.UsageError(cformat('%{red!}You must be within a git repository to run this script.'))\\n\\n    if not error:\\n        print(cformat('%{green}\\\\u2705 All headers are up to date'))\\n    elif ci:\\n        print(cformat('%{red}\\\\u274C Some headers need to be updated or added'))\\n        sys.exit(1)\\n    else:\\n        print(cformat('%{yellow}\\\\U0001F504 Some headers have been updated (or are missing)'))\"]}, {'features': [], 'snippets': ['def add(request):\\n    print \"jojo\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, xPos, yPos):\\n        self.x = xPos\\n        self.y = yPos\\n        self.th = 32\\n        self.tw = 32', 'def checkCollision(self, otherSprite):\\n        if (self.x < otherSprite.x + otherSprite.tw and otherSprite.x < self.x + self.tw\\n            and self.y < otherSprite.y + otherSprite.th and otherSprite.y < self.y + self.th):\\n            return True\\n        else:\\n            return False', 'def __init__(self, xPos, yPos):\\n        super(Actor, self).__init__(xPos, yPos)\\n        self.speed = 5\\n        self.dy = 0\\n        self.d = 3\\n        self.dir = \"right\"\\n        # self.newdir = \"right\"\\n        self.state = \"standing\"\\n        self.walkR = []\\n        self.walkL = []', 'def loadPics(self):\\n        self.standing = loadImage(\"gripe_stand.png\")\\n        self.falling = loadImage(\"grfalling.png\")\\n        for i in range(8):\\n            imageName = \"gr\" + str(i) + \".png\"\\n            self.walkR.append(loadImage(imageName))\\n        for i in range(8):\\n            imageName = \"gl\" + str(i) + \".png\"\\n            self.walkL.append(loadImage(imageName))', 'def checkWall(self, wall):\\n        if wall.state == \"hidden\":\\n            if (self.x >= wall.x - self.d and\\n                    (self.x + 32 <= wall.x + 32 + self.d)):\\n                return False', 'def move(self):\\n        if self.dir == \"right\":\\n            if self.state == \"walking\":\\n                self.im = self.walkR[frameCount % 8]\\n                self.dx = self.speed\\n            elif self.state == \"standing\":\\n                self.im = self.standing\\n                self.dx = 0\\n            elif self.state == \"falling\":\\n                self.im = self.falling\\n                self.dx = 0\\n                self.dy = 5\\n        elif self.dir == \"left\":\\n            if self.state == \"walking\":\\n                self.im = self.walkL[frameCount % 8]\\n                self.dx = -self.speed\\n            elif self.state == \"standing\":\\n                self.im = self.standing\\n                self.dx = 0\\n            elif self.state == \"falling\":\\n                self.im = self.falling\\n                self.dx = 0\\n                self.dy = 5\\n        else:\\n            self.dx = 0\\n        self.x += self.dx\\n        self.y += self.dy\\n\\n        if self.x <= 0:\\n            self.x = 0\\n        if self.x >= 640 - self.tw:\\n            self.x = 640 -self.tw', 'def display(self):\\n        image(self.im, self.x, self.y)', 'def __init__(self, xPos, yPos):\\n        super(Block, self).__init__(xPos, yPos)\\n        self.state = \"visible\"', 'def loadPics(self):\\n        self.im = loadImage(\"block.png\")', 'def display(self):\\n        if self.state == \"visible\":\\n            image(self.im, self.x, self.y)']}, {'features': [], 'snippets': ['def setUp(self):\\n        pass', \"def test_words(self):\\n        result = spell_checker.check(u'한아이가 장난깜을 갖고놀고있다. 그만하게 할가?')\\n        assert result.errors == 4\", \"def test_list(self):\\n        results = spell_checker.check([u'안녕 하세요.', u'저는 한국인 입니다.'])\\n        assert results[0].checked == u'안녕하세요.'\\n        assert results[1].checked == u'저는 한국인입니다.'\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def addOperators(self, num, target):\\n        \"\"\"\\n        Adapted from https://leetcode.com/discuss/58614/java-standard-backtrace-ac-solutoin-short-and-clear\\n\\n        Algorithm:\\n        1. DFS\\n        2. Special handling for multiplication\\n        3. Detect invalid number with leading 0\\'s\\n        :type num: str\\n        :type target: int\\n        :rtype: List[str]\\n        \"\"\"\\n        ret = []\\n        self.dfs(num, target, 0, \"\", 0, 0, ret)\\n        return ret']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def session():\\n    Session = sessionmaker()\\n    engine = create_engine(MYSQL_CONNECTION_STRING)\\n    Session.configure(bind=engine)\\n    metadata.create_all(engine)\\n    try:\\n        yield Session()\\n    except:\\n        pass', 'def patch_sqlalchemy():\\n    mysqldb_hooks.install_patches()\\n    try:\\n        yield\\n    finally:\\n        mysqldb_hooks.reset_patches()', \"def assert_span(span, operation, parent=None):\\n    assert span.operation_name == 'MySQLdb:' + operation\\n    assert span.tags.get(tags.SPAN_KIND) == tags.SPAN_KIND_RPC_CLIENT\\n    if parent:\\n        assert span.parent_id == parent.context.span_id\\n        assert span.context.trace_id == parent.context.trace_id\\n    else:\\n        assert span.parent_id is None\"]}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Deleting model 'Participant'\\n        db.delete_table(u'pa_participant')\\n\\n        # Removing M2M table for field user on 'Participant'\\n        db.delete_table('pa_participant_user')\\n\\n        # Adding M2M table for field user on 'ReportingPeriod'\\n        db.create_table(u'pa_reportingperiod_user', (\\n            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\\n            ('reportingperiod', models.ForeignKey(orm[u'pa.reportingperiod'], null=False)),\\n            ('user', models.ForeignKey(orm[u'pa.user'], null=False))\\n        ))\\n        db.create_unique(u'pa_reportingperiod_user', ['reportingperiod_id', 'user_id'])\"]}, {'features': [], 'snippets': ['def __init__(self, file_name=None):\\n        self._file_name = file_name\\n        self._puzzle_input = None\\n        self._solved_output = (\\n            \"The text file had {0} nice strings using the original rules\\\\n\"\\n            \"and it had {1} nice strings using the new rules.\"\\n        )\\n        self.__regex_vowels = re.compile(\\'[aeiou]\\')\\n        self.__regex_double_char = re.compile(\\'(\\\\w)\\\\\\\\1+\\')\\n        self.__regex_naughty = re.compile(\\'ab|cd|pq|xy\\')\\n        self.__regex_double_pair = re.compile(\\'(\\\\w{2})\\\\w*\\\\\\\\1\\')\\n        self.__regex_triplet = re.compile(\\'(\\\\w)\\\\w\\\\\\\\1\\')', 'def __is_nice_string_using_old_rules(self, string):\\n        return (self.__regex_naughty.search(string) is None\\n            and len(self.__regex_vowels.findall(string)) > 2\\n            and self.__regex_double_char.search(string))', 'def _solve_puzzle_parts(self):\\n        old_nice_count = 0\\n        new_nice_count = 0\\n        for string in self._puzzle_input:\\n            if not string:\\n                continue\\n            if self.__is_nice_string_using_old_rules(string):\\n                old_nice_count += 1\\n            if self.__is_nice_string_using_new_rules(string):\\n                new_nice_count += 1\\n        return (old_nice_count, new_nice_count)', 'def _run_test_case(self, test_case):\\n        correct_output = self._solved_output.format(\\n            test_case.expected1,\\n            test_case.expected2\\n        )\\n        test_output = self.get_puzzle_solution(test_case.input)\\n        if correct_output == test_output:\\n            print(\"Test passed for input \\'{0}\\'\".format(test_case.input))\\n        else:\\n            print(\"Test failed for input \\'{0}\\'\".format(test_case.input))\\n            print(test_output)']}, {'features': [], 'snippets': ['def __init__(self,\\n                 total,\\n                 prefix=\\'\\',\\n                 suffix=\\'\\',\\n                 decimals=0,\\n                 bar_length=60,\\n                 keep_alive=False,\\n                 display_time=False):\\n        \"\"\"\\n        Creates a new progress bar using the given information.\\n        :param total:                       The total number of iteration for this progress bar.\\n        :param prefix:                      [Optional] The text that should be displayed at the left side of the\\n                                            progress bar. Note that progress bars will always stay left-aligned at the\\n                                            shortest possible.\\n        :param suffix:                      [Optional] The text that should be displayed at the very right side of the\\n                                            progress bar.\\n        :param decimals:                    [Optional] The number of decimals to display for the percentage.\\n        :param bar_length:                  [Optional] The graphical bar size displayed on screen. Unit is character.\\n        :param keep_alive:                  [Optional] Specify whether the progress bar should stay displayed forever\\n                                            once completed or if it should vanish.\\n        :param display_time:                [Optional] Specify whether the duration since the progress has begun should\\n                                            be displayed. Running time will be displayed between parenthesis, whereas it\\n                                            will be displayed between brackets when the progress has completed.\\n        \"\"\"\\n        super(TaskProgress, self).__init__()\\n\\n        self.progress = 0\\n\\n        # Minimum number of seconds at maximum completion before a progress bar is removed from display\\n        # The progress bar may vanish at a further time as the redraw rate depends upon chrono AND method calls\\n        self.timeout_chrono = None\\n        self.begin_time = None\\n        self.end_time = None\\n        self.elapsed_time_at_end = None\\n\\n        # Graphics related information\\n        self.keep_alive = keep_alive\\n        self.display_time = display_time\\n\\n        self.total = total\\n        self.prefix = prefix\\n        self.suffix = suffix\\n        self.decimals = decimals\\n        self.bar_length = bar_length', 'def __init__(self,\\n                 message_number=default_message_number,\\n                 exception_number=default_exception_number,\\n                 permanent_progressbar_slots=default_permanent_progressbar_slots,\\n                 redraw_frequency_millis=default_redraw_frequency_millis,\\n                 console_level=default_level,\\n                 task_millis_to_removal=default_task_millis_to_removal,\\n                 console_format_strftime=default_console_format_strftime,\\n                 console_format=default_console_format,\\n                 file_handlers=None,\\n                 application_name=None):\\n        \"\"\"\\n        Initializes a new logger and starts its process immediately using given configuration.\\n        :param message_number:              [Optional] Number of simultaneously displayed messages below progress bars.\\n        :param exception_number:            [Optional] Number of simultaneously displayed exceptions below messages.\\n        :param permanent_progressbar_slots: [Optional] The amount of vertical space (bar slots) to keep at all times,\\n                                            so the message logger will not move anymore if the bar number is equal or\\n                                            lower than this parameter.\\n        :param redraw_frequency_millis:     [Optional] Minimum time lapse in milliseconds between two redraws. It may be\\n                                            more because the redraw rate depends upon time AND method calls.\\n        :param console_level:               [Optional] The logging level (from standard logging module).\\n        :param task_millis_to_removal:      [Optional] Minimum time lapse in milliseconds at maximum completion before\\n                                            a progress bar is removed from display. The progress bar may vanish at a\\n                                            further time as the redraw rate depends upon time AND method calls.\\n        :param console_format_strftime:     [Optional] Specify the time format for console log lines using python\\n                                            strftime format. Defaults to format: \\'29 november 2016 21:52:12\\'.\\n        :param console_format:              [Optional] Specify the format of the console log lines. There are two\\n                                            variables available: {T} for timestamp, {L} for level. Will then add some\\n                                            tabulations in order to align text beginning for all levels.\\n                                            Defaults to format: \\'{T} [{L}]\\'\\n                                            Which will produce: \\'29 november 2016 21:52:12 [INFO]      my log text\\'\\n                                                                \\'29 november 2016 21:52:13 [WARNING]   my log text\\'\\n                                                                \\'29 november 2016 21:52:14 [DEBUG]     my log text\\'\\n        :param file_handlers:               [Optional] Specify the file handlers to use. Each file handler will use its\\n                                            own regular formatter and level. Console logging is distinct from file\\n                                            logging. Console logging uses custom stdout formatting, while file logging\\n                                            uses regular python logging rules. All handlers are permitted except\\n                                            StreamHandler if used with stdout or stderr which are reserved by this\\n                                            library for custom console output.\\n        :param application_name:            [Optional] Used only if \\'file_handlers\\' parameter is ignored. Specifies the\\n                                            application name to use to format the default file logger using format:\\n                                            application_%Y-%m-%d_%H-%M-%S.log\\n        \"\"\"\\n        super(FancyLogger, self).__init__()\\n\\n        # Define default file handlers\\n        if not file_handlers:\\n            if not application_name:\\n                app_name = \\'application\\'\\n            else:\\n                app_name = application_name\\n\\n            handler = RotatingFileHandler(filename=os.path.join(os.getcwd(), \\'{}_{}.log\\'\\n                                                                .format(app_name, strftime(\\'%Y-%m-%d_%H-%M-%S\\'))),\\n                                          encoding=\\'utf8\\',\\n                                          maxBytes=5242880,  # 5 MB\\n                                          backupCount=10,\\n                                          delay=True)\\n            handler.setLevel(logging.INFO)\\n            handler.setFormatter(fmt=Formatter(fmt=\\'%(asctime)s [%(levelname)s]\\\\t%(message)s\\',\\n                                               datefmt=self.default_console_format_strftime))\\n            self.default_file_handlers.append(handler)\\n\\n            file_handlers = self.default_file_handlers\\n\\n        if not self.queue:\\n            self.queue = Queue()\\n            self.process = MultiprocessingLogger(queue=self.queue,\\n                                                 console_level=console_level,\\n                                                 message_number=message_number,\\n                                                 exception_number=exception_number,\\n                                                 permanent_progressbar_slots=permanent_progressbar_slots,\\n                                                 redraw_frequency_millis=redraw_frequency_millis,\\n                                                 task_millis_to_removal=task_millis_to_removal,\\n                                                 console_format_strftime=console_format_strftime,\\n                                                 console_format=console_format,\\n                                                 file_handlers=file_handlers)\\n            self.process.start()', 'def terminate(self):\\n        \"\"\"\\n        Tells the logger process to exit immediately. If you do not call \\'flush\\' method before, you may lose some\\n        messages of progresses that have not been displayed yet. This method blocks until logger process has stopped.\\n        \"\"\"\\n        self.queue.put(dill.dumps(ExitCommand()))\\n\\n        if self.process:\\n            self.process.join()', 'def set_level(self,\\n                  level,\\n                  console_only=False):\\n        \"\"\"\\n        Defines the logging level (from standard logging module) for log messages.\\n        :param level:           Level of logging for the file logger.\\n        :param console_only:    [Optional] If True then the file logger will not be affected.\\n        \"\"\"\\n        self.queue.put(dill.dumps(SetLevelCommand(level=level,\\n                                                  console_only=console_only)))', 'def set_task(self,\\n                 task_id,\\n                 total,\\n                 prefix,\\n                 suffix=\\'\\',\\n                 decimals=0,\\n                 bar_length=60,\\n                 keep_alive=False,\\n                 display_time=False):\\n        \"\"\"\\n        Defines a new progress bar with the given information.\\n        :param task_id:         Unique identifier for this progress bar. Will erase if already existing.\\n        :param total:           The total number of iteration for this progress bar.\\n        :param prefix:          The text that should be displayed at the left side of the progress bar. Note that\\n                                progress bars will always stay left-aligned at the shortest possible.\\n        :param suffix:          [Optional] The text that should be displayed at the very right side of the progress bar.\\n        :param decimals:        [Optional] The number of decimals to display for the percentage.\\n        :param bar_length:      [Optional] The graphical bar size displayed on screen. Unit is character.\\n        :param keep_alive:      [Optional] Specify whether the progress bar should stay displayed forever once completed\\n                                or if it should vanish.\\n        :param display_time:    [Optional] Specify whether the duration since the progress has begun should be\\n                                displayed. Running time will be displayed between parenthesis, whereas it will be\\n                                displayed between brackets when the progress has completed.\\n        \"\"\"\\n        self.queue.put(dill.dumps(NewTaskCommand(task_id=task_id,\\n                                                 task=TaskProgress(total,\\n                                                                   prefix,\\n                                                                   suffix,\\n                                                                   decimals,\\n                                                                   bar_length,\\n                                                                   keep_alive,\\n                                                                   display_time))))', 'def debug(self, text):\\n        \"\"\"\\n        Posts a debug message adding a timestamp and logging level to it for both file and console handlers.\\n        Logger uses a redraw rate because of console flickering. That means it will not draw new messages or progress\\n        at the very time they are being logged but their timestamp will be captured at the right time. Logger will\\n        redraw at a given time period AND when new messages or progress are logged. If you still want to force redraw\\n        immediately (may produce flickering) then call \\'flush\\' method.\\n        :param text: The text to log into file and console.\\n        \"\"\"\\n        self.queue.put(dill.dumps(LogMessageCommand(text=text, level=logging.DEBUG)))', 'def warning(self, text):\\n        \"\"\"\\n        Posts a warning message adding a timestamp and logging level to it for both file and console handlers.\\n        Logger uses a redraw rate because of console flickering. That means it will not draw new messages or progress\\n        at the very time they are being logged but their timestamp will be captured at the right time. Logger will\\n        redraw at a given time period AND when new messages or progress are logged. If you still want to force redraw\\n        immediately (may produce flickering) then call \\'flush\\' method.\\n        :param text: The text to log into file and console.\\n        \"\"\"\\n        self.queue.put(dill.dumps(LogMessageCommand(text=text, level=logging.WARNING)))', 'def critical(self, text):\\n        \"\"\"\\n        Posts a critical message adding a timestamp and logging level to it for both file and console handlers.\\n        Logger uses a redraw rate because of console flickering. That means it will not draw new messages or progress\\n        at the very time they are being logged but their timestamp will be captured at the right time. Logger will\\n        redraw at a given time period AND when new messages or progress are logged. If you still want to force redraw\\n        immediately (may produce flickering) then call \\'flush\\' method.\\n        :param text: The text to log into file and console.\\n        \"\"\"\\n        self.queue.put(dill.dumps(LogMessageCommand(text=text, level=logging.CRITICAL)))', 'def progress(self,\\n                 enumerable,\\n                 task_progress_object=None):\\n        \"\"\"\\n        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.\\n        :param enumerable:              Collection to iterate over.\\n        :param task_progress_object:    [Optional] TaskProgress object holding the progress bar information.\\n        :return:                        The logger instance.\\n        \"\"\"\\n        self.list = enumerable\\n        self.list_length = len(enumerable)\\n        self.task_id = uuid.uuid4()\\n        self.index = 0\\n\\n        if task_progress_object:\\n            # Force total attribute\\n            task_progress_object.total = self.list_length\\n        else:\\n            task_progress_object = TaskProgress(total=self.list_length,\\n                                                display_time=True,\\n                                                prefix=\\'Progress\\')\\n\\n        # Create a task progress\\n        self.set_task_object(task_id=self.task_id,\\n                             task_progress_object=task_progress_object)\\n\\n        return self', 'def __next__(self):\\n        \"\"\"\\n        Enables the object to be used as an iterator. Each iteration will produce a progress update in the logger.\\n        :return: The current object of the iterator.\\n        \"\"\"\\n        if self.index >= self.list_length:\\n            raise StopIteration\\n        else:\\n            self.index += 1\\n            self.update(task_id=self.task_id,\\n                        progress=self.index)\\n\\n            return self.list[self.index - 1]']}, {'features': [], 'snippets': ['def findForce(system, forcetype, add=True):\\n  \"\"\" Finds a specific force in the system force list - added if not found.\"\"\"\\n  for force in system.getForces():\\n    if isinstance(force, forcetype):\\n      return force\\n  if add==True:\\n    system.addForce(forcetype())\\n    return findForce(system, forcetype)\\n  return None', 'def atomIndexInResidue(residue):\\n  \"\"\" list of atom index in residue \"\"\"\\n  index=[]\\n  for a in list(residue.atoms()):\\n    index.append(a.index)\\n  return index', 'def uniquePairs(index):\\n  \"\"\" list of unique, internal pairs \"\"\"\\n  return list(combinations( range(index[0],index[-1]+1),2 ) )', 'def addExclusions(nonbondedforce, pairlist):\\n  \"\"\" add nonbonded exclusions between pairs \"\"\"\\n  for i,j in pairlist:\\n    nonbondedforce.addExclusion(i,j)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def brushSize(size=-1):\\n    \"\"\"\"\\n    Set brush size\\n    \"\"\"\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if size < 1:\\n        size = random.randrange(2, ((image.height + image.width) / 8))\\n    gimp_be.pdb.gimp_context_set_brush_size(size)', 'def brushOpacity(op=-1):\\n    if op == -1:\\n        op = random.randrange(15, 100)\\n    gimp_be.pdb.gimp_brushes_set_opacity(op)\\n    return op', 'def brushColor(r1=-1, g1=-1, b1=-1, r2=-1, g2=-1, b2=-1):\\n    if not r1 == -1:\\n        gimp_be.pdb.gimp_context_set_foreground((r1, g1, b1))\\n    if not r2 == -1:\\n        gimp_be.pdb.gimp_context_set_background((r2, g2, b2))\\n    elif r1 == -1:\\n        r1 = random.randrange(0, 255)\\n        g1 = random.randrange(0, 255)\\n        b1 = random.randrange(0, 255)\\n        r2 = random.randrange(0, 255)\\n        g2 = random.randrange(0, 255)\\n        b2 = random.randrange(0, 255)\\n        gimp_be.pdb.gimp_context_set_foreground((r1, g1, b1))\\n        gimp_be.pdb.gimp_context_set_background((r2, g2, b2))\\n    return (r1, g1, b1, r2, g2, b2)', 'def grayColor(gray_color):\\n    gimp_be.pdb.gimp_context_set_foreground((gray_color, gray_color, gray_color))', \"def randomBrush():\\n    num_brushes, brush_list = gimp_be.pdb.gimp_brushes_get_list('')\\n    brush_pick = brush_list[random.randrange(0, len(brush_list))]\\n    gimp_be.pdb.gimp_brushes_set_brush(brush_pick)\\n    return brush_pick\", \"def randomDynamics():\\n    dynamics_pick = random.choice(gimp_be.pdb.gimp_dynamics_get_list('')[1])\\n    gimp_be.pdb.gimp_context_set_dynamics(dynamics_pick)\\n    return dynamics_pick\", 'def drawLine(points):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    gimp_be.pdb.gimp_paintbrush_default(drawable, len(points), points)', 'def drawRays(rays=32, rayLength=100, centerX=0, centerY=0):\\n    \"\"\"\"\\n    draw N rays from center in active drawable with current brush\\n    \"\"\"\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if centerX == 0:\\n        centerX = image.width/2\\n    if centerY == 0:\\n        centerY = image.height/2\\n    ray_gap = int(360.0/rays)\\n    for ray in range(0,rays):\\n        ctrlPoints = centerX, centerY, centerX + rayLength * math.sin(math.radians(ray*ray_gap)), centerY + rayLength * math.cos(math.radians(ray*ray_gap))\\n        drawLine(ctrlPoints)', 'def spikeBallStack(depth=20, layer_mode=6, flatten=0):\\n    for x in range(1,depth):\\n        image = gimp_be.gimp.image_list()[0]\\n        drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n        qL()\\n        gimp_be.pdb.gimp_layer_set_mode(gimp_be.pdb.gimp_image_get_active_layer(image), layer_mode)\\n        drawRandomRays(rays=random.choice([32,64,128,4]), length=(image.height/2-image.height/12), centerX=image.width/2, centerY=image.height/2,noise=random.choice([0.3,0.1,0.8]))\\n        if flatten:\\n            if not x%flatten:\\n                gimp_be.pdb.gimp_image_flatten(image)', 'def drawBars(barNum=10, opt=3):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    barWidth =image.width/ barNum\\n    barLeft = 0\\n    color = -1\\n    for loopNum in range(0, barNum):\\n        gimp_be.pdb.gimp_image_select_rectangle(image, 2, barLeft, 0, barWidth, image.height)\\n        barLeft = barLeft + barWidth\\n        if opt == 3:\\n            randomBlend()\\n        elif opt == 2:\\n            color = brushColor()\\n            gimp_be.pdb.gimp_edit_bucket_fill_full(drawable, 0, 0, 100, 0, 1, 0, gimp_be.SELECT_CRITERION_COMPOSITE, 0, 0)\\n        else:\\n            gimp_be.pdb.gimp_edit_bucket_fill_full(drawable, 0, 0, 100, 0, 1, 0, gimp_be.SELECT_CRITERION_COMPOSITE, 0, 0)\\n    gimp_be.pdb.gimp_selection_none(image)\\n    return (barNum, opt, color)', 'def drawCNT():\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    drawSinWave(1, 4, image.height * .42, 0, image.height / 2)\\n    gimp_be.pdb.gimp_paintbrush(drawable, 0, 4, (0, (image.height - 80),image.width, (image.height - 80)), 0, 0)\\n    gimp_be.pdb.gimp_paintbrush(drawable, 0, 4, (0, 80,image.width, 80), 0, 0)', 'def drawSinWave(bar_space=32, bar_length=-1, mag=70, x_offset=-1, y_offset=-1):\\n    image = gimp_be.gimp.image_list()[0]\\n    if y_offset == -1:\\n        y_offset = image.height/2\\n    if x_offset == -1:\\n        x_offset = 0\\n    if bar_length == -1:\\n        bar_length = image.height/6\\n    steps = image.width / bar_space\\n    x = 0\\n    for cStep in range(0, steps):\\n        x = cStep * bar_space + x_offset\\n        y = int(round(math.sin(x) * mag) + y_offset)\\n        ctrlPoints = x, int(y - round(bar_length / 2)), x, int(y + round(bar_length / 2))\\n        drawLine(ctrlPoints)', 'def drawSinWaveDouble(barSpace, barLen, mag):\\n    image = gimp_be.gimp.image_list()[0]\\n    steps =image.width/ barSpace\\n    x = 0\\n    for cStep in range(1, steps):\\n        x = cStep * barSpace\\n        y = int(abs(round(math.sin(x) * mag + image.height / 2)))\\n        ctrlPoints = x, int(y - round(barLen / 2)), x, int(y + round(barLen / 2))\\n        drawLine(ctrlPoints)', 'def drawBrush(x1, y1):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    ctrlPoints = (x1, y1, x1, y1)\\n    drawLine(ctrlPoints)', 'def drawMultiBrush(brush_strokes=24):\\n    image = gimp_be.gimp.image_list()[0]\\n    grid_width=image.width/int(math.sqrt(brush_strokes))\\n    grid_height=image.height/int(math.sqrt(brush_strokes))\\n    coord_x=0\\n    coord_y = 0\\n    for i in range(0, int(math.sqrt(brush_strokes))):\\n        coord_x = coord_x + grid_width\\n        for x in range(0, int(math.sqrt(brush_strokes))):\\n            coord_y = coord_y + grid_height\\n            drawBrush(coord_x, coord_y)\\n        coord_y = 0', 'def dotGrid():\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    for i in range(10,image.width-10,20):\\n        for x in range(10, image.height-10,20):\\n            grayColor(abs(i^3-x^3)%256)\\n            drawBrush(i+10,x+10)', 'def randomCircleFill(num=20, size=100, opt=3, sq=1):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    for loopNum in range(0, num):\\n        cirPar = [random.randrange(0,image.width), random.randrange(0, image.height), random.randrange(10, size),\\n                  random.randrange(10, size)]\\n        if opt % 2 == 0:\\n            brushColor()\\n        if sq:\\n            gimp_be.pdb.gimp_ellipse_select(image, cirPar[0], cirPar[1], cirPar[2], cirPar[2], 2, 1, 0, 0)\\n        else:\\n            gimp_be.pdb.gimp_ellipse_select(image, cirPar[0], cirPar[1], cirPar[2], cirPar[3], 2, 1, 0, 0)\\n        if opt % 3 == 3:\\n            randomBlend()\\n        else:\\n            gimp_be.pdb.gimp_edit_bucket_fill_full(drawable, 0, 0, 100, 0, 1, 0, gimp_be.SELECT_CRITERION_COMPOSITE, 0, 0)\\n    gimp_be.pdb.gimp_selection_none(image)', 'def randomBlend():\\n    # Random Blend tool test\\n    blend_mode = 0\\n    paint_mode = 0\\n    gradient_type = random.randrange(0, 10)\\n    opacity = random.randrange(20, 100)\\n    offset = 0\\n    repeat = random.randrange(0, 2)\\n    reverse = 0\\n    supersample = 0\\n    max_depth = random.randrange(1, 9)\\n    threshold = 0\\n    threshold = random.randrange(0, 1)\\n    dither = 0\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    brushColor()\\n    x1 = random.randrange(0,image.width)\\n    y1 = random.randrange(0, image.height)\\n    x2 = random.randrange(0,image.width)\\n    y2 = random.randrange(0, image.height)\\n    gimp_be.pdb.gimp_blend(drawable, blend_mode, paint_mode, gradient_type, opacity, offset, repeat, reverse, supersample, max_depth, threshold, dither, x1, y1, x2, y2)', \"def drawInkBlot(option=''):\\n    image=gimp_be.gimp.image_list()[0]\\n    layer=gimp_be.pdb.gimp_image_get_active_layer(image)\\n    if 'trippy' in option:\\n        layer_copy = gimp_be.pdb.gimp_layer_copy(layer, 0)\\n        gimp_be.pdb.gimp_image_add_layer(image, layer_copy,1)\\n        randomBlend()\\n        mask = gimp_be.pdb.gimp_layer_create_mask(layer,5)\\n        gimp_be.pdb.gimp_image_add_layer_mask(image, layer,mask)\\n        editLayerMask(1)\\n    randomCircleFill(num=15,size=800)\\n    brushColor(255,255,255)\\n    randomCircleFill(num=50,size=100)\\n    randomCircleFill(num=5,size=300)\\n    brushColor(0)\\n    randomCircleFill(num=20,size=600)\\n    randomCircleFill(num=50,size=400)\\n    randomCircleFill(num=100,size=100)\\n    brushColor(255,255,255)\\n    randomCircleFill(num=50,size=100)\\n    brushColor(0)\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    brushSize()\\n    strokes=[random.randrange(0,image.width/2),random.randrange(0,image.height),random.randrange(0,image.width/2),random.randrange(0,image.height)]\\n    gimp_be.pdb.gimp_smudge(drawable, random.choice([1,5,10,50,100]), len(strokes), strokes)\\n    brushSize()\\n    strokes=[random.randrange(0,image.width/2),random.randrange(0,image.height),random.randrange(0,image.width/2),random.randrange(0,image.height)]\\n    gimp_be.pdb.gimp_smudge(drawable, random.choice([1,5,10,50,100]), len(strokes), strokes)\\n    mirror('h')\\n    if 'trippy' in option and random.choice([0,1]):\\n        drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n        gimp_be.pdb.gimp_invert(drawable)\\n        editLayerMask(0)\", 'def gridCenters(grid=[]):\\n    if grid==[]:\\n        grid=[4,3]\\n    image = gimp_be.gimp.image_list()[0]\\n    row_width = image.width/(grid[0])\\n    columb_height = image.height/(grid[1])\\n    tile_centers = [] \\n    for row in range(0,grid[0]):\\n        for columb in range(0,grid[1]):\\n            tile_centers.append([row_width*row+row_width/2,columb_height*columb+columb_height/2])\\n    return tile_centers', \"def drawAkuTree(branches=6,tree_height=0, position=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if position==0:\\n        position=[]\\n        position.append(random.randrange(image.width))\\n        position.append(random.randrange(4*tree_height/3, 3*image.height/4))\\n    if tree_height == 0:\\n        tree_height=random.randrange(position[1]/3, position[1]-position[1]/25)\\n    print 'position:' + str(position)\\n    #draw trunk\\n    trunk=[position[0],position[1],position[0],position[1]-tree_height]\\n    trunk_size=tree_height/40+3\\n    print str(trunk)\\n    print 'tree_height: ' + str(tree_height)\\n    print 'trunk size: ' + str(trunk_size)\\n    brushSize(trunk_size)\\n    drawLine(trunk)\\n    for node in range(branches):\\n        node_base=[position[0],position[1]-((node*tree_height+1)/branches+tree_height/25+random.randrange(-1*tree_height/12,tree_height/12))]\\n        base_length=tree_height/25\\n        node_end=[]\\n        if node%2==0:\\n            node_end=[node_base[0]+base_length/2,node_base[1]-base_length/2]\\n            brushSize(2*trunk_size/3)\\n            drawLine([node_base[0],node_base[1],node_end[0],node_end[1]])\\n            brushSize(trunk_size/3)\\n            drawLine([node_end[0],node_end[1],node_end[0],node_end[1]-tree_height/12-(tree_height/48)])\\n        else:\\n            node_end=[node_base[0]-base_length/2,node_base[1]-base_length/2]\\n            brushSize(2*trunk_size/3)\\n            drawLine([node_base[0],node_base[1],node_end[0],node_end[1]])\\n            brushSize(trunk_size/3)\\n            drawLine([node_end[0],node_end[1],node_end[0],node_end[1]-(tree_height/12)])\", 'def drawTree(x1=-1, y1=-1, angle=270, depth=9, recursiondepth=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if x1 == -1:\\n        x1 = image.width/2\\n    if y1 == -1:\\n        y1 = image.height/2\\n    x2 = x1 + int(math.cos(math.radians(angle)) * depth * 10.0)\\n    y2 = y1 + int(math.sin(math.radians(angle)) * depth * 10.0)\\n    ctrlPoints = (x1, y1, x2, y2)\\n    if recursiondepth <= 2:\\n        brushColor(87, 53, 12)\\n    elif depth == 1:\\n        brushColor(152, 90, 17)\\n    elif depth <= 3:\\n        brushColor(7, 145, 2)\\n    brushSize(depth * 4 + 5)\\n    gimp_be.pdb.gimp_paintbrush_default(drawable, len(ctrlPoints), ctrlPoints)\\n    if depth > 0:\\n        drawTree(x2, y2, angle - 20, depth - 1, recursiondepth + 1)\\n        drawTree(x2, y2, angle + 20, depth - 1, recursiondepth + 1)', 'def drawTriTree(x1=-1, y1=-1, angle=270, depth=6, recursiondepth=0, size=10):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if x1 == -1:\\n        x1 = image.width/2\\n    if y1 == -1:\\n        y1 = image.height/2\\n    if depth:\\n        x2 = x1 + int(math.cos(math.radians(angle)) * depth * size) + random.randrange(-12, 12)\\n        y2 = y1 + int(math.sin(math.radians(angle)) * depth * size) + random.randrange(-12, 12)\\n        ctrlPoints = (x1, y1, x2, y2)\\n        brushSize(depth + int(size/10))\\n        brushColor()\\n        gimp_be.pdb.gimp_paintbrush_default(drawable, len(ctrlPoints), ctrlPoints)\\n        drawTriTree(x2, y2, angle - 30, depth - 1, recursiondepth + 1,size)\\n        drawTriTree(x2, y2, angle, depth - 1, recursiondepth + 1,size)\\n        drawTriTree(x2, y2, angle + 30, depth - 1, recursiondepth + 1,size)', 'def drawColorTriTree(x1=-1, y1=-1, angle=270, depth=9, recursiondepth=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if x1 == -1:\\n        x1 = image.width/2\\n    if y1 == -1:\\n        y1 = image.height/2\\n    brushSize(depth + 1)\\n    if depth:\\n        x2 = x1 + int(math.cos(math.radians(angle)) * depth * 10.0) + random.randrange(-12, 12)\\n        y2 = y1 + int(math.sin(math.radians(angle)) * depth * 10.0) + random.randrange(-12, 12)\\n        ctrlPoints = (x1, y1, x2, y2)\\n        gimp_be.pdb.gimp_paintbrush_default(drawable, len(ctrlPoints), ctrlPoints)\\n        drawColorTriTree(x2, y2, angle - 20 + random.choice(-10, -5, 0, 5, 10), depth - 1, recursiondepth + 1)\\n        drawColorTriTree(x2, y2, angle + random.choice(-10, -5, 0, 5, 10), depth - 1, recursiondepth + 1)\\n        drawColorTriTree(x2, y2, angle + 20 + random.choice(-10, -5, 0, 5, 10), depth - 1, recursiondepth + 1)', 'def drawOddTree(x1=-1, y1=-1, angle=270, depth=9, recursiondepth=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if x1 == -1:\\n        x1 = image.width/2\\n    if y1 == -1:\\n        y1 = image.height/2\\n    brushSize((depth * 8 + 30))\\n    if depth:\\n        x2 = x1 + int(math.cos(math.radians(angle)) * depth * 10.0)\\n        y2 = y1 + int(math.sin(math.radians(angle)) * depth * 10.0)\\n        ctrlPoints = (x1, y1, x2, y2)\\n        gimp_be.pdb.gimp_paintbrush_default(drawable, len(ctrlPoints), ctrlPoints)\\n        if not random.randrange(0, 23) == 23:\\n            drawTree(x2, y2, angle - 20, depth - 1, recursiondepth + 1)\\n            if depth % 2 == 0:\\n                drawTree(x2, y2, angle + 20, depth - 1, recursiondepth + 1)\\n            if (depth + 1) % 4 == 0:\\n                drawTree(x2, y2, angle + 20, depth - 1, recursiondepth + 1)\\n            if depth == 5:\\n                drawTree(x2, y2, angle - 45, depth - 1, recursiondepth + 1)\\n                drawTree(x2, y2, angle + 45, depth - 1, recursiondepth + 1)', 'def drawForestTree(x1=-1, y1=-1, angle=270, depth=7, size=10, recursiondepth=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if x1 == -1:\\n        x1 = image.width/2\\n    if y1 == -1:\\n        y1 = image.height/2\\n    if depth:\\n        x2 = x1 + int(math.cos(math.radians(angle)) * depth * 10.0)\\n        y2 = y1 + int(math.sin(math.radians(angle)) * depth * 10.0)\\n        ctrlPoints = (x1, y1, x2, y2)\\n        brushSize(depth * depth * (int(size / ((image.height - y1)) / image.height)) + 4)\\n        gimp_be.pdb.gimp_paintbrush_default(drawable, len(ctrlPoints), ctrlPoints)\\n        if not random.randrange(0, 23) == 23:\\n            drawForestTree(x2, y2, angle - 20, depth - 1, size, recursiondepth + 1)\\n            if random.randrange(0, 23) == 23:\\n                drawForestTree(x2, y2, angle - random.randrange(-30, 30), depth - 1, size, recursiondepth + 1)\\n                drawForestTree(x2, y2, angle - random.randrange(-30, 30), depth - 1, size, recursiondepth + 1)\\n                drawForestTree(x2, y2, angle - random.randrange(-30, 30), depth - 1, size, recursiondepth + 1)\\n            else:\\n                drawForestTree(x2, y2, angle - random.randrange(15, 50), depth - 1, size, recursiondepth + 1)\\n                if depth % 2 == 0:\\n                    drawForestTree(x2, y2, angle + 20, depth - 1, size, recursiondepth + 1)\\n                if (depth + 1) % 4 == 0:\\n                    drawForestTree(x2, y2, angle + 20, depth - 1, size, recursiondepth + 1)\\n                if depth == 5:\\n                    drawForestTree(x2, y2, angle - 45, depth - 1, size, recursiondepth + 1)\\n                    drawForestTree(x2, y2, angle + 45, depth - 1, size, recursiondepth + 1)', 'def drawForest(trees, options):\\n    image = gimp_be.gimp.image_list()[0]\\n    for tree in range(0, trees):\\n        y1 = 2 * (image.height / 3) + random.randrange(-1 * (image.height / 5), image.height / 5)\\n        x1 = random.randrange(image.width / 20, 19 * (image.width / 20))\\n        angle = random.randrange(250, 290)\\n        size = (y1 / (2.0 * (image.height / 3.0) + (image.height / 5.0))) + 4\\n        depth = random.randrange(3, 7)\\n        drawForestTree(x1, y1, angle, depth, size)', 'def drawPolygon(sides=5,size=300,x_pos=0,y_pos=0, angle_offset=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if y_pos==0:\\n        y_pos=image.height/2\\n        if x_pos==0:\\n            x_pos=image.width/2\\n    degree_between_points=360/sides\\n    points_list=[]\\n    for x in range(0,sides+1):\\n        point_degree=degree_between_points*x+angle_offset\\n        points_list.append(int(round(math.sin(math.radians(point_degree))*size))+x_pos)\\n        points_list.append(int(round(math.cos(math.radians(point_degree))*size))+y_pos)\\n    fade_out=0\\n    method=0\\n    gradient_length=0\\n    gimp_be.pdb.gimp_paintbrush(drawable, fade_out, len(points_list), points_list, method, gradient_length)', 'def drawPolygonGrid(size=60,sides=3, angle_offset=0):\\n    image = gimp_be.gimp.image_list()[0]\\n    drawable = gimp_be.pdb.gimp_image_active_drawable(image)\\n    if sides%2 == 1 or sides>4:\\n        for y in range(0-image.height/10,image.height+image.height/10, size):\\n            x_loop=0\\n            for x in range(0-image.width/10, image.width+image.width/10, size):\\n                if x_loop%2==1:\\n                    drawPolygon(sides,size-size/2,x-(size/2),y,360/sides)\\n                else:\\n                    drawPolygon(sides,size-size/2,x,y,0)\\n                x_loop=x_loop+1\\n    else:\\n        for x in range(0-image.height/10,image.height+image.height/10, size):\\n            for y in range(0-image.width/10, image.width+image.width/10, size):\\n                drawPolygon(sides,size/3,x,y,0)\\n    degree_between_points=360/sides\\n    points_list=[]\\n    for x in range(0,sides+1):\\n        point_degree=math.radians(degree_between_points*x+angle_offset)\\n        points_list.append(int(round(math.sin(point_degree)*size)))\\n        points_list.append(int(round(math.cos(point_degree)*size)))\\n    fade_out=0\\n    method=0\\n    gradient_length=0\\n    gimp_be.pdb.gimp_paintbrush(drawable, fade_out, len(points_list), points_list, method, gradient_length)']}, {'features': [], 'snippets': [\"def __init__(self, **kw):\\n        self.name = kw.get('name', None)\\n        self.ddl = kw.get('ddl', '')\\n        self._default = kw.get('default', None)\\n\\n        self.comment = kw.get('comment', '')\\n        self.nullable = kw.get('nullable', False)\\n        self.updatable = kw.get('updatable', True)\\n        self.insertable = kw.get('insertable', True)\\n\\n        self.unique_key = kw.get('unique_key', False)\\n        self.non_unique_key = kw.get('key', False)\\n        self.primary_key = kw.get('primary_key', False)\\n\\n        self._order = Field._count\\n        Field._count += 1\", 'def default(self):\\n        d = self._default\\n        return d() if callable(d) else d', \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = ''\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'varchar(255)'\\n        super(StringField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = 0\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'bigint'\\n        super(IntegerField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = 0.0\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'real'\\n        super(FloatField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = False\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'bool'\\n        super(BooleanField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = ''\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'text'\\n        super(TextField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if not 'default' in kw:\\n            kw['default'] = ''\\n        if not 'ddl' in kw:\\n            kw['ddl'] = 'blob'\\n        super(BlobField, self).__init__(**kw)\", \"def __init__(self, name=None):\\n        super(VersionField, self).__init__(name=name, default=0, ddl='bigint')\", \"def __init__(self, **kw):\\n        if 'ddl' not in kw:\\n            kw['ddl'] = 'datetime'\\n        super(DateTimeField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if 'ddl' not in kw:\\n            kw['ddl'] = 'date'\\n        super(DateField, self).__init__(**kw)\", \"def __init__(self, **kw):\\n        if 'ddl' not in kw:\\n            kw['ddl'] = 'enum'\\n        super(EnumField, self).__init__(**kw)\", 'def _gen_sql(table_name, mappings):\\n    pk, unique_keys, keys = None, [], []\\n    sql = [\\'-- generating SQL for %s:\\' % table_name, \\'create table `%s` (\\' % table_name]\\n    for f in sorted(mappings.values(), lambda x, y: cmp(x._order, y._order)):\\n        if not hasattr(f, \\'ddl\\'):\\n            raise StandardError(\\'no ddl in field \"%s\".\\' % f)\\n        ddl = f.ddl\\n        nullable = f.nullable\\n        has_comment = not (f.comment == \\'\\')\\n        has_default = f._default is not None\\n        left = nullable and \\'  `%s` %s\\' % (f.name, ddl) or \\'  `%s` %s not null\\' % (f.name, ddl)\\n        mid = has_default and \\' default \\\\\\'%s\\\\\\'\\' % f._default or None\\n        right = has_comment and \\' comment \\\\\\'%s\\\\\\',\\' % f.comment or \\',\\'\\n        line = mid and \\'%s%s%s\\' % (left, mid, right) or \\'%s%s\\' % (left, right)\\n        if f.primary_key:\\n            pk = f.name\\n            line = \\'  `%s` %s not null auto_increment,\\' % (f.name, ddl)\\n        elif f.unique_key:\\n            unique_keys.append(f.name)\\n        elif f.non_unique_key:\\n            keys.append(f.name)\\n        sql.append(line)\\n    for uk in unique_keys:\\n        sql.append(\\'  unique key(`%s`),\\' % uk)\\n    for k in keys:\\n        sql.append(\\'  key(`%s`),\\' % k)\\n    sql.append(\\'  primary key(`%s`)\\' % pk)\\n    sql.append(\\')ENGINE=InnoDB DEFAULT CHARSET=utf8;\\')\\n    return \\'\\\\n\\'.join(sql)', \"def __new__(cls, name, bases, attrs):\\n        # skip base Model class:\\n        if name == 'Model':\\n            return type.__new__(cls, name, bases, attrs)\\n\\n        # store all subclasses info:\\n        if not hasattr(cls, 'subclasses'):\\n            cls.subclasses = {}\\n        if not name in cls.subclasses:\\n            cls.subclasses[name] = name\\n        else:\\n            logging.warning('Redefine class: %s', name)\\n\\n        logging.info('Scan ORMapping %s...', name)\\n        mappings = dict()\\n        primary_key = None\\n        for k, v in attrs.iteritems():\\n            if isinstance(v, Field):\\n                if not v.name:\\n                    v.name = k\\n                logging.debug('Found mapping: %s => %s' % (k, v))\\n                # check duplicate primary key:\\n                if v.primary_key:\\n                    if primary_key:\\n                        raise TypeError('Cannot define more than 1 primary key in class: %s' % name)\\n                    if v.updatable:\\n                        # logging.warning('NOTE: change primary key to non-updatable.')\\n                        v.updatable = False\\n                    if v.nullable:\\n                        # logging.warning('NOTE: change primary key to non-nullable.')\\n                        v.nullable = False\\n                    primary_key = v\\n                mappings[k] = v\\n        # check exist of primary key:\\n        if not primary_key:\\n            raise TypeError('Primary key not defined in class: %s' % name)\\n        for k in mappings.iterkeys():\\n            attrs.pop(k)\\n        if '__table__' not in attrs:\\n            attrs['__table__'] = name.lower()\\n        attrs['__mappings__'] = mappings\\n        attrs['__primary_key__'] = primary_key\\n        attrs['__sql__'] = lambda self: _gen_sql(attrs['__table__'], mappings)\\n        for trigger in _triggers:\\n            if trigger not in attrs:\\n                attrs[trigger] = None\\n        return type.__new__(cls, name, bases, attrs)\", 'def __init__(self, **kw):\\n        super(Model, self).__init__(**kw)', 'def __setattr__(self, key, value):\\n        self[key] = value', 'def get(cls, key_name, key_value):\\n        \"\"\"\\n        Get by primary/unique key.\\n        \"\"\"\\n        d = db.select_one(\\'select * from %s where %s=?\\' % (cls.__table__, key_name), key_value)\\n        if not d:\\n            # TODO: change to logging?\\n            raise AttributeError(\"Can\\'t find in [%s] where %s=[%s]\" % (cls.__table__, key_name, key_value))\\n        return cls(**d) if d else None', 'def find_first(cls, where, *args):\\n        \"\"\"\\n        Find by where clause and return one result. If multiple results found, \\n        only the first one returned. If no result found, return None.\\n        \"\"\"\\n        d = db.select_one(\\'select * from %s %s\\' % (cls.__table__, where), *args)\\n        return cls(**d) if d else None', 'def find_all(cls, *args):\\n        \"\"\"\\n        Find all and return list.\\n        \"\"\"\\n        L = db.select(\\'select * from `%s`\\' % cls.__table__)\\n        return [cls(**d) for d in L]', 'def find_by(cls, cols, where, *args):\\n        \"\"\"\\n        Find by where clause and return list.\\n        \"\"\"\\n        L = db.select(\\'select %s from `%s` %s\\' % (cols, cls.__table__, where), *args)\\n        if cols.find(\\',\\') == -1 and cols.strip() != \\'*\\':\\n            return [d[0] for d in L]\\n        return [cls(**d) for d in L]', 'def count_all(cls):\\n        \"\"\"\\n        Find by \\'select count(pk) from table\\' and return integer.\\n        \"\"\"\\n        return db.select_int(\\'select count(`%s`) from `%s`\\' % (cls.__primary_key__.name, cls.__table__))', 'def count_by(cls, where, *args):\\n        \"\"\"\\n        Find by \\'select count(pk) from table where ... \\' and return int.\\n        \"\"\"\\n        return db.select_int(\\'select count(`%s`) from `%s` %s\\' % (cls.__primary_key__.name, cls.__table__, where), *args)', \"def delete(self):\\n        self.pre_delete and self.pre_delete()\\n        pk = self.__primary_key__.name\\n        args = (getattr(self, pk), )\\n        db.update('delete from `%s` where `%s`=?' % (self.__table__, pk), *args)\\n        return self\"]}, {'features': [], 'snippets': ['def make_author():\\n    return Author(\\n        id=fake.random_int(),\\n        first_name=fake.first_name(),\\n        last_name=fake.last_name(),\\n        twitter=fake.domain_word(),\\n    )', 'def make_comment(with_author=True):\\n    author = make_author() if with_author else None\\n    return Comment(id=fake.random_int(), body=fake.bs(), author=author)', 'def author():\\n    return make_author()', 'def authors():\\n    return [make_author() for _ in range(3)]', 'def comments():\\n    return [make_comment() for _ in range(3)]', 'def post():\\n    return make_post()', 'def post_with_null_comment():\\n    return make_post(with_comments=False)', 'def post_with_null_author():\\n    return make_post(with_author=False)']}, {'features': [], 'snippets': ['def recurPowerNew(base, exp):\\n\\n    # Base case is when exp = 0\\n    if exp <= 0:\\n        return 1']}, {'features': [], 'snippets': ['def setUpClass(cls):\\r\\n        print \"Testing \", cls.__name__', \"def test_user_not_authorized(self):\\r\\n        '''\\r\\n        Check that user in not able to get course list without authenticating.\\r\\n        '''\\r\\n        print '(' + self.test_user_not_authorized.__name__ + ')', \\\\\\r\\n            self.test_user_not_authorized.__doc__\", \"def test_user_authorized(self):\\r\\n        '''\\r\\n        Check that authenticated user is able to get course list.\\r\\n        '''\\r\\n        print '(' + self.test_user_authorized.__name__ + ')', \\\\\\r\\n            self.test_user_authorized.__doc__\", \"def test_course_get(self):\\r\\n        '''\\r\\n        Check data consistency of Course/GET and CourseList/GET.\\r\\n        '''\", \"def _course_get(self, resource_url):\\r\\n        '''\\r\\n        Check data consistency of CourseList/GET.\\r\\n        '''\", \"def test_course_post(self):\\r\\n        '''\\r\\n        Check that a new course can be created.\\r\\n        '''\\r\\n        print '(' + self.test_course_post.__name__ + ')', \\\\\\r\\n            self.test_course_post.__doc__\", \"def test_course_put(self):\\r\\n        '''\\r\\n        Check that an existing course can be modified.\\r\\n        '''\\r\\n        print '(' + self.test_course_put.__name__ + ')', \\\\\\r\\n            self.test_course_put.__doc__\", \"def test_course_delete(self):\\r\\n        '''\\r\\n        Check that course in not able to get course list without authenticating.\\r\\n        '''\\r\\n        print '(' + self.test_course_delete.__name__ + ')', \\\\\\r\\n            self.test_course_delete.__doc__\", \"def test_for_method_not_allowed(self):\\r\\n        '''\\r\\n        For inconsistency check for 405, method not allowed.\\r\\n        '''\", \"def _isIdentical(self, api_item, db_item):\\r\\n        '''\\r\\n        Check whether template data corresponds to data stored in the database.\\r\\n        '''\", \"def _convert(self, template_data):\\r\\n        '''\\r\\n        Convert template data to a dictionary representing the format the data is saved in the database.\\r\\n        '''\", \"def _create_dict(self,item):\\r\\n        '''\\r\\n        Create a dictionary from template data for easier handling.\\r\\n        '''\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def peek(self):\\n\\t\\toldPos = self.tell()\\n\\t\\tb = self.read(1)\\n\\t\\tnewPos = self.tell()\\n\\t\\tif((newPos == (oldPos+1)) and (b != '')):\\n\\t\\t\\tr = ord(b)\\n\\t\\telse:\\n\\t\\t\\tr = None\\n\\n\\t\\tself.seek(oldPos, 0)\\n\\t\\treturn r\", 'def get(self):\\n\\t\\treturn ord(self.read(1))', 'def put(self, b):\\n\\t\\tself.write(chr(b))\\n\\t\\treturn', 'def erase(self):\\n\\t\\tself.truncate(0)\\n\\t\\tself.seek(0, 0)\\n\\t\\treturn', 'def setvalue(self, s):\\n\\t\\tself.erase()\\n\\t\\tself.write(s)\\n\\t\\treturn', 'def enc8(x):\\n\\tif (x > 255):\\n\\t\\traise Exception(\"The integer %d cannot be encoded on 8 bits.\" % x)\\n\\telse:\\n\\t\\treturn x']}, {'features': [], 'snippets': ['def __init__(\\n        self, plotly_name=\"bordercolor\", parent_name=\"sankey.hoverlabel\", **kwargs']}, {'features': [], 'snippets': ['def __init__(self, model, data):\\n        # try and import pytorch\\n        global torch\\n        if torch is None:\\n            import torch\\n            if version.parse(torch.__version__) < version.parse(\"0.4\"):\\n                warnings.warn(\"Your PyTorch version is older than 0.4 and not supported.\")\\n\\n        # check if we have multiple inputs\\n        self.multi_input = False\\n        if type(data) == list:\\n            self.multi_input = True\\n        if type(data) != list:\\n            data = [data]\\n        self.data = data\\n        self.layer = None\\n        self.input_handle = None\\n        self.interim = False\\n        self.interim_inputs_shape = None\\n        self.expected_value = None  # to keep the DeepExplainer base happy\\n        if type(model) == tuple:\\n            self.interim = True\\n            model, layer = model\\n            model = model.eval()\\n            self.layer = layer\\n            self.add_target_handle(self.layer)\\n\\n            # if we are taking an interim layer, the \\'data\\' is going to be the input\\n            # of the interim layer; we will capture this using a forward hook\\n            with torch.no_grad():\\n                _ = model(*data)\\n                interim_inputs = self.layer.target_input\\n                if type(interim_inputs) is tuple:\\n                    # this should always be true, but just to be safe\\n                    self.interim_inputs_shape = [i.shape for i in interim_inputs]\\n                else:\\n                    self.interim_inputs_shape = [interim_inputs.shape]\\n            self.target_handle.remove()\\n            del self.layer.target_input\\n        self.model = model.eval()\\n\\n        self.multi_output = False\\n        self.num_outputs = 1\\n        with torch.no_grad():\\n            outputs = model(*data)\\n\\n            # also get the device everything is running on\\n            self.device = outputs.device\\n            if outputs.shape[1] > 1:\\n                self.multi_output = True\\n                self.num_outputs = outputs.shape[1]\\n            self.expected_value = outputs.mean(0).cpu().numpy()', 'def add_handles(self, model, forward_handle, backward_handle):\\n        \"\"\"\\n        Add handles to all non-container layers in the model.\\n        Recursively for non-container layers\\n        \"\"\"\\n        handles_list = []\\n        model_children = list(model.children())\\n        if model_children:\\n            for child in model_children:\\n                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\\n        else:  # leaves\\n            handles_list.append(model.register_forward_hook(forward_handle))\\n            handles_list.append(model.register_backward_hook(backward_handle))\\n        return handles_list', 'def gradient(self, idx, inputs):\\n        self.model.zero_grad()\\n        X = [x.requires_grad_() for x in inputs]\\n        outputs = self.model(*X)\\n        selected = [val for val in outputs[:, idx]]\\n        grads = []\\n        if self.interim:\\n            interim_inputs = self.layer.target_input\\n            for idx, input in enumerate(interim_inputs):\\n                grad = torch.autograd.grad(selected, input,\\n                                           retain_graph=True if idx + 1 < len(interim_inputs) else None,\\n                                           allow_unused=True)[0]\\n                if grad is not None:\\n                    grad = grad.cpu().numpy()\\n                else:\\n                    grad = torch.zeros_like(X[idx]).cpu().numpy()\\n                grads.append(grad)\\n            del self.layer.target_input\\n            return grads, [i.detach().cpu().numpy() for i in interim_inputs]\\n        else:\\n            for idx, x in enumerate(X):\\n                grad = torch.autograd.grad(selected, x,\\n                                           retain_graph=True if idx + 1 < len(X) else None,\\n                                           allow_unused=True)[0]\\n                if grad is not None:\\n                    grad = grad.cpu().numpy()\\n                else:\\n                    grad = torch.zeros_like(X[idx]).cpu().numpy()\\n                grads.append(grad)\\n            return grads', 'def deeplift_grad(module, grad_input, grad_output):\\n    \"\"\"The backward hook which computes the deeplift\\n    gradient for an nn.Module\\n    \"\"\"\\n    # first, get the module type\\n    module_type = module.__class__.__name__\\n    # first, check the module is supported\\n    if module_type in op_handler:\\n        if op_handler[module_type].__name__ not in [\\'passthrough\\', \\'linear_1d\\']:\\n            return op_handler[module_type](module, grad_input, grad_output)\\n    else:\\n        print(\\'Warning: unrecognized nn.Module: {}\\'.format(module_type))\\n        return grad_input', 'def get_target_input(module, input, output):\\n    \"\"\"A forward hook which saves the tensor - attached to its graph.\\n    Used if we want to explain the interim outputs of a model\\n    \"\"\"\\n    try:\\n        del module.target_input\\n    except AttributeError:\\n        pass\\n    setattr(module, \\'target_input\\', input)', 'def deeplift_tensor_grad(grad):\\n    return_grad = complex_module_gradients[-1]\\n    del complex_module_gradients[-1]\\n    return return_grad', 'def passthrough(module, grad_input, grad_output):\\n    \"\"\"No change made to gradients\"\"\"\\n    return None', 'def linear_1d(module, grad_input, grad_output):\\n    \"\"\"No change made to gradients.\"\"\"\\n    return None']}, {'features': [], 'snippets': [\"def __init__(self, reddit, term, config, oauth, url=None, submission=None):\\n        super(SubmissionPage, self).__init__(reddit, term, config, oauth)\\n\\n        self.controller = SubmissionController(self, keymap=config.keymap)\\n\\n        if url:\\n            self.content = SubmissionContent.from_url(\\n                reddit, url, term.loader,\\n                max_comment_cols=config['max_comment_cols'])\\n        else:\\n            self.content = SubmissionContent(\\n                submission, term.loader,\\n                max_comment_cols=config['max_comment_cols'])\\n        # Start at the submission post, which is indexed as -1\\n        self.nav = Navigator(self.content.get, page_index=-1)\\n        self.selected_subreddit = None\", 'def toggle_comment(self):\\n        \"Toggle the selected comment tree between visible and hidden\"\\n\\n        current_index = self.nav.absolute_index\\n        self.content.toggle(current_index)\\n\\n        # This logic handles a display edge case after a comment toggle. We\\n        # want to make sure that when we re-draw the page, the cursor stays at\\n        # its current absolute position on the screen. In order to do this,\\n        # apply a fixed offset if, while inverted, we either try to hide the\\n        # bottom comment or toggle any of the middle comments.\\n        if self.nav.inverted:\\n            data = self.content.get(current_index)\\n            if data[\\'hidden\\'] or self.nav.cursor_index != 0:\\n                window = self._subwindows[-1][0]\\n                n_rows, _ = window.getmaxyx()\\n                self.nav.flip(len(self._subwindows) - 1)\\n                self.nav.top_item_height = n_rows', 'def exit_submission(self):\\n        \"Close the submission and return to the subreddit page\"\\n\\n        self.active = False', 'def refresh_content(self, order=None, name=None):\\n        \"Re-download comments and reset the page index\"\\n\\n        order = order or self.content.order\\n        url = name or self.content.name\\n\\n        with self.term.loader(\\'Refreshing page\\'):\\n            self.content = SubmissionContent.from_url(\\n                self.reddit, url, self.term.loader, order=order,\\n                max_comment_cols=self.config[\\'max_comment_cols\\'])\\n        if not self.term.loader.exception:\\n            self.nav = Navigator(self.content.get, page_index=-1)', 'def prompt_subreddit(self):\\n        \"Open a prompt to navigate to a different subreddit\"\\n\\n        name = self.term.prompt_input(\\'Enter page: /\\')\\n        if name is not None:\\n            with self.term.loader(\\'Loading page\\'):\\n                content = SubredditContent.from_name(\\n                    self.reddit, name, self.term.loader)\\n            if not self.term.loader.exception:\\n                self.selected_subreddit = content\\n                self.active = False', 'def open_link(self):\\n        \"Open the selected item with the webbrowser\"\\n\\n        data = self.get_selected_item()\\n        url = data.get(\\'permalink\\')\\n        if url:\\n            self.term.open_browser(url)\\n        else:\\n            self.term.flash()', 'def open_pager(self):\\n        \"Open the selected item with the system\\'s pager\"\\n        data = self.get_selected_item()\\n        if data[\\'type\\'] == \\'Submission\\':\\n            text = \\'\\\\n\\\\n\\'.join((data[\\'permalink\\'], data[\\'text\\']))\\n            self.term.open_pager(text)\\n        elif data[\\'type\\'] == \\'Comment\\':\\n            text = \\'\\\\n\\\\n\\'.join((data[\\'permalink\\'], data[\\'body\\']))\\n            self.term.open_pager(text)\\n        else:\\n            self.term.flash()', 'def add_comment(self):\\n        \"\"\"\\n        Submit a reply to the selected item.\\n\\n        Selected item:\\n            Submission - add a top level comment\\n            Comment - add a comment reply\\n        \"\"\"\\n\\n        data = self.get_selected_item()\\n        if data[\\'type\\'] == \\'Submission\\':\\n            body = data[\\'text\\']\\n            reply = data[\\'object\\'].add_comment\\n        elif data[\\'type\\'] == \\'Comment\\':\\n            body = data[\\'body\\']\\n            reply = data[\\'object\\'].reply\\n        else:\\n            self.term.flash()\\n            return\\n\\n        # Construct the text that will be displayed in the editor file.\\n        # The post body will be commented out and added for reference\\n        lines = [\\'#  |\\' + line for line in body.split(\\'\\\\n\\')]\\n        content = \\'\\\\n\\'.join(lines)\\n        comment_info = docs.COMMENT_FILE.format(\\n            author=data[\\'author\\'],\\n            type=data[\\'type\\'].lower(),\\n            content=content)\\n\\n        with self.term.open_editor(comment_info) as comment:\\n            if not comment:\\n                self.term.show_notification(\\'Canceled\\')\\n                return\\n\\n            with self.term.loader(\\'Posting\\', delay=0):\\n                reply(comment)\\n                # Give reddit time to process the submission\\n                time.sleep(2.0)\\n\\n            if self.term.loader.exception is None:\\n                self.refresh_content()\\n            else:\\n                raise TemporaryFileError()', 'def delete_comment(self):\\n        \"Delete the selected comment\"\\n\\n        if self.get_selected_item()[\\'type\\'] == \\'Comment\\':\\n            self.delete_item()\\n        else:\\n            self.term.flash()', \"def comment_urlview(self):\\n        data = self.get_selected_item()\\n        comment = data.get('body') or data.get('text') or data.get('url_full')\\n        if comment:\\n            self.term.open_urlview(comment)\\n        else:\\n            self.term.flash()\", \"def _draw_comment(self, win, data, inverted):\\n\\n        n_rows, n_cols = win.getmaxyx()\\n        n_cols -= 1\\n\\n        # Handle the case where the window is not large enough to fit the text.\\n        valid_rows = range(0, n_rows)\\n        offset = 0 if not inverted else -(data['n_rows'] - n_rows)\\n\\n        # If there isn't enough space to fit the comment body on the screen,\\n        # replace the last line with a notification.\\n        split_body = data['split_body']\\n        if data['n_rows'] > n_rows:\\n            # Only when there is a single comment on the page and not inverted\\n            if not inverted and len(self._subwindows) == 0:\\n                cutoff = data['n_rows'] - n_rows + 1\\n                split_body = split_body[:-cutoff]\\n                split_body.append('(Not enough space to display)')\\n\\n        row = offset\\n        if row in valid_rows:\\n\\n            attr = curses.A_BOLD\\n            attr |= (Color.BLUE if not data['is_author'] else Color.GREEN)\\n            self.term.add_line(win, '{author} '.format(**data), row, 1, attr)\\n\\n            if data['flair']:\\n                attr = curses.A_BOLD | Color.YELLOW\\n                self.term.add_line(win, '{flair} '.format(**data), attr=attr)\\n\\n            text, attr = self.term.get_arrow(data['likes'])\\n            self.term.add_line(win, text, attr=attr)\\n            self.term.add_line(win, ' {score} {created} '.format(**data))\\n\\n            if data['gold']:\\n                text, attr = self.term.guilded\\n                self.term.add_line(win, text, attr=attr)\\n\\n            if data['stickied']:\\n                text, attr = '[stickied]', Color.GREEN\\n                self.term.add_line(win, text, attr=attr)\\n\\n            if data['saved']:\\n                text, attr = '[saved]', Color.GREEN\\n                self.term.add_line(win, text, attr=attr)\\n\\n        for row, text in enumerate(split_body, start=offset+1):\\n            if row in valid_rows:\\n                self.term.add_line(win, text, row, 1)\\n\\n        # Unfortunately vline() doesn't support custom color so we have to\\n        # build it one segment at a time.\\n        attr = Color.get_level(data['level'])\\n        x = 0\\n        for y in range(n_rows):\\n            self.term.addch(win, y, x, self.term.vline, attr)\\n\\n        return attr | self.term.vline\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _coerce_iterator_output(self, expr, state=None):\\n        import supriya.patterns\\n\\n        if not isinstance(expr, supriya.patterns.Event):\\n            expr = supriya.patterns.NoteEvent(**expr)\\n        if expr.get(\"uuid\") is None:\\n            expr = new(expr, uuid=uuid.uuid4())\\n        return expr', 'def play(self, clock=None, server=None):\\n        import supriya.patterns\\n        import supriya.realtime\\n\\n        event_player = supriya.patterns.RealtimeEventPlayer(\\n            self, clock=clock, server=server or supriya.realtime.Server.default()\\n        )\\n        event_player.start()\\n        return event_player', 'def with_effect(self, synthdef, release_time=0.25, **settings):\\n        import supriya.patterns\\n\\n        return supriya.patterns.Pfx(\\n            self, synthdef=synthdef, release_time=release_time, **settings\\n        )']}, {'features': [], 'snippets': ['def to_return(self):\\n        result = {}\\n        try:\\n            for returnable in self.returnables:\\n                result[returnable] = getattr(self, returnable)\\n            result = self._filter_params(result)\\n        except Exception:\\n            raise\\n        return result', \"def device_password(self):\\n        if self._values['device_password'] is None:\\n            return None\\n        return self._values['device_password']\", \"def device_username(self):\\n        if self._values['device_username'] is None:\\n            return None\\n        return self._values['device_username']\", \"def device_address(self):\\n        if self.device_is_address:\\n            return self._values['device']\", \"def device_port(self):\\n        if self._values['device_port'] is None:\\n            return None\\n        return int(self._values['device_port'])\", 'def device_is_address(self):\\n        if is_valid_ip(self.device):\\n            return True\\n        return False', \"def device_is_id(self):\\n        pattern = r'[A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12}'\\n        if re.match(pattern, self.device):\\n            return True\\n        return False\", 'def device_is_name(self):\\n        if not self.device_is_address and not self.device_is_id:\\n            return True\\n        return False', 'def device_reference(self):\\n        if not self.managed:\\n            return None\\n        if self.device_is_address:\\n            # This range lookup is how you do lookups for single IP addresses. Weird.\\n            filter = \"address+eq+\\'{0}...{0}\\'\".format(self.device)\\n        elif self.device_is_name:\\n            filter = \"hostname+eq+\\'{0}\\'\".format(self.device)\\n        elif self.device_is_id:\\n            filter = \"uuid+eq+\\'{0}\\'\".format(self.device)\\n        else:\\n            raise F5ModuleError(\\n                \"Unknown device format \\'{0}\\'\".format(self.device)\\n            )\\n\\n        uri = \"https://{0}:{1}/mgmt/shared/resolver/device-groups/cm-bigip-allBigIpDevices/devices/\" \\\\\\n              \"?$filter={2}&$top=1\".format(self.client.provider[\\'server\\'],\\n                                           self.client.provider[\\'server_port\\'], filter)\\n        resp = self.client.api.get(uri)\\n        try:\\n            response = resp.json()\\n        except ValueError as ex:\\n            raise F5ModuleError(str(ex))\\n        if resp.status == 200 and response[\\'totalItems\\'] == 0:\\n            raise F5ModuleError(\\n                \"No device with the specified address was found.\"\\n            )\\n        elif \\'code\\' in response and response[\\'code\\'] == 400:\\n            if \\'message\\' in response:\\n                raise F5ModuleError(response[\\'message\\'])\\n            else:\\n                raise F5ModuleError(resp._content)\\n        id = response[\\'items\\'][0][\\'uuid\\']\\n        result = dict(\\n            link=\\'https://localhost/mgmt/shared/resolver/device-groups/cm-bigip-allBigIpDevices/devices/{0}\\'.format(id)\\n        )\\n        return result', 'def pool_id(self):\\n        filter = \"(name%20eq%20\\'{0}\\')\".format(self.pool)\\n        uri = \\'https://{0}:{1}/mgmt/cm/device/licensing/pool/regkey/licenses?$filter={2}&$top=1\\'.format(\\n            self.client.provider[\\'server\\'],\\n            self.client.provider[\\'server_port\\'],\\n            filter\\n        )\\n        resp = self.client.api.get(uri)\\n        try:\\n            response = resp.json()\\n        except ValueError as ex:\\n            raise F5ModuleError(str(ex))\\n        if resp.status == 200 and response[\\'totalItems\\'] == 0:\\n            raise F5ModuleError(\\n                \"No pool with the specified name was found.\"\\n            )\\n        elif \\'code\\' in response and response[\\'code\\'] == 400:\\n            if \\'message\\' in response:\\n                raise F5ModuleError(response[\\'message\\'])\\n            else:\\n                raise F5ModuleError(resp._content)\\n        return response[\\'items\\'][0][\\'id\\']', 'def member_id(self):\\n        if self.device_is_address:\\n            # This range lookup is how you do lookups for single IP addresses. Weird.\\n            filter = \"deviceAddress+eq+\\'{0}...{0}\\'\".format(self.device)\\n        elif self.device_is_name:\\n            filter = \"deviceName+eq+\\'{0}\\'\".format(self.device)\\n        elif self.device_is_id:\\n            filter = \"deviceMachineId+eq+\\'{0}\\'\".format(self.device)\\n        else:\\n            raise F5ModuleError(\\n                \"Unknown device format \\'{0}\\'\".format(self.device)\\n            )\\n        uri = \\'https://{0}:{1}/mgmt/cm/device/licensing/pool/regkey/licenses/{2}/offerings/{3}/members/\\' \\\\\\n              \\'?$filter={4}\\'.format(self.client.provider[\\'server\\'], self.client.provider[\\'server_port\\'],\\n                                    self.pool_id, self.key, filter)\\n        resp = self.client.api.get(uri)\\n        try:\\n            response = resp.json()\\n        except ValueError as ex:\\n            raise F5ModuleError(str(ex))\\n\\n        if resp.status == 200 and response[\\'totalItems\\'] == 0:\\n            return None\\n        elif \\'code\\' in response and response[\\'code\\'] == 400:\\n            if \\'message\\' in response:\\n                raise F5ModuleError(response[\\'message\\'])\\n            else:\\n                raise F5ModuleError(resp._content)\\n        result = response[\\'items\\'][0][\\'id\\']\\n        return result', \"def device_port(self):\\n        if self._values['managed']:\\n            return None\\n        return self._values['device_port']\", \"def device_username(self):\\n        if self._values['managed']:\\n            return None\\n        return self._values['device_username']\", \"def device_password(self):\\n        if self._values['managed']:\\n            return None\\n        return self._values['device_password']\", \"def device_reference(self):\\n        if not self._values['managed']:\\n            return None\\n        return self._values['device_reference']\", \"def device_address(self):\\n        if self._values['managed']:\\n            return None\\n        return self._values['device_address']\", 'def managed(self):\\n        return None', 'def __init__(self, want, have=None):\\n        self.want = want\\n        self.have = have', 'def __default(self, param):\\n        attr1 = getattr(self.want, param)\\n        try:\\n            attr2 = getattr(self.have, param)\\n            if attr1 != attr2:\\n                return attr1\\n        except AttributeError:\\n            return attr1', \"def __init__(self, *args, **kwargs):\\n        self.module = kwargs.get('module', None)\\n        self.client = F5RestClient(**self.module.params)\\n        self.want = ModuleParameters(params=self.module.params, client=self.client)\\n        self.have = ApiParameters()\\n        self.changes = UsableChanges()\", 'def _update_changed_options(self):\\n        diff = Difference(self.want, self.have)\\n        updatables = Parameters.updatables\\n        changed = dict()\\n        for k in updatables:\\n            change = diff.compare(k)\\n            if change is None:\\n                continue\\n            else:\\n                if isinstance(change, dict):\\n                    changed.update(change)\\n                else:\\n                    changed[k] = change\\n        if changed:\\n            self.changes = UsableChanges(params=changed)\\n            return True\\n        return False', 'def exec_module(self):\\n        start = datetime.now().isoformat()\\n        version = bigiq_version(self.client)\\n        changed = False\\n        result = dict()\\n        state = self.want.state\\n\\n        if state == \"present\":\\n            changed = self.present()\\n        elif state == \"absent\":\\n            changed = self.absent()\\n\\n        reportable = ReportableChanges(params=self.changes.to_return())\\n        changes = reportable.to_return()\\n        result.update(**changes)\\n        result.update(dict(changed=changed))\\n        self._announce_deprecations(result)\\n        send_teem(start, self.module, version)\\n        return result', 'def present(self):\\n        if self.exists():\\n            return False\\n        return self.create()', 'def remove(self):\\n        self._set_changed_options()\\n        if self.module.check_mode:\\n            return True\\n        self.remove_from_device()\\n        if self.exists():\\n            raise F5ModuleError(\"Failed to delete the resource.\")\\n        # Artificial sleeping to wait for remote licensing (on BIG-IP) to complete\\n        #\\n        # This should be something that BIG-IQ can do natively in 6.1-ish time.\\n        time.sleep(60)\\n        return True', \"def create_on_device(self):\\n        params = self.changes.api_params()\\n        uri = 'https://{0}:{1}/mgmt/cm/device/licensing/pool/regkey/licenses/{2}/offerings/{3}/members/'.format(\\n            self.client.provider['server'],\\n            self.client.provider['server_port'],\\n            self.want.pool_id,\\n            self.want.key\\n        )\\n\\n        if not self.want.managed:\\n            params['username'] = self.want.device_username\\n            params['password'] = self.want.device_password\\n\\n        resp = self.client.api.post(uri, json=params)\\n        try:\\n            response = resp.json()\\n        except ValueError as ex:\\n            raise F5ModuleError(str(ex))\\n\\n        if 'code' in response and response['code'] == 400:\\n            if 'message' in response:\\n                raise F5ModuleError(response['message'])\\n            else:\\n                raise F5ModuleError(resp.content)\", 'def absent(self):\\n        if self.exists():\\n            return self.remove()\\n        return False', \"def __init__(self):\\n        self.supports_check_mode = True\\n        argument_spec = dict(\\n            pool=dict(required=True),\\n            key=dict(required=True, no_log=True),\\n            device=dict(required=True),\\n            managed=dict(type='bool'),\\n            device_port=dict(type='int', default=443),\\n            device_username=dict(no_log=True),\\n            device_password=dict(no_log=True),\\n            state=dict(default='present', choices=['absent', 'present'])\\n        )\\n        self.argument_spec = {}\\n        self.argument_spec.update(f5_argument_spec)\\n        self.argument_spec.update(argument_spec)\\n        self.required_if = [\\n            ['state', 'present', ['key', 'managed']],\\n            ['managed', False, ['device', 'device_username', 'device_password']],\\n            ['managed', True, ['device']]\\n        ]\"]}, {'features': [], 'snippets': ['def __init__(self, fget):\\n        self.fget = fget\\n        self.func_name = fget.__name__']}, {'features': [], 'snippets': ['def find_version(filename):\\n    _version_re = re.compile(r\"__version__ = \\'(.*)\\'\")\\n    for line in open(filename):\\n        version_match = _version_re.match(line)\\n        if version_match:\\n            return version_match.group(1)']}, {'features': [], 'snippets': ['def f(lst): return [w * w for w in lst]', 'def example_1():\\n    print \"example_1\"\\n    print \"op(f, x): f is a function from a list to a list\"\\n    print \"x is a stream \\\\n\"\\n\\n    # FUNCTIONS FROM LIST TO LIST\\n\\n    # This example uses the following list operators:\\n    # functions from a list to a list.\\n    # f, g, h, r\\n\\n\\n    # Example A: function using list comprehension\\n    def f(lst): return [w*w for w in lst]\\n\\n    # Example B: function using filter\\n    threshold = 6\\n    def predicate(w):\\n        return w > threshold\\n    def g(lst):\\n        return filter(predicate, lst)\\n\\n    # Example C: function using map\\n    # Raise each element of the list to the n-th power.   \\n    n = 3\\n    def power(w):\\n        return w**n\\n    def h(lst):\\n        return map(power, lst)\\n\\n    # Example D: function using another list comprehension\\n    # Discard any element of x that is not a\\n    # multiple of a parameter n, and divide the\\n    # elements that are multiples of n by n.\\n    n = 3\\n    def r(lst):\\n        result = []\\n        for w in lst:\\n            if w%n == 0: result.append(w/n)\\n        return result', 'def F(x): return op(f,x)', 'def main():\\n    example_1()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def worker_func(args):\\n    self = args[0]\\n    m = args[1]\\n    k = args[2]\\n    r = args[3]\\n\\n    return (self.eval_func(m, k, r) -\\n            self.eval_func(m, k, self.rt) -\\n            self.temporal_diff_sum(m, k)) ** 2', 'def worker_func_der(args):\\n    self = args[0]\\n    m = args[1]\\n    k = args[2]\\n    r = args[3]\\n    i = args[4]\\n\\n    return ((self.eval_func(m, k, r) -\\n             self.eval_func(m, k, self.rt) -\\n             self.temporal_diff_sum(m, k)) * 2 *\\n            self.eval_func_der(m, k, r, i))', \"def __init__(self):\\n        self.lf = 0.2  # Learning factor lambda\\n        self.data = []  # The features' values for all the games\\n        self.rewards = []  # Reward values for moving from 1 state to the next\\n        self.rt = np.array([])\\n        self.max_iter = 50\", 'def set_rt(self, rt):\\n        assert(len(rt) == self.num_features)\\n        self.rt = rt', 'def set_data(self, data):\\n        self.data = []\\n        self.rewards = []\\n\\n        for game in data:\\n            game = np.vstack((game, np.zeros(self.num_features + 1)))\\n            self.data.append(game[:, :-1])\\n            self.rewards.append(game[:, -1:])', 'def eval_func_der(self, m, k, r, i):\\n        \"\"\"\\n        Find the derivative of the evaluation function with respect\\n        to the ith component of the vector r\\n        \"\"\"\\n        return self.data[m][k][i]', 'def temporal_diff(self, m, s):\\n        \"\"\"\\n        The temporal diffence value for state s to state (s+1) in the mth game\\n        \"\"\"\\n        return (self.get_reward(m, s) + self.eval_func(m, s + 1, self.rt) -\\n                self.eval_func(m, s, self.rt))', 'def optimized_func(self, r):\\n        result = 0\\n        M = len(self.data)\\n        pool = Pool(processes=4)\\n\\n        for m in range(M):\\n            Nm = self.data[m].shape[0] - 1\\n\\n            k_args = range(Nm + 1)\\n            self_args = [self] * len(k_args)\\n            m_args = [m] * len(k_args)\\n            r_args = [r] * len(k_args)\\n\\n            result += sum(pool.map(worker_func,\\n                                   zip(self_args, m_args, k_args, r_args)))\\n\\n        return result', 'def optimized_func_der(self, r):\\n        p = Pool(processes=4)\\n\\n        self_args = [self] * len(r)\\n        i_args = range(len(r))\\n        r_args = [r] * len(r)\\n\\n        return np.array(p.map(optimized_func_i_der,\\n                              zip(self_args, r_args, i_args)))']}, {'features': [], 'snippets': [\"def signup():\\n    # Create new user\\n    new_user = User()\\n    new_user.name = request.form['name']\\n    new_user.email = request.form['email']\\n    new_user.password = sha1(request.form['password']).hexdigest()\\n    new_user.token = str(uuid.uuid4())\\n    new_user.save()\\n    return JSON(message='User created successfully')\"]}, {'features': [], 'snippets': ['def get_team_name(code):\\n    return team_mapping[code]', 'def get_match_description(response):\\n    match_container = response.xpath(\"//td[@colspan = \\'5\\' and @align = \\'center\\']\")[0]\\n    match_details = match_container.xpath(\".//text()\").extract()\\n    return {\\n        \"round\": match_details[1],\\n        \"venue\": match_details[3],\\n        \"date\": match_details[6],\\n        \"attendance\": match_details[8],\\n        \"homeTeam\": response.xpath(\"(//a[contains(@href, \\'teams/\\')])[1]/text()\").extract_first(),\\n        \"awayTeam\": response.xpath(\"(//a[contains(@href, \\'teams/\\')])[2]/text()\").extract_first(),\\n        \"homeScore\": int(response.xpath(\"//table[1]/tr[2]/td[5]/b/text()\").extract_first()),\\n        \"awayScore\": int(response.xpath(\"//table[1]/tr[3]/td[5]/b/text()\").extract_first())\\n    }']}, {'features': [], 'snippets': ['def dummyPreprocessInput(image):\\n    image -= 127.5\\n    return image']}, {'features': [], 'snippets': [\"def __init__(self, pool_size=2, strides=None, padding='valid'):\\n        if strides is None:\\n            strides = pool_size\\n        assert padding in {'valid', 'same'}, 'border_mode must be in {valid, same}'\\n        self.pool_length = pool_size\\n        self.stride = strides\\n        self.border_mode = padding\", \"def __init__(self, pool_size=2, strides=None, padding='valid'):\\n        super(LW_MaxPooling1D, self).__init__(pool_size, strides, padding)\", \"def __init__(self, pool_size=2, strides=None, padding='valid'):\\n        super(LW_AveragePooling1D, self).__init__(pool_size, strides, padding)\", \"def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):\\n        if data_format == 'default':\\n            data_format = default_data_format\\n        assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {channels_last, channels_first}'\\n        self.pool_size = tuple(pool_size)\\n        if strides is None:\\n            strides = self.pool_size\\n        self.strides = tuple(strides)\\n        assert padding in {'valid', 'same'}, 'border_mode must be in {valid, same}'\\n        self.border_mode = padding\\n        self.dim_ordering = data_format\", \"def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):\\n        super(LW_MaxPooling2D, self).__init__(pool_size, strides, padding, data_format)\", \"def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format='default'):\\n        super(LW_AveragePooling2D, self).__init__(pool_size, strides, padding, data_format)\", \"def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):\\n        if dim_ordering == 'default':\\n            dim_ordering = default_data_format\\n        assert dim_ordering in {'channels_last', 'channels_first'}, 'data_format must be in {channels_last, channels_first}'\\n        self.pool_size = tuple(pool_size)\\n        if strides is None:\\n            strides = self.pool_size\\n        self.strides = tuple(strides)\\n        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\\n        self.border_mode = border_mode\\n        self.dim_ordering = dim_ordering\", \"def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):\\n        super(LW_MaxPooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)\", \"def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default'):\\n        super(LW_AveragePooling3D, self).__init__(pool_size, strides, border_mode, dim_ordering)\", 'def __init__(self):\\n        pass', \"def __init__(self, data_format='default'):\\n        if data_format == 'default':\\n            data_format = default_data_format\\n        self.dim_ordering = data_format\", \"def __init__(self, data_format='default'):\\n        if data_format == 'default':\\n            data_format = default_data_format\\n        self.dim_ordering = data_format\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, access_token=None, timeout=None):\\r\\n        self.access_token = access_token\\r\\n        self.timeout = timeout', 'def get_object(self, id, **args):\\r\\n        \"\"\"Fetchs the given object from the graph.\"\"\"\\r\\n        return self.request(id, args)', 'def get_objects(self, ids, **args):\\r\\n        \"\"\"Fetchs all of the given object from the graph.', 'def get_connections(self, id, connection_name, **args):\\r\\n        \"\"\"Fetchs the connections for given object.\"\"\"\\r\\n        return self.request(id + \"/\" + connection_name, args)', 'def put_object(self, parent_object, connection_name, **data):\\r\\n        \"\"\"Writes the given object to the graph, connected to the given parent.', 'def put_wall_post(self, message, attachment={}, profile_id=\"me\"):\\r\\n        \"\"\"Writes a wall post to the given profile\\'s wall.', 'def put_comment(self, object_id, message):\\r\\n        \"\"\"Writes the given comment on the given post.\"\"\"\\r\\n        return self.put_object(object_id, \"comments\", message=message)', 'def put_like(self, object_id):\\r\\n        \"\"\"Likes the given post.\"\"\"\\r\\n        return self.put_object(object_id, \"likes\")', 'def delete_object(self, id):\\r\\n        \"\"\"Deletes the object with the given ID from the graph.\"\"\"\\r\\n        self.request(id, post_args={\"method\": \"delete\"})', 'def delete_request(self, user_id, request_id):\\r\\n        \"\"\"Deletes the Request with the given ID for the given user.\"\"\"\\r\\n        conn = httplib.HTTPSConnection(\\'graph.facebook.com\\')', 'def put_photo(self, image, message=None, album_id=None, **kwargs):\\r\\n        \"\"\"Uploads an image using multipart/form-data.', 'def _encode_multipart_form(self, fields):\\r\\n        \"\"\"Encode files as \\'multipart/form-data\\'.', 'def request(self, path, args=None, post_args=None):\\r\\n        \"\"\"Fetches the given path in the Graph API.', 'def fql(self, query, args=None, post_args=None):\\r\\n        \"\"\"FQL query.', 'def extend_access_token(self, app_id, app_secret):\\r\\n        \"\"\"', 'def __init__(self, result):\\r\\n        #Exception.__init__(self, message)\\r\\n        #self.type = type\\r\\n        self.result = result\\r\\n        try:\\r\\n            self.type = result[\"error_code\"]\\r\\n        except:\\r\\n            self.type = \"\"', 'def get_user_from_cookie(cookies, app_id, app_secret):\\r\\n    \"\"\"Parses the cookie set by the official Facebook JavaScript SDK.', 'def parse_signed_request(signed_request, app_secret):\\r\\n    \"\"\" Return dictionary with signed request data.', 'def auth_url(app_id, canvas_url, perms=None, **kwargs):\\r\\n    url = \"https://www.facebook.com/dialog/oauth?\"\\r\\n    kvps = {\\'client_id\\': app_id, \\'redirect_uri\\': canvas_url}\\r\\n    if perms:\\r\\n        kvps[\\'scope\\'] = \",\".join(perms)\\r\\n    kvps.update(kwargs)\\r\\n    return url + urllib.urlencode(kvps)', 'def get_access_token_from_code(code, redirect_uri, app_id, app_secret):\\r\\n    \"\"\"Get an access token from the \"code\" returned from an OAuth dialog.', 'def get_app_access_token(app_id, app_secret):\\r\\n    \"\"\"Get the access_token for the app.']}, {'features': [], 'snippets': ['def __init__(self, state):\\n        self._state = state\\n        self.tx_pool = TransactionPool(None)\\n        self._last_block = Block.deserialize(GenesisBlock().serialize())\\n        self.current_difficulty = StringToUInt256(str(config.user.genesis_difficulty))\\n\\n        self.trigger_miner = False\\n        self.lock = threading.RLock()', 'def height(self):\\n        with self.lock:\\n            if not self._last_block:\\n                return -1\\n            return self._last_block.block_number', 'def last_block(self) -> Block:\\n        with self.lock:\\n            return self._last_block', 'def total_coin_supply(self):\\n        with self.lock:\\n            return self._state.total_coin_supply', 'def get_cumulative_difficulty(self):\\n        with self.lock:\\n            last_block_metadata = self._state.get_block_metadata(self._last_block.headerhash)\\n            return last_block_metadata.cumulative_difficulty', 'def get_block_header_hash_by_number(self, block_number) -> Optional[bytes]:\\n        with self.lock:\\n            return self._state.get_block_header_hash_by_number(block_number)', 'def get_address_balance(self, address: bytes) -> int:\\n        with self.lock:\\n            return self._state.get_address_balance(address)', 'def get_address_state(self, address: bytes) -> AddressState:\\n        with self.lock:\\n            return self._state.get_address_state(address)', 'def get_tx_metadata(self, transaction_hash) -> list:\\n        with self.lock:\\n            return self._state.get_tx_metadata(transaction_hash)', 'def get_unconfirmed_transaction(self, transaction_hash) -> list:\\n        with self.lock:\\n            for tx_set in self.tx_pool.transactions:\\n                tx = tx_set[1].transaction\\n                if tx.txhash == transaction_hash:\\n                    return [tx, tx_set[1].timestamp]\\n            if transaction_hash in self.tx_pool.pending_tx_pool_hash:\\n                for tx_set in self.tx_pool.pending_tx_pool:\\n                    tx = tx_set[1].transaction\\n                    if tx.txhash == transaction_hash:\\n                        return [tx, tx_set[1].timestamp]\\n\\n            return []', 'def get_blockheader_and_metadata(self, block_number=0) -> Tuple:\\n        with self.lock:\\n            block_number = block_number or self.height  # if both are non-zero, then block_number takes priority\\n\\n            result = (None, None)\\n            block = self.get_block_by_number(block_number)\\n            if block:\\n                blockheader = block.blockheader\\n                blockmetadata = self.get_block_metadata(blockheader.headerhash)\\n                result = (blockheader, blockmetadata)\\n\\n            return result', 'def get_measurement(self, block_timestamp, parent_headerhash, parent_metadata: BlockMetadata):\\n        with self.lock:\\n            return self._state.get_measurement(block_timestamp, parent_headerhash, parent_metadata)', 'def get_block_is_duplicate(self, block: Block) -> bool:\\n        with self.lock:\\n            return self._state.get_block(block.headerhash) is not None', 'def get_headerhashes(self, start_blocknumber):\\n        with self.lock:\\n            start_blocknumber = max(0, start_blocknumber)\\n            end_blocknumber = min(self._last_block.block_number,\\n                                  start_blocknumber + 2 * config.dev.reorg_limit)\\n\\n            total_expected_headerhash = end_blocknumber - start_blocknumber + 1\\n\\n            node_header_hash = qrl_pb2.NodeHeaderHash()\\n            node_header_hash.block_number = start_blocknumber\\n\\n            block = self._state.get_block_by_number(end_blocknumber)\\n            block_headerhash = block.headerhash\\n            node_header_hash.headerhashes.append(block_headerhash)\\n            end_blocknumber -= 1\\n\\n            while end_blocknumber >= start_blocknumber:\\n                block_metadata = self._state.get_block_metadata(block_headerhash)\\n                for headerhash in block_metadata.last_N_headerhashes[-1::-1]:\\n                    node_header_hash.headerhashes.append(headerhash)\\n                end_blocknumber -= len(block_metadata.last_N_headerhashes)\\n                if len(block_metadata.last_N_headerhashes) == 0:\\n                    break\\n                block_headerhash = block_metadata.last_N_headerhashes[0]\\n\\n            node_header_hash.headerhashes[:] = node_header_hash.headerhashes[-1::-1]\\n            del node_header_hash.headerhashes[:len(node_header_hash.headerhashes) - total_expected_headerhash]\\n\\n            return node_header_hash', \"def load(self, genesis_block):\\n        # load() has the following tasks:\\n        # Write Genesis Block into State immediately\\n        # Register block_number <-> blockhash mapping\\n        # Calculate difficulty Metadata for Genesis Block\\n        # Generate AddressStates from Genesis Block balances\\n        # Apply Genesis Block's transactions to the state\\n        # Detect if we are forked from genesis block and if so initiate recovery.\\n        height = self._state.get_mainchain_height()\\n\\n        if height == -1:\\n            self._state.put_block(genesis_block, None)\\n            block_number_mapping = qrl_pb2.BlockNumberMapping(headerhash=genesis_block.headerhash,\\n                                                              prev_headerhash=genesis_block.prev_headerhash)\\n\\n            self._state.put_block_number_mapping(genesis_block.block_number, block_number_mapping, None)\\n            parent_difficulty = StringToUInt256(str(config.user.genesis_difficulty))\\n\\n            self.current_difficulty, _ = DifficultyTracker.get(\\n                measurement=config.dev.mining_setpoint_blocktime,\\n                parent_difficulty=parent_difficulty)\\n\\n            block_metadata = BlockMetadata.create()\\n            block_metadata.set_block_difficulty(self.current_difficulty)\\n            block_metadata.set_cumulative_difficulty(self.current_difficulty)\\n\\n            self._state.put_block_metadata(genesis_block.headerhash, block_metadata, None)\\n            addresses_state = dict()\\n            for genesis_balance in GenesisBlock().genesis_balance:\\n                bytes_addr = genesis_balance.address\\n                addresses_state[bytes_addr] = AddressState.get_default(bytes_addr)\\n                addresses_state[bytes_addr]._data.balance = genesis_balance.balance\\n\\n            for tx_idx in range(1, len(genesis_block.transactions)):\\n                tx = Transaction.from_pbdata(genesis_block.transactions[tx_idx])\\n                for addr in tx.addrs_to:\\n                    addresses_state[addr] = AddressState.get_default(addr)\\n\\n            coinbase_tx = Transaction.from_pbdata(genesis_block.transactions[0])\\n\\n            if not isinstance(coinbase_tx, CoinBase):\\n                return False\\n\\n            addresses_state[coinbase_tx.addr_to] = AddressState.get_default(coinbase_tx.addr_to)\\n\\n            if not coinbase_tx.validate_extended(genesis_block.block_number):\\n                return False\\n\\n            coinbase_tx.apply_state_changes(addresses_state)\\n\\n            for tx_idx in range(1, len(genesis_block.transactions)):\\n                tx = Transaction.from_pbdata(genesis_block.transactions[tx_idx])\\n                tx.apply_state_changes(addresses_state)\\n\\n            self._state.put_addresses_state(addresses_state)\\n            self._state.update_tx_metadata(genesis_block, None)\\n            self._state.update_mainchain_height(0, None)\\n        else:\\n            self._last_block = self.get_block_by_number(height)\\n            self.current_difficulty = self._state.get_block_metadata(self._last_block.headerhash).block_difficulty\\n            fork_state = self._state.get_fork_state()\\n            if fork_state:\\n                block = self._state.get_block(fork_state.initiator_headerhash)\\n                self._fork_recovery(block, fork_state)\", 'def _update_chainstate(self, block: Block, batch):\\n        self._last_block = block\\n        self._update_block_number_mapping(block, batch)\\n        self.tx_pool.remove_tx_in_block_from_pool(block)\\n        self._state.update_mainchain_height(block.block_number, batch)\\n        self._state.update_tx_metadata(block, batch)', 'def _remove_block_from_mainchain(self, block: Block, latest_block_number: int, batch):\\n        addresses_set = self._state.prepare_address_list(block)\\n        addresses_state = self._state.get_state_mainchain(addresses_set)\\n        for tx_idx in range(len(block.transactions) - 1, -1, -1):\\n            tx = Transaction.from_pbdata(block.transactions[tx_idx])\\n            tx.revert_state_changes(addresses_state, self)\\n\\n        self.tx_pool.add_tx_from_block_to_pool(block, latest_block_number)\\n        self._state.update_mainchain_height(block.block_number - 1, batch)\\n        self._state.rollback_tx_metadata(block, batch)\\n        self._state.remove_blocknumber_mapping(block.block_number, batch)\\n        self._state.put_addresses_state(addresses_state, batch)', 'def _rollback(self, forked_header_hash: bytes, fork_state: qrlstateinfo_pb2.ForkState = None):\\n        \"\"\"\\n        Rollback from last block to the block just before the forked_header_hash\\n        :param forked_header_hash:\\n        :param fork_state:\\n        :return:\\n        \"\"\"\\n        hash_path = []\\n        while self._last_block.headerhash != forked_header_hash:\\n            block = self._state.get_block(self._last_block.headerhash)\\n            mainchain_block = self._state.get_block_by_number(block.block_number)\\n\\n            if block is None:\\n                logger.warning(\"self.state.get_block(self.last_block.headerhash) returned None\")\\n\\n            if mainchain_block is None:\\n                logger.warning(\"self.get_block_by_number(block.block_number) returned None\")\\n\\n            if block.headerhash != mainchain_block.headerhash:\\n                break\\n            hash_path.append(self._last_block.headerhash)\\n\\n            batch = self._state.batch\\n            self._remove_block_from_mainchain(self._last_block, block.block_number, batch)\\n\\n            if fork_state:\\n                fork_state.old_mainchain_hash_path.extend([self._last_block.headerhash])\\n                self._state.put_fork_state(fork_state, batch)\\n\\n            self._state.write_batch(batch)\\n\\n            self._last_block = self._state.get_block(self._last_block.prev_headerhash)\\n\\n        return hash_path', 'def _fork_recovery(self, block: Block, fork_state: qrlstateinfo_pb2.ForkState) -> bool:\\n        logger.info(\"Triggered Fork Recovery\")\\n        # This condition only becomes true, when fork recovery was interrupted\\n        if fork_state.fork_point_headerhash:\\n            logger.info(\"Recovering from last fork recovery interruption\")\\n            forked_header_hash, hash_path = fork_state.fork_point_headerhash, fork_state.new_mainchain_hash_path\\n        else:\\n            forked_header_hash, hash_path = self._get_fork_point(block)\\n            fork_state.fork_point_headerhash = forked_header_hash\\n            fork_state.new_mainchain_hash_path.extend(hash_path)\\n            self._state.put_fork_state(fork_state)\\n\\n        rollback_done = False\\n        if fork_state.old_mainchain_hash_path:\\n            b = self._state.get_block(fork_state.old_mainchain_hash_path[-1])\\n            if b and b.prev_headerhash == fork_state.fork_point_headerhash:\\n                rollback_done = True\\n\\n        if not rollback_done:\\n            logger.info(\"Rolling back\")\\n            old_hash_path = self._rollback(forked_header_hash, fork_state)\\n        else:\\n            old_hash_path = fork_state.old_mainchain_hash_path\\n\\n        if not self.add_chain(hash_path[-1::-1], fork_state):\\n            logger.warning(\"Fork Recovery Failed... Recovering back to old mainchain\")\\n            # If above condition is true, then it means, the node failed to add_chain\\n            # Thus old chain state, must be retrieved\\n            self._rollback(forked_header_hash)\\n            self.add_chain(old_hash_path[-1::-1], fork_state)  # Restores the old chain state\\n            return False\\n\\n        logger.info(\"Fork Recovery Finished\")\\n\\n        self.trigger_miner = True\\n        return True', \"def add_block(self, block: Block, check_stale=True) -> bool:\\n        with self.lock:\\n            if block.block_number < self.height - config.dev.reorg_limit:\\n                logger.debug('Skipping block #%s as beyond re-org limit', block.block_number)\\n                return False\\n\\n            if self.get_block_is_duplicate(block):\\n                return False\\n\\n            batch = self._state.batch\\n            block_flag, fork_flag = self._add_block(block, batch=batch, check_stale=check_stale)\\n            if block_flag:\\n                if not fork_flag:\\n                    self._state.write_batch(batch)\\n                logger.info('Added Block #%s %s', block.block_number, bin2hstr(block.headerhash))\\n                return True\\n\\n            return False\"]}, {'features': [], 'snippets': ['def create_block(hashprev=None, coinbase=None, ntime=None, *, version=None, tmpl=None, txlist=None):\\n    \"\"\"Create a block (with regtest difficulty).\"\"\"\\n    block = CBlock()\\n    if tmpl is None:\\n        tmpl = {}\\n    block.nVersion = version or tmpl.get(\\'version\\') or VERSIONBITS_LAST_OLD_BLOCK_VERSION\\n    block.nTime = ntime or tmpl.get(\\'curtime\\') or int(time.time() + 600)\\n    block.hashPrevBlock = hashprev or int(tmpl[\\'previousblockhash\\'], 0x10)\\n    if tmpl and not tmpl.get(\\'bits\\') is None:\\n        block.nBits = struct.unpack(\\'>I\\', bytes.fromhex(tmpl[\\'bits\\']))[0]\\n    else:\\n        block.nBits = 0x207fffff  # difficulty retargeting is disabled in REGTEST chainparams\\n    if coinbase is None:\\n        coinbase = create_coinbase(height=tmpl[\\'height\\'])\\n    block.vtx.append(coinbase)\\n    if txlist:\\n        for tx in txlist:\\n            if not hasattr(tx, \\'calc_sha256\\'):\\n                tx = tx_from_hex(tx)\\n            block.vtx.append(tx)\\n    block.hashMerkleRoot = block.calc_merkle_root()\\n    block.calc_sha256()\\n    return block', 'def add_witness_commitment(block, nonce=0):\\n    \"\"\"Add a witness commitment to the block\\'s coinbase transaction.\\n\\n    According to BIP141, blocks with witness rules active must commit to the\\n    hash of all in-block transactions including witness.\"\"\"\\n    # First calculate the merkle root of the block\\'s\\n    # transactions, with witnesses.\\n    witness_nonce = nonce\\n    witness_root = block.calc_witness_merkle_root()\\n    # witness_nonce should go to coinbase witness.\\n    block.vtx[0].wit.vtxinwit = [CTxInWitness()]\\n    block.vtx[0].wit.vtxinwit[0].scriptWitness.stack = [ser_uint256(witness_nonce)]\\n\\n    # witness commitment is the last OP_RETURN output in coinbase\\n    block.vtx[0].vout.append(CTxOut(0, get_witness_script(witness_root, witness_nonce)))\\n    block.vtx[0].rehash()\\n    block.hashMerkleRoot = block.calc_merkle_root()\\n    block.rehash()', 'def create_coinbase(height, pubkey=None, extra_output_script=None, fees=0, nValue=50):\\n    \"\"\"Create a coinbase transaction.\\n\\n    If pubkey is passed in, the coinbase output will be a P2PK output;\\n    otherwise an anyone-can-spend output.\\n\\n    If extra_output_script is given, make a 0-value output to that\\n    script. This is useful to pad block weight/sigops as needed. \"\"\"\\n    coinbase = CTransaction()\\n    coinbase.vin.append(CTxIn(COutPoint(0, 0xffffffff), script_BIP34_coinbase_height(height), 0xffffffff))\\n    coinbaseoutput = CTxOut()\\n    coinbaseoutput.nValue = nValue * COIN\\n    if nValue == 50:\\n        halvings = int(height / 150)  # regtest\\n        coinbaseoutput.nValue >>= halvings\\n        coinbaseoutput.nValue += fees\\n    if pubkey is not None:\\n        coinbaseoutput.scriptPubKey = CScript([pubkey, OP_CHECKSIG])\\n    else:\\n        coinbaseoutput.scriptPubKey = CScript([OP_TRUE])\\n    coinbase.vout = [coinbaseoutput]\\n    if extra_output_script is not None:\\n        coinbaseoutput2 = CTxOut()\\n        coinbaseoutput2.nValue = 0\\n        coinbaseoutput2.scriptPubKey = extra_output_script\\n        coinbase.vout.append(coinbaseoutput2)\\n    coinbase.calc_sha256()\\n    return coinbase', 'def create_transaction(node, txid, to_address, *, amount):\\n    \"\"\" Return signed transaction spending the first output of the\\n        input txid. Note that the node must have a wallet that can\\n        sign for the output that is being spent.\\n    \"\"\"\\n    raw_tx = create_raw_transaction(node, txid, to_address, amount=amount)\\n    tx = tx_from_hex(raw_tx)\\n    return tx', 'def get_legacy_sigopcount_block(block, accurate=True):\\n    count = 0\\n    for tx in block.vtx:\\n        count += get_legacy_sigopcount_tx(tx, accurate)\\n    return count', 'def witness_script(use_p2wsh, pubkey):\\n    \"\"\"Create a scriptPubKey for a pay-to-witness TxOut.\\n\\n    This is either a P2WPKH output for the given pubkey, or a P2WSH output of a\\n    1-of-1 multisig for the given pubkey. Returns the hex encoding of the\\n    scriptPubKey.\"\"\"\\n    if not use_p2wsh:\\n        # P2WPKH instead\\n        pkscript = key_to_p2wpkh_script(pubkey)\\n    else:\\n        # 1-of-1 multisig\\n        witness_script = CScript([OP_1, bytes.fromhex(pubkey), OP_1, OP_CHECKMULTISIG])\\n        pkscript = script_to_p2wsh_script(witness_script)\\n    return pkscript.hex()', 'def send_to_witness(use_p2wsh, node, utxo, pubkey, encode_p2sh, amount, sign=True, insert_redeem_script=\"\"):\\n    \"\"\"Create a transaction spending a given utxo to a segwit output.\\n\\n    The output corresponds to the given pubkey: use_p2wsh determines whether to\\n    use P2WPKH or P2WSH; encode_p2sh determines whether to wrap in P2SH.\\n    sign=True will have the given node sign the transaction.\\n    insert_redeem_script will be added to the scriptSig, if given.\"\"\"\\n    tx_to_witness = create_witness_tx(node, use_p2wsh, utxo, pubkey, encode_p2sh, amount)\\n    if (sign):\\n        signed = node.signrawtransactionwithwallet(tx_to_witness)\\n        assert \"errors\" not in signed or len([\"errors\"]) == 0\\n        return node.sendrawtransaction(signed[\"hex\"])\\n    else:\\n        if (insert_redeem_script):\\n            tx = tx_from_hex(tx_to_witness)\\n            tx.vin[0].scriptSig += CScript([bytes.fromhex(insert_redeem_script)])\\n            tx_to_witness = tx.serialize().hex()\\n\\n    return node.sendrawtransaction(tx_to_witness)']}, {'features': [], 'snippets': ['def fetch_queue_lengths(queue_names):\\n    \"\"\"Connect to Redis server and request queue lengths.', 'def configure_callback(conf):\\n    \"\"\"Receive configuration block\"\"\"\\n    global REDIS_HOST, REDIS_PORT, VERBOSE_LOGGING, QUEUE_NAMES\\n    for node in conf.children:\\n        if node.key == \\'Host\\':\\n            REDIS_HOST = node.values[0]\\n        elif node.key == \\'Port\\':\\n            REDIS_PORT = int(node.values[0])\\n        elif node.key == \\'Verbose\\':\\n            VERBOSE_LOGGING = bool(node.values[0])\\n        elif node.key == \\'Queues\\':\\n            QUEUE_NAMES = list(node.values)\\n        else:\\n            collectd.warning(\\'redis_queues plugin: Unknown config key: %s.\\'\\n                             % node.key)\\n    log_verbose(\\'Configured with host=%s, port=%s\\' % (REDIS_HOST, REDIS_PORT))\\n    for queue in QUEUE_NAMES:\\n        log_verbose(\\'Watching queue %s\\' % queue)\\n    if not QUEUE_NAMES:\\n        log_verbose(\\'Not watching any queues\\')', \"def log_verbose(msg):\\n    if not VERBOSE_LOGGING:\\n        return\\n    collectd.info('redis plugin [verbose]: %s' % msg)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, *args, **kwargs):\\n        super(EspecialidadeMedicoFilterSet, self).__init__(*args, **kwargs)\\n\\n        row1 = to_row([('especialidade', 12)])\\n\\n        self.form.helper = FormHelper()\\n        self.form.helper.form_method = 'GET'\\n        self.form.helper.layout = Layout(\\n            Fieldset(_('Pesquisar Médico'),\\n                     row1, form_actions(save_label='Filtrar'))\\n        )\", \"def __init__(self, *args, **kwargs):\\n        super(UsuarioForm, self).__init__(*args, **kwargs)\\n        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'\\n        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'\", \"def clean(self):\\n\\n        if ('password' not in self.cleaned_data or\\n                'password_confirm' not in self.cleaned_data):\\n            raise ValidationError(_('Favor informar senhas atuais ou novas'))\\n\\n        msg = _('As senhas não conferem.')\\n        self.valida_igualdade(\\n            self.cleaned_data['password'],\\n            self.cleaned_data['password_confirm'],\\n            msg)\\n\\n        try:\\n            validate_password(self.cleaned_data['password'])\\n        except ValidationError as error:\\n            raise ValidationError(error)\\n\\n        return self.cleaned_data\", \"def save(self, commit=False):\\n        usuario = super(UsuarioForm, self).save(commit)\\n\\n        # Cria User\\n        u = User.objects.create(username=usuario.username, email=usuario.email)\\n        u.set_password(self.cleaned_data['password'])\\n        u.is_active = True\\n        u.groups.add(get_or_create_grupo(self.cleaned_data['tipo'].descricao))\\n\\n        u.save()\\n        usuario.user = u\\n        usuario.save()\\n        return usuario\", \"def __init__(self, *args, **kwargs):\\n        super(UsuarioEditForm, self).__init__(*args, **kwargs)\\n        self.fields['primeiro_telefone'].widget.attrs['class'] = 'telefone'\\n        self.fields['segundo_telefone'].widget.attrs['class'] = 'telefone'\", \"def clean_primeiro_numero(self):\\n        cleaned_data = self.cleaned_data\\n\\n        telefone = Telefone()\\n        telefone.tipo = self.data['primeiro_tipo']\\n        telefone.ddd = self.data['primeiro_ddd']\\n        telefone.numero = self.data['primeiro_numero']\\n        telefone.principal = self.data['primeiro_principal']\\n\\n        cleaned_data['primeiro_telefone'] = telefone\\n        return cleaned_data\", \"def save(self, commit=False):\\n        usuario = super(UsuarioEditForm, self).save(commit)\\n\\n        # Primeiro telefone\\n        tel = usuario.primeiro_telefone\\n\\n        tel.tipo = self.data['primeiro_tipo']\\n        tel.ddd = self.data['primeiro_ddd']\\n        tel.numero = self.data['primeiro_numero']\\n        tel.principal = self.data['primeiro_principal']\\n        tel.save()\\n\\n        usuario.primeiro_telefone = tel\\n\\n        # Segundo telefone\\n        tel = usuario.segundo_telefone\\n\\n        if tel:\\n            tel.tipo = self.data['segundo_tipo']\\n            tel.ddd = self.data['segundo_ddd']\\n            tel.numero = self.data['segundo_numero']\\n            tel.principal = self.data['segundo_principal']\\n            tel.save()\\n            usuario.segundo_telefone = tel\\n\\n        # User\\n        u = usuario.user\\n        u.email = usuario.email\\n        u.groups.remove(u.groups.first())\\n        u.groups.add(get_or_create_grupo(self.cleaned_data['tipo'].descricao))\\n\\n        u.save()\\n        usuario.save()\\n        return usuario\"]}, {'features': [], 'snippets': ['def strip(s):\\n    return s.strip()', \"def bytearray_to_utf8(x):\\n    return x.decode('utf-8')\", 'def __init__(self, ser: serial.Serial, parent=None):\\n        super().__init__(parent)\\n        self.serial = ser\\n        self._quit = False', 'def quit(self):\\n        self._quit = True', 'def __init__(self, parent=None):\\n        super().__init__(parent)\\n        self.recording_enabled = False\\n        self.serial = serial.Serial()\\n        self.rootnode = JsonNode(\\'\\')\\n        self._connected = False\\n        self._dirty = False\\n        self._filename = None\\n\\n        # settings\\n        self.settings = QSettingsManager()\\n        set_default_settings(self.settings)\\n\\n        # Controller Settings\\n        self.settingsDialog = None\\n\\n        # object explorer\\n        self.objectexplorer = ObjectExplorer(self.rootnode, self)\\n        self.objectexplorer.nodevalue_changed.connect(self.send_serialdata)\\n        self.objectexplorer.nodeproperty_changed.connect(self.set_dirty)\\n        self.objectexplorerDockWidget = QDockWidget(self.tr(\"object explorer\"),\\n                                                    self)\\n        self.objectexplorerDockWidget.setObjectName(\\n            \"objectexplorer_dockwidget\")\\n        self.objectexplorerDockWidget.setWidget(self.objectexplorer)\\n\\n        # plot widget\\n        self.plot = PlotWidget(self.rootnode, self.settings, self)\\n\\n        # plot settings\\n        self.plotsettings = PlotSettingsWidget(self.settings, self.plot, self)\\n        self.plotsettingsDockWidget = QDockWidget(self.tr(\"plot settings\"),\\n                                                  self)\\n        self.plotsettingsDockWidget.setObjectName(\"plotsettings_dockwidget\")\\n        self.plotsettingsDockWidget.setWidget(self.plotsettings)\\n\\n        # log widget\\n        self.loggingWidget = LoggingWidget(self)\\n        self.loggingDockWidget = QDockWidget(self.tr(\"logger\"), self)\\n        self.loggingDockWidget.setObjectName(\"logging_dockwidget\")\\n        self.loggingDockWidget.setWidget(self.loggingWidget)\\n\\n        # record widget\\n        self.recordWidget = RecordWidget(self.rootnode, self)\\n        self.recordDockWidget = QDockWidget(self.tr(\"data recording\"), self)\\n        self.recordDockWidget.setObjectName(\"record_dockwidget\")\\n        self.recordDockWidget.setWidget(self.recordWidget)\\n\\n        # actions and menus\\n        self._init_actions()\\n        self._init_menus()\\n\\n        # statusbar\\n        statusbar = self.statusBar()\\n        statusbar.setVisible(True)\\n        self.connectionstateLabel = QLabel(self.tr(\"Not connected\"))\\n        statusbar.addPermanentWidget(self.connectionstateLabel)\\n        statusbar.showMessage(self.tr(\"Ready\"))\\n\\n        # layout\\n        self.setCentralWidget(self.plot)\\n        self.addDockWidget(Qt.LeftDockWidgetArea,\\n                           self.objectexplorerDockWidget)\\n        self.addDockWidget(Qt.LeftDockWidgetArea, self.plotsettingsDockWidget)\\n        self.addDockWidget(Qt.BottomDockWidgetArea, self.loggingDockWidget)\\n        self.addDockWidget(Qt.BottomDockWidgetArea, self.recordDockWidget)\\n\\n        self.load_settings()', 'def _init_menus(self):\\n        # file menu\\n        self.fileMenu = self.menuBar().addMenu(self.tr(\"File\"))\\n        self.fileMenu.addAction(self.newAction)\\n        self.fileMenu.addAction(self.loadAction)\\n        self.fileMenu.addAction(self.saveAction)\\n        self.fileMenu.addAction(self.saveasAction)\\n        self.fileMenu.addSeparator()\\n        self.fileMenu.addAction(self.connectAction)\\n        self.fileMenu.addAction(self.serialdlgAction)\\n        self.fileMenu.addSeparator()\\n        self.fileMenu.addAction(self.quitAction)\\n\\n        # view menu\\n        self.viewMenu = self.menuBar().addMenu(self.tr(\"View\"))\\n        self.viewMenu.addAction(\\n            self.objectexplorerDockWidget.toggleViewAction())\\n        self.viewMenu.addAction(self.plotsettingsDockWidget.toggleViewAction())\\n        self.viewMenu.addAction(self.loggingDockWidget.toggleViewAction())\\n        self.viewMenu.addAction(self.recordDockWidget.toggleViewAction())\\n\\n        # record menu\\n        self.recordMenu = self.menuBar().addMenu(self.tr(\"Record\"))\\n        self.recordMenu.addAction(self.startrecordingAction)\\n        self.recordMenu.addAction(self.stoprecordingAction)\\n        self.recordMenu.addAction(self.exportcsvAction)\\n        self.recordMenu.addSeparator()\\n        self.recordMenu.addAction(self.clearrecordAction)\\n        self.recordMenu.addSeparator()\\n        self.recordMenu.addAction(self.recordsettingsAction)\\n\\n        # info menu\\n        self.menuBar().addAction(self.infoAction)', 'def load_file(self, filename):\\n        old_filename = self.filename if self.filename != filename else None\\n        self.filename = filename\\n\\n        try:\\n            with open(filename, \\'rb\\') as f:\\n                try:\\n                    self.objectexplorer.model().beginResetModel()\\n                    self.rootnode.load(bytearray_to_utf8(f.read()))\\n                    self.objectexplorer.model().endResetModel()\\n                except ValueError as e:\\n                    critical(self, \"File \\'%s\\' is not a valid config file.\"\\n                             % filename)\\n                    logger.error(str(e))\\n                    if old_filename is not None:\\n                        self.load_file(old_filename)\\n                    else:\\n                        self.filename = None\\n\\n        except FileNotFoundError as e:\\n            logger.error(str(e))\\n            self.filename = None\\n\\n        self.objectexplorer.refresh()', 'def save_settings(self):\\n        settings = QSettings()\\n        settings.setValue(WINDOWSTATE_SETTING, self.saveState())\\n        settings.setValue(GEOMETRY_SETTING, self.saveGeometry())\\n        settings.setValue(FILENAME_SETTING, self.filename)', 'def new(self):\\n        self.objectexplorer.model().beginResetModel()\\n        self.rootnode.clear()\\n        self.objectexplorer.model().endResetModel()', 'def receive_serialdata(self, time, data):\\n        self.loggingWidget.log_input(data)\\n\\n        try:\\n            self.rootnode.from_json(data)\\n        except ValueError as e:\\n            logger.error(str(e))\\n\\n        # refresh widgets\\n        self.objectexplorer.refresh()\\n        self.plot.refresh(time)\\n        if self.recording_enabled:\\n            self.recordWidget.add_data(time, self.rootnode)', 'def show_serialdlg(self):\\n        dlg = SerialDialog(self.settings, self)\\n        dlg.exec_()', 'def connect(self):\\n        # Load port setting\\n        port = self.settings.get(PORT_SETTING)\\n        baudrate = self.settings.get(BAUDRATE_SETTING)\\n\\n        # If no port has been selected before show serial settings dialog\\n        if port is None:\\n            if self.show_serialdlg() == QDialog.Rejected:\\n                return\\n            port = self.settings.get(PORT_SETTING)\\n            baudrate = self.settings.get(BAUDRATE_SETTING)\\n\\n        # Serial connection\\n        try:\\n            self.serial.port = port\\n            self.serial.baudrate = baudrate\\n            self.serial.open()\\n        except ValueError:\\n            QMessageBox.critical(\\n                self, QCoreApplication.applicationName(),\\n                self.tr(\"Serial parameters e.g. baudrate, databits are out \"\\n                        \"of range.\")\\n            )\\n        except SerialException:\\n            QMessageBox.critical(\\n                self, QCoreApplication.applicationName(),\\n                self.tr(\"The device \\'%s\\' can not be found or can not be \"\\n                        \"configured.\" % port)\\n            )\\n        else:\\n            self.worker = SerialWorker(self.serial, self)\\n            self.worker.data_received.connect(self.receive_serialdata)\\n            self.worker.start()\\n\\n            self.connectAction.setText(self.tr(\"Disconnect\"))\\n            self.connectAction.setIcon(QIcon(pixmap(\"network-disconnect-3.png\")))\\n            self.serialdlgAction.setEnabled(False)\\n            self.connectionstateLabel.setText(\\n                self.tr(\"Connected to %s\") % port)\\n            self._connected = True\\n            self.objectexplorer.refresh()', 'def show_savecfg_dlg(self):\\n        filename, _ = QFileDialog.getSaveFileName(\\n            self, self.tr(\"Save configuration file...\"),\\n            directory=os.path.expanduser(\"~\"),\\n            filter=\"Json file (*.json)\"\\n        )\\n\\n        if filename:\\n            self.filename = filename\\n            self.save_file()', 'def show_opencfg_dlg(self):\\n        # show file dialog\\n        filename, _ = QFileDialog.getOpenFileName(\\n            self, self.tr(\"Open configuration file...\"),\\n            directory=os.path.expanduser(\"~\"),\\n            filter=self.tr(\"Json file (*.json);;All files (*.*)\")\\n        )\\n\\n        # load config file\\n        if filename:\\n            self.load_file(filename)', 'def start_recording(self):\\n        self.recording_enabled = True\\n        self.startrecordingAction.setEnabled(False)\\n        self.stoprecordingAction.setEnabled(True)', 'def export_csv(self):\\n        filename, _ = QFileDialog.getSaveFileName(\\n            self, QCoreApplication.applicationName(),\\n            filter=\"CSV files(*.csv);;All files (*.*)\"\\n        )\\n\\n        if filename == \"\":\\n            return\\n\\n        # get current dataframe and export to csv\\n        df = self.recordWidget.dataframe\\n        decimal = self.settings.get(DECIMAL_SETTING)\\n        df = df.applymap(lambda x: str(x).replace(\".\", decimal))\\n        df.to_csv(\\n            filename, index_label=\"time\",\\n            sep=self.settings.get(SEPARATOR_SETTING)\\n        )', 'def show_recordsettings(self):\\n        dlg = CSVSettingsDialog(self)\\n        dlg.exec_()', 'def filename(self):\\n        return self._filename', 'def filename(self, value=\"\"):\\n        self._filename = value\\n        self.refresh_window_title()', 'def dirty(self):\\n        return self._dirty', 'def dirty(self, value):\\n        self._dirty = value\\n        self.refresh_window_title()']}, {'features': [], 'snippets': [\"def yowater():\\n\\n    payload = request.args if request.args else request.get_json(force=True)\\n    username = payload.get('username')\\n\\n    reminder = db.reminders.find_one({'username': username})\\n\\n    reply_object = payload.get('reply')\\n\\n    if reply_object is None:\\n\\n        if db.reminders.find_one({'username': username}) is None:\\n\\n            address = get_remote_addr(request)\\n            data = get_location_data(address)\\n            if not data:\\n                return 'Timezone needed'\\n\\n            user_data = {'created': datetime.now(pytz.utc),\\n                         'username': username}\\n\\n            if data.get('time_zone'):\\n                user_data.update({'timezone': data.get('time_zone')})\\n\\n            db.reminders.insert(user_data)\\n\\n            return 'OK'\\n\\n    else:\\n        reply_text = reply_object.get('text')\\n\\n        if reply_text == u'Can\\\\'t right now 😖':\\n            reminder['trigger_date'] = datetime.now(pytz.utc) + timedelta(minutes=15)\\n        else:\\n\\n            reminder['step'] += 1\\n            reminder['trigger_date'] = datetime.now(pytz.utc) + timedelta(minutes=60)\\n\\n        reminder['last_reply_date'] = datetime.now(pytz.utc)\\n\\n        db.reminders.update({'username': username},\\n                            reminder)\\n\\n        db.replies.insert({'username': username,\\n                           'created': datetime.now(pytz.utc),\\n                           'reply': reply_text})\\n\\n        return 'OK'\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def step_impl(context):\\n    context.execute_steps(u\\'\\'\\'\\n        given I open History dialog\\n    \\'\\'\\')\\n    history = context.browser.find_element_by_id(\"HistoryPopup\")\\n    entries = history.find_elements_by_xpath(\\'.//li[not(@data-clone-template)]\\')\\n    assert len(entries) > 0, \"There are no entries in the history\"\\n    item = entries[0]\\n    item.find_elements_by_xpath(\\'.//*[@data-share-item]\\')[0].click()', 'def step_impl(context, url):\\n    # Wait for modal to appear\\n    WebDriverWait(context.browser, 10).until(\\n        expected_conditions.visibility_of_element_located(\\n            (By.ID, \\'ShareRequestForm\\')))\\n    output = context.browser.execute_script(\"return restman.ui.editors.get(\\'#ShareRequestEditor\\').getValue();\")\\n\\n    snippet = json.loads(output)\\n\\n    assert url == snippet[\"url\"], \"URL: \\\\\"{}\\\\\" not in output.\\\\nOutput: {}\".format(value, output)\\n    for row in context.table:\\n        assert row[\\'key\\'] in snippet[\\'headers\\'], \"Header {} is not in output\".format(row[\\'key\\'])\\n        assert row[\\'value\\'] == snippet[\\'headers\\'][row[\\'key\\']], \"Header value is not correct. Expected: {}; Actual: {}\".format(value, snippet[\\'headers\\'][name])', \"def step_impl(context):\\n    context.execute_steps(u'''\\n        given I open History dialog\\n    ''')\\n    # Click on import\\n    context.browser.find_element_by_id('ImportHistory').click()\\n    WebDriverWait(context.browser, 10).until(\\n        expected_conditions.visibility_of_element_located(\\n            (By.ID, 'ImportRequestForm')))\", 'def step_impl(context, url):\\n    req = json.dumps({\\n        \"method\": \"POST\",\\n        \"url\": url,\\n        \"headers\": {\\n            \"Content-Type\": \"application/json\",\\n            \"X-Test-Header\": \"shared_request\"\\n        },\\n        \"body\": {\\n            \"type\": \"form\",\\n            \"content\": {\\n                \"SomeKey\": \"SomeValue11233\",\\n                \"SomeOtherKey\": \"SomeOtherValue019\",\\n            }\\n        }\\n    })\\n    context.browser.execute_script(\"return restman.ui.editors.setValue(\\'#ImportRequestEditor\\', atob(\\'{}\\'));\".format(base64.b64encode(req)))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        self.events_count = 0\\n        self.events_count_by_type = dict()', 'def count_all(self):\\n        \"\"\"Tells how many events have been counted globally\\n\\n        Returns:\\n            int\\n\\n        \"\"\"\\n        return self.events_count']}, {'features': [], 'snippets': ['def __init__(self, image, edge_points3d, edge_points2d):\\n        \"\"\"\\n        Constructor for a surface defined by a texture image and\\n        4 boundary points. Choose the first point as the origin\\n        of the surface\\'s coordinate system.\\n\\n        :param image: image array\\n        :param edge_points3d: array of 3d coordinates of 4 corner points in clockwise direction\\n        :param edge_points2d: array of 2d coordinates of 4 corner points in clockwise direction\\n        \"\"\"\\n        assert len(edge_points3d) == 4 and len(edge_points2d) == 4\\n\\n        self.image = image\\n        self.edge_points3d = edge_points3d\\n        self.edge_points2d = np.float32(edge_points2d)  # This is required for using cv2\\'s getPerspectiveTransform\\n        self.normal = self._get_normal_vector()', 'def top_right_corner3d(self):\\n        return self.edge_points3d[1]', 'def bottom_left_corner3d(self):\\n        return self.edge_points3d[3]', 'def _get_normal_vector(self):\\n        \"\"\"\\n        :return: the normal vector of the surface. It determined the front side\\n        of the surface and it\\'s not necessarily a unit vector\\n        \"\"\"\\n        p0 = self.edge_points3d[0]\\n        p1 = self.edge_points3d[1]\\n        p3 = self.edge_points3d[3]\\n        v1 = p3 - p0\\n        v2 = p1 - p0\\n        normal = np.cross(v1, v2)\\n        norm = np.linalg.norm(normal)\\n        return normal / norm', 'def __init__(self, surfaces):\\n        self.surfaces = surfaces', 'def __init__(self, models=None):\\n        self.models = models or []', 'def __init__(self, point1, point2):\\n        \"\"\"\\n        Using the line equation a*x + b*y + c = 0 with b >= 0\\n        :param point1: starting point\\n        :param point2: ending point\\n        :return: a Line object\\n        \"\"\"\\n        assert len(point1) == 2 and len(point2) == 2\\n\\n        self.a = point2[1] - point1[1]\\n        self.b = point1[0] - point2[0]\\n        self.c = point1[1] * point2[0] - point1[0] * point2[1]\\n\\n        if self.b < 0:\\n            self.a = -self.a\\n            self.b = -self.b\\n            self.c = -self.c', 'def is_point_on_right(self, point):\\n        return self.a * point[0] + self.b * point[1] + self.c < 0', 'def get_y_from_x(self, x):\\n        if self.b == 0:\\n            return 0.0\\n\\n        return 1.0 * (-self.c - self.a * x) / self.b']}, {'features': [], 'snippets': ['def __init__(self, uri=None, path=None, host=None):\\n        \"\"\"\\n        ContributorOrcid - a model defined in Swagger\\n\\n        :param dict swaggerTypes: The key is attribute name\\n                                  and the value is attribute type.\\n        :param dict attributeMap: The key is attribute name\\n                                  and the value is json key in definition.\\n        \"\"\"\\n        self.swagger_types = {\\n            \\'uri\\': \\'str\\',\\n            \\'path\\': \\'str\\',\\n            \\'host\\': \\'str\\'\\n        }\\n\\n        self.attribute_map = {\\n            \\'uri\\': \\'uri\\',\\n            \\'path\\': \\'path\\',\\n            \\'host\\': \\'host\\'\\n        }\\n\\n        self._uri = uri\\n        self._path = path\\n        self._host = host', 'def uri(self):\\n        \"\"\"\\n        Gets the uri of this ContributorOrcid.\\n\\n        :return: The uri of this ContributorOrcid.\\n        :rtype: str\\n        \"\"\"\\n        return self._uri', 'def uri(self, uri):\\n        \"\"\"\\n        Sets the uri of this ContributorOrcid.\\n\\n        :param uri: The uri of this ContributorOrcid.\\n        :type: str\\n        \"\"\"\\n\\n        self._uri = uri', 'def path(self):\\n        \"\"\"\\n        Gets the path of this ContributorOrcid.\\n\\n        :return: The path of this ContributorOrcid.\\n        :rtype: str\\n        \"\"\"\\n        return self._path', 'def path(self, path):\\n        \"\"\"\\n        Sets the path of this ContributorOrcid.\\n\\n        :param path: The path of this ContributorOrcid.\\n        :type: str\\n        \"\"\"\\n\\n        self._path = path', 'def host(self):\\n        \"\"\"\\n        Gets the host of this ContributorOrcid.\\n\\n        :return: The host of this ContributorOrcid.\\n        :rtype: str\\n        \"\"\"\\n        return self._host', 'def host(self, host):\\n        \"\"\"\\n        Sets the host of this ContributorOrcid.\\n\\n        :param host: The host of this ContributorOrcid.\\n        :type: str\\n        \"\"\"\\n\\n        self._host = host', 'def to_str(self):\\n        \"\"\"\\n        Returns the string representation of the model\\n        \"\"\"\\n        return pformat(self.to_dict())', 'def __eq__(self, other):\\n        \"\"\"\\n        Returns true if both objects are equal\\n        \"\"\"\\n        if not isinstance(other, ContributorOrcid):\\n            return False\\n\\n        return self.__dict__ == other.__dict__']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def step_impl(context, username, password, email, first_name, last_name):\\n    context.base_user = User(username=username, email=email, password=password, first_name=first_name,\\n                        last_name=last_name)', 'def step_impl(context):\\n    context.user_service.save(context.base_user)', \"def step_impl(context, user_name):\\n    user_exists = context.user_service.exists(user_name)\\n    assert context.base_user.username == user_exists['username']\\n    assert context.base_user.password == user_exists['password']\\n    assert context.base_user.email == user_exists['email']\\n    assert context.base_user.first_name == user_exists['first_name']\\n    assert context.base_user.last_name == user_exists['last_name']\\n    assert user_exists['_id'] is not None\", 'def step_impl(context, username, field, value):\\n    user = context.user_service.exists(username)\\n\\n    if user is not None:\\n        user[field] = value\\n        context.user_service.update(user.to_json())\\n    else:\\n        raise UserNotFound(username, \"User was not found\")']}, {'features': [], 'snippets': ['def setUp(self):\\n\\n        self.data = MockData()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def member_list(request):\\n    env = Env()\\n    MEMBERS_PASSWORD = env(\\'MEMBERS_PASSWORD\\')\\n\\n    # handle form submission\\n    if request.POST:\\n        pw_form = PasswordForm(request.POST)\\n\\n        if pw_form.is_valid() and pw_form.cleaned_data[\\'password\\'] == MEMBERS_PASSWORD:\\n            request.session[\\'password\\'] = pw_form.cleaned_data[\\'password\\']\\n            return redirect(\\'members:member_list\\')\\n\\n        messages.error(request, \"The password you entered was incorrect, please try again.\")\\n\\n    # form not being submitted, check password\\n    if (request.session.get(\\'password\\') and request.session[\\'password\\'] == MEMBERS_PASSWORD):\\n        member_list = Member.objects.all()\\n        return render(request, \\'members/member_list.html\\', {\\n            \\'member_list\\': member_list,\\n        })\\n\\n    # password is wrong, render form\\n    pw_form = PasswordForm()\\n    return render(request, \\'members/members_password_form.html\\', {\\n        \\'pw_form\\': pw_form,\\n    })', 'def build_frames(pwidth, pheight, ncols):\\n    frames = []\\n    for i in range(ncols):\\n        f = Frame(x1=(i*((pwidth-30) / ncols)+15),\\n                  y1=0,\\n                  width=((pwidth-30) / ncols),\\n                  height=pheight+2,\\n                  leftPadding=15,\\n                  rightPadding=15,\\n                  topPadding=15,\\n                  bottomPadding=15,\\n                  showBoundary=True)\\n        frames.append(f)\\n    frames[0].showBoundary=False\\n    frames[3].showBoundary=False\\n    return frames']}, {'features': [], 'snippets': [\"def testFun():\\n    print('Starting')\\n    while True:\\n        time.sleep(3)\\n        print('looping')\\n        time.sleep(3)\\n        print('3 Seconds Later')\", 'def root():\\n\\n    return \\'Started a background process with PID \\' + str(backProc.pid) + \" is running: \" + str(backProc.is_alive())', \"def kill():\\n    backProc.terminate()\\n    return 'killed: ' + str(backProc.pid)\", \"def kill_all():\\n    proc = multiprocessing.active_children()\\n    for p in proc:\\n        p.terminate()\\n    return 'killed all'\", 'def active():\\n    proc = multiprocessing.active_children()\\n    arr = []\\n    for p in proc:\\n        print(p.pid)\\n        arr.append(p.pid)\\n\\n    return str(arr)', \"def start():\\n    global backProc\\n    backProc = multiprocessing.Process(target=testFun, args=(), daemon=True)\\n    backProc.start()\\n    return 'started: ' + str(backProc.pid)\"]}, {'features': [], 'snippets': [\"def __init__(self,domain='gw.api.taobao.com',port=80):\\r\\n\\t\\tRestApi.__init__(self,domain, port)\\r\\n\\t\\tself.user_nick = None\"]}, {'features': [], 'snippets': ['def setUp(self):\\n        self.config = ConfigReader(\"\"\"\\n            <root>\\n                <person>\\n                    <name>山田</name>\\n                    <age>15</age>\\n                </person>\\n                <person>\\n                    <name>佐藤</name>\\n                    <age>43</age>\\n                </person>\\n            </root>\\n            \"\"\")']}, {'features': [], 'snippets': ['def test_correct_login(self):\\n        # Ensure login behaves correctly with correct credentials.\\n        with self.client:\\n            response = self.client.post(\\n                \"/login\",\\n                data=dict(email=\"ad@min.com\", password=\"admin_user\"),\\n                follow_redirects=True,\\n            )\\n            self.assertIn(b\"Welcome\", response.data)\\n            self.assertIn(b\"Logout\", response.data)\\n            self.assertIn(b\"Members\", response.data)\\n            self.assertTrue(current_user.email == \"ad@min.com\")\\n            self.assertTrue(current_user.is_active())\\n            self.assertEqual(response.status_code, 200)', 'def test_logout_route_requires_login(self):\\n        # Ensure logout route requres logged in user.\\n        response = self.client.get(\"/logout\", follow_redirects=True)\\n        self.assertIn(b\"Please log in to access this page\", response.data)', 'def test_validate_success_login_form(self):\\n        # Ensure correct data validates.\\n        form = LoginForm(email=\"ad@min.com\", password=\"admin_user\")\\n        self.assertTrue(form.validate())', 'def test_get_by_id(self):\\n        # Ensure id is correct for the current/logged in user.\\n        with self.client:\\n            self.client.post(\\n                \"/login\",\\n                data=dict(email=\"ad@min.com\", password=\"admin_user\"),\\n                follow_redirects=True,\\n            )\\n            self.assertTrue(current_user.id == 1)', 'def test_check_password(self):\\n        # Ensure given password is correct after unhashing.\\n        user = User.query.filter_by(email=\"ad@min.com\").first()\\n        self.assertTrue(\\n            bcrypt.check_password_hash(user.password, \"admin_user\")\\n        )\\n        self.assertFalse(bcrypt.check_password_hash(user.password, \"foobar\"))', 'def test_register_route(self):\\n        # Ensure about route behaves correctly.\\n        response = self.client.get(\"/register\", follow_redirects=True)\\n        self.assertIn(b\"<h1>Register</h1>\\\\n\", response.data)']}, {'features': [], 'snippets': ['def coll(sx, sy, dx, dy):\\n    m = 0\\n    for p in range(32):\\n        m2 = m + 2**(-p)\\n        if inside(sx + dx * m2, sy + dy * m2): m = m2\\n    return (sx + dx*m, sy + dy*m)']}, {'features': [], 'snippets': ['def __init__(self):\\n        \"\"\"\\n        Construct empty object.\\n        \"\"\"\\n        self.num_lines = 0\\n        self.remaining_lines = MAX_NUM_LINES\\n        self.lines = []', 'def merge_after(self, obj):\\n        \"\"\"\\n        Merge with another CmdText object by appending the input objects content.\\n        \"\"\"\\n        self.lines', 'def strip_lines(self):\\n        \"\"\"\\n        Remove excessive number of lines. This deletes the oldest half.\\n        \"\"\"\\n        if (self.num_lines > MAX_NUM_STORED_LINES):\\n            for i in range(MAX_NUM_STORED_LINES // 2):\\n                self.lines.pop(i)', 'def update_num_lines(self):\\n        \"\"\"\\n        Update the number of lines member.\\n        \"\"\"\\n        self.num_lines = len(self.lines)', 'def print_screen(self):\\n        \"\"\"\\n        Return MAX_NUM_LINES lines.\\n        \"\"\"\\n        return self.lines[-MAX_NUM_LINES:]', 'def __iter__(self):\\n        \"\"\"\\n        Iterator for CmdText object.\\n        \"\"\"\\n        for l in self.lines:\\n            yield l', 'def __getitem__(self, ind):\\n        return self.lines[ind]', 'def get_lines(string):\\n    \"\"\"\\n    Return list of lines extracted from string.\\n    \"\"\"\\n    line_list = string.split(\\'\\\\n\\')\\n\\n    new_list = []\\n    for l in line_list:\\n        new_list += [l[i*LINEWIDTH:(i+1)*LINEWIDTH] for i in range(len(l) // LINEWIDTH + 1)]', 'def __init__(self, string, rind=None):\\n        CmdText.__init__(self)\\n        self.insert(string)\\n\\n        if (rind is not None):\\n            self.response = rind', 'def __init__(self, string, cind=None):\\n        CmdText.__init__(self)\\n        self.insert(string)\\n\\n        if (cind is not None):\\n            self.command = cind', 'def run(cls):\\n        \"\"\"\\n        Runs all tests (methods which begin with \\'test\\').\\n        \"\"\"\\n        #print(cls)\\n        max_len = max([len(a) for a in cls.__dict__])\\n        for key in cls.__dict__:\\n            if key.startswith(\"test\"):\\n                fill = max_len - len(key)\\n                sys.stdout.write(\"Testing {} ...{} \".format(key, \\'.\\'*fill))\\n                try:\\n                    cls.__dict__[key]()\\n                except:\\n                    raise\\n                else:\\n                    print(\"Test passed!\")\\n        print(\"All tests passed!\")', 'def test_get_lines_with_empty_string():\\n        assert get_lines(\"\") == [\"\"]', 'def test_get_lines_with_short_string():\\n        assert len(get_lines(\"a\"*(LINEWIDTH-1))) == 1', 'def test_get_lines_with_long_string():\\n        assert len(get_lines(\"a\"*(2*LINEWIDTH-1))) == 2', 'def test_get_lines_with_very_long_string():\\n        assert len(get_lines(\"a\"*(4*LINEWIDTH-1))) == 4', 'def test_get_lines_with_long_text_string():\\n        text = \"This is a test string, which should simulate real text. The command should\" \\\\\\n         + \" correctly split this text into two lines.\"\\n        LINEWIDTH = 80\\n        correct_lines = [text[:LINEWIDTH], text[LINEWIDTH:]]\\n        assert len(get_lines(text)) == len(text) // LINEWIDTH + 1\\n        assert get_lines(text) == correct_lines']}, {'features': [], 'snippets': ['def __init__(self, do_open):\\n        self._do_open = do_open']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def filename(self):\\n        pass']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def test___doc__(self):\\n        self.assertEqual(\\n            ctds.Parameter.__doc__,\\n            '''\\\\\", \"def test_parameter(self):\\n        param1 = ctds.Parameter(b'123', output=True)\\n        self.assertEqual(param1.value, b'123')\\n        self.assertTrue(isinstance(param1, ctds.Parameter))\\n\\n        param2 = ctds.Parameter(b'123')\\n        self.assertEqual(param1.value, b'123')\\n        self.assertEqual(type(param1), type(param2))\\n        self.assertTrue(isinstance(param2, ctds.Parameter))\", 'def _test__cmp__(self, __cmp__, expected, oper):\\n        cases = (\\n            (ctds.Parameter(b\\'1234\\'), ctds.Parameter(b\\'123\\')),\\n            (ctds.Parameter(b\\'123\\'), ctds.Parameter(b\\'123\\')),\\n            (ctds.Parameter(b\\'123\\'), ctds.Parameter(b\\'123\\', output=True)),\\n            (ctds.Parameter(b\\'123\\'), ctds.Parameter(b\\'1234\\')),\\n            (ctds.Parameter(b\\'123\\'), b\\'123\\'),\\n            (ctds.Parameter(b\\'123\\'), ctds.Parameter(123)),\\n            (ctds.Parameter(b\\'123\\'), unicode_(\\'123\\')),\\n            (ctds.Parameter(b\\'123\\'), ctds.SqlBinary(None)),\\n            (ctds.Parameter(b\\'123\\'), 123),\\n            (ctds.Parameter(b\\'123\\'), None),\\n        )\\n\\n        for index, args in enumerate(cases):\\n            operation = \\'[{0}]: {1} {2} {3}\\'.format(index, repr(args[0]), oper, repr(args[1]))\\n            if expected[index] == TypeError:\\n                try:\\n                    __cmp__(*args)\\n                except TypeError as ex:\\n                    regex = (\\n                        r\"\\'{0}\\' not supported between instances of \\'[^\\']+\\' and \\'[^\\']+\\'\".format(oper)\\n                        if not PY3 or PY36\\n                        else\\n                        r\\'unorderable types: \\\\S+ {0} \\\\S+\\'.format(oper)\\n                    )\\n                    self.assertTrue(re.match(regex, str(ex)), ex)\\n                else:\\n                    self.fail(\\'{0} did not fail as expected\\'.format(operation)) # pragma: nocover\\n            else:\\n                self.assertEqual(__cmp__(*args), expected[index], operation)', \"def test___cmp__ne(self):\\n        self._test__cmp__(\\n            lambda left, right: left != right,\\n            (\\n                True,\\n                False,\\n                False,\\n                True,\\n                False,\\n                True,\\n                PY3,\\n                True,\\n                True,\\n                True,\\n            ),\\n            '!='\\n        )\", \"def test___cmp__le(self):\\n        self._test__cmp__(\\n            lambda left, right: left <= right,\\n            (\\n                False,\\n                True,\\n                True,\\n                True,\\n                True,\\n                TypeError if PY3 else False,\\n                TypeError if PY3 else True,\\n                TypeError if PY3 else False,\\n                TypeError if PY3 else False,\\n                TypeError if PY3 else False,\\n            ),\\n            '<='\\n        )\", \"def test___cmp__ge(self):\\n        self._test__cmp__(\\n            lambda left, right: left >= right,\\n            (\\n                True,\\n                True,\\n                True,\\n                False,\\n                True,\\n                TypeError if PY3 else True,\\n                TypeError if PY3 else True,\\n                TypeError if PY3 else True,\\n                TypeError if PY3 else True,\\n                TypeError if PY3 else True,\\n            ),\\n            '>='\\n        )\"]}, {'features': [], 'snippets': ['def get_updated_line_contents(updates=None):\\n    test_contents = copy.deepcopy(test_line_contents)\\n    if updates is not None:\\n        test_contents.update(updates)\\n    return test_contents', 'def test_retrieval(self, expected, actual):\\n        assert expected == actual', \"def test_passing_invalid_ip_address_throws_exception(self):\\n        with pytest.raises(InvalidIpAddressException):\\n            line = Line(line_contents=get_updated_line_contents({'ip_address': 'foobar'}))\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, c_data: _C_MultiHMatrix, **params):\\n        # Users should use one of the two constructors below.\\n\\n        self.c_data = c_data\\n        self.shape = (self.lib.multi_nbrows(c_data), self.lib.multi_nbcols(c_data))\\n        self.size = self.lib.nbhmats(c_data)\\n\\n\\n        self.lib.getHMatrix.restype=ctypes.POINTER(_C_HMatrix)\\n        self.lib.getHMatrix.argtypes=[ctypes.POINTER(_C_MultiHMatrix), ctypes.c_int]\\n\\n        self.hmatrices = []\\n        for l in range(0,self.size):\\n            c_data_hmatrix = self.lib.getHMatrix(self.c_data,l)\\n            self.hmatrices.append(HMatrix(c_data_hmatrix,**params))\\n\\n\\n        self.params = params.copy()', 'def from_coefs(cls, getcoefs, nm, points_target, points_source=None, **params):\\n        \"\"\"Construct an instance of the class from a evaluation function.\\n\\n        Parameters\\n        ----------\\n        getcoefs: Callable\\n            A function evaluating an array of matrices at given coordinates.\\n        points_target: np.ndarray of shape (N, 3)\\n            The coordinates of the target points. If points_source=None, also the coordinates of the target points\\n        points_source: np.ndarray of shape (N, 3)\\n            If not None; the coordinates of the source points.\\n        epsilon: float, keyword-only, optional\\n            Tolerance of the Adaptive Cross Approximation\\n        eta: float, keyword-only, optional\\n            Criterion to choose the blocks to compress\\n        minclustersize: int, keyword-only, optional\\n            Minimum shape of a block\\n        maxblocksize: int, keyword-only, optional\\n            Maximum number of coefficients in a block\\n\\n        Returns\\n        -------\\n        MultiHMatrix or ComplexMultiHMatrix\\n        \"\"\"\\n        # Set params.\\n        cls._set_building_params(**params)', 'def from_submatrices(cls, getsubmatrix, nm, points_target, points_source=None, **params):\\n        \"\"\"Construct an instance of the class from a evaluation function.\\n\\n        Parameters\\n        ----------\\n        points: np.ndarray of shape (N, 3)\\n            The coordinates of the points.\\n        getsubmatrix: Callable\\n            A function evaluating the matrix in a given range.\\n        epsilon: float, keyword-only, optional\\n            Tolerance of the Adaptive Cross Approximation\\n        eta: float, keyword-only, optional\\n            Criterion to choose the blocks to compress\\n        minclustersize: int, keyword-only, optional\\n            Minimum shape of a block\\n        maxblocksize: int, keyword-only, optional\\n            Maximum number of coefficients in a block\\n\\n        Returns\\n        -------\\n        HMatrix or ComplexHMatrix\\n        \"\"\"\\n        # Set params.\\n        cls._set_building_params(**params)\\n\\n        # Boilerplate code for Python/C++ interface.\\n        _getsumatrix_func_type = ctypes.CFUNCTYPE(\\n                None, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int),\\n                ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_double)\\n            )\\n        if points_source is None:\\n            cls.lib.MultiHMatrixCreatewithsubmatSym.restype = ctypes.POINTER(_C_MultiHMatrix)\\n            cls.lib.MultiHMatrixCreatewithsubmatSym.argtypes = [\\n                np.ctypeslib.ndpointer(dtype=np.float64, ndim=2, flags=\\'C_CONTIGUOUS\\'),\\n                ctypes.c_int,\\n                _getsumatrix_func_type,\\n                ctypes.c_int\\n            ]\\n\\n            # Call the C++ backend.\\n            c_data = cls.lib.MultiHMatrixCreatewithsubmatSym(points_target, points_target.shape[0], _getsumatrix_func_type(getsubmatrix),nm)\\n        else:\\n            cls.lib.MultiHMatrixCreatewithsubmat.restype = ctypes.POINTER(_C_MultiHMatrix)\\n            cls.lib.MultiHMatrixCreatewithsubmat.argtypes = [\\n                np.ctypeslib.ndpointer(dtype=np.float64, ndim=2, flags=\\'C_CONTIGUOUS\\'),\\n                ctypes.c_int,\\n                np.ctypeslib.ndpointer(dtype=np.float64, ndim=2, flags=\\'C_CONTIGUOUS\\'),\\n                ctypes.c_int,\\n                _getsumatrix_func_type,\\n                ctypes.c_int\\n            ]\\n\\n            # Call the C++ backend.\\n            c_data = cls.lib.MultiHMatrixCreatewithsubmat(points_target,points_target.shape[0],points_source, points_source.shape[0], _getsumatrix_func_type(getsubmatrix),nm)\\n\\n        return cls(c_data, **params)', 'def _set_building_params(cls, *, eta=None, minclustersize=None, epsilon=None, maxblocksize=None):\\n        \"\"\"Put the parameters in the C++ backend.\"\"\"\\n        if epsilon is not None:\\n            cls.lib.setepsilon.restype = None\\n            cls.lib.setepsilon.argtypes = [ ctypes.c_double ]\\n            cls.lib.setepsilon(epsilon)\\n\\n        if eta is not None:\\n            cls.lib.seteta.restype = None\\n            cls.lib.seteta.argtypes = [ ctypes.c_double ]\\n            cls.lib.seteta(eta)\\n\\n        if minclustersize is not None:\\n            cls.lib.setminclustersize.restype = None\\n            cls.lib.setminclustersize.argtypes = [ ctypes.c_int ]\\n            cls.lib.setminclustersize(minclustersize)\\n\\n        if maxblocksize is not None:\\n            cls.lib.setmaxblocksize.restype = None\\n            cls.lib.setmaxblocksize.argtypes = [ ctypes.c_int ]\\n            cls.lib.setmaxblocksize(maxblocksize)', 'def __getitem__(self, key):\\n\\n        # self.lib.getHMatrix.restype=ctypes.POINTER(_C_HMatrix)\\n        # self.lib.getHMatrix.argtypes=[ctypes.POINTER(_C_MultiHMatrix), ctypes.c_int]\\n        # c_data_hmatrix = self.lib.getHMatrix(self.c_data,key)\\n        # return HMatrix(c_data_hmatrix,**self.params)\\n        return self.hmatrices[key]']}, {'features': [], 'snippets': ['def lcm(a, b):\\n        return a * b / gcd(a, b)', 'def gcd(a, b):\\n    while b != 0:\\n        (a, b) = (b, a % b)\\n    return a', 'def egcd(a, b):\\n    if a == 0:\\n        return (0, 1)\\n    else:\\n        y, x = egcd(b % a, a)\\n        return (x - (b // a) * y, y)', 'def modInverse(a, m):\\n    x, y = egcd(a, m)\\n    if gcd(a, m) == 1:\\n        return x % m', 'def reduceCongr(a, b, m):\\n        gcdAB = gcd(a, b)\\n        a /= gcdAB\\n        b /= gcdAB\\n        m /= gcd(gcdAB, m)\\n        modinv = modInverse(a, m)\\n        b *= modinv\\n        return (1, b, m)', 'def linCongr(a, b, m):\\n        solutions = set()\\n        if (b % gcd(a, m) == 0):\\n                numSols = gcd(a, m)\\n                sol = (b * egcd(a, m)[0] / numSols) % m\\n                for i in xrange(0, numSols):\\n                        solutions.add((sol + m * i / numSols) % m)\\n        return solutions', 'def crt(congruences):\\n        x = 0\\n        M = 1\\n        for i in xrange(len(congruences)):\\n                M *= congruences[i][2]\\n                congruences[i] = reduceCongr(congruences[i][0], congruences[i][1], congruences[i][2])\\n\\n        for j in xrange(len(congruences)):\\n                m = congruences[j][2]\\n                if gcd(m, M/m) != 1:\\n                        return None\\n                x += congruences[j][1] * modInverse(M/m, m) * M / m\\n\\n        return x % M', 'def linCongrSystem(congruences):\\n        newCongruences = []\\n        for i in xrange(len(congruences)):\\n                congruences[i] = reduceCongr(congruences[i][0], congruences[i][1], congruences[i][2])']}, {'features': [], 'snippets': ['def f():\\n    x = `1` # 4 str']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def json_loads(data):\\n            # on Python 3.5 json.loads only supports str not bytes\\n            return json.loads(data.decode())', 'def get(self, name, default=None):\\n        \"\"\"Return the first value, either the default or actual\"\"\"\\n        return super().get(name, [default])[0]', 'def __init__(self, buffer_size=100):\\n        self._queue = asyncio.Queue(buffer_size)', 'def is_full(self):\\n        return self._queue.full()', 'def __init__(self, url_bytes, headers, version, method, transport):\\n        self.raw_url = url_bytes\\n        # TODO: Content-Encoding detection\\n        self._parsed_url = parse_url(url_bytes)\\n        self.app = None\\n\\n        self.headers = headers\\n        self.version = version\\n        self.method = method\\n        self.transport = transport\\n\\n        # Init but do not inhale\\n        self.body_init()\\n        self.parsed_json = None\\n        self.parsed_form = None\\n        self.parsed_files = None\\n        self.parsed_args = None\\n        self.uri_template = None\\n        self._cookies = None\\n        self.stream = None\\n        self.endpoint = None', 'def __bool__(self):\\n        if self.transport:\\n            return True\\n        return False', 'def body_push(self, data):\\n        self.body.append(data)', 'def json(self):\\n        if self.parsed_json is None:\\n            self.load_json()\\n\\n        return self.parsed_json', 'def token(self):\\n        \"\"\"Attempt to return the auth header token.\\n\\n        :return: token related to request\\n        \"\"\"\\n        prefixes = (\"Bearer\", \"Token\")\\n        auth_header = self.headers.get(\"Authorization\")\\n\\n        if auth_header is not None:\\n            for prefix in prefixes:\\n                if prefix in auth_header:\\n                    return auth_header.partition(prefix)[-1].strip()\\n\\n        return auth_header', 'def form(self):\\n        if self.parsed_form is None:\\n            self.parsed_form = RequestParameters()\\n            self.parsed_files = RequestParameters()\\n            content_type = self.headers.get(\\n                \"Content-Type\", DEFAULT_HTTP_CONTENT_TYPE\\n            )\\n            content_type, parameters = parse_header(content_type)\\n            try:\\n                if content_type == \"application/x-www-form-urlencoded\":\\n                    self.parsed_form = RequestParameters(\\n                        parse_qs(self.body.decode(\"utf-8\"))\\n                    )\\n                elif content_type == \"multipart/form-data\":\\n                    # TODO: Stream this instead of reading to/from memory\\n                    boundary = parameters[\"boundary\"].encode(\"utf-8\")\\n                    self.parsed_form, self.parsed_files = parse_multipart_form(\\n                        self.body, boundary\\n                    )\\n            except Exception:\\n                error_logger.exception(\"Failed when parsing form\")\\n\\n        return self.parsed_form', 'def files(self):\\n        if self.parsed_files is None:\\n            self.form  # compute form to get files\\n\\n        return self.parsed_files', 'def args(self):\\n        if self.parsed_args is None:\\n            if self.query_string:\\n                self.parsed_args = RequestParameters(\\n                    parse_qs(self.query_string)\\n                )\\n            else:\\n                self.parsed_args = RequestParameters()\\n        return self.parsed_args', 'def raw_args(self):\\n        return {k: v[0] for k, v in self.args.items()}', 'def cookies(self):\\n        if self._cookies is None:\\n            cookie = self.headers.get(\"Cookie\")\\n            if cookie is not None:\\n                cookies = SimpleCookie()\\n                cookies.load(cookie)\\n                self._cookies = {\\n                    name: cookie.value for name, cookie in cookies.items()\\n                }\\n            else:\\n                self._cookies = {}\\n        return self._cookies', 'def ip(self):\\n        if not hasattr(self, \"_socket\"):\\n            self._get_address()\\n        return self._ip', 'def port(self):\\n        if not hasattr(self, \"_socket\"):\\n            self._get_address()\\n        return self._port', 'def socket(self):\\n        if not hasattr(self, \"_socket\"):\\n            self._get_address()\\n        return self._socket', 'def remote_addr(self):\\n        \"\"\"Attempt to return the original client ip based on X-Forwarded-For.\\n\\n        :return: original client ip.\\n        \"\"\"\\n        if not hasattr(self, \"_remote_addr\"):\\n            forwarded_for = self.headers.get(\"X-Forwarded-For\", \"\").split(\",\")\\n            remote_addrs = [\\n                addr\\n                for addr in [addr.strip() for addr in forwarded_for]\\n                if addr\\n            ]\\n            if len(remote_addrs) > 0:\\n                self._remote_addr = remote_addrs[0]\\n            else:\\n                self._remote_addr = \"\"\\n        return self._remote_addr', 'def scheme(self):\\n        if (\\n            self.app.websocket_enabled\\n            and self.headers.get(\"upgrade\") == \"websocket\"\\n        ):\\n            scheme = \"ws\"\\n        else:\\n            scheme = \"http\"\\n\\n        if self.transport.get_extra_info(\"sslcontext\"):\\n            scheme += \"s\"\\n\\n        return scheme', 'def host(self):\\n        # it appears that httptools doesn\\'t return the host\\n        # so pull it from the headers\\n        return self.headers.get(\"Host\", \"\")', 'def content_type(self):\\n        return self.headers.get(\"Content-Type\", DEFAULT_HTTP_CONTENT_TYPE)', 'def match_info(self):\\n        \"\"\"return matched info after resolving route\"\"\"\\n        return self.app.router.get(self)[2]', 'def path(self):\\n        return self._parsed_url.path.decode(\"utf-8\")', 'def query_string(self):\\n        if self._parsed_url.query:\\n            return self._parsed_url.query.decode(\"utf-8\")\\n        else:\\n            return \"\"', 'def url(self):\\n        return urlunparse(\\n            (self.scheme, self.host, self.path, None, self.query_string, None)\\n        )']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_bug_info(self, repository, bug_id):\\n        \"\"\"Get the information for the specified bug.\\n\\n        This should return a dictionary with \\'summary\\', \\'description\\', and\\n        \\'status\\' keys.\\n\\n        This is cached for 60 seconds to reduce the number of queries to the\\n        bug trackers and make things seem fast after the first infobox load,\\n        but is still a short enough time to give relatively fresh data.\\n        \"\"\"\\n        return cache_memoize(self.make_bug_cache_key(repository, bug_id),\\n                             lambda: self.get_bug_info_uncached(repository,\\n                                                                bug_id),\\n                             expiration=60)']}, {'features': [], 'snippets': ['def parse_expression_into_parts(expression):\\n    \"\"\"\\n    Parse expression into list of parts\\n    :rtype : list\\n    :param expression: str # i.e. \"2 * 3 + ( 2 - 3 )\"\\n    \"\"\"\\n    raise NotImplementedError(\"complete me!\")', 'def evaluate_postfix(parts):\\n    raise NotImplementedError(\"complete me!\")']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_create_experiment_view(self):\\n        \"\"\" Tests edit_experiment template renders for url \\'create_experiment\\' \"\"\"\\n        response = self.client.get(reverse(\"ab_testing_tool_create_experiment\"))\\n        self.assertOkay(response)\\n        self.assertTemplateUsed(response, \"ab_tool/edit_experiment.html\")', 'def test_create_experiment_view_unauthorized(self):\\n        \"\"\" Tests edit_experiment template does not render for url \\'create_experiment\\'\\n            when unauthorized \"\"\"\\n        self.set_roles([])\\n        response = self.client.get(reverse(\"ab_testing_tool_create_experiment\"), follow=True)\\n        self.assertTemplateNotUsed(response, \"ab_tool/create_experiment.html\")\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")', 'def test_edit_experiment_view(self):\\n        \"\"\" Tests edit_experiment template renders when authenticated \"\"\"\\n        experiment = self.create_test_experiment()\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(experiment.id,)))\\n        self.assertTemplateUsed(response, \"ab_tool/edit_experiment.html\")', 'def test_edit_experiment_view_started_experiment(self):\\n        \"\"\" Tests edit_experiment template renders when experiment has started \"\"\"\\n        experiment = self.create_test_experiment()\\n        experiment.tracks_finalized = True\\n        experiment.save()\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(experiment.id,)))\\n        self.assertTemplateUsed(response, \"ab_tool/edit_experiment.html\")', 'def test_edit_experiment_view_with_tracks_weights(self):\\n        \"\"\" Tests edit_experiment template renders properly with track weights \"\"\"\\n        experiment = self.create_test_experiment()\\n        experiment.assignment_method = Experiment.WEIGHTED_PROBABILITY_RANDOM\\n        track1 = self.create_test_track(name=\"track1\", experiment=experiment)\\n        track2 = self.create_test_track(name=\"track2\", experiment=experiment)\\n        self.create_test_track_weight(experiment=experiment, track=track1)\\n        self.create_test_track_weight(experiment=experiment, track=track2)\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(experiment.id,)))\\n        self.assertTemplateUsed(response, \"ab_tool/edit_experiment.html\")', 'def test_edit_experiment_view_unauthorized(self):\\n        \"\"\" Tests edit_experiment template doesn\\'t render when unauthorized \"\"\"\\n        self.set_roles([])\\n        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(experiment.id,)),\\n                                   follow=True)\\n        self.assertTemplateNotUsed(response, \"ab_tool/edit_experiment.html\")\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")', 'def test_edit_experiment_view_nonexistent(self):\\n        \"\"\"Tests edit_experiment when experiment does not exist\"\"\"\\n        e_id = NONEXISTENT_EXPERIMENT_ID\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(e_id,)))\\n        self.assertTemplateNotUsed(response, \"ab_tool/edit_experiment.html\")\\n        self.assertEquals(response.status_code, 404)', 'def test_edit_experiment_view_wrong_course(self):\\n        \"\"\" Tests edit_experiment when attempting to access a experiment from a different course \"\"\"\\n        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)\\n        response = self.client.get(reverse(\"ab_testing_tool_edit_experiment\", args=(experiment.id,)))\\n        self.assertError(response, UNAUTHORIZED_ACCESS)', 'def test_submit_create_experiment(self):\\n        \"\"\" Tests that create_experiment creates a Experiment object verified by\\n            DB count when uniformRandom is true\"\"\"\\n        Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        num_experiments = Experiment.objects.count()\\n        experiment = {\\n                \"name\": \"experiment\", \"notes\": \"hi\", \"uniformRandom\": True,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": None, \"name\": \"A\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_create_experiment\"), follow=True,\\n            content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertEquals(num_experiments + 1, Experiment.objects.count(), response)', 'def test_submit_create_experiment_with_weights_as_assignment_method(self):\\n        \"\"\" Tests that create_experiment creates a Experiment object verified by\\n            DB count when uniformRandom is false and the tracks have weightings \"\"\"\\n        Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        num_experiments = Experiment.objects.count()\\n        experiment = {\\n                \"name\": \"experiment\", \"notes\": \"hi\", \"uniformRandom\": False,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": 100, \"name\": \"A\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_create_experiment\"), follow=True,\\n            content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertEquals(num_experiments + 1, Experiment.objects.count(), response)', 'def test_submit_create_experiment_unauthorized(self):\\n        \"\"\"Tests that create_experiment creates a Experiment object verified by DB count\"\"\"\\n        self.set_roles([])\\n        Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        num_experiments = Experiment.objects.count()\\n        experiment = {\"name\": \"experiment\", \"notes\": \"hi\"}\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_create_experiment\"), follow=True,\\n            content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")', 'def test_submit_edit_experiment(self):\\n        \"\"\" Tests that submit_edit_experiment does not change DB count but does change Experiment\\n            attribute\"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\")\\n        experiment_id = experiment.id\\n        num_experiments = Experiment.objects.count()\\n        experiment = {\\n                \"name\": \"new_name\", \"notes\": \"hi\", \"uniformRandom\": True,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": None, \"name\": \"A\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertOkay(response)\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        experiment = Experiment.objects.get(id=experiment_id)\\n        self.assertEquals(experiment.name, \"new_name\")', 'def test_submit_edit_experiment_changes_assignment_method_to_weighted(self):\\n        \"\"\" Tests that submit_edit_experiment changes an Experiment\\'s assignment\\n            method from uniform (default) to weighted\"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\")\\n        experiment_id = experiment.id\\n        num_experiments = Experiment.objects.count()\\n        no_track_weights = experiment.track_probabilites.count()\\n        experiment = {\\n                \"name\": \"new_name\", \"notes\": \"hi\", \"uniformRandom\": False,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": 20, \"name\": \"A\"},\\n                           {\"id\": None, \"weighting\": 80, \"name\": \"B\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertOkay(response)\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        experiment = Experiment.objects.get(id=experiment_id)\\n        self.assertEquals(experiment.assignment_method, Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        self.assertEquals(experiment.track_probabilites.count(), no_track_weights + 2)', 'def test_submit_edit_experiment_changes_assignment_method_to_uniform(self):\\n        \"\"\" Tests that submit_edit_experiment changes an Experiment\\'s assignment\\n            method from weighted uniform \"\"\"\\n        experiment = self.create_test_experiment(\\n                name=\"old_name\", assignment_method=Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        experiment_id = experiment.id\\n        num_experiments = Experiment.objects.count()\\n        no_tracks = experiment.tracks.count()\\n        experiment = {\\n                \"name\": \"new_name\", \"notes\": \"hi\", \"uniformRandom\": True,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": None, \"name\": \"A\"},\\n                           {\"id\": None, \"weighting\": None, \"name\": \"B\"},\\n                           {\"id\": None, \"weighting\": None, \"name\": \"C\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertOkay(response)\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        experiment = Experiment.objects.get(id=experiment_id)\\n        self.assertEquals(experiment.assignment_method, Experiment.UNIFORM_RANDOM)\\n        self.assertEquals(experiment.tracks.count(), no_tracks + 3)', 'def test_submit_edit_experiment_unauthorized(self):\\n        \"\"\" Tests submit_edit_experiment when unauthorized\"\"\"\\n        self.set_roles([])\\n        experiment = self.create_test_experiment(name=\"old_name\")\\n        experiment_id = experiment.id\\n        experiment = {\"name\": \"new_name\", \"notes\": \"\"}\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            content_type=\"application/json\", data=json.dumps(experiment), follow=True\\n        )\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")', 'def test_submit_edit_experiment_nonexistent(self):\\n        \"\"\" Tests that submit_edit_experiment method raises error for non-existent Experiment \"\"\"\\n        experiment_id = NONEXISTENT_EXPERIMENT_ID\\n        experiment = {\"name\": \"new_name\", \"notes\": \"\"}\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertEquals(response.status_code, 404)', 'def test_submit_edit_experiment_wrong_course(self):\\n        \"\"\" Tests that submit_edit_experiment method raises error for existent Experiment but\\n            for wrong course\"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\",\\n                                       course_id=TEST_OTHER_COURSE_ID)\\n        data = {\"name\": \"new_name\", \"notes\": \"\"}\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment.id,)),\\n            content_type=\"application/json\", data=json.dumps(data)\\n        )\\n        self.assertError(response, UNAUTHORIZED_ACCESS)', 'def test_submit_edit_started_experiment_changes_name_and_notes(self):\\n        \"\"\" Tests that submit_edit_experiment changes an Experiment\\'s\\n            name and notes and track names only if the experiment has already been started \"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\", notes=\"old_notes\",\\n                                                 tracks_finalized=True)\\n        experiment_id = experiment.id\\n        num_experiments = Experiment.objects.count()\\n        old_track = self.create_test_track(experiment=experiment, name=\"old_name_track\")\\n        experiment_json = {\\n                \"name\": \"new_name\", \"notes\": \"new_notes\", \"tracks\": [{\"id\": old_track.id,\\n                  \"name\": \"new_track_name\"}],\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment_json)\\n        )\\n        self.assertOkay(response)\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        experiment = Experiment.objects.get(id=experiment_id)\\n        self.assertEquals(experiment.name, \"new_name\")\\n        self.assertEquals(experiment.notes, \"new_notes\")\\n        self.assertEquals(experiment.tracks.all()[0].name, \"new_track_name\")', 'def test_submit_edit_started_experiment_does_not_change_tracks(self):\\n        \"\"\" Tests that submit_edit_experiment doesn\\'t change tracks for\\n            an experiment that has already been started \"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\", tracks_finalized=True,\\n                assignment_method=Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        experiment_id = experiment.id\\n        num_experiments = Experiment.objects.count()\\n        no_tracks = experiment.tracks.count()\\n        experiment = {\\n                \"name\": \"new_name\", \"notes\": \"hi\", \"uniformRandom\": True,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": None, \"weighting\": None, \"name\": \"A\"},\\n                           {\"id\": None, \"weighting\": None, \"name\": \"B\"},\\n                           {\"id\": None, \"weighting\": None, \"name\": \"C\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment_id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment)\\n        )\\n        self.assertOkay(response)\\n        self.assertEquals(num_experiments, Experiment.objects.count())\\n        experiment = Experiment.objects.get(id=experiment_id)\\n        self.assertEquals(experiment.assignment_method, Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        self.assertEquals(experiment.tracks.count(), no_tracks)', 'def test_submit_edit_started_experiment_changes_existing_tracks(self):\\n        \"\"\" Tests that submit_edit_experiment does change track objects for\\n            an experiment that has not yet been started \"\"\"\\n        experiment = self.create_test_experiment(name=\"old_name\", tracks_finalized=False,\\n                assignment_method=Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        track1 = self.create_test_track(experiment=experiment, name=\"A\")\\n        track2 = self.create_test_track(experiment=experiment, name=\"B\")\\n        self.create_test_track_weight(experiment=experiment, track=track1)\\n        self.create_test_track_weight(experiment=experiment, track=track2)\\n        track_count = experiment.tracks.count()\\n        experiment_json = {\\n                \"name\": \"new_name\", \"notes\": \"hi\", \"uniformRandom\": False,\\n                \"csvUpload\": False,\\n                \"tracks\": [{\"id\": track1.id, \"weighting\": 30, \"name\": \"C\"},\\n                           {\"id\": track2.id, \"weighting\": 70, \"name\": \"D\"}]\\n        }\\n        response = self.client.post(\\n            reverse(\"ab_testing_tool_submit_edit_experiment\", args=(experiment.id,)),\\n            follow=True, content_type=\"application/json\", data=json.dumps(experiment_json)\\n        )\\n        self.assertOkay(response)\\n        experiment = Experiment.objects.get(id=experiment.id)\\n        self.assertEquals(experiment.assignment_method, Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        self.assertEquals(experiment.tracks.count(), track_count)\\n        track1 = experiment.tracks.get(id=track1.id)\\n        track2 = experiment.tracks.get(id=track2.id)\\n        self.assertEquals(track1.name, \"C\") #Checks name has changed\\n        self.assertEquals(track2.name, \"D\")\\n        self.assertEquals(track1.weight.weighting, 30) #Checks weighting has changed\\n        self.assertEquals(track2.weight.weighting, 70)', 'def test_delete_experiment(self):\\n        \"\"\" Tests that delete_experiment method properly deletes a experiment when authorized\"\"\"\\n        first_num_experiments = Experiment.objects.count()\\n        experiment = self.create_test_experiment()\\n        self.assertEqual(first_num_experiments + 1, Experiment.objects.count())\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                    follow=True)\\n        second_num_experiments = Experiment.objects.count()\\n        self.assertOkay(response)\\n        self.assertEqual(first_num_experiments, second_num_experiments)', 'def test_delete_experiment_already_finalized(self):\\n        \"\"\" Tests that delete experiment doesn\\'t work when experiments are finalized \"\"\"\\n        experiment = self.create_test_experiment()\\n        experiment.update(tracks_finalized=True)\\n        first_num_experiments = Experiment.objects.count()\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                    follow=True)\\n        second_num_experiments = Experiment.objects.count()\\n        self.assertError(response, EXPERIMENT_TRACKS_ALREADY_FINALIZED)\\n        self.assertEqual(first_num_experiments, second_num_experiments)', 'def test_delete_experiment_has_installed_intervention_point(self, _mock1):\\n        \"\"\" Tests that delete experiment doesn\\'t work when there is an associated\\n            intervention point is installed \"\"\"\\n        experiment = self.create_test_experiment()\\n        first_num_experiments = Experiment.objects.count()\\n        ret_val = [True]\\n        with patch(\"ab_tool.canvas.CanvasModules.experiment_has_installed_intervention\",\\n                   return_value=ret_val):\\n            response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                        follow=True)\\n            second_num_experiments = Experiment.objects.count()\\n            self.assertError(response, INTERVENTION_POINTS_ARE_INSTALLED)\\n            self.assertEqual(first_num_experiments, second_num_experiments)', 'def test_delete_experiment_unauthorized(self):\\n        \"\"\" Tests that delete_experiment method raises error when unauthorized \"\"\"\\n        self.set_roles([])\\n        experiment = self.create_test_experiment()\\n        first_num_experiments = Experiment.objects.count()\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                    follow=True)\\n        second_num_experiments = Experiment.objects.count()\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")\\n        self.assertEqual(first_num_experiments, second_num_experiments)', 'def test_delete_experiment_nonexistent(self):\\n        \"\"\" Tests that delete_experiment method raises successfully redirects\\n            despite non-existent Experiment. This is by design, as the Http404\\n            is caught since multiple users may be editing the A/B dashboard on\\n            in the same course \"\"\"\\n        self.create_test_experiment()\\n        t_id = NONEXISTENT_EXPERIMENT_ID\\n        first_num_experiments = Experiment.objects.count()\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(t_id,)), follow=True)\\n        second_num_experiments = Experiment.objects.count()\\n        self.assertEqual(first_num_experiments, second_num_experiments)\\n        self.assertOkay(response)', 'def test_delete_experiment_wrong_course(self):\\n        \"\"\" Tests that delete_experiment method raises error for existent Experiment but for\\n            wrong course \"\"\"\\n        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)\\n        first_num_experiments = Experiment.objects.count()\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                   follow=True)\\n        second_num_experiments = Experiment.objects.count()\\n        self.assertEqual(first_num_experiments, second_num_experiments)\\n        self.assertError(response, UNAUTHORIZED_ACCESS)', 'def test_delete_experiment_deletes_intervention_point_urls(self):\\n        \"\"\" Tests that intervention_point_urls of a experiment are deleted when the experiment is \"\"\"\\n        experiment = self.create_test_experiment()\\n        track1 = self.create_test_track(name=\"track1\", experiment=experiment)\\n        track2 = self.create_test_track(name=\"track2\", experiment=experiment)\\n        intervention_point = self.create_test_intervention_point()\\n        InterventionPointUrl.objects.create(intervention_point=intervention_point,\\n                                            track=track1, url=\"example.com\")\\n        InterventionPointUrl.objects.create(intervention_point=intervention_point,\\n                                            track=track2, url=\"example.com\")\\n        first_num_intervention_point_urls = InterventionPointUrl.objects.count()\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_experiment\", args=(experiment.id,)),\\n                                    follow=True)\\n        second_num_intervention_point_urls = InterventionPointUrl.objects.count()\\n        self.assertOkay(response)\\n        self.assertEqual(first_num_intervention_point_urls - 2, second_num_intervention_point_urls)', 'def test_finalize_tracks(self):\\n        \"\"\" Tests that the finalize tracks page sets the appropriate course \"\"\"\\n        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        self.assertFalse(experiment.tracks_finalized)\\n        self.create_test_track()\\n        response = self.client.post(reverse(\"ab_testing_tool_finalize_tracks\", args=(experiment.id,)),\\n                                    follow=True)\\n        self.assertOkay(response)\\n        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        self.assertTrue(experiment.tracks_finalized)', 'def test_finalize_tracks_missing_urls(self):\\n        \"\"\" Tests that finalize fails if there are missing urls \"\"\"\\n        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        self.assertFalse(experiment.tracks_finalized)\\n        track1 = self.create_test_track(name=\"track1\", experiment=experiment)\\n        self.create_test_track(name=\"track2\", experiment=experiment)\\n        intervention_point = self.create_test_intervention_point()\\n        InterventionPointUrl.objects.create(intervention_point=intervention_point,\\n                                            track=track1, url=\"example.com\")\\n        response = self.client.post(reverse(\"ab_testing_tool_finalize_tracks\", args=(experiment.id,)), follow=True)\\n        self.assertOkay(response)\\n        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        self.assertFalse(experiment.tracks_finalized)', 'def test_finalize_tracks_no_tracks(self):\\n        \"\"\" Tests that finalize fails if there are no tracks for an experiment \"\"\"\\n        experiment = Experiment.get_placeholder_course_experiment(TEST_COURSE_ID)\\n        response = self.client.post(reverse(\"ab_testing_tool_finalize_tracks\", args=(experiment.id,)),\\n                                    follow=True)\\n        self.assertError(response, NO_TRACKS_FOR_EXPERIMENT)', 'def test_finalize_tracks_missing_track_weights(self):\\n        \"\"\" Tests that finalize fails if there are no track weights for an weighted\\n            probability experiment \"\"\"\\n        experiment = self.create_test_experiment(assignment_method=Experiment.WEIGHTED_PROBABILITY_RANDOM)\\n        self.create_test_track(name=\"track1\", experiment=experiment)\\n        response = self.client.post(reverse(\"ab_testing_tool_finalize_tracks\", args=(experiment.id,)), follow=True)\\n        self.assertOkay(response)\\n        self.assertFalse(experiment.tracks_finalized)', 'def test_copy_experiment(self):\\n        \"\"\" Tests that copy_experiment creates a new experiment \"\"\"\\n        experiment = self.create_test_experiment()\\n        num_experiments = Experiment.objects.count()\\n        url = reverse(\"ab_testing_tool_copy_experiment\", args=(experiment.id,))\\n        response = self.client.post(url, follow=True)\\n        self.assertOkay(response)\\n        self.assertEqual(Experiment.objects.count(), num_experiments + 1)', 'def test_copy_experiment_unauthorized(self):\\n        \"\"\" Tests that copy_experiment fails when unauthorized \"\"\"\\n        self.set_roles([])\\n        experiment = self.create_test_experiment()\\n        url = reverse(\"ab_testing_tool_copy_experiment\", args=(experiment.id,))\\n        response = self.client.post(url, follow=True)\\n        self.assertTemplateUsed(response, \"ab_tool/not_authorized.html\")', 'def test_copy_experiment_inavlid_id(self):\\n        \"\"\" Tests that copy_experiment fails with bad experiment_id \"\"\"\\n        url = reverse(\"ab_testing_tool_copy_experiment\", args=(12345,))\\n        response = self.client.post(url, follow=True)\\n        self.assertEquals(response.status_code, 404)', 'def test_copy_experiment_wrong_course(self):\\n        \"\"\" Tests that copy_experiment fails if experiment is different coruse \"\"\"\\n        experiment = self.create_test_experiment(course_id=TEST_OTHER_COURSE_ID)\\n        url = reverse(\"ab_testing_tool_copy_experiment\", args=(experiment.id,))\\n        response = self.client.post(url, follow=True)\\n        self.assertError(response, UNAUTHORIZED_ACCESS)', 'def test_delete_track(self):\\n        \"\"\" Tests that delete_track method properly deletes a track of an experiment when authorized\"\"\"\\n        experiment = self.create_test_experiment()\\n        track = self.create_test_track(experiment=experiment)\\n        self.assertEqual(experiment.tracks.count(), 1)\\n        response = self.client.post(reverse(\"ab_testing_tool_delete_track\", args=(track.id,)),\\n                                    follow=True)\\n        self.assertEqual(experiment.tracks.count(), 0)\\n        self.assertOkay(response)']}, {'features': [], 'snippets': ['def saveProfileImage(self, filestorage):\\n        buffer = filestorage.stream\\n        buffer.seek(0)\\n        image = Image.open(buffer)\\n        image = ImageUtil.crop_image(image, 64)\\n        current_app.logger.info(image)\\n        dirpath = getDirectoryPath(current_app, \\'_settings\\')\\n        filepath = os.path.join(dirpath, \"profile.png\")\\n        image.save(filepath, optimize=True)']}, {'features': [], 'snippets': ['def __init__(self, module, message):\\n        \"\"\"\\n        SearchException constructor.\\n\\n        Args:\\n            module (str): name of module/class that\\'s raising exception\\n            message (str): exception message to be displayed\\n\\n        Usage:\\n            raise SearchException(\"Test\", \"this is an error\")\\n\\n        \"\"\"\\n        message = \"{0} - {1}\".format(module, message)\\n        Exception.__init__(self, message)', 'def __init__(self, engine, message, code=None):\\n        \"\"\"\\n        EngineException constructor.\\n\\n        Args:\\n            engine (str): name of engine that\\'s raising exception\\n            message (str): exception message to be displayed (ignored usually here)\\n\\n        Kwargs:\\n            code (int): response status code of issued request\\n\\n        Usage:\\n            raise EngineException(\"Bing\", \"\", code=200)\\n\\n        \"\"\"\\n        self.message = message\\n        self.code = code\\n\\n        if code:\\n            self.message = ERROR.get(code, ERROR[\\'default\\']).format(self.code)\\n\\n        SearchException.__init__(self, engine, self.message)']}, {'features': [], 'snippets': ['def makeExcellon(manufacturer=\\'default\\'):\\n    \"\"\"\\n    \"\"\"\\n\\n    ns = {\\'pcbmode\\':config.cfg[\\'ns\\'][\\'pcbmode\\'],\\n          \\'svg\\':config.cfg[\\'ns\\'][\\'svg\\']} \\n\\n    # Open the board\\'s SVG\\n    svg_in = utils.openBoardSVG()\\n    drills_layer = svg_in.find(\"//svg:g[@pcbmode:sheet=\\'drills\\']\",\\n                               namespaces=ns)\\n\\n    excellon = Excellon(drills_layer)\\n\\n    # Save to file\\n    base_dir = os.path.join(config.cfg[\\'base-dir\\'], \\n                            config.cfg[\\'locations\\'][\\'build\\'], \\n                            \\'production\\')\\n    base_name = \"%s_rev_%s\" % (config.brd[\\'config\\'][\\'name\\'],\\n                               config.brd[\\'config\\'][\\'rev\\'])\\n\\n    filename_info = config.cfg[\\'manufacturers\\'][manufacturer][\\'filenames\\'][\\'drills\\']\\n\\n    add = \\'_%s.%s\\' % (\\'drills\\',\\n                      filename_info[\\'plated\\'].get(\\'ext\\') or \\'txt\\')\\n    filename = os.path.join(base_dir, base_name + add)\\n\\n    with open(filename, \"wb\") as f:\\n        for line in excellon.getExcellon():\\n            f.write(line)', 'def __init__(self, svg):\\n        \"\"\"\\n        \"\"\"\\n\\n        self._svg = svg\\n\\n        self._ns = {\\'pcbmode\\':config.cfg[\\'ns\\'][\\'pcbmode\\'],\\n                    \\'svg\\':config.cfg[\\'ns\\'][\\'svg\\']} \\n\\n        # Get all drill paths except for the ones used in the\\n        # drill-index\\n        drill_paths = self._svg.findall(\".//svg:g[@pcbmode:type=\\'component-shapes\\']//svg:path\",\\n                                     namespaces=self._ns)\\n\\n        drills_dict = {}\\n        for drill_path in drill_paths:\\n            diameter = drill_path.get(\\'{\\'+config.cfg[\\'ns\\'][\\'pcbmode\\']+\\'}diameter\\')\\n            location = self._getLocation(drill_path)\\n            if diameter not in drills_dict:\\n                drills_dict[diameter] = {}\\n                drills_dict[diameter][\\'locations\\'] = []\\n            drills_dict[diameter][\\'locations\\'].append(location)\\n\\n        self._preamble = self._createPreamble()\\n        self._content = self._createContent(drills_dict)\\n        self._postamble = self._createPostamble()', 'def _createContent(self, drills):\\n        \"\"\"\\n        \"\"\"\\n        ex = []\\n        for i, diameter in enumerate(drills):\\n            # This is probably not necessary, but I\\'m not 100% certain\\n            # that if the item order of a dict is gurenteed. If not\\n            # the result can be quite devastating where drill\\n            # diameters are wrong!\\n            # Drill index must be greater than 0\\n            drills[diameter][\\'index\\'] = i+1\\n            ex.append(\"T%dC%s\\\\n\" % (i+1, diameter)) \\n\\n        ex.append(\\'M95\\\\n\\') # End of a part program header\\n\\n        for diameter in drills:\\n            ex.append(\"T%s\\\\n\" % drills[diameter][\\'index\\'])\\n            for coord in drills[diameter][\\'locations\\']:\\n                ex.append(self._getPoint(coord))\\n\\n        return ex', 'def _createPostamble(self):\\n        \"\"\"\\n        \"\"\"\\n        ex = []\\n        ex.append(\\'M30\\\\n\\') # End of Program, rewind\\n        return ex']}, {'features': [], 'snippets': ['def test_1dim_distance():\\n    \"\"\"See if this contraption works in 1 dimension\"\"\"\\n    num1 = random.random()\\n    num2 = random.random()\\n    assert kmeans.ndim_euclidean_distance(num1, num2) == abs(num1-num2)', 'def test_maxiters():\\n    \"\"\"ensure the iteration ceiling works\"\"\"\\n  #  assert kmeans.should_iter([], [], iterations=29) == True\\n    assert kmeans.should_iter([], [], iterations=30) == False\\n    assert kmeans.should_iter([], [], iterations=31) == False']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def getParentAndBase(path):\\n    match = PREFIX.match(path)\\n    if match is None:\\n        if path.endswith('/'):\\n            stripped_path = path[:-1]\\n        else:\\n            stripped_path = path\\n        base = FNAME_MATCH.search(stripped_path)\\n        if base is None:\\n            raise ValueError('Invalid path')\\n        parent = FNAME_MATCH.sub('', stripped_path)\\n        return parent, base.group(1)\\n    else:\\n        prefix, leading_slash, uri = match.groups()\\n        parts = uri.split('/')\\n        parent_path = '/'.join(parts[:-1])\\n\\n        if leading_slash is not None:\\n            parent_path = '{prefix}/{uri}'.format(prefix=prefix, uri='/'.join(parts[:-1]))\\n        else:\\n            parent_path = '{prefix}{uri}'.format(prefix=prefix, uri='/'.join(parts[:-1]))\\n        return parent_path, parts[-1]\", 'def md5_for_file(fname):\\n    hash_md5 = hashlib.md5()\\n    with open(fname, \"rb\") as f:\\n        for chunk in iter(lambda: f.read(4096), b\"\"):\\n            hash_md5.update(chunk)\\n    return str(hash_md5.hexdigest())']}, {'features': [], 'snippets': [\"def main(use_idf=False, random_state=None, std=False, n_jobs=-1, verbose=2):\\n    wc_idf_map = None\\n    if use_idf:\\n        # ingredients inverse document frequencies\\n        wc_components = build_tfidf_wc(verbose=(verbose > 0))\\n        wc_idf = wc_components['model'].idf_\\n        wc_idf_words = wc_components['model'].get_feature_names()\\n        wc_idf_map = dict(zip(wc_idf_words, wc_idf))\\n    # word2vec recipe feature vectors\\n    wc_components = build_word2vec_wc(feature_vec_size=120, avg=True, idf=wc_idf_map, verbose=(verbose > 0))\\n    y_train = wc_components['train']['df']['cuisine_code'].as_matrix()\\n    X_train = wc_components['train']['features_matrix']\\n    # standardize features aka mean ~ 0, std ~ 1\\n    if std:\\n        scaler = StandardScaler()\\n        scaler.fit(X_train)\\n        X_train = scaler.transform(X_train)\\n    # random forest supervised classifier\\n    time_0 = time.time()\\n    clf = RandomForestClassifier(n_estimators=100, max_depth=None,\\n        n_jobs=n_jobs, random_state=random_state, verbose=verbose)\\n    # perform cross validation\\n    cv_n_fold = 8\\n    print 'cross validating %s ways...' % cv_n_fold\\n    scores_cv = cross_val_score(clf, X_train, y_train, cv=cv_n_fold, n_jobs=-1)\\n    print 'accuracy: %0.5f (+/- %0.5f)' % (scores_cv.mean(), scores_cv.std() * 2)\\n    time_1 = time.time()\\n    elapsed_time = time_1 - time_0\\n    print 'cross validation took %.3f seconds' % elapsed_time\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        self.latitude = 32.074322\\n        self.longitude = 34.792081\\n        self.radius_meters = 100\\n        self.number_of_vertices = 36\\n        self.polycircle = \\\\\\n            polycircles.Polycircle(latitude=self.latitude,\\n                                   longitude=self.longitude,\\n                                   radius=self.radius_meters,\\n                                   number_of_vertices=self.number_of_vertices)', 'def test_lon_lat_output(self):\\n        \"\"\"Asserts that the vertices in the lat-lon output are in the\\n        right order (lat before long).\"\"\"\\n        for vertex in self.polycircle.to_lon_lat():\\n            assert_almost_equal(vertex[0], self.longitude, places=2)\\n            assert_almost_equal(vertex[1], self.latitude, places=2)', 'def test_kml_equals_lon_lat(self):\\n        \"\"\"Asserts that the return value of to_kml() property is identical to\\n        the return value of to_lon_lat().\"\"\"\\n        assert_equal(self.polycircle.to_kml(), self.polycircle.to_lon_lat())']}, {'features': [], 'snippets': ['def close(self):\\n        return', 'def get_pool(mpi=False, threads=None):\\n    \"\"\" Get a pool object to pass to emcee for parallel processing.\\n        If mpi is False and threads is None, pool is None.\\n\\n        Parameters\\n        ----------\\n        mpi : bool\\n            Use MPI or not. If specified, ignores the threads kwarg.\\n        threads : int (optional)\\n            If mpi is False and threads is specified, use a Python\\n            multiprocessing pool with the specified number of threads.\\n    \"\"\"\\n\\n    if mpi:\\n        from emcee.utils import MPIPool\\n\\n        # Initialize the MPI pool\\n        pool = MPIPool()\\n\\n        # Make sure the thread we\\'re running on is the master\\n        if not pool.is_master():\\n            pool.wait()\\n            sys.exit(0)\\n        logger.debug(\"Running with MPI...\")\\n\\n    elif threads > 1:\\n        logger.debug(\"Running with multiprocessing on {} cores...\"\\n                     .format(threads))\\n        pool = multiprocessing.Pool(threads)\\n\\n    else:\\n        logger.debug(\"Running serial...\")\\n        pool = SerialPool()\\n\\n    return pool', 'def __init__(self, backend):\\n        import matplotlib.pyplot as plt\\n        from IPython.core.interactiveshell import InteractiveShell\\n        from IPython.core.pylabtools import backend2gui\\n\\n        self.shell = InteractiveShell.instance()\\n        self.old_backend = backend2gui[str(plt.get_backend())]\\n        self.new_backend = backend', 'def __exit__(self, type, value, tb):\\n        gui, backend = self.shell.enable_matplotlib(self.old_backend)', 'def __init__(self, somedict):\\n        self._dict = dict(somedict)   # make a copy\\n        self._hash = None', 'def __len__(self):\\n        return len(self._dict)', 'def __hash__(self):\\n        if self._hash is None:\\n            self._hash = hash(frozenset(self._dict.items()))\\n        return self._hash']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def naturaltime(seconds: float, future=False) -> str:\\n    assert future\\n    if seconds < 120:\\n        return \"in {} second{}\".format(\\n            int(seconds), \"s\" if seconds >= 2 else \"\"\\n        )\\n    minutes = seconds / 60\\n    if minutes < 120:\\n        return \"in {} minute{}\".format(\\n            int(minutes), \"s\" if minutes >= 2 else \"\"\\n        )\\n    hours = minutes / 60\\n    if hours < 48:\\n        return \"in {} hour{}\".format(int(hours), \"s\" if hours >= 2 else \"\")\\n    days = hours / 24\\n    return \"in {} day{}\".format(int(days), \"s\" if days >= 2 else \"\")', 'def next_sleep_interval() -> float:\\n    now = get_now(datetime.timezone.utc)\\n    target = now.replace(second=0) + WAKEUP_INTERVAL\\n    return (target - now).total_seconds()', 'def web_site_from_url(runner: web.AppRunner, url: str) -> web.BaseSite:\\n    parsed = urlparse(url)\\n    if parsed.scheme == \"http\":\\n        assert parsed.hostname is not None\\n        assert parsed.port is not None\\n        return web.TCPSite(runner, parsed.hostname, parsed.port)\\n    elif parsed.scheme == \"unix\":\\n        return web.UnixSite(runner, parsed.path)\\n    else:\\n        logger.warning(\\n            \"Ignoring web listen url %s: scheme %r not supported\",\\n            url,\\n            parsed.scheme,\\n        )\\n        raise ValueError(url)', 'def __init__(\\n        self, config_arg: Optional[str], *, config_yaml: Optional[str] = None', 'def signal_shutdown(self) -> None:\\n        logger.debug(\"Signalling shutdown\")\\n        self._stop_event.set()', 'def job_should_run(startup: bool, job: JobConfig) -> bool:\\n        if (\\n            startup\\n            and isinstance(job.schedule, str)\\n            and job.schedule == \"@reboot\"\\n        ):\\n            logger.debug(\\n                \"Job %s (%s) is scheduled for startup (@reboot)\",\\n                job.name,\\n                job.schedule_unparsed,\\n            )\\n            return True\\n        elif isinstance(job.schedule, CronTab):\\n            crontab = job.schedule  # type: CronTab\\n            if crontab.test(get_now(job.timezone).replace(second=0)):\\n                logger.debug(\\n                    \"Job %s (%s) is scheduled for now\",\\n                    job.name,\\n                    job.schedule_unparsed,\\n                )\\n                return True\\n            else:\\n                logger.debug(\\n                    \"Job %s (%s) not scheduled for now\",\\n                    job.name,\\n                    job.schedule_unparsed,\\n                )\\n                return False\\n        else:\\n            return False']}, {'features': [], 'snippets': [\"def genetic_modification(testapp, lab, award):\\n    item = {\\n        'award': award['@id'],\\n        'lab': lab['@id'],\\n        'modified_site_by_coordinates': {\\n            'assembly': 'GRCh38',\\n            'chromosome': '11',\\n            'start': 20000,\\n            'end': 21000\\n        },\\n        'purpose': 'repression',\\n        'category': 'deletion',\\n        'method': 'CRISPR',\\n        'zygosity': 'homozygous'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def genetic_modification_RNAi(testapp, lab, award):\\n    item = {\\n        'award': award['@id'],\\n        'lab': lab['@id'],\\n        'modified_site_by_coordinates': {\\n            'assembly': 'GRCh38',\\n            'chromosome': '11',\\n            'start': 20000,\\n            'end': 21000\\n        },\\n        'purpose': 'repression',\\n        'category': 'deletion',\\n        'method': 'RNAi'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def genetic_modification_source(testapp, lab, award, source, gene):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'introduced_gene': gene['@id'],\\n        'purpose': 'expression',\\n        'method': 'CRISPR',\\n        'reagents': [\\n            {\\n                'source': source['@id'],\\n                'identifier': 'sigma:ABC123'\\n            }\\n        ]\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def crispr_deletion(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'deletion',\\n        'purpose': 'repression',\\n        'method': 'CRISPR'\\n    }\", \"def crispr_deletion_1(testapp, lab, award, target):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'deletion',\\n        'purpose': 'repression',\\n        'method': 'CRISPR',\\n        'modified_site_by_target_id': target['@id'],\\n        'guide_rna_sequences': ['ACCGGAGA']\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def tale_deletion(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'deletion',\\n        'purpose': 'repression',\\n        'method': 'TALEN',\\n        'zygosity': 'heterozygous'\\n    }\", \"def crispr_tag(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'method': 'CRISPR'\\n    }\", \"def bombardment_tag(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'nucleic_acid_delivery_method': ['bombardment']\\n    }\", \"def recomb_tag(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'method': 'site-specific recombination'\\n    }\", \"def transfection_tag(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'nucleic_acid_delivery_method': ['stable transfection']\\n    }\", \"def crispri(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'interference',\\n        'purpose': 'repression',\\n        'method': 'CRISPR'\\n    }\", \"def rnai(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'interference',\\n        'purpose': 'repression',\\n        'method': 'RNAi'\\n    }\", \"def mutagen(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'mutagenesis',\\n        'purpose': 'repression',\\n        'method': 'mutagen treatment'\\n    }\", \"def tale_replacement(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'replacement',\\n        'purpose': 'characterization',\\n        'method': 'TALEN',\\n        'zygosity': 'heterozygous'\\n    }\", \"def mpra(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'characterization',\\n        'nucleic_acid_delivery_method': ['transduction']\\n    }\", \"def starr_seq(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'episome',\\n        'purpose': 'characterization',\\n        'nucleic_acid_delivery_method': ['transient transfection']\\n    }\", \"def introduced_elements(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'episome',\\n        'purpose': 'characterization',\\n        'nucleic_acid_delivery_method': ['transient transfection'],\\n        'introduced_elements': 'genomic DNA regions'\\n    }\", \"def crispr_tag_1(testapp, lab, award, ctcf):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'method': 'CRISPR',\\n        'modified_site_by_gene_id': ctcf['@id'],\\n        'introduced_tags': [{'name': 'mAID-mClover', 'location': 'C-terminal'}]\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def mpra_1(testapp, lab, award):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'characterization',\\n        'nucleic_acid_delivery_method': ['transduction'],\\n        'introduced_elements': 'synthesized DNA',\\n        'modified_site_nonspecific': 'random'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def recomb_tag_1(testapp, lab, award, target, treatment_5, document):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'method': 'site-specific recombination',\\n        'modified_site_by_target_id': target['@id'],\\n        'modified_site_nonspecific': 'random',\\n        'category': 'insertion',\\n        'treatments': [treatment_5['@id']],\\n        'documents': [document['@id']],\\n        'introduced_tags': [{'name': 'eGFP', 'location': 'C-terminal'}]\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def rnai_1(testapp, lab, award, source, target):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'interference',\\n        'purpose': 'repression',\\n        'method': 'RNAi',\\n        'reagents': [{'source': source['@id'], 'identifier': 'addgene:12345'}],\\n        'rnai_sequences': ['ATTACG'],\\n        'modified_site_by_target_id': target['@id']\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def genetic_modification_1(lab, award):\\n    return {\\n        'modification_type': 'deletion',\\n        'award': award['uuid'],\\n        'lab': lab['uuid'],\\n        'modifiction_description': 'some description'\\n    }\", \"def genetic_modification_2(lab, award):\\n    return {\\n        'modification_type': 'deletion',\\n        'award': award['uuid'],\\n        'lab': lab['uuid'],\\n        'modification_description': 'some description',\\n        'modification_zygocity': 'homozygous',\\n        'modification_purpose': 'tagging',\\n        'modification_treatments': [],\\n        'modification_genome_coordinates': [{\\n            'chromosome': '11',\\n            'start': 5309435,\\n            'end': 5309451\\n            }]\\n    }\", 'def crispr_gm(lab, award, source):\\n    return {\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'source\\': source[\\'uuid\\'],\\n        \\'guide_rna_sequences\\': [\\n            \"ACA\",\\n            \"GCG\"\\n        ],\\n        \\'insert_sequence\\': \\'TCGA\\',\\n        \\'aliases\\': [\\'encode:crispr_technique1\\'],\\n        \\'@type\\': [\\'Crispr\\', \\'ModificationTechnique\\', \\'Item\\'],\\n        \\'@id\\': \\'/crisprs/79c1ec08-c878-4419-8dba-66aa4eca156b/\\',\\n        \\'uuid\\': \\'79c1ec08-c878-4419-8dba-66aa4eca156b\\'\\n    }', \"def genetic_modification_5(lab, award, crispr_gm):\\n    return {\\n        'modification_type': 'deletion',\\n        'award': award['uuid'],\\n        'lab': lab['uuid'],\\n        'description': 'blah blah description blah',\\n        'zygosity': 'homozygous',\\n        'treatments': [],\\n        'source': 'sigma',\\n        'product_id': '12345',\\n        'modification_techniques': [crispr_gm],\\n        'modified_site': [{\\n            'assembly': 'GRCh38',\\n            'chromosome': '11',\\n            'start': 5309435,\\n            'end': 5309451\\n            }]\\n    }\", 'def genetic_modification_6(lab, award, crispr_gm, source):\\n    return {\\n        \\'purpose\\': \\'validation\\',\\n        \\'category\\': \\'deeltion\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'description\\': \\'blah blah description blah\\',\\n        \"method\": \"CRISPR\",\\n        \"modified_site_by_target_id\": \"/targets/FLAG-ZBTB43-human/\",\\n        \"reagents\": [\\n            {\\n                \"identifier\": \"placeholder_id\",\\n                \"source\": source[\\'uuid\\']\\n            }\\n        ]\\n    }', 'def genetic_modification_7_invalid_reagent(lab, award, crispr_gm):\\n    return {\\n        \\'purpose\\': \\'characterization\\',\\n        \\'category\\': \\'deletion\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'description\\': \\'blah blah description blah\\',\\n        \"method\": \"CRISPR\",\\n        \"modified_site_by_target_id\": \"/targets/FLAG-ZBTB43-human/\",\\n        \"reagents\": [\\n            {\\n                \"identifier\": \"placeholder_id\",\\n                \"source\": \"/sources/sigma/\"\\n            }\\n        ]\\n    }', 'def genetic_modification_7_valid_reagent(lab, award, crispr_gm):\\n    return {\\n        \\'purpose\\': \\'characterization\\',\\n        \\'category\\': \\'deletion\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'description\\': \\'blah blah description blah\\',\\n        \"method\": \"CRISPR\",\\n        \"modified_site_by_target_id\": \"/targets/FLAG-ZBTB43-human/\",\\n        \"reagents\": [\\n            {\\n                \"identifier\": \"ABC123\",\\n                \"source\": \"/sources/sigma/\"\\n            }\\n        ]\\n    }', \"def genetic_modification_7_addgene_source(testapp):\\n    item = {\\n        'name': 'addgene',\\n        'title': 'Addgene',\\n        'status': 'released'\\n    }\\n    return testapp.post_json('/source', item).json['@graph'][0]\", 'def genetic_modification_7_multiple_matched_identifiers(lab, award, crispr_gm):\\n    return {\\n        \\'purpose\\': \\'characterization\\',\\n        \\'category\\': \\'deletion\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'description\\': \\'blah blah description blah\\',\\n        \"method\": \"CRISPR\",\\n        \"modified_site_by_target_id\": \"/targets/FLAG-ZBTB43-human/\",\\n        \"reagents\": [\\n            {\\n                \"identifier\": \"12345\",\\n                \"source\": \"/sources/addgene/\"\\n            }\\n        ]\\n    }', 'def genetic_modification_7_multiple_reagents(lab, award, crispr_gm):\\n    return {\\n        \\'purpose\\': \\'characterization\\',\\n        \\'category\\': \\'deletion\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \\'description\\': \\'blah blah description blah\\',\\n        \"method\": \"CRISPR\",\\n        \"modified_site_by_target_id\": \"/targets/FLAG-ZBTB43-human/\",\\n        \"reagents\": [\\n            {\\n                \"identifier\": \"12345\",\\n                \"source\": \"/sources/addgene/\",\\n                \"url\": \"http://www.addgene.org\"\\n            },\\n            {\\n                \"identifier\": \"67890\",\\n                \"source\": \"/sources/addgene/\",\\n                \"url\": \"http://www.addgene.org\"\\n            }\\n        ]\\n    }', 'def genetic_modification_8(lab, award):\\n    return {\\n        \\'purpose\\': \\'analysis\\',\\n        \\'category\\': \\'interference\\',\\n        \\'award\\': award[\\'uuid\\'],\\n        \\'lab\\': lab[\\'uuid\\'],\\n        \"method\": \"CRISPR\",\\n    }', \"def construct_genetic_modification(\\n        testapp,\\n        lab,\\n        award,\\n        document,\\n        target_ATF5_genes,\\n        target_promoter):\\n    item = {\\n        'award': award['@id'],\\n        'documents': [document['@id']],\\n        'lab': lab['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'nucleic_acid_delivery_method': ['stable transfection'],\\n        'introduced_tags': [{'name':'eGFP', 'location': 'C-terminal', 'promoter_used': target_promoter['@id']}],\\n        'modified_site_by_target_id': target_ATF5_genes['@id']\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def construct_genetic_modification_N(\\n        testapp,\\n        lab,\\n        award,\\n        document,\\n        target):\\n    item = {\\n        'award': award['@id'],\\n        'documents': [document['@id']],\\n        'lab': lab['@id'],\\n        'category': 'insertion',\\n        'purpose': 'tagging',\\n        'nucleic_acid_delivery_method': ['stable transfection'],\\n        'introduced_tags': [{'name':'eGFP', 'location': 'N-terminal'}],\\n        'modified_site_by_target_id': target['@id']\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def interference_genetic_modification(\\n        testapp,\\n        lab,\\n        award,\\n        document,\\n        target):\\n    item = {\\n        'award': award['@id'],\\n        'documents': [document['@id']],\\n        'lab': lab['@id'],\\n        'category': 'interference',\\n        'purpose': 'repression',\\n        'method': 'RNAi',        \\n        'modified_site_by_target_id': target['@id']\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def crispr_knockout(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'knockout',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\", 'def recombination_knockout(lab, award):\\n    return {\\n        \\'lab\\': lab[\\'@id\\'],\\n        \\'award\\': award[\\'@id\\'],\\n        \\'category\\': \\'knockout\\',\\n        \\'purpose\\': \\'repression\\',\\n        \\'method\\': \\'site-specific recombination\\',\\n        \\'modified_site_by_coordinates\\': {\\n            \"assembly\": \"GRCh38\",\\n            \"chromosome\": \"11\",\\n            \"start\": 60000,\\n            \"end\": 62000\\n        }\\n    }', \"def characterization_insertion_transfection(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'characterization',\\n        'nucleic_acid_delivery_method': ['stable transfection'],\\n        'modified_site_nonspecific': 'random',\\n        'introduced_elements': 'synthesized DNA'\\n    }\", \"def characterization_insertion_CRISPR(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR',\\n        'modified_site_nonspecific': 'random',\\n        'introduced_elements': 'synthesized DNA'\\n    }\", \"def disruption_genetic_modification(testapp, lab, award):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'CRISPR cutting',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def activation_genetic_modification(testapp, lab, award):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'CRISPRa',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def binding_genetic_modification(testapp, lab, award):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'CRISPR dCas',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def HR_knockout(lab, award, target):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'knockout',\\n        'purpose': 'repression',\\n        'method': 'homologous recombination',\\n        'modified_site_by_target_id': target['@id']\\n    }\", \"def CRISPR_introduction(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'expression',\\n        'nucleic_acid_delivery_method': ['transient transfection']\\n    }\", \"def genetic_modification_9(lab, award, human_donor_1):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'donor': human_donor_1['@id'],\\n        'category': 'insertion',\\n        'purpose': 'expression',\\n        'method': 'transient transfection'\\n    }\", \"def transgene_insertion(testapp, lab, award, ctcf):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'in vivo enhancer characterization',\\n        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],\\n        'modified_site_by_gene_id': ctcf['@id'],\\n        'introduced_sequence': 'ATCGTA'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def guides_transduction_GM(testapp, lab, award):\\n    item = {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'expression',\\n        'nucleic_acid_delivery_method': ['transduction'],\\n        'introduced_elements': 'gRNAs and CRISPR machinery',\\n        'MOI': 'high',\\n        'guide_type': 'sgRNA'\\n    }\\n    return testapp.post_json('/genetic_modification', item).json['@graph'][0]\", \"def genetic_modification_10(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'insertion',\\n        'purpose': 'expression',\\n        'nucleic_acid_delivery_method': ['transduction'],\\n        'introduced_elements': 'gRNAs and CRISPR machinery',\\n    }\", \"def genetic_modification_11(lab, award):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'disruption',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\", \"def transgene_insertion_2(testapp, lab, award, ctcf):\\n    return {\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'transgene insertion',\\n        'purpose': 'in vivo enhancer characterization',\\n        'nucleic_acid_delivery_method': ['mouse pronuclear microinjection'],\\n        'modified_site_by_gene_id': ctcf['@id'],\\n        'introduced_sequence': 'ATCGTA'\\n    }\", \"def activation_genetic_modification_2(testapp, lab, award):\\n    return{\\n        'lab': lab['@id'],\\n        'award': award['@id'],\\n        'category': 'activation',\\n        'purpose': 'characterization',\\n        'method': 'CRISPR'\\n    }\"]}, {'features': [], 'snippets': ['def test_make_gym_venv_nostack(name, num_envs, state_shape, reward_scale):\\n    seed = 0\\n    frame_op = None\\n    frame_op_len = None\\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\\n    venv.reset()\\n    for i in range(5):\\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\\n\\n    assert isinstance(state, np.ndarray)\\n    assert state.shape == (num_envs,) + state_shape\\n    assert isinstance(reward, np.ndarray)\\n    assert reward.shape == (num_envs,)\\n    assert isinstance(done, np.ndarray)\\n    assert done.shape == (num_envs,)\\n    assert len(info) == num_envs\\n    venv.close()', \"def test_make_gym_concat(name, num_envs, state_shape, reward_scale):\\n    seed = 0\\n    frame_op = 'concat'  # used for image, or for concat vector\\n    frame_op_len = 4\\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\\n    venv.reset()\\n    for i in range(5):\\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\\n\\n    assert isinstance(state, np.ndarray)\\n    stack_shape = (num_envs, frame_op_len * state_shape[0],) + state_shape[1:]\\n    assert state.shape == stack_shape\\n    assert isinstance(reward, np.ndarray)\\n    assert reward.shape == (num_envs,)\\n    assert isinstance(done, np.ndarray)\\n    assert done.shape == (num_envs,)\\n    assert len(info) == num_envs\\n    venv.close()\", \"def test_make_gym_stack(name, num_envs, state_shape, reward_scale):\\n    seed = 0\\n    frame_op = 'stack'  # used for rnn\\n    frame_op_len = 4\\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\\n    venv.reset()\\n    for i in range(5):\\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\\n\\n    assert isinstance(state, np.ndarray)\\n    stack_shape = (num_envs, frame_op_len,) + state_shape\\n    assert state.shape == stack_shape\\n    assert isinstance(reward, np.ndarray)\\n    assert reward.shape == (num_envs,)\\n    assert isinstance(done, np.ndarray)\\n    assert done.shape == (num_envs,)\\n    assert len(info) == num_envs\\n    venv.close()\"]}, {'features': [], 'snippets': ['def compute_rbf_kernel_matrix(X):\\n    \"\"\"Compute the RBF kernel matrix with sigma2 as the median pairwise\\n    distance.\\n    \"\"\"\\n    sigma2 = np.median(pairwise_distances(X, metric=\\'euclidean\\'))**2\\n    K = pairwise_kernels(X, X, metric=\\'rbf\\', gamma=1.0/sigma2, n_jobs=-1)\\n    return K', 'def compute_svm_cv(K, y, C=100.0, n_folds=5,\\n                   scoring=balanced_accuracy_scoring):\\n    \"\"\"Compute cross-validated score of SVM with given precomputed kernel.\\n    \"\"\"\\n    cv = StratifiedKFold(y, n_folds=n_folds)\\n    clf = SVC(C=C, kernel=\\'precomputed\\', class_weight=\\'auto\\')\\n    scores = cross_val_score(clf, K, y,\\n                             scoring=scoring, cv=cv)\\n    return scores.mean()', 'def permutation_subjects(y):\\n    \"\"\"Permute class labels of Contextual Disorder dataset.\\n    \"\"\"\\n    y_perm = np.random.randint(0, 2, len(y)/2)\\n    y_perm = np.concatenate((y_perm, np.logical_not(y_perm).astype(int)))\\n    return y_perm', 'def compute_svm_score_nestedCV(K, y, n_folds,\\n                               scoring=balanced_accuracy_scoring,\\n                               random_state=None,\\n                               param_grid=[{\\'C\\': np.logspace(-5, 5, 25)}]):\\n    \"\"\"Compute cross-validated score of SVM using precomputed kernel.\\n    \"\"\"\\n    cv = StratifiedKFold(y, n_folds=n_folds, shuffle=True,\\n                         random_state=random_state)\\n    scores = np.zeros(n_folds)\\n    for i, (train, test) in enumerate(cv):\\n        cvclf = SVC(kernel=\\'precomputed\\')\\n        y_train = y[train]\\n        cvcv = StratifiedKFold(y_train, n_folds=n_folds,\\n                               shuffle=True,\\n                               random_state=random_state)\\n        clf = GridSearchCV(cvclf, param_grid=param_grid, scoring=scoring,\\n                           cv=cvcv, n_jobs=1)\\n        clf.fit(K[train, :][:, train], y_train)\\n        # print clf.best_params_\\n        scores[i] = clf.score(K[test, :][:, train], y[test])\\n\\n    return scores.mean()', 'def apply_ktst(K, y, iterations=10000, subjects=False, verbose=True):\\n    \"\"\"\\n    Compute MMD^2_u, its null distribution and the p-value of the\\n    kernel two-sample test.\\n\\n    Parameters:\\n    ----------\\n    K: array-like\\n        Kernel matrix\\n    y: array_like\\n        class labels\\n    verbose: bool\\n        Verbosity\\n\\n    Returns:\\n    -------\\n    mmd2u: float\\n        MMD^2_u value.\\n    acc_null: array\\n        Null distribution of the MMD^2_u\\n    p_value: float\\n         p-value\\n    \"\"\"\\n    assert len(np.unique(y)) == 2, \\'KTST only works on binary problems\\'\\n\\n    # Assuming that the first m rows of the kernel matrix are from one\\n    # class and the other n rows from the second class.\\n    m = len(y[y == 0])\\n    n = len(y[y == 1])\\n    mmd2u = MMD2u(K, m, n)\\n    if verbose:\\n        print(\"MMD^2_u = %s\" % mmd2u)\\n        print(\"Computing the null distribution.\")\\n    if subjects:\\n        perms = [permutation_subjects_ktst(y) for i in range(iterations)]\\n        mmd2u_null = compute_null_distribution_given_permutations(K, m, n,\\n                                                                  perms,\\n                                                                  iterations)\\n    else:\\n        mmd2u_null = compute_null_distribution(K, m, n, iterations,\\n                                               verbose=verbose)\\n\\n    p_value = max(1.0/iterations, (mmd2u_null > mmd2u).sum()\\n                  / float(iterations))\\n    if verbose:\\n        print(\"p-value ~= %s \\\\t (resolution : %s)\" % (p_value, 1.0/iterations))\\n\\n    return mmd2u, mmd2u_null, p_value']}, {'features': [], 'snippets': [\"def parse(self, x, y):\\r\\n        if (x,y) not in self.resolutions:\\r\\n            resolutions = ', '.join(['%sx%s' % (a, b) for a,b in self.resolutions])\\r\\n            raise Exception('Resolution %s x %s not supported. Available resolutions: %s' % (x,y, resolutions)  )\\r\\n        return Resolution(x, y)\", \"def __colors(self):\\r\\n        return [key for key in self.__dict__.keys() if not key.startswith('_') and key != 'named']\", \"def named(self, name):\\r\\n        if not hasattr(self, name):\\r\\n            colors = ', '.join(self.__colors())\\r\\n            raise Exception('Unknown color %s. Available colors are: %s' % (name, colors))\\r\\n        return getattr(self, name)\", \"def try_parse(value):\\r\\n    try:    return int(value)\\r\\n    except: return { 'true': True, 'false': False }.get(value.lower(), value)\", \"def read_config():\\r\\n    with open('config.cfg', 'r') as cfg_file:\\r\\n        lines = cfg_file.readlines()\\r\\n        lines = [\\r\\n            line.strip().replace(' ', '').split('=')\\r\\n            for line in lines\\r\\n            if line.strip() and '=' in line\\r\\n        ]\\r\\n        cfg = {key:try_parse(value) for key,value in lines}\\r\\n        return cfg\"]}, {'features': [], 'snippets': ['def setUp(self):\\n        fixture_path = \"spec/fixtures/responses/whois.nic.pw/status_available.txt\"\\n        host         = \"whois.nic.pw\"\\n        part         = yawhois.record.Part(open(fixture_path, \"r\").read(), host)\\n        self.record  = yawhois.record.Record(None, [part])', 'def test_available(self):\\n        eq_(self.record.available, True)', \"def test_nameservers(self):\\n        eq_(self.record.nameservers.__class__.__name__, 'list')\\n        eq_(self.record.nameservers, [])\", 'def test_registered(self):\\n        eq_(self.record.registered, False)', 'def test_registrar(self):\\n        eq_(self.record.registrar, None)', \"def test_technical_contacts(self):\\n        eq_(self.record.technical_contacts.__class__.__name__, 'list')\\n        eq_(self.record.technical_contacts, [])\", 'def test_domain_id(self):\\n        eq_(self.record.domain_id, None)']}, {'features': [], 'snippets': ['def missingNumber(self, nums):\\n        \"\"\"\\n        :type nums: List[int]\\n        :rtype: int\\n        \"\"\"\\n        xor = len(nums)\\n\\n        for i, n in enumerate(nums):\\n            xor ^= n\\n            xor ^= i\\n\\n        return xor']}, {'features': [], 'snippets': ['def __init__(\\n        self, plotly_name=\"minexponent\", parent_name=\"choropleth.colorbar\", **kwargs']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def score_syntax_errors(program_lines):\\n    points = {')': 3, ']': 57, '}': 1197, '>': 25137}\\n    s = 0\\n    scores_auto = []\\n\\n    for line in program_lines:\\n        corrupted, stack = corrupted_character(line)\\n\\n        if corrupted:\\n            s += points[corrupted]\\n        else:\\n            scores_auto.append(score_autocomplete(stack))\\n\\n    return s, sorted(scores_auto)[floor(len(scores_auto)/2)]\", 'def stack_converter(st):\\n        return [lookup[element] for element in st[::-1]]', \"def score_autocomplete(stack):\\n    points_autocomplete = {')': 1, ']': 2, '}': 3, '>': 4}\\n    s_auto = 0\\n\\n    for char in stack:\\n        s_auto *= 5\\n        s_auto += points_autocomplete[char]\\n\\n    return s_auto\", \"def test_score_syntax_errors():\\n    assert score_syntax_errors(open('input/10.test').read().splitlines()) == (26397, 288957)\", \"def test_scoring_autocomplete():\\n    assert score_autocomplete('}}]])})]') == 288957\\n    assert score_autocomplete(')}>]})') == 5566\\n    assert score_autocomplete('}}>}>))))') == 1480781\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def isplayfile(pathname) :\\n    if os.path.isfile(pathname) == False:\\n        return False\\n    ext = os.path.splitext(pathname)[1]\\n    ext = ext.lower()\\n    if (ext == '.mp2') : return True;\\n    if (ext == '.mp3') : return True;\\n    if (ext == '.ogg') : return True;\\n    return False\", \"def validate_password(self, login, password):\\n    if login in users :\\n        if encrypt(password) == users[login] :\\n            cherrypy.session['username'] = login\\n            cherrypy.session['database'] = userdatabase(login)\\n            return True\\n\\n    return False\", 'def index(self):\\n        html = header\\n        (_1, _2, id) = getradio(\\'0\\')\\n        (radio, genre, url) = getradio(id)\\n\\n        if id != 0:\\n            html += \\'\\'\\'<h3><a href=\"#\" onClick=\"fplayradio(\\'%s\\')\"> \\'\\'\\' % id\\n            html += \\'\\'\\'Play Last Radio %s <span class=\"glyphicon glyphicon-play\"></span></a></h3>\\'\\'\\' % radio\\n        html += getfooter()\\n        return html', 'def music(self, directory=\\'/mnt/Media/Music/\\'):\\n        html = header\\n        count = 0\\n        html += \\'\\'\\'<table class=\"table table-condensed\">\\'\\'\\'\\n        filelist = os.listdir(directory)\\n        filelist.sort()\\n        for f in filelist:\\n          file = os.path.join(directory, f)\\n          html += \\'\\'\\'<tr>\\'\\'\\'\\n          if isplayfile(file):\\n            html += \\'\\'\\'<td ><a href=\"#\" onClick=\"fplayradio(\\'%s\\')\">\\'\\'\\' % file\\n            html += \\'\\'\\'Play %s<span class=\"glyphicon glyphicon-play\"></span></a></td>\\'\\'\\' % (file)\\n          if os.path.isdir(file):\\n            html += \\'\\'\\'<td ><a href=\"/music?directory=%s\">%s</a> </td>\\'\\'\\' % (file, f)\\n          html += \\'\\'\\'</tr>\\'\\'\\'\\n          count += 1\\n\\n        html += \\'\\'\\'</table>\\'\\'\\'\\n        html += \\'\\'\\'</div> </div>\\'\\'\\'\\n\\n        html += getfooter()\\n        return html', 'def g(self, name=\"\", genre=\"\", randomlist=\\'false\\'):\\n        list = searchradio(name.decode(\\'utf8\\'), genre)\\n        count = 0\\n\\n        # Randomlist\\n        if randomlist == \\'true\\' : shuffle(list)\\n\\n        listhtml = \\'\\'\\'<table class=\"table table-condensed\">\\'\\'\\'\\n        for id,radio,gen,url in list:\\n            listhtml += \\'\\'\\'<tr>\\'\\'\\'\\n            listhtml += \\'\\'\\'<td width=\"200px\"><a href=\"#\" onClick=\"fmodradio(\\'%s\\')\" alt=\"%s\">%s</a></td>\\'\\'\\' % (id, url, radio)\\n            listhtml += \\'\\'\\'<td width=\"100px\">%s</td>\\'\\'\\' % gen\\n            listhtml += \\'\\'\\'<td ><a href=\"#\" onClick=\"fplayradio(\\'%s\\')\">Play <span class=\"glyphicon glyphicon-play\"></span></a></td>\\'\\'\\' % (id)\\n            listhtml += \\'\\'\\'</tr>\\'\\'\\'\\n            count += 1\\n        listhtml += \\'\\'\\'</table>\\'\\'\\'\\n        listhtml += \\'\\'\\'</div> </div>\\'\\'\\'\\n\\n        html = \\'\\'\\n        html += \\'\\'\\'<div class=\"row\"> <div class=\"col-md-8\"> \\'\\'\\'\\n        if randomlist == \\'false\\':\\n            html += \\'\\'\\'<h2><a href=\"#\" onClick=\"frandom(name=\\'%s\\', genre=\\'%s\\', randomlist=\\'true\\')\">%d Results for \\'%s\\' + \\'%s\\'</a></h2>\\'\\'\\' % (name, genre, count, name, genre)\\n        else:\\n            html += \\'\\'\\'<h2><a href=\"#\" onClick=\"fsearch(name=\\'%s\\', genre=\\'%s\\')\">%d Random for \\'%s\\' + \\'%s\\'</a></h2>\\'\\'\\' % (name, genre, count, name, genre)\\n\\n        html += listhtml\\n\\n        return html', 'def i(self, name=\"\", genre=\"\", url=\"\"):\\n        html = \"<h2>Insert</h2>\"\\n        if name == \"\" or name == None :\\n          html += \"Error no name\"\\n          return html\\n\\n        if insert(name, genre, url) == False:\\n            html += \"Error db \"\\n            return html\\n\\n        html += \\'\\'\\'<h3>This radio has been inserted</h3>\\'\\'\\'\\n        html += \\'\\'\\'<p><table class=\"table table-condensed\">\\'\\'\\'\\n        html += \\'\\'\\' <tr> \\'\\'\\'\\n        html += \\'\\'\\'  <td>radio: <strong>%s</strong></td> \\'\\'\\' % name\\n        html += \\'\\'\\'  <td>genre: <strong>%s</strong></td> \\'\\'\\' % genre\\n        html += \\'\\'\\'  <td>url: <strong><a href=\"%s\" target=\"_blank\">%s</a></strong></td> \\'\\'\\' % (url, url)\\n        html += \\'\\'\\'  <td width=\"300px\"><a href=\"#\" onClick=\"fplayradio(\\'%s\\')\"> Play \\'\\'\\' % url\\n        html += \\'\\'\\'<span class=\"glyphicon glyphicon-play\"></span></a></td>\\'\\'\\'\\n        html += \\'\\'\\' </tr> \\'\\'\\'\\n        html += \\'\\'\\'</table>\\'\\'\\'\\n\\n        return html', 'def d(self, id=\"\"):\\n        html = \"<h2>Delete</h2>\"\\n        if id == \"\" or id == None :\\n            html += \"Error\"\\n            return html\\n\\n        if id == \"0\" :\\n          html += \"0 is reserved, sorry\"\\n          return html\\n\\n        #if delete(id) == False:\\n        if nonexist(id) == False:\\n            html += \"Delete error in id\" % id\\n            html += getfooter()\\n            return html\\n\\n        html += \"Item %s set as non existent\" % id\\n        return html', 'def p(self, id):\\n        html = \"\"\\n        if id == \"\" or id == None :\\n            html += \"Error no radio id\"\\n            return html\\n        if id == \"0\" :\\n          html += \"0 is reserved, sorry\"\\n          return html\\n\\n        (radio, genre, url) = playradio(id)\\n        if  url == \\'\\':\\n            html += \"Error in parameter %s\" % url\\n            return html\\n\\n        cherrypy.session[\\'playing\\'] = id\\n        html += \\'\\'\\'<h3>Now Playing: \\'\\'\\'\\n        html += \\'\\'\\'<a href=\"%s\">%s</a>\\'\\'\\' % (url, radio)\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"fplayradio(\\'%s\\')\">\\'\\'\\' % id\\n        html += \\'\\'\\'<span class=\"glyphicon glyphicon-play\"></span></a>\\'\\'\\'\\n        html += \\'\\'\\'&nbsp;<a href=\"#\" onClick=\"fmodradio(\\'%s\\')\"><span class=\"glyphicon glyphicon-pencil\"></span></a></small>&nbsp;\\'\\'\\' % id\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"fdelradio(\\'%s\\')\"><span class=\"glyphicon glyphicon-trash\"></span></a>&nbsp;\\'\\'\\' % id\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"faddfav(\\'%s\\')\"><span class=\"glyphicon glyphicon-star\"></span></a>\\'\\'\\' % id\\n        html += \\'\\'\\'</h3>\\'\\'\\'\\n        return html', 'def v(self, vol=\"\"):\\n        html = \"\"\\n        if vol == \"\" or vol == None :\\n           html += \"Error\"\\n        v = volume(vol)\\n\\n        html += \"<h6>%s (%s) </h6>\" % (v, vol)\\n        return html', 'def m(self, id):\\n        html = \\'\\'\\'<h2>Modify</h2>\\'\\'\\'\\n\\n        if id == \"\" or id == None :\\n          html += \"Error\"\\n          return html\\n        if id == \"0\" :\\n          html += \"0 is reserved, sorry\"\\n          return html\\n\\n        (name, genre, url) = getradio(id)\\n        html += \\'<h3>%s | %s | %s</h3>\\' % (name, genre, url)\\n        html += \\'\\'\\'<input type=\"hidden\" id=\"idm\" name=\"id\" value=\"%s\">\\'\\'\\' % id\\n        html += \\'\\'\\'<input type=\"text\" id=\"namem\" name=\"name\" value=\"%s\">\\'\\'\\' % name\\n        html += \\'\\'\\'genre: <input type=\"text\" id=\"genrem\" name=\"genre\" value=\"%s\"> \\'\\'\\' % genre\\n        html += \\'\\'\\'url: <input type=\"text\" style=\"min-width: 280px\" id=\"urlm\" name=\"url\" value=\"%s\"> \\'\\'\\' % url\\n        html += \\'\\'\\'<button id=\"button-modify\">Change</button>\\'\\'\\'\\n        html += \\'\\'\\'<h3><a href=\"#\" onClick=\"fdelradio(\\'%s\\')\">Delete? <span class=\"glyphicon glyphicon-trash\"></span></a></h3>\\'\\'\\' % id\\n        html += \\'\\'\\'<h3><a href=\"%s\" target=\"_blank\">Play in browser <span class=\"glyphicon glyphicon-music\"></span></a>\\'\\'\\' % url\\n\\n\\n        return html', 'def f(self, id=\"\", name=\"\", genre=\"\", url=\"\"):\\n        html = \\'\\'\\'<h2>Modified</h2>\\'\\'\\'\\n        if id == \"\" or id == None :\\n          html += \"Error missing id\"\\n          return html\\n\\n        if id == \"0\" :\\n          html += \"0 is reserved, sorry\"\\n          return html\\n\\n        if modify(id, name, url, genre) == False:\\n            html += \"Error in DB\"\\n            return html\\n\\n        (name, genre, url) = getradio(id)\\n        html += \\'\\'\\'<p><table class=\"table table-condensed\">\\'\\'\\'\\n        html += \\'\\'\\'<tr>\\'\\'\\'\\n        html += \\'\\'\\'<td width=\"100px\"><a href=\"#\" onClick=\"fmodradio(\\'%s\\')\">\\'\\'\\' % id\\n        html += \\'\\'\\'Mod <span class=\"glyphicon glyphicon-pencil\"></span></a></td>\\'\\'\\'\\n        html += \\'\\'\\'<td width=\"200px\">%s</td>\\'\\'\\' % name\\n        html += \\'\\'\\'<td width=\"200px\">%s</td>\\'\\'\\' % genre\\n        html += \\'\\'\\'<td><a href=\"%s\" target=\"_blank\">%s</a></td>\\'\\'\\' % (url, url)\\n        html += \\'\\'\\'<td width=\"300px\"><a href=\"#\" onClick=\"fplayradio(\\'%s\\')\">\\'\\'\\'% url\\n        html += \\'\\'\\'Play <span class=\"glyphicon glyphicon-play\"></span></a></td>\\'\\'\\'\\n        html += \\'\\'\\'</tr>\\'\\'\\'\\n        html += \\'\\'\\'</table>\\'\\'\\'\\n\\n        return html', 'def haddfav(self, id=\"\"):\\n        if id == \"\" or id == None :\\n          html += \"Error missing id\"\\n          return html\\n\\n        if id == \"0\" :\\n          html += \"0 is reserved, sorry\"\\n          return html\\n\\n        (name, genre, url) = getradio(id)\\n        if \\'Fav\\' in genre:\\n            genre = genre.replace(\\', Fav\\', \\'\\')\\n            star = False\\n        else:\\n            genre += \\', Fav\\'\\n            star = True\\n\\n        if addgen(id, genre) == False:\\n            return \\'\\'\\n\\n        (name, genre, url) = getradio(id)\\n        cherrypy.session[\\'playing\\'] = id\\n        html = \\'<h3>Now Playing: \\'\\n        html += \\'\\'\\'<a href=\"%s\">%s</a>\\'\\'\\' % (url, name)\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"fplayradio(\\'%s\\')\">\\'\\'\\' % url\\n        html += \\'\\'\\'<span class=\"glyphicon glyphicon-play\"></span></a>\\'\\'\\'\\n        html += \\'\\'\\'&nbsp;<a href=\"#\" onClick=\"fmodradio(\\'%s\\')\"><span class=\"glyphicon glyphicon-pencil\"></span></a></small>&nbsp;\\'\\'\\' % id\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"fdelradio(\\'%s\\')\"><span class=\"glyphicon glyphicon-trash\"></span></a>&nbsp;\\'\\'\\' % id\\n        html += \\'\\'\\'<a href=\"#\" onClick=\"faddfav(\\'%s\\')\"><span class=\"glyphicon glyphicon-star\"></span></a>\\'\\'\\' % id\\n        if star:\\n            html += \\'\\'\\'Starred\\'\\'\\'\\n        html += \\'\\'\\'</h3>\\'\\'\\'\\n\\n        return html', 'def k(self):\\n        html = \"<h2>Stopping</h2>\"\\n        killall()\\n        return html', 'def getfooter() :\\n    global footer, version\\n\\n    db = cherrypy.session[\\'database\\']\\n    try:\\n        con = lite.connect( db )\\n        cur = con.cursor()\\n        sql = \"select radio, genre, url from Radio where id=0\"\\n        cur.execute(sql)\\n        (radio, genre, url) = cur.fetchone()\\n    except:\\n        (radio, genre, url) = (\\'ERROR\\', sql, \\'\\')\\n\\n    con.close()\\n\\n    hostname = socket.gethostname()\\n    f = \\'\\'\\'<footer class=\"footer\"> <div class=\"container\">\\'\\'\\'\\n    f += \\'\\'\\'<p class=\"text-muted\">\\'\\'\\'\\n    f += \\'\\'\\'Session id: %s - Session Database %s<br>\\'\\'\\' % (cherrypy.session.id, cherrypy.session[\\'database\\'])\\n    f += \\'\\'\\'Host: %s - Version: %s - Updated: %s // Last: %s\\'\\'\\' % (hostname, version, genre, url)\\n    f += \\'\\'\\'</p>\\'\\'\\'\\n    f += \\'\\'\\'</div></footer>\\'\\'\\'\\n    return f + footer', 'def delete(id) :\\n    db = cherrypy.session[\\'database\\']\\n    try:\\n        con = lite.connect( db )\\n        cur = con.cursor()\\n        sql =  \"DELETE from Radio WHERE id = \\'%s\\'\" % (id)\\n        cur.execute(sql)\\n        ret = True\\n    except:\\n        ret = False\\n\\n    updateversiondb(cur)\\n    con.commit()\\n    con.close()\\n    return ret', 'def insert(radio, genre, url) :\\n    db = cherrypy.session[\\'database\\']\\n    sql =  \"INSERT INTO Radio (radio, genre, url, exist) VALUES(\\'%s\\', \\'%s\\', \\'%s\\', 1)\" % (radio, genre, url)\\n    try:\\n        con = lite.connect( db )\\n        cur = con.cursor()\\n        cur.execute(sql)\\n        ret = True\\n    except:\\n        ret = False\\n\\n    updateversiondb(cur)\\n    con.commit()\\n    con.close()\\n    return ret', 'def addgen(id, genre) :\\n    db = cherrypy.session[\\'database\\']\\n\\n    sql = \"UPDATE Radio SET genre=\\'%s\\' WHERE id = %s\" % (genre, id)\\n    try:\\n        con = lite.connect( db )\\n        cur = con.cursor()\\n        cur.execute(sql)\\n        ret = True\\n    except:\\n        ret = False\\n\\n    updateversiondb(cur)\\n    con.commit()\\n    con.close()\\n    return ret', 'def searchradio(radio, genre) :\\n    db = cherrypy.session[\\'database\\']\\n    #o = \\'order by radio\\'\\n    o = \\'\\'\\n    sql = \"select id, radio, genre, url from Radio where exist > 0 and radio like \\'%%%s%%\\' and genre like \\'%%%s%%\\' and id > 0 %s\" % (radio, genre, o)\\n    try:\\n        con = lite.connect( db )\\n        cur = con.cursor()\\n        cur.execute(sql)\\n    except:\\n        return [(0, sql, o, genre)]\\n\\n    rows = cur.fetchall()\\n    con.close()\\n    return rows', 'def userdatabase(user) :\\n    db = database\\n    if not os.path.isfile(db):\\n        return None\\n    return db', 'def setplayer(p):\\n    global player\\n    player = p', 'def killall():\\n    global player\\n    status = 0\\n    if player == \\'omxplayer\\':\\n        control = \"/usr/local/bin/omxcontrol\"\\n        status = subprocess.call([control,  \"stop\"])\\n    status = subprocess.call([\"pkill\", player])\\n\\n    return status', 'def volume_alsa(vol):\\n    # With ALSA on CHIP\\n    if vol == \\'up\\':\\n        db = subprocess.check_output([\"amixer set \\'Power Amplifier\\' 5%+\"], shell=True)\\n        #db = os.system(\"amixer set \\'Power Amplifier\\' 5%+\")\\n    if vol == \\'down\\':\\n        db = subprocess.check_output([\"amixer set \\'Power Amplifier\\' 5%-\"], shell=True)\\n        #db = os.system(\"amixer set \\'Power Amplifier\\' 5%-\")\\n    i = db.rfind(\\':\\')\\n    return db[i+1:]', \"def writemypid(pidfile):\\n    pid = str(os.getpid())\\n    with open(pidfile, 'w') as f:\\n        f.write(pid)\\n    f.close\", 'def error_page_404(status, message, traceback, version):\\n    html = header\\n    html += \"%s<br>\" % (status)\\n    html += \"%s\" % (traceback)\\n    html += getfooter()\\n    return html', 'def secureheaders():\\n    headers = cherrypy.response.headers\\n    headers[\\'X-Frame-Options\\'] = \\'DENY\\'\\n    headers[\\'X-XSS-Protection\\'] = \\'1; mode=block\\'\\n    headers[\\'Content-Security-Policy\\'] = \"default-src=\\'self\\'\"']}, {'features': [], 'snippets': ['def assertListEq(self, l1, l2, ignore):\\r\\n        \\'\\'\\' succeed iff {l1} - {ignore} == {l2} - {ignore} \\'\\'\\'\\r\\n        missing = (set(l1) ^ set(l2)) - set(ignore)\\r\\n        if missing:\\r\\n            print >>sys.stderr, \"l1=%r\\\\nl2=%r\\\\nignore=%r\" % (l1, l2, ignore)\\r\\n            self.fail(\"%r missing\" % missing.pop())', 'def assertHasattr(self, obj, attr, ignore):\\r\\n        \\'\\'\\' succeed iff hasattr(obj,attr) or attr in ignore. \\'\\'\\'\\r\\n        if attr in ignore: return\\r\\n        if not hasattr(obj, attr): print \"???\", attr\\r\\n        self.failUnless(hasattr(obj, attr),\\r\\n                        \\'expected hasattr(%r, %r)\\' % (obj, attr))', 'def assertHaskey(self, obj, key, ignore):\\r\\n        \\'\\'\\' succeed iff key in obj or key in ignore. \\'\\'\\'\\r\\n        if key in ignore: return\\r\\n        if key not in obj:\\r\\n            print >>sys.stderr, \"***\", key\\r\\n        self.assertTrue(key in obj)', \"def assertEqualsOrIgnored(self, a, b, ignore):\\r\\n        ''' succeed iff a == b or a in ignore or b in ignore '''\\r\\n        if a not in ignore and b not in ignore:\\r\\n            self.assertEqual(a, b)\", \"def checkModule(self, moduleName, module=None, ignore=()):\\r\\n        ''' succeed iff pyclbr.readmodule_ex(modulename) corresponds\\r\\n            to the actual module object, module.  Any identifiers in\\r\\n            ignore are ignored.   If no module is provided, the appropriate\\r\\n            module is loaded with __import__.'''\", 'def ismethod(oclass, obj, name):\\r\\n            classdict = oclass.__dict__\\r\\n            if isinstance(obj, FunctionType):\\r\\n                if not isinstance(classdict[name], StaticMethodType):\\r\\n                    return False\\r\\n            else:\\r\\n                if not  isinstance(obj, MethodType):\\r\\n                    return False\\r\\n                if obj.im_self is not None:\\r\\n                    if (not isinstance(classdict[name], ClassMethodType) or\\r\\n                        obj.im_self is not oclass):\\r\\n                        return False\\r\\n                else:\\r\\n                    if not isinstance(classdict[name], FunctionType):\\r\\n                        return False', 'def defined_in(item, module):\\r\\n            if isinstance(item, ClassType):\\r\\n                return item.__module__ == module.__name__\\r\\n            if isinstance(item, FunctionType):\\r\\n                return item.func_globals is module.__dict__\\r\\n            return False', \"def test_easy(self):\\r\\n        self.checkModule('pyclbr')\\r\\n        self.checkModule('doctest')\\r\\n        # Silence Py3k warning\\r\\n        rfc822 = import_module('rfc822', deprecated=True)\\r\\n        self.checkModule('rfc822', rfc822)\\r\\n        self.checkModule('difflib')\", \"def test_decorators(self):\\r\\n        # XXX: See comment in pyclbr_input.py for a test that would fail\\r\\n        #      if it were not commented out.\\r\\n        #\\r\\n        self.checkModule('test.pyclbr_input')\", 'def test_others(self):\\r\\n        cm = self.checkModule', 'def test_main():\\r\\n    run_unittest(PyclbrTest)']}, {'features': [], 'snippets': ['def __unicode__(self):\\n        return self.name']}, {'features': [], 'snippets': ['def __init__(\\n        self,\\n        djvu,\\n        index,\\n        pages: Optional[tuple] = None,\\n        **kwargs', 'def page_number_gen(self):\\n        \"\"\"Generate pages numbers from specified page intervals.\"\"\"\\n        last = 0\\n        for start, end in sorted(self._pages):\\n            start = max(last, start)\\n            last = end + 1\\n            yield from range(start, last)', 'def generator(self):\\n        \"\"\"Generate pages from specified page interval.\"\"\"\\n        for page_number in self.page_number_gen():\\n            title = \\'{page_ns}:{prefix}/{number}\\'.format(\\n                page_ns=self._page_ns,\\n                prefix=self._prefix,\\n                number=page_number)\\n            page = ProofreadPage(self._index.site, title)\\n            page.page_number = page_number  # remember page number in djvu file\\n            yield page', 'def main(*args: str) -> None:\\n    \"\"\"\\n    Process command line arguments and invoke bot.\\n\\n    If args is an empty list, sys.argv is used.\\n\\n    :param args: command line arguments\\n    \"\"\"\\n    index = None\\n    djvu_path = \\'.\\'  # default djvu file directory\\n    pages = \\'1-\\'\\n    options = {}\\n\\n    # Parse command line arguments.\\n    local_args = pywikibot.handle_args(args)\\n    for arg in local_args:\\n        opt, _, value = arg.partition(\\':\\')\\n        if opt == \\'-index\\':\\n            index = value\\n        elif opt == \\'-djvu\\':\\n            djvu_path = value\\n        elif opt == \\'-pages\\':\\n            pages = value\\n        elif opt == \\'-summary\\':\\n            options[\\'summary\\'] = value\\n        elif opt in (\\'-force\\', \\'-always\\'):\\n            options[opt[1:]] = True\\n        else:\\n            pywikibot.output(\\'Unknown argument \\' + arg)\\n\\n    # index is mandatory.\\n    if not index:\\n        pywikibot.bot.suggest_help(missing_parameters=[\\'-index\\'])\\n        return\\n\\n    # If djvu_path is not a file, build djvu_path from dir+index.\\n    djvu_path = os.path.expanduser(djvu_path)\\n    djvu_path = os.path.abspath(djvu_path)\\n    if not os.path.exists(djvu_path):\\n        pywikibot.error(\\'No such file or directory: \\' + djvu_path)\\n        return\\n\\n    if os.path.isdir(djvu_path):\\n        djvu_path = os.path.join(djvu_path, index)\\n\\n    # Check the djvu file exists and, if so, create the DjVuFile wrapper.\\n    djvu = DjVuFile(djvu_path)\\n\\n    if not djvu.has_text():\\n        pywikibot.error(\\'No text layer in djvu file {}\\'.format(djvu.file))\\n        return\\n\\n    # Parse pages param.\\n    pages = pages.split(\\',\\')\\n    for i, page_interval in enumerate(pages):\\n        start, sep, end = page_interval.partition(\\'-\\')\\n        start = int(start or 1)\\n        end = int(end or djvu.number_of_images()) if sep else start\\n        pages[i] = (start, end)\\n\\n    site = pywikibot.Site()\\n    if not site.has_extension(\\'ProofreadPage\\'):\\n        pywikibot.error(\\'Site {} must have ProofreadPage extension.\\'\\n                        .format(site))\\n        return\\n\\n    index_page = pywikibot.Page(site, index, ns=site.proofread_index_ns)\\n\\n    if not index_page.exists():\\n        raise NoPageError(index)\\n\\n    pywikibot.output(\\'uploading text from {} to {}\\'\\n                     .format(djvu.file, index_page.title(as_link=True)))\\n\\n    bot = DjVuTextBot(djvu, index_page, pages=pages, site=site, **options)\\n    bot.run()']}, {'features': [], 'snippets': ['def build_get_request(\\n    resource_group_name: str,\\n    managed_instance_name: str,\\n    database_name: str,\\n    query_id: str,\\n    subscription_id: str,\\n    **kwargs: Any', 'def build_list_by_query_request(\\n    resource_group_name: str,\\n    managed_instance_name: str,\\n    database_name: str,\\n    query_id: str,\\n    subscription_id: str,\\n    *,\\n    start_time: Optional[str] = None,\\n    end_time: Optional[str] = None,\\n    interval: Optional[Union[str, \"_models.QueryTimeGrainType\"]] = None,\\n    **kwargs: Any', 'def __init__(self, client, config, serializer, deserializer):\\n        self._client = client\\n        self._serialize = serializer\\n        self._deserialize = deserializer\\n        self._config = config', 'def get(\\n        self,\\n        resource_group_name: str,\\n        managed_instance_name: str,\\n        database_name: str,\\n        query_id: str,\\n        **kwargs: Any', 'def list_by_query(\\n        self,\\n        resource_group_name: str,\\n        managed_instance_name: str,\\n        database_name: str,\\n        query_id: str,\\n        start_time: Optional[str] = None,\\n        end_time: Optional[str] = None,\\n        interval: Optional[Union[str, \"_models.QueryTimeGrainType\"]] = None,\\n        **kwargs: Any', 'def prepare_request(next_link=None):\\n            if not next_link:', 'def extract_data(pipeline_response):\\n            deserialized = self._deserialize(\"ManagedInstanceQueryStatistics\", pipeline_response)\\n            list_of_elem = deserialized.value\\n            if cls:\\n                list_of_elem = cls(list_of_elem)\\n            return deserialized.next_link or None, iter(list_of_elem)']}, {'features': [], 'snippets': [\"def index():\\n    return render_template('index.html'), 200\", \"def app_settings():\\n\\n    app_settings = {'GRAPHITE_HOST': settings.GRAPHITE_HOST,\\n                    'OCULUS_HOST': settings.OCULUS_HOST,\\n                    'FULL_NAMESPACE': settings.FULL_NAMESPACE,\\n                    }\\n\\n    resp = json.dumps(app_settings)\\n    return resp, 200\", 'def data():\\n    metric = request.args.get(\\'metric\\', None)\\n    try:\\n        raw_series = REDIS_CONN.get(metric)\\n        if not raw_series:\\n            resp = json.dumps({\\'results\\': \\'Error: No metric by that name\\'})\\n            return resp, 404\\n        else:\\n            unpacker = Unpacker(use_list = False)\\n            unpacker.feed(raw_series)\\n            timeseries = [item[:2] for item in unpacker]\\n            resp = json.dumps({\\'results\\': timeseries})\\n            return resp, 200\\n    except Exception as e:\\n        error = \"Error: \" + e\\n        resp = json.dumps({\\'results\\': error})\\n        return resp, 500', \"def __init__(self):\\n        self.stdin_path = '/dev/null'\\n        self.stdout_path = settings.LOG_PATH + '/webapp.log'\\n        self.stderr_path = settings.LOG_PATH + '/webapp.log'\\n        self.pidfile_path = settings.PID_PATH + '/webapp.pid'\\n        self.pidfile_timeout = 5\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, c):\\n        self.c = c\\n        self.is_star = False']}, {'features': [], 'snippets': [\"def validate_hidden_field(form, field):\\n        raise ValidationError('Always wrong')\", \"def index():\\n        form = ExampleForm()\\n        form.validate_on_submit()  # to get error messages to the browser\\n        flash('critical message', 'critical')\\n        flash('error message', 'error')\\n        flash('warning message', 'warning')\\n        flash('info message', 'info')\\n        flash('debug message', 'debug')\\n        flash('different message', 'different')\\n        flash('uncategorized message')\\n        return render_template('index.html', form=form)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def show(graphs, request, titles, prog=\\'neato\\', size=None,\\n         type_format=None, filename=None):\\n    \"\"\"\\n    Display the results using matplotlib.\\n    \"\"\"\\n    if not size:\\n        size = _get_size(len(graphs))\\n    fig, axarr = plt.subplots(size[0], size[1], figsize=(18, 10))\\n    fig.set_facecolor(\\'white\\')\\n    x_val = 0\\n    y_val = 0\\n    index = 0\\n\\n    if size[0] == 1:\\n        axarr = np.array(axarr).reshape((1, size[1]))\\n\\n    for candidate in graphs:\\n        # axarr[x_val, y_val].axis(\\'off\\')\\n        axarr[x_val, y_val].xaxis.set_major_formatter(plt.NullFormatter())\\n        axarr[x_val, y_val].yaxis.set_major_formatter(plt.NullFormatter())\\n        axarr[x_val, y_val].xaxis.set_ticks([])\\n        axarr[x_val, y_val].yaxis.set_ticks([])\\n        axarr[x_val, y_val].set_title(titles[index])', 'def _plot_subplot(graph, new_nodes, prog, type_format, axes):\\n    \"\"\"\\n    Plot a single candidate graph.\\n    \"\"\"\\n    pos = nx.nx_agraph.graphviz_layout(graph, prog=prog)\\n\\n    # draw the nodes\\n    for node, values in graph.nodes(data=True):\\n        shape = \\'o\\'\\n        if values[stitcher.TYPE_ATTR] in type_format:\\n            shape = type_format[values[stitcher.TYPE_ATTR]]\\n        color = \\'g\\'\\n        alpha = 0.8\\n        if node in new_nodes:\\n            color = \\'b\\'\\n            alpha = 0.2\\n        elif \\'rank\\' in values and values[\\'rank\\'] > 7:\\n            color = \\'r\\'\\n        elif \\'rank\\' in values and values[\\'rank\\'] < 7 and values[\\'rank\\'] > 3:\\n            color = \\'y\\'\\n        nx.draw_networkx_nodes(graph, pos, nodelist=[node], node_color=color,\\n                               node_shape=shape, alpha=alpha, ax=axes)\\n\\n    # draw the edges\\n    dotted_line = []\\n    normal_line = []\\n    for src, trg in graph.edges():\\n        if src in new_nodes and trg not in new_nodes:\\n            dotted_line.append((src, trg))\\n        else:\\n            normal_line.append((src, trg))\\n    nx.draw_networkx_edges(graph, pos, edgelist=dotted_line, style=\\'dotted\\',\\n                           ax=axes)\\n    nx.draw_networkx_edges(graph, pos, edgelist=normal_line, ax=axes)\\n\\n    # draw labels\\n    nx.draw_networkx_labels(graph, pos, ax=axes)', 'def _plot_3d_subplot(graph, request, prog, axes):\\n    \"\"\"\\n    Plot a single candidate graph in 3d.\\n    \"\"\"\\n    cache = {}\\n\\n    tmp = graph.copy()\\n    for node in request.nodes():\\n        tmp.remove_node(node)\\n\\n    pos = nx.nx_agraph.graphviz_layout(tmp, prog=prog)\\n\\n    # the container\\n    for item in tmp.nodes():\\n        axes.plot([pos[item][0]], [pos[item][1]], [0], linestyle=\"None\",\\n                  marker=\"o\", color=\\'gray\\')\\n        axes.text(pos[item][0], pos[item][1], 0, item)\\n\\n    for src, trg in tmp.edges():\\n        axes.plot([pos[src][0], pos[trg][0]],\\n                  [pos[src][1], pos[trg][1]],\\n                  [0, 0], color=\\'gray\\')\\n\\n    # the new nodes\\n    for item in graph.nodes():\\n        if item in request.nodes():\\n            for nghb in graph.neighbors(item):\\n                if nghb in tmp.nodes():\\n                    x_val = pos[nghb][0]\\n                    y_val = pos[nghb][1]\\n                    if (x_val, y_val) in list(cache.values()):\\n                        x_val = pos[nghb][0] + random.randint(10, SPACE)\\n                        y_val = pos[nghb][0] + random.randint(10, SPACE)\\n                    cache[item] = (x_val, y_val)\\n\\n                    # edge\\n                    axes.plot([x_val, pos[nghb][0]],\\n                              [y_val, pos[nghb][1]],\\n                              [SPACE, 0], color=\\'blue\\')\\n\\n            axes.plot([x_val], [y_val], [SPACE], linestyle=\"None\", marker=\"o\",\\n                      color=\\'blue\\')\\n            axes.text(x_val, y_val, SPACE, item)\\n\\n    for src, trg in request.edges():\\n        if trg in cache and src in cache:\\n            axes.plot([cache[src][0], cache[trg][0]],\\n                      [cache[src][1], cache[trg][1]],\\n                      [SPACE, SPACE], color=\\'blue\\')']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_helper():\\n    return \"test helper text\"', 'def module_version(module, label=None):\\n    \"\"\"\\n    Helper function for getting the module (\"module\") in the current\\n    namespace and their versions.', 'def file_contents(filename, label=None):\\n    \"\"\"\\n    Helper function for getting the contents of a file,\\n    provided the filename.', 'def svn_information(svndir=None, label=None):\\n    \"\"\"\\n    Helper function for obtaining the SVN repository\\n    information for the current directory (default)\\n    or the directory supplied in the svndir argument.', 'def get_git_hash(gitpath=None, label=None):\\n    \"\"\"\\n    Helper function for obtaining the git repository hash.\\n    for the current directory (default)                                          \\n    or the directory supplied in the gitpath argument.\\n\\n    Returns a dictionary keyed (by default) as \\'GIT HASH\\'\\n    where the value is a string containing essentially what\\n    is returned by subprocess.  \\n\\n    The optional argument \\'label\\' allows you to set the string \\n    used as the dictionary key in the returned dictionary.\\n    \"\"\"\\n    if gitpath:\\n        thisdir = os.getcwd()\\n        os.chdir(gitpath)', 'def get_source_code(scode,sourcepath=None, label=None):\\n    \"\"\"\\n    Helper function for obtaining the source code.\\n    for the current directory (default) or the directory\\n    supplied in the sourcepath argument.\\n\\n    Returns a dictionary keyed (by default) as \\'source code\\'\\n    where the value is a string containing the source code.  \\n\\n    The optional argument \\'label\\' allows you to set the string \\n    used as the dictionary key in the returned dictionary.\\n    \"\"\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, gan=None, config=None, trainer=None):\\n      super().__init__(config=config, gan=gan, trainer=trainer)\\n      self.d_grads = None\\n      self.g_grads = None']}, {'features': [], 'snippets': ['def main():\\n    # Contingency Table from Wilks (2011) Table 8.3\\n    table = np.array([[50, 91, 71],\\n                      [47, 2364, 170],\\n                      [54, 205, 3288]])\\n    mct = MulticlassContingencyTable(table, n_classes=table.shape[0],\\n                                     class_names=np.arange(table.shape[0]).astype(str))\\n    print(mct.peirce_skill_score())\\n    print(mct.gerrity_score())', 'def __init__(self, table=None, n_classes=2, class_names=(\"1\", \"0\")):\\n        self.table = table\\n        self.n_classes = n_classes\\n        self.class_names = class_names\\n        if table is None:\\n            self.table = np.zeros((self.n_classes, self.n_classes), dtype=int)', 'def peirce_skill_score(self):\\n        \"\"\"\\n        Multiclass Peirce Skill Score (also Hanssen and Kuipers score, True Skill Score)\\n        \"\"\"\\n        n = float(self.table.sum())\\n        nf = self.table.sum(axis=1)\\n        no = self.table.sum(axis=0)\\n        correct = float(self.table.trace())\\n        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (no * no).sum() / n ** 2)', 'def heidke_skill_score(self):\\n        n = float(self.table.sum())\\n        nf = self.table.sum(axis=1)\\n        no = self.table.sum(axis=0)\\n        correct = float(self.table.trace())\\n        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (nf * no).sum() / n ** 2)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, band_names=[\\'delta\\', \\'theta\\', \\'alpha\\', \\'beta\\'],\\n                 ch_names=[\\'TP9\\', \\'AF7\\', \\'AF8\\', \\'TP10\\']):\\n        \"\"\"\\n        \"\"\"\\n        self.band_names = band_names\\n        self.ch_names = ch_names\\n        self.n_bars = self.band_names * self.ch_names\\n\\n        self.x =\\n\\n        self.fig, self.ax = plt.subplots()\\n        self.ax.set_ylim((0, 1))\\n\\n        y = np.zeros((self.n_bars,))\\n        x = range(self.n_bars)\\n\\n        self.rects = self.ax.bar(x, y)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __unicode__(self):\\n        return \"%s: %s\" %(self.TIPOS[self.tipo][1],self.msg)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_multi_buffer(self):\\n        grid = Grid((3, 3))\\n        f = TimeFunction(name=\"f\", grid=grid)\\n        g = TimeFunction(name=\"g\", grid=grid, save=Buffer(7))\\n\\n        op = Operator([Eq(f.forward, 1), Eq(g, f.forward)])\\n        op(time_M=3)\\n        # f looped all time_order buffer and is 1 everywhere\\n        assert np.allclose(f.data, 1)\\n        # g looped indices 0 to 3, rest is still 0\\n        assert np.allclose(g.data[0:4], 1)\\n        assert np.allclose(g.data[4:], 0)', 'def test_interior(self, opt):\\n        \"\"\"\\n        Tests application of an Operator consisting of a single equation\\n        over the ``interior`` subdomain.\\n        \"\"\"\\n        grid = Grid(shape=(4, 4, 4))\\n        x, y, z = grid.dimensions\\n\\n        interior = grid.interior\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid)\\n\\n        eqn = [Eq(u.forward, u + 2, subdomain=interior)]\\n\\n        op = Operator(eqn, opt=opt)\\n        op.apply(time_M=2)\\n        assert np.all(u.data[1, 1:-1, 1:-1, 1:-1] == 6.)\\n        assert np.all(u.data[1, :, 0] == 0.)\\n        assert np.all(u.data[1, :, -1] == 0.)\\n        assert np.all(u.data[1, :, :, 0] == 0.)\\n        assert np.all(u.data[1, :, :, -1] == 0.)', 'def test_subdim_middle(self, opt):\\n        \"\"\"\\n        Tests that instantiating SubDimensions using the classmethod\\n        constructors works correctly.\\n        \"\"\"\\n        grid = Grid(shape=(4, 4, 4))\\n        x, y, z = grid.dimensions\\n        t = grid.stepping_dim  # noqa\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid)  # noqa\\n        xi = SubDimension.middle(name=\\'xi\\', parent=x,\\n                                 thickness_left=1,\\n                                 thickness_right=1)\\n        eqs = [Eq(u.forward, u + 1)]\\n        eqs = [e.subs(x, xi) for e in eqs]\\n\\n        op = Operator(eqs, opt=opt)\\n\\n        u.data[:] = 1.0\\n        op.apply(time_M=1)\\n        assert np.all(u.data[1, 0, :, :] == 1)\\n        assert np.all(u.data[1, -1, :, :] == 1)\\n        assert np.all(u.data[1, 1:3, :, :] == 2)', 'def test_bcs(self, opt):\\n        \"\"\"\\n        Tests application of an Operator consisting of multiple equations\\n        defined over different sub-regions, explicitly created through the\\n        use of SubDimensions.\\n        \"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions\\n        t = grid.stepping_dim\\n        thickness = 4\\n\\n        u = TimeFunction(name=\\'u\\', save=None, grid=grid, space_order=0, time_order=1)\\n\\n        xleft = SubDimension.left(name=\\'xleft\\', parent=x, thickness=thickness)\\n        xi = SubDimension.middle(name=\\'xi\\', parent=x,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n        xright = SubDimension.right(name=\\'xright\\', parent=x, thickness=thickness)\\n\\n        yi = SubDimension.middle(name=\\'yi\\', parent=y,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n\\n        t_in_centre = Eq(u[t+1, xi, yi], 1)\\n        leftbc = Eq(u[t+1, xleft, yi], u[t+1, xleft+1, yi] + 1)\\n        rightbc = Eq(u[t+1, xright, yi], u[t+1, xright-1, yi] + 1)\\n\\n        op = Operator([t_in_centre, leftbc, rightbc], opt=opt)\\n\\n        op.apply(time_m=1, time_M=1)\\n\\n        assert np.all(u.data[0, :, 0:thickness] == 0.)\\n        assert np.all(u.data[0, :, -thickness:] == 0.)\\n        assert all(np.all(u.data[0, i, thickness:-thickness] == (thickness+1-i))\\n                   for i in range(thickness))\\n        assert all(np.all(u.data[0, -i, thickness:-thickness] == (thickness+2-i))\\n                   for i in range(1, thickness + 1))\\n        assert np.all(u.data[0, thickness:-thickness, thickness:-thickness] == 1.)', 'def test_iteration_property_parallel(self, exprs, expected):\\n        \"\"\"Tests detection of sequental and parallel Iterations when applying\\n        equations over different subdomains.\"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions  # noqa\\n        t = grid.time_dim  # noqa\\n\\n        interior = grid.interior  # noqa\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid, save=10, time_order=1)  # noqa\\n\\n        # List comprehension would need explicit locals/globals mappings to eval\\n        for i, e in enumerate(list(exprs)):\\n            exprs[i] = eval(e)\\n\\n        op = Operator(exprs, opt=\\'noop\\')\\n        iterations = FindNodes(Iteration).visit(op)\\n        assert all(i.is_Sequential for i in iterations if i.dim.name != expected)\\n        assert all(i.is_Parallel for i in iterations if i.dim.name == expected)', 'def test_iteration_property_vector(self, exprs, expected):\\n        \"\"\"Tests detection of vector Iterations when using subdimensions.\"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions  # noqa\\n        time = grid.time_dim  # noqa\\n\\n        # The leftmost 10 elements\\n        yleft = SubDimension.left(name=\\'yleft\\', parent=y, thickness=10) # noqa\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid, save=10, time_order=0, space_order=1)  # noqa\\n\\n        # List comprehension would need explicit locals/globals mappings to eval\\n        for i, e in enumerate(list(exprs)):\\n            exprs[i] = eval(e)\\n\\n        op = Operator(exprs, opt=\\'simd\\')\\n        iterations = FindNodes(Iteration).visit(op)\\n        vectorized = [i.dim.name for i in iterations if i.is_Vectorized]\\n        assert set(vectorized) == set(expected)', 'def test_subdimmiddle_parallel(self, opt):\\n        \"\"\"\\n        Tests application of an Operator consisting of a subdimension\\n        defined over different sub-regions, explicitly created through the\\n        use of SubDimensions.\\n        \"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions\\n        t = grid.stepping_dim\\n        thickness = 4\\n\\n        u = TimeFunction(name=\\'u\\', save=None, grid=grid, space_order=0, time_order=1)\\n\\n        xi = SubDimension.middle(name=\\'xi\\', parent=x,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n\\n        yi = SubDimension.middle(name=\\'yi\\', parent=y,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n\\n        # a 5 point stencil that can be computed in parallel\\n        centre = Eq(u[t+1, xi, yi], u[t, xi, yi] + u[t, xi-1, yi]\\n                                    + u[t, xi+1, yi] + u[t, xi, yi-1] + u[t, xi, yi+1])\\n\\n        u.data[0, 10, 10] = 1.0\\n\\n        op = Operator([centre], opt=opt)\\n        print(op.ccode)\\n\\n        iterations = FindNodes(Iteration).visit(op)\\n        assert all(i.is_Affine and i.is_Parallel for i in iterations if i.dim in [xi, yi])\\n\\n        op.apply(time_m=0, time_M=0)\\n\\n        assert np.all(u.data[1, 9:12, 10] == 1.0)\\n        assert np.all(u.data[1, 10, 9:12] == 1.0)\\n\\n        # Other than those, it should all be 0\\n        u.data[1, 9:12, 10] = 0.0\\n        u.data[1, 10, 9:12] = 0.0\\n        assert np.all(u.data[1, :] == 0)', 'def test_subdimmiddle_notparallel(self):\\n        \"\"\"\\n        Tests application of an Operator consisting of a subdimension\\n        defined over different sub-regions, explicitly created through the\\n        use of SubDimensions.\\n\\n        Different from ``test_subdimmiddle_parallel`` because an interior\\n        dimension cannot be evaluated in parallel.\\n        \"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions\\n        t = grid.stepping_dim\\n        thickness = 4\\n\\n        u = TimeFunction(name=\\'u\\', save=None, grid=grid, space_order=0, time_order=1)\\n\\n        xi = SubDimension.middle(name=\\'xi\\', parent=x,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n\\n        yi = SubDimension.middle(name=\\'yi\\', parent=y,\\n                                 thickness_left=thickness, thickness_right=thickness)\\n\\n        # flow dependencies in x and y which should force serial execution\\n        # in reverse direction\\n        centre = Eq(u[t+1, xi, yi], u[t, xi, yi] + u[t+1, xi+1, yi+1])\\n        u.data[0, 10, 10] = 1.0\\n\\n        op = Operator([centre])\\n\\n        iterations = FindNodes(Iteration).visit(op)\\n        assert all(i.is_Affine and i.is_Sequential for i in iterations if i.dim == xi)\\n        assert all(i.is_Affine and i.is_Parallel for i in iterations if i.dim == yi)\\n\\n        op.apply(time_m=0, time_M=0)\\n\\n        for i in range(4, 11):\\n            assert u.data[1, i, i] == 1.0\\n            u.data[1, i, i] = 0.0\\n\\n        assert np.all(u.data[1, :] == 0)', 'def test_subdim_fd(self):\\n        \"\"\"\\n        Test that the FD shortcuts are handled correctly with SubDimensions\\n        \"\"\"\\n        grid = Grid(shape=(20, 20))\\n        x, y = grid.dimensions\\n\\n        u = TimeFunction(name=\\'u\\', save=None, grid=grid, space_order=1, time_order=1)\\n        u.data[:] = 2.\\n\\n        # Flows inward (i.e. forward) rather than outward\\n        eq = [Eq(u.forward, u.dx + u.dy, subdomain=grid.interior)]\\n\\n        op = Operator(eq)\\n\\n        op.apply(time_M=0)\\n\\n        assert np.all(u.data[1, -1, :] == 2.)\\n        assert np.all(u.data[1, :, 0] == 2.)\\n        assert np.all(u.data[1, :, -1] == 2.)\\n        assert np.all(u.data[1, 0, :] == 2.)\\n        assert np.all(u.data[1, 1:18, 1:18] == 0.)', 'def test_expandingbox_like(self, opt):\\n        \"\"\"\\n        Make sure SubDimensions aren\\'t an obstacle to expanding boxes.\\n        \"\"\"\\n        grid = Grid(shape=(8, 8))\\n        x, y = grid.dimensions\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid)\\n        xi = SubDimension.middle(name=\\'xi\\', parent=x, thickness_left=2, thickness_right=2)\\n        yi = SubDimension.middle(name=\\'yi\\', parent=y, thickness_left=2, thickness_right=2)\\n\\n        eqn = Eq(u.forward, u + 1)\\n        eqn = eqn.subs({x: xi, y: yi})\\n\\n        op = Operator(eqn, opt=opt)\\n\\n        op.apply(time=3, x_m=2, x_M=5, y_m=2, y_M=5,\\n                 xi_ltkn=0, xi_rtkn=0, yi_ltkn=0, yi_rtkn=0)\\n\\n        assert np.all(u.data[0, 2:-2, 2:-2] == 4.)\\n        assert np.all(u.data[1, 2:-2, 2:-2] == 3.)\\n        assert np.all(u.data[:, :2] == 0.)\\n        assert np.all(u.data[:, -2:] == 0.)\\n        assert np.all(u.data[:, :, :2] == 0.)\\n        assert np.all(u.data[:, :, -2:] == 0.)', \"def test_basic(self):\\n        nt = 19\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n\\n        u = TimeFunction(name='u', grid=grid)\\n        assert(grid.stepping_dim in u.indices)\\n\\n        u2 = TimeFunction(name='u2', grid=grid, save=nt)\\n        assert(time in u2.indices)\\n\\n        factor = 4\\n        time_subsampled = ConditionalDimension('t_sub', parent=time, factor=factor)\\n        usave = TimeFunction(name='usave', grid=grid, save=(nt+factor-1)//factor,\\n                             time_dim=time_subsampled)\\n        assert(time_subsampled in usave.indices)\\n\\n        eqns = [Eq(u.forward, u + 1.), Eq(u2.forward, u2 + 1.), Eq(usave, u)]\\n        op = Operator(eqns)\\n        op.apply(t_M=nt-2)\\n        assert np.all(np.allclose(u.data[(nt-1) % 3], nt-1))\\n        assert np.all([np.allclose(u2.data[i], i) for i in range(nt)])\\n        assert np.all([np.allclose(usave.data[i], i*factor)\\n                      for i in range((nt+factor-1)//factor)])\", 'def test_spacial_subsampling(self, opt):\\n        \"\"\"\\n        Test conditional dimension for the spatial ones.\\n        This test saves u every two grid points :\\n        u2[x, y] = u[2*x, 2*y]\\n        \"\"\"\\n        nt = 19\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid, save=nt)\\n        assert(grid.time_dim in u.indices)\\n\\n        # Creates subsampled spatial dimensions and accordine grid\\n        dims = tuple([ConditionalDimension(d.name+\\'sub\\', parent=d, factor=2)\\n                      for d in u.grid.dimensions])\\n        grid2 = Grid((6, 6), dimensions=dims, time_dimension=time)\\n        u2 = TimeFunction(name=\\'u2\\', grid=grid2, save=nt)\\n        assert(time in u2.indices)\\n\\n        eqns = [Eq(u.forward, u + 1.), Eq(u2, u)]\\n        op = Operator(eqns, opt=opt)\\n        op.apply(time_M=nt-2)\\n        # Verify that u2[x,y]= u[2*x, 2*y]\\n        assert np.allclose(u.data[:-1, 0::2, 0::2], u2.data[:-1, :, :])', 'def test_issue_1592(self):\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n        time_sub = ConditionalDimension(\\'t_sub\\', parent=time, factor=2)\\n        v = TimeFunction(name=\"v\", grid=grid, space_order=4, time_dim=time_sub, save=5)\\n        w = Function(name=\"w\", grid=grid, space_order=4)\\n        Operator(Eq(w, v.dx))(time=6)\\n        op = Operator(Eq(v.forward, v.dx))\\n        op.apply(time=6)\\n        exprs = FindNodes(Expression).visit(op)\\n        assert exprs[-1].expr.lhs.indices[0] == IntDiv(time, 2) + 1', 'def test_nothing_in_negative(self):\\n        \"\"\"Test the case where when the condition is false, there is nothing to do.\"\"\"\\n        nt = 4\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n\\n        u = TimeFunction(name=\\'u\\', save=nt, grid=grid)\\n        assert(grid.time_dim in u.indices)\\n\\n        factor = 4\\n        time_subsampled = ConditionalDimension(\\'t_sub\\', parent=time, factor=factor)\\n        usave = TimeFunction(name=\\'usave\\', grid=grid, save=(nt+factor-1)//factor,\\n                             time_dim=time_subsampled)\\n        assert(time_subsampled in usave.indices)\\n\\n        eqns = [Eq(usave, u)]\\n        op = Operator(eqns)\\n\\n        u.data[:] = 1.0\\n        usave.data[:] = 0.0\\n        op.apply(time_m=1, time_M=1)\\n        assert np.allclose(usave.data, 0.0)\\n\\n        op.apply(time_m=0, time_M=0)\\n        assert np.allclose(usave.data, 1.0)', \"def test_as_expr(self):\\n        nt = 19\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n\\n        u = TimeFunction(name='u', grid=grid)\\n        assert(grid.stepping_dim in u.indices)\\n\\n        u2 = TimeFunction(name='u2', grid=grid, save=nt)\\n        assert(time in u2.indices)\\n\\n        factor = 4\\n        time_subsampled = ConditionalDimension('t_sub', parent=time, factor=factor)\\n        usave = TimeFunction(name='usave', grid=grid, save=(nt+factor-1)//factor,\\n                             time_dim=time_subsampled)\\n        assert(time_subsampled in usave.indices)\\n\\n        eqns = [Eq(u.forward, u + 1.), Eq(u2.forward, u2 + 1.),\\n                Eq(usave, time_subsampled * u)]\\n        op = Operator(eqns)\\n        op.apply(t=nt-2)\\n        assert np.all(np.allclose(u.data[(nt-1) % 3], nt-1))\\n        assert np.all([np.allclose(u2.data[i], i) for i in range(nt)])\\n        assert np.all([np.allclose(usave.data[i], i*factor*i)\\n                      for i in range((nt+factor-1)//factor)])\", 'def test_no_index(self):\\n        \"\"\"Test behaviour when the ConditionalDimension is used as a symbol in\\n        an expression.\"\"\"\\n        nt = 19\\n        grid = Grid(shape=(11, 11))\\n        time = grid.time_dim\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid)\\n        assert(grid.stepping_dim in u.indices)\\n\\n        v = Function(name=\\'v\\', grid=grid)\\n\\n        factor = 4\\n        time_subsampled = ConditionalDimension(\\'t_sub\\', parent=time, factor=factor)\\n\\n        eqns = [Eq(u.forward, u + 1), Eq(v, v + u*u*time_subsampled)]\\n        op = Operator(eqns)\\n        op.apply(t_M=nt-2)\\n        assert np.all(np.allclose(u.data[(nt-1) % 3], nt-1))\\n        # expected result is 1024\\n        # v = u[0]**2 * 0 + u[4]**2 * 1 + u[8]**2 * 2 + u[12]**2 * 3 + u[16]**2 * 4\\n        # with u[t] = t\\n        # v = 16 * 1 + 64 * 2 + 144 * 3 + 256 * 4 = 1600\\n        assert np.all(np.allclose(v.data, 1600))', 'def test_symbolic_factor(self):\\n        \"\"\"\\n        Test ConditionalDimension with symbolic factor (provided as a Constant).\\n        \"\"\"\\n        g = Grid(shape=(4, 4, 4))\\n\\n        u = TimeFunction(name=\\'u\\', grid=g, time_order=0)\\n\\n        fact = Constant(name=\\'fact\\', dtype=np.int32, value=4)\\n        tsub = ConditionalDimension(name=\\'tsub\\', parent=g.time_dim, factor=fact)\\n        usave = TimeFunction(name=\\'usave\\', grid=g, time_dim=tsub, save=4)\\n\\n        op = Operator([Eq(u, u + 1), Eq(usave, u)])\\n\\n        op.apply(time=7)  # Use `fact`\\'s default value, 4\\n        assert np.all(usave.data[0] == 1)\\n        assert np.all(usave.data[1] == 5)\\n\\n        u.data[:] = 0.\\n        op.apply(time=7, fact=2)\\n        assert np.all(usave.data[0] == 1)\\n        assert np.all(usave.data[1] == 3)\\n        assert np.all(usave.data[2] == 5)\\n        assert np.all(usave.data[3] == 7)', 'def test_grouping(self):\\n        \"\"\"\\n        Test that Clusters over the same set of ConditionalDimensions fall within\\n        the same Conditional. This is a follow up to issue #1610.\\n        \"\"\"\\n        grid = Grid(shape=(10, 10))\\n        time = grid.time_dim\\n        cond = ConditionalDimension(name=\\'cond\\', parent=time, condition=time < 5)\\n\\n        u = TimeFunction(name=\\'u\\', grid=grid, space_order=4)\\n\\n        # We use a SubDomain only to keep the two Eqs separated\\n        eqns = [Eq(u.forward, u + 1, subdomain=grid.interior),\\n                Eq(u.forward, u.dx.dx + 1., implicit_dims=[cond])]\\n\\n        op = Operator(eqns, opt=(\\'advanced-fsg\\', {\\'cire-mincost-sops\\': 1}))\\n\\n        conds = FindNodes(Conditional).visit(op)\\n        assert len(conds) == 1\\n        assert len(retrieve_iteration_tree(conds[0].then_body)) == 2', 'def test_expr_like_lowering(self):\\n        \"\"\"\\n        Test the lowering of an expr-like ConditionalDimension\\'s condition.\\n        This test makes an Operator that should indexify and lower the condition\\n        passed in the Conditional Dimension\\n        \"\"\"\\n\\n        grid = Grid(shape=(3, 3))\\n        g1 = Function(name=\\'g1\\', grid=grid)\\n        g2 = Function(name=\\'g2\\', grid=grid)\\n\\n        g1.data[:] = 0.49\\n        g2.data[:] = 0.49\\n        x, y = grid.dimensions\\n        ci = ConditionalDimension(name=\\'ci\\', parent=y, condition=Le((g1 + g2),\\n                                  1.01*(g1 + g2)))\\n\\n        f = Function(name=\\'f\\', shape=grid.shape, dimensions=(x, ci))\\n        Operator(Eq(f, g1+g2)).apply()\\n\\n        assert np.all(f.data[:] == g1.data[:] + g2.data[:])', 'def test_relational_classes(self, setup_rel, rhs, c1, c2, c3, c4):\\n        \"\"\"\\n        Test ConditionalDimension using conditions based on Relations over SubDomains.\\n        \"\"\"\\n\\n        class InnerDomain(SubDomain):\\n            name = \\'inner\\'\\n\\n            def define(self, dimensions):\\n                return {d: (\\'middle\\', 2, 2) for d in dimensions}\\n\\n        inner_domain = InnerDomain()\\n        grid = Grid(shape=(8, 8), subdomains=(inner_domain,))\\n        g = Function(name=\\'g\\', grid=grid)\\n        g2 = Function(name=\\'g2\\', grid=grid)\\n\\n        for i in [g, g2]:\\n            i.data[:4, :4] = 1\\n            i.data[4:, :4] = 2\\n            i.data[4:, 4:] = 3\\n            i.data[:4, 4:] = 4\\n\\n        xi, yi = grid.subdomains[\\'inner\\'].dimensions\\n\\n        cond = setup_rel(0.25*g + 0.75*g2, rhs, subdomain=grid.subdomains[\\'inner\\'])\\n        ci = ConditionalDimension(name=\\'ci\\', parent=yi, condition=cond)\\n        f = Function(name=\\'f\\', shape=grid.shape, dimensions=(xi, ci))\\n\\n        eq1 = Eq(f, 0.4*g + 0.6*g2)\\n        eq2 = Eq(f, 5)\\n\\n        Operator([eq1, eq2]).apply()\\n        assert np.all(f.data[2:6, c1:c2] == 5.)\\n        assert np.all(f.data[:, c3:c4] < 5.)', 'def test_no_fusion_simple(self):\\n        \"\"\"\\n        If ConditionalDimensions are present, then Clusters must not be fused so\\n        that ultimately Eqs get scheduled to different loop nests.\\n        \"\"\"\\n        grid = Grid(shape=(4, 4, 4))\\n        time = grid.time_dim\\n\\n        f = TimeFunction(name=\\'f\\', grid=grid)\\n        g = Function(name=\\'g\\', grid=grid)\\n        h = Function(name=\\'h\\', grid=grid)\\n\\n        # No ConditionalDimensions yet. Will be fused and optimized\\n        eqns = [Eq(f.forward, f + 1),\\n                Eq(h, f + 1),\\n                Eq(g, f + 1)]\\n\\n        op = Operator(eqns)\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf0\\'].root)\\n        assert len(exprs) == 4\\n        assert exprs[1].expr.rhs is exprs[0].output\\n        assert exprs[2].expr.rhs is exprs[0].output\\n        assert exprs[3].expr.rhs is exprs[0].output\\n\\n        # Now with a ConditionalDimension. No fusion, no optimization\\n        ctime = ConditionalDimension(name=\\'ctime\\', parent=time, condition=time > 4)\\n\\n        eqns = [Eq(f.forward, f + 1),\\n                Eq(h, f + 1),\\n                Eq(g, f + 1, implicit_dims=[ctime])]\\n\\n        op = Operator(eqns)\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf0\\'].root)\\n        assert len(exprs) == 3\\n        assert exprs[1].expr.rhs is exprs[0].output\\n        assert exprs[2].expr.rhs is exprs[0].output\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf1\\'].root)\\n        assert len(exprs) == 1', 'def test_no_fusion_convoluted(self):\\n        \"\"\"\\n        Conceptually like `test_no_fusion_simple`, but with more expressions\\n        and non-trivial data flow.\\n        \"\"\"\\n        grid = Grid(shape=(4, 4, 4))\\n        time = grid.time_dim\\n\\n        f = TimeFunction(name=\\'f\\', grid=grid)\\n        g = Function(name=\\'g\\', grid=grid)\\n        h = Function(name=\\'h\\', grid=grid)\\n\\n        ctime = ConditionalDimension(name=\\'ctime\\', parent=time, condition=time > 4)\\n\\n        eqns = [Eq(f.forward, f + 1),\\n                Eq(h, f + 1),\\n                Eq(g, f + 1, implicit_dims=[ctime]),\\n                Eq(f.forward, f + 1, implicit_dims=[ctime]),\\n                Eq(f.forward, f + 1),\\n                Eq(g, f + 1)]\\n\\n        op = Operator(eqns)\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf0\\'].root)\\n        assert len(exprs) == 3\\n        assert exprs[1].expr.rhs is exprs[0].output\\n        assert exprs[2].expr.rhs is exprs[0].output\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf1\\'].root)\\n        assert len(exprs) == 3\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf2\\'].root)\\n        assert len(exprs) == 3\\n        assert exprs[1].expr.rhs is exprs[0].output\\n        assert exprs[2].expr.rhs is exprs[0].output', 'def test_topofusion_w_subdims_conddims(self):\\n        \"\"\"\\n        Check that topological fusion works across guarded Clusters over different\\n        iteration spaces and in presence of anti-dependences.\\n\\n        This test uses both SubDimensions (via SubDomains) and ConditionalDimensions.\\n        \"\"\"\\n        grid = Grid(shape=(4, 4, 4))\\n        time = grid.time_dim\\n\\n        f = TimeFunction(name=\\'f\\', grid=grid, time_order=2)\\n        g = TimeFunction(name=\\'g\\', grid=grid, time_order=2)\\n        h = TimeFunction(name=\\'h\\', grid=grid, time_order=2)\\n        fsave = TimeFunction(name=\\'fsave\\', grid=grid, time_order=2, save=5)\\n        gsave = TimeFunction(name=\\'gsave\\', grid=grid, time_order=2, save=5)\\n\\n        ctime = ConditionalDimension(name=\\'ctime\\', parent=time, condition=time > 4)\\n\\n        eqns = [Eq(f.forward, f + 1),\\n                Eq(g.forward, g + 1),\\n                Eq(fsave, f.dt2, implicit_dims=[ctime]),\\n                Eq(h, f + g, subdomain=grid.interior),\\n                Eq(gsave, g.dt2, implicit_dims=[ctime])]\\n\\n        op = Operator(eqns)\\n\\n        # Check generated code -- expect the gsave equation to be scheduled together\\n        # in the same loop nest with the fsave equation\\n        assert len(op._func_table) == 3\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf0\\'].root)\\n        assert len(exprs) == 2\\n        assert exprs[0].write is f\\n        assert exprs[1].write is g\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf1\\'].root)\\n        assert len(exprs) == 3\\n        assert exprs[1].write is fsave\\n        assert exprs[2].write is gsave\\n\\n        exprs = FindNodes(Expression).visit(op._func_table[\\'bf2\\'].root)\\n        assert len(exprs) == 1\\n        assert exprs[0].write is h']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def can_write(self, **kwargs):\\n        user = kwargs.get('user', None)\\n        if not settings.ANONYMOUS and (not user or user.is_anonymous()):\\n            return False\\n        return ReusablePlugin.can_write(self, **kwargs)\", 'def __unicode__(self):\\n        return \"%s: %s\" % (self.article.current_revision.title, self.original_filename)', 'def upload_path(instance, filename):\\n    from os import path\\n\\n    extension = extension_allowed(filename)\\n\\n    # Has to match original extension filename\\n    if instance.id and instance.attachment and instance.attachment.original_filename:\\n        original_extension = instance.attachment.original_filename.split(\".\")[-1]\\n        if not extension.lower() == original_extension:\\n            raise IllegalFileExtension(\"File extension has to be \\'%s\\', not \\'%s\\'.\" %\\n                                       (original_extension, extension.lower()))\\n    elif instance.attachment:\\n        instance.attachment.original_filename = filename\\n\\n    upload_path = settings.UPLOAD_PATH\\n    upload_path = upload_path.replace(\\'%aid\\', str(instance.attachment.article.id))\\n    if settings.UPLOAD_PATH_OBSCURIFY:\\n        import random\\n        import hashlib\\n        m = hashlib.md5(str(random.randint(0, 100000000000000)))\\n        upload_path = path.join(upload_path, m.hexdigest())\\n\\n    if settings.APPEND_EXTENSION:\\n        filename += \\'.upload\\'\\n    return path.join(upload_path, filename)', 'def get_filename(self):\\n        \"\"\"Used to retrieve the filename of a revision.\\n        But attachment.original_filename should always be used in the frontend\\n        such that filenames stay consistent.\"\"\"\\n        # TODO: Perhaps we can let file names change when files are replaced?\\n        if not self.file:\\n            return None\\n        filename = self.file.name.split(\"/\")[-1]\\n        return \".\".join(filename.split(\".\")[:-1])', \"def save(self, *args, **kwargs):\\n        if (not self.id and\\n            not self.previous_revision and\\n            self.attachment and\\n            self.attachment.current_revision and\\n            self.attachment.current_revision != self):\\n\\n            self.previous_revision = self.attachment.current_revision\\n\\n        if not self.revision_number:\\n            try:\\n                previous_revision = self.attachment.attachmentrevision_set.latest()\\n                self.revision_number = previous_revision.revision_number + 1\\n            # NB! The above should not raise the below exception, but somehow it does.\\n            except AttachmentRevision.DoesNotExist, Attachment.DoesNotExist:\\n                self.revision_number = 1\\n\\n        super(AttachmentRevision, self).save(*args, **kwargs)\\n\\n        if not self.attachment.current_revision:\\n            # If I'm saved from Django admin, then article.current_revision is me!\\n            self.attachment.current_revision = self\\n            self.attachment.save()\", 'def on_revision_delete(instance, *args, **kwargs):\\n    if not instance.file:\\n        return\\n\\n    # Remove file\\n    path = instance.file.path.split(\"/\")[:-1]\\n    instance.file.delete(save=False)\\n\\n    # Clean up empty directories\\n\\n    # Check for empty folders in the path. Delete the first two.\\n    if len(path[-1]) == 32:\\n        # Path was (most likely) obscurified so we should look 2 levels down\\n        max_depth = 2\\n    else:\\n        max_depth = 1\\n    for depth in range(0, max_depth):\\n        delete_path = \"/\".join(path[:-depth] if depth > 0 else path)\\n        try:\\n            if len(os.listdir(os.path.join(django_settings.MEDIA_ROOT, delete_path))) == 0:\\n                os.rmdir(delete_path)\\n        except OSError:\\n            # Raised by os.listdir if directory is missing\\n            pass']}, {'features': [], 'snippets': ['def build_uniprot_to_index_to_core(sable_db_obj):\\n    uniprot_to_index_to_core = {}\\n    for line in sable_db_obj:\\n        tokens = line.split()\\n        try:\\n            # PARSING ID\\n            prot = tokens[0]\\n            index = int(tokens[1])\\n            core = tokens[2]\\n            # PARSING ID\\n            if uniprot_to_index_to_core.has_key(prot):\\n                uniprot_to_index_to_core[prot][index] = core\\n            else:\\n                uniprot_to_index_to_core[prot] = {index: core}\\n        except ValueError:\\n            print \"Cannot parse: \" + line[0:len(line) - 1]\\n    return uniprot_to_index_to_core']}, {'features': [], 'snippets': ['def initialize(self, base_block):\\n        self.vtx = copy.deepcopy(base_block.vtx)\\n        self.hashMerkleRoot = self.calc_merkle_root()', 'def normal_serialize(self):\\n        return super().serialize()', 'def set_test_params(self):\\n        self.num_nodes = 1\\n        self.setup_clean_chain = True\\n        self.extra_args = [[]]', 'def add_transactions_to_block(self, block, tx_list):\\n        [tx.rehash() for tx in tx_list]\\n        block.vtx.extend(tx_list)', 'def create_tx(self, spend_tx, n, value, script=CScript([OP_TRUE, OP_DROP] * 15 + [OP_TRUE])):\\n        return create_tx_with_script(spend_tx, n, amount=value, script_pub_key=script)', 'def sign_tx(self, tx, spend_tx):\\n        scriptPubKey = bytearray(spend_tx.vout[0].scriptPubKey)\\n        if (scriptPubKey[0] == OP_TRUE):  # an anyone-can-spend\\n            tx.vin[0].scriptSig = CScript()\\n            return\\n        (sighash, err) = SignatureHash(spend_tx.vout[0].scriptPubKey, tx, 0, SIGHASH_ALL)\\n        tx.vin[0].scriptSig = CScript([self.coinbase_key.sign(sighash) + bytes(bytearray([SIGHASH_ALL]))])', 'def next_block(self, number, spend=None, additional_coinbase_value=0, script=CScript([OP_TRUE]), solve=True):\\n        if self.tip is None:\\n            base_block_hash = self.genesis_hash\\n            block_time = int(time.time()) + 1\\n        else:\\n            base_block_hash = self.tip.sha256\\n            block_time = self.tip.nTime + 1\\n        # First create the coinbase\\n        height = self.block_heights[base_block_hash] + 1\\n        coinbase = create_coinbase(height, self.coinbase_pubkey)\\n        coinbase.vout[0].nValue += additional_coinbase_value\\n        coinbase.rehash()\\n        if spend is None:\\n            block = create_block(base_block_hash, coinbase, block_time)\\n        else:\\n            coinbase.vout[0].nValue += spend.vout[0].nValue - 1  # all but one satoshi to fees\\n            coinbase.rehash()\\n            block = create_block(base_block_hash, coinbase, block_time)\\n            tx = self.create_tx(spend, 0, 1, script)  # spend 1 satoshi\\n            self.sign_tx(tx, spend)\\n            self.add_transactions_to_block(block, [tx])\\n            block.hashMerkleRoot = block.calc_merkle_root()\\n        if solve:\\n            block.solve()\\n        self.tip = block\\n        self.block_heights[block.sha256] = height\\n        assert number not in self.blocks\\n        self.blocks[number] = block\\n        return block', 'def save_spendable_output(self):\\n        self.log.debug(\"saving spendable output %s\" % self.tip.vtx[0])\\n        self.spendable_outputs.append(self.tip)', 'def get_spendable_output(self):\\n        self.log.debug(\"getting spendable output %s\" % self.spendable_outputs[0].vtx[0])\\n        return self.spendable_outputs.pop(0).vtx[0]', 'def move_tip(self, number):\\n        self.tip = self.blocks[number]', 'def update_block(self, block_number, new_transactions):\\n        block = self.blocks[block_number]\\n        self.add_transactions_to_block(block, new_transactions)\\n        old_sha256 = block.sha256\\n        block.hashMerkleRoot = block.calc_merkle_root()\\n        block.solve()\\n        # Update the internal state just like in next_block\\n        self.tip = block\\n        if block.sha256 != old_sha256:\\n            self.block_heights[block.sha256] = self.block_heights[old_sha256]\\n            del self.block_heights[old_sha256]\\n        self.blocks[block_number] = block\\n        return block', 'def reconnect_p2p(self):\\n        \"\"\"Tear down and bootstrap the P2P connection to the node.\\n\\n        The node gets disconnected several times in this test. This helper\\n        method reconnects the p2p and restarts the network thread.\"\"\"\\n        self.nodes[0].disconnect_p2ps()\\n        self.bootstrap_p2p()']}, {'features': [], 'snippets': [\"def __init__(self,domain='rds.aliyuncs.com',port=80):\\n\\t\\tRestApi.__init__(self,domain, port)\\n\\t\\tself.AccountName = None\\n\\t\\tself.DBInstanceId = None\\n\\t\\tself.resourceOwnerAccount = None\"]}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Adding field 'Idea.color'\\n        db.add_column(u'brainstorming_idea', 'color',\\n                      self.gf('django.db.models.fields.CharField')(default='', max_length=100, blank=True),\\n                      keep_default=False)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def dumpIndexes():\\n    with open(indexes_path,'wb') as fh:\\n        json.dump(indexes,fh)\", 'def getActiveName():\\n    # ACTIVE INDEX NUMER\\n    activeIndex = indexes[\\'active\\']\\n    if activeIndex is None:\\n        print \"No active index.  Use \\'list -i\\' to list available indexies and \\'use\\' to set an active index.\"\\n        sys.exit()\\n\\n    # GET THE NAME OF THE INDEX\\n    try:\\n        activeIndexName = indexes[\\'names\\'][indexes[\\'active\\']]\\n    except:\\n        print \"Invalid index number\"\\n        sys.exit()\\n    return activeIndexName', 'def readSettings(name):\\n    \"\"\" A utility function which loads the index settings from file\\n    \"\"\"\\n    try:\\n        with open(os.path.join(user_data_dir,name+\".settings\"),\\'rb\\') as fh:\\n            settings = json.load(fh)\\n    except Exception as e:\\n        print \"Error reading index settings\"\\n        import pdb\\n        pdb.set_trace()\\n        sys.exit()\\n    return settings', 'def dumpSettings(settings,name):\\n    \"\"\" A utility function which saves the index settings to file\\n    \"\"\"\\n    try:\\n        with open(os.path.join(user_data_dir,name+\".settings\"),\\'wb\\') as fh: \\n            json.dump(settings,fh)\\n    except Exception as e:\\n        print \"Error writing index settings\"\\n        import pdb\\n        pdb.set_trace()\\n        sys.exit()']}, {'features': [], 'snippets': ['def __init__(self, size):\\n        self._data = np.zeros((size,))\\n        self._capacity = size\\n        self._size = 0', 'def __getitem__(self, index):\\n        \"\"\"Get the value at the given index.\\n\\n        Parameters\\n        ----------\\n        index : int\\n            The index into the array.\\n\\n        \"\"\"\\n        return self._data[index]', 'def __init__(self, x=0.0, y=0.0):\\n        self.x = x\\n        self.y = y', 'def __init__(self, x=0.0, y=0.0, z=0.0):\\n        self.x = x\\n        self.y = y\\n        self.z = z', 'def __init__(self, x=0.0, y=0.0, z=0.0):\\n        super(Vector3D, self).__init__(x, y, z)', 'def __init__(self):\\n        self._queue = []', 'def __contains__(self, item):\\n        try:\\n            self._queue.index(item)\\n            return True\\n        except Exception:\\n            return False', \"def __str__(self):\\n        return '[' + ', '.join('{}'.format(el) for el in self._queue) + ']'\", 'def push(self, item):\\n        \"\"\"Push a new element on the queue\\n\\n        Parameters\\n        ----------\\n        item :\\n            The element to push on the queue\\n\\n        \"\"\"\\n        raise NotImplementedError', 'def pop(self):\\n        \"\"\"Pop an element from the queue.\"\"\"\\n        raise NotImplementedError', 'def extend(self, items):\\n        \"\"\"Extend the queue by a number of elements.\\n\\n        Parameters\\n        ----------\\n        items : list\\n            A list of items.\\n\\n        \"\"\"\\n        for item in items:\\n            self.push(item)', 'def remove(self, item):\\n        \"\"\"Remove an element from the queue.\\n\\n        Parameters\\n        ----------\\n        item :\\n            The element to remove.\\n\\n        \"\"\"\\n        self._queue.remove(item)', 'def __init__(self):\\n        super(FIFOQueue, self).__init__()', 'def pop(self):\\n        \"\"\"Return the element at the front of the queue.\\n\\n        Returns\\n        -------\\n        The first element in the queue.\\n\\n        \"\"\"\\n        return self._queue.pop(0)', 'def __init__(self, func=lambda x: x):\\n        super(PriorityQueue, self).__init__()\\n\\n        self.func = func', \"def __str__(self):\\n        return '[' + ', '.join('({},{})'.format(*el) for el in self._queue) + ']'\", 'def pop(self):\\n        \"\"\"Get the element with the highest priority.\\n\\n        Get the element with the highest priority (i.e., smallest value).\\n\\n        Returns\\n        -------\\n        The element with the highest priority.\\n\\n        \"\"\"\\n        return heapq.heappop(self._queue)[1]']}, {'features': [], 'snippets': ['def test_img2patch(self):\\n        img = np.arange(16).reshape(1, 4, 4, 1)\\n        patch = img2patch(img, size=3, step=1)\\n        expected = np.asarray([\\n            [img[0, 0:3, 0:3, 0], img[0, 0:3, 1:4, 0]],\\n            [img[0, 1:4, 0:3, 0], img[0, 1:4, 1:4, 0]]\\n        ])\\n        expected = expected[None, ..., None]\\n        self.assertTrue((patch == expected).all())\\n\\n        imgs = [\\n            np.random.randn(2, 5, 6, 3),\\n            np.random.randn(3, 10, 10, 2),\\n            np.random.randn(1, 23, 17, 5)\\n        ]\\n        sizes = [\\n            (1, 1),\\n            2,\\n            (3, 4)\\n        ]\\n        steps = [\\n            (1, 2),\\n            (3, 1),\\n            3\\n        ]\\n        shapes = [\\n            (2, 5, 3, 1, 1, 3),\\n            (3, 3, 9, 2, 2, 2),\\n            (1, 7, 5, 3, 4, 5)\\n        ]\\n        for img, size, step, shape in zip(imgs, sizes, steps, shapes):\\n            self.assertEqual(shape, img2patch(img, size, step).shape)', 'def test_patch2img(self):\\n        img = np.arange(16).reshape(1, 4, 4, 1)\\n        patch = img2patch(img, size=2, step=2)\\n        self.assertTrue((img == patch2img(patch, (2, 2), (1, 4, 4, 1))).all())\\n        patch = img2patch(img, size=3, step=1)\\n        expected = np.arange(0, 32, 2).reshape(1, 4, 4, 1)\\n        expected[0, 0, 0, 0] /= 2\\n        expected[0, 0, -1, 0] /= 2\\n        expected[0, -1, 0, 0] /= 2\\n        expected[0, -1, -1, 0] /= 2\\n        expected[0, 1:3, 1:3, 0] *= 2\\n        self.assertTrue((expected == patch2img(patch, (1, 1), (1, 4, 4, 1))).all())']}, {'features': [], 'snippets': [\"def add_fields(_, level, event_dict):\\n    ''' Add custom fields to each record. '''\\n    now = dt.datetime.now()\\n    #event_dict['timestamp'] = TZ.localize(now, True).astimezone(pytz.utc).isoformat()\\n    event_dict['timestamp'] = TZ.localize(now, True).astimezone\\\\\\n        (pytz.timezone(app.config['TIMEZONE'])).strftime(app.config['TIME_FMT'])\\n    event_dict['level'] = level\\n    if request:\\n        try:\\n            #event_dict['ip_address'] = request.headers['X-Forwarded-For'].split(',')[0].strip()\\n            event_dict['ip_address'] = request.headers.get('X-Forwarded-For', request.remote_addr)\\n            #event_dict['ip_address'] = request.header.get('X-Real-IP')\\n        except:\\n            event_dict['ip_address'] = 'unknown'\\n\\n    return event_dict\"]}, {'features': [], 'snippets': ['def __init__(self, plotly_name=\"textfont\", parent_name=\"scattersmith\", **kwargs):\\n        super(TextfontValidator, self).__init__(\\n            plotly_name=plotly_name,\\n            parent_name=parent_name,\\n            data_class_str=kwargs.pop(\"data_class_str\", \"Textfont\"),\\n            data_docs=kwargs.pop(\\n                \"data_docs\",\\n                \"\"\"\\n            color\\n\\n            colorsrc\\n                Sets the source reference on Chart Studio Cloud\\n                for `color`.\\n            family\\n                HTML font family - the typeface that will be\\n                applied by the web browser. The web browser\\n                will only be able to apply a font if it is\\n                available on the system which it operates.\\n                Provide multiple font families, separated by\\n                commas, to indicate the preference in which to\\n                apply fonts if they aren\\'t available on the\\n                system. The Chart Studio Cloud (at\\n                https://chart-studio.plotly.com or on-premise)\\n                generates images on a server, where only a\\n                select number of fonts are installed and\\n                supported. These include \"Arial\", \"Balto\",\\n                \"Courier New\", \"Droid Sans\",, \"Droid Serif\",\\n                \"Droid Sans Mono\", \"Gravitas One\", \"Old\\n                Standard TT\", \"Open Sans\", \"Overpass\", \"PT Sans\\n                Narrow\", \"Raleway\", \"Times New Roman\".\\n            familysrc\\n                Sets the source reference on Chart Studio Cloud\\n                for `family`.\\n            size\\n\\n            sizesrc\\n                Sets the source reference on Chart Studio Cloud\\n                for `size`.']}, {'features': [], 'snippets': ['def post_create(request):\\n    if not request.user.is_staff or not request.user.is_superuser:\\n        raise Http404\\n\\n    form = PostForm(request.POST or None, request.FILES or None)\\n    if form.is_valid():\\n        instance = form.save(commit=False)\\n        instance.user = request.user\\n        instance.save()\\n        # message success\\n        messages.success(request, \"Successfully Created\")\\n        return HttpResponseRedirect(instance.get_absolute_url())\\n    context = {\\n        \"form\": form,\\n    }\\n    return render(request, \"post_form.html\", context)', 'def post_list(request):\\n    today = timezone.now().date()\\n    queryset_list = Post.objects.active()  # .order_by(\"-timestamp\")\\n    if request.user.is_staff or request.user.is_superuser:\\n        queryset_list = Post.objects.all()\\n\\n    query = request.GET.get(\"q\")\\n    if query:\\n        queryset_list = queryset_list.filter(\\n            Q(title__icontains=query) |\\n            Q(content__icontains=query) |\\n            Q(user__first_name__icontains=query) |\\n            Q(user__last_name__icontains=query)\\n        ).distinct()\\n    paginator = Paginator(queryset_list, 8)  # Show 25 contacts per page\\n    page_request_var = \"page\"\\n    page = request.GET.get(page_request_var)\\n    try:\\n        queryset = paginator.page(page)\\n    except PageNotAnInteger:\\n        # If page is not an integer, deliver first page.\\n        queryset = paginator.page(1)\\n    except EmptyPage:\\n        # If page is out of range (e.g. 9999), deliver last page of results.\\n        queryset = paginator.page(paginator.num_pages)\\n\\n    context = {\\n        \"object_list\": queryset,\\n        \"title\": \"List\",\\n        \"page_request_var\": page_request_var,\\n        \"today\": today,\\n    }\\n    return render(request, \"post_list.html\", context)']}, {'features': [], 'snippets': ['def __init__(self, json_dict=None):\\r\\n        self.id = None  # (Integer)\\r\\n        self.company = None  # *(CompanyReference)\\r\\n        self.contact = None  # (ContactReference)\\r\\n        self.phone = None  # (String)\\r\\n        self.phoneExt = None  # (String)\\r\\n        self.email = None  # (String)\\r\\n        self.site = None  # (SiteReference)\\r\\n        self.status = None  # *(OrderStatusReference)\\r\\n        self.opportunity = None  # (OpportunityReference)\\r\\n        self.orderDate = None  # (String)\\r\\n        self.dueDate = None  # (String)\\r\\n        self.billingTerms = None  # (BillingTermsReference)\\r\\n        self.taxCode = None  # (TaxCodeReference)\\r\\n        self.poNumber = None  # (String(50))\\r\\n        self.locationId = None  # (Integer)\\r\\n        self.businessUnitId = None  # (Integer)\\r\\n        self.salesRep = None  # *(MemberReference)\\r\\n        self.notes = None  # (String)\\r\\n        self.billClosedFlag = None  # (Boolean)\\r\\n        self.billShippedFlag = None  # (Boolean)\\r\\n        self.restrictDownpaymentFlag = None  # (Boolean)\\r\\n        self.description = None  # (String)\\r\\n        self.topCommentFlag = None  # (Boolean)\\r\\n        self.bottomCommentFlag = None  # (Boolean)\\r\\n        self.shipToCompany = None  # (CompanyReference)\\r\\n        self.shipToContact = None  # (ContactReference)\\r\\n        self.shipToSite = None  # (SiteReference)\\r\\n        self.billToCompany = None  # (CompanyReference)\\r\\n        self.billToContact = None  # (ContactReference)\\r\\n        self.billToSite = None  # (SiteReference)\\r\\n        self.productIds = None  # (Integer[])\\r\\n        self.documentIds = None  # (Integer[])\\r\\n        self.invoiceIds = None  # (Integer[])\\r\\n        self.configIds = None  # (Integer[])\\r\\n        self.total = None  # (Number)\\r\\n        self.taxTotal = None  # (Number)\\r\\n        self._info = None  # (Metadata)']}, {'features': [], 'snippets': ['def __init__(self):\\r\\n        super(MainWindow, self).__init__()', 'def loadToolbar(self):\\r\\n        \"\"\"\\r\\n        Loads the toolbar actions, restore toolbar state, and restore window geometry.\\r\\n        \"\"\"', 'def loadMenu(self):\\r\\n        \"\"\"\\r\\n        Load the menu root items and items into the QTabWidget with the appropriate pages. \\r\\n        \"\"\"\\r\\n        show_empty_root_items = cbpos.config[\\'menu\\', \\'show_empty_root_items\\']\\r\\n        show_disabled_items = cbpos.config[\\'menu\\', \\'show_disabled_items\\']\\r\\n        hide_tab_bar = not cbpos.config[\\'menu\\', \\'show_tab_bar\\']', 'def onCurrentTabChanged(self, index, tabs=None):\\r\\n        if tabs is None:\\r\\n            tabs = self.tabs\\r\\n        widget = tabs.widget(index)\\r\\n        try:\\r\\n            signal = widget.shown\\r\\n        except AttributeError:\\r\\n            pass\\r\\n        else:\\r\\n            signal.emit()', 'def getTabWidget(self, items):\\r\\n        \"\"\"\\r\\n        Returns the appropriate window to be placed in the main QTabWidget,\\r\\n        depending on the number of children of a root menu item.\\r\\n        \"\"\"\\r\\n        count = len(items)\\r\\n        if count == 0:\\r\\n            # If there are no child items, just return an empty widget\\r\\n            widget = QtGui.QWidget()\\r\\n            widget.setEnabled(False)\\r\\n            return widget\\r\\n        elif count == 1:\\r\\n            # If there is only one item, show it as is.\\r\\n            logger.debug(\\'Loading menu page for %s\\', items[0].name)\\r\\n            widget = items[0].page()\\r\\n            widget.setEnabled(items[0].enabled)\\r\\n            return widget\\r\\n        else:\\r\\n            # If there are many children, add them in a QTabWidget\\r\\n            tabs = QtGui.QTabWidget()\\r\\n            tabs.currentChanged.connect(lambda i, t=tabs: self.onCurrentTabChanged(i, t))', 'def saveWindowState(self):\\r\\n        \"\"\"\\r\\n        Saves the main window state (position, size, toolbar positions)\\r\\n        \"\"\"\\r\\n        mwState = self.saveState().toBase64() \\r\\n        mwGeom  = self.saveGeometry().toBase64() \\r\\n        cbpos.config[\\'mainwindow\\', \\'state\\'] = unicode(mwState)\\r\\n        cbpos.config[\\'mainwindow\\', \\'geometry\\'] = unicode(mwGeom)\\r\\n        cbpos.config.save()', 'def closeEvent(self, event):\\r\\n        \"\"\"\\r\\n        Perform necessary operations before closing the window.\\r\\n        \"\"\"\\r\\n        self.saveWindowState()\\r\\n        #do any other thing before closing...\\r\\n        event.accept()', 'def addInit(cls, init):\\r\\n        \"\"\"\\r\\n        Adds the `init` method to the list of extensions of the `MainWindow.__init__`.\\r\\n        \"\"\"\\r\\n        cls.__inits.append(init)']}, {'features': [], 'snippets': [\"def configuration(parent_package='', top_path=None):\\n    import warnings\\n    from numpy.distutils.misc_util import Configuration\\n    from numpy.distutils.system_info import get_info, BlasNotFoundError\\n    config = Configuration('odr', parent_package, top_path)\\n\\n    libodr_files = ['d_odr.f',\\n                    'd_mprec.f',\\n                    'dlunoc.f']\\n\\n    blas_info = get_info('blas_opt')\\n    if blas_info:\\n        libodr_files.append('d_lpk.f')\\n    else:\\n        warnings.warn(BlasNotFoundError.__doc__)\\n        libodr_files.append('d_lpkbls.f')\\n\\n    odrpack_src = [join('odrpack', x) for x in libodr_files]\\n    config.add_library('odrpack', sources=odrpack_src)\\n\\n    sources = ['__odrpack.c']\\n    libraries = ['odrpack'] + blas_info.pop('libraries', [])\\n    include_dirs = ['.'] + blas_info.pop('include_dirs', [])\\n    config.add_extension('__odrpack',\\n                         sources=sources,\\n                         libraries=libraries,\\n                         include_dirs=include_dirs,\\n                         depends=(['odrpack.h'] + odrpack_src),\\n                         **blas_info\\n                         )\\n\\n    config.add_data_dir('tests')\\n    return config\"]}, {'features': [], 'snippets': ['def __init__(self, service_name, service_port, serialport):\\n        super(motorserver, self).__init__(service_name, service_port)\\n        self.serial_port = serialport\\n        self.input_buffer = \"\"\\n        self.evthandler = ioloop_mod.IOLoop.instance().add_handler(self.serial_port.fileno(), self.handle_serial_event, ioloop_mod.IOLoop.instance().READ)\\n        self.last_command_time = time.time()\\n        self.pcb = ioloop_mod.PeriodicCallback(self.check_data_reveived, COMMAND_GRACE_TIME)\\n        self.pcb.start()', 'def _setspeeds(self, m1speed, m2speed):\\n        self.serial_port.write(\"S%04X%04X\\\\n\" % ((m1speed & 0xffff), (m2speed & 0xffff)))', 'def setspeeds(self, resp, m1speed, m2speed):\\n        self.last_command_time = time.time()\\n        #print(\"Got speeds %s,%s\" % (m1speed, m2speed))\\n        self._setspeeds(m1speed, m2speed)\\n        # TODO: actually handle ACK/NACK somehow (we need to read it from the serialport but we can\\'t block while waiting for it...)\\n        resp.send(\"ACK\")', 'def message_received(self, message):\\n        #print(\"DEBUG: msg=%s\" % message)\\n        try:\\n            # Currently we have no incoming messages from this board\\n            pass\\n        except Exception as e:\\n            print \"message_received exception: Got exception %s\" % repr(e)\\n            # Ignore indexerrors, they just mean we could not parse the command\\n            pass\\n        pass', 'def run(self):\\n        print(\"Starting motorserver\")\\n        super(motorserver, self).run()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(\\n        self,', 'def load_mongo(self, mongo_uri: Union[str, None] = None):\\n        if mongo_uri:\\n            self.mongo_uri = mongo_uri\\n            self.client = MongoClient(mongo_uri)\\n        else:\\n            self.mongo_uri = \"localhost:27017\"\\n            self.client = MongoClient()\\n\\n        self._core = self.client[\"core\"]', 'def _eq_loaded(self):\\n        if self.lc:\\n            return True\\n        else:\\n            print(\"Load eQulibrator local cache.\")\\n            return False', 'def load_thermo_from_postgres(\\n        self, postgres_uri: str = \"postgresql:///eq_compounds\"', 'def load_thermo_from_sqlite(\\n        self, sqlite_filename: str = \"compounds.sqlite\"', 'def get_eQ_compound_from_cid(\\n        self, c_id: str, pickaxe: Pickaxe = None, db_name: str = None', 'def standard_dg_formation_from_cid(\\n        self, c_id: str, pickaxe: Pickaxe = None, db_name: str = None', 'def get_eQ_reaction_from_rid(\\n        self, r_id: str, pickaxe: Pickaxe = None, db_name: str = None', 'def physiological_dg_prime_from_rid(\\n        self, r_id: str, pickaxe: Pickaxe = None, db_name: str = None', 'def standard_dg_prime_from_rid(\\n        self, r_id: str, pickaxe: Pickaxe = None, db_name: str = None', 'def dg_prime_from_rid(\\n        self,\\n        r_id: str,\\n        pickaxe: Pickaxe = None,\\n        db_name: str = None,\\n        p_h: Q_ = default_physiological_p_h,\\n        p_mg: Q_ = default_physiological_p_mg,\\n        ionic_strength: Q_ = default_physiological_ionic_strength,']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def string_to_bytes( text ):\\n    return bin(int.from_bytes(text.encode(), 'big'))\", 'def char_to_bytes(char):\\n    return bin(ord(char))', 'def decodes():\\n    pass']}, {'features': [], 'snippets': ['def __init__(\\n        self,\\n        *,\\n        sampling_strategy=\"auto\",\\n        random_state=None,\\n        shrinkage=None,', 'def _check_X_y(self, X, y):\\n        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)\\n        X, y = self._validate_data(\\n            X,\\n            y,\\n            reset=True,\\n            accept_sparse=[\"csr\", \"csc\"],\\n            dtype=None,\\n            force_all_finite=False,\\n        )\\n        return X, y, binarize_y']}, {'features': [], 'snippets': ['def __init__(self, *a, **k):\\n        super(SessionRecordingComponent, self).__init__(*a, **a)\\n        self.set_trigger_recording_on_release(not (self._record_button.is_pressed))', 'def set_trigger_recording_on_release(self, trigger_recording):\\n        self._should_trigger_recording = trigger_recording', 'def _on_record_button_pressed(self):\\n        pass', 'def _on_record_button_released(self):\\n        if self._should_trigger_recording:\\n            self._trigger_recording()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, name, region, vpc, description, inbound=None, outbound=None):\\n        \"\"\"\\n        :param name: String, name of SG\\n        :param region: String, AWS region\\n        :param vpc: String, IP of the VPC this SG belongs to\\n        :param description: String\\n        :param inbound: List of dicts, IP Permissions that should exist\\n        :param outbound: List of dicts, IP Permissions that should exist\\n        \"\"\"\\n        self.id = None\\n        self.name = name\\n        self.region = region\\n        self.vpc = vpc\\n        self.description = description\\n        self.IpPermissions = []\\n        self.IpPermissionsEgress = []\\n        self.owner = None\\n        self.changed = False\\n        try:\\n            self._get()\\n        except AWSNotFound:\\n            self._create()\\n        self._merge_rules(must.be_list(inbound), self.IpPermissions)\\n        self._merge_rules(must.be_list(outbound), self.IpPermissionsEgress, egress=True)\\n        if self.changed:\\n            self._get()', 'def _merge_rules(self, requested, active, egress=False):\\n        \"\"\"\\n        :param requested: List of dicts, IP Permissions that should exist\\n        :param active: List of dicts, IP Permissions that already exist\\n        :param egress: Bool, addressing outbound rules or not?\\n        :return: Bool\\n        \"\"\"\\n        if not isinstance(requested, list):\\n            raise ParseError(\\n                \\'SecurityGroup {0}, need a list of dicts, instead got \"{1}\"\\'.format(self.name, requested))\\n        for rule in requested:\\n            if rule not in active:\\n                self._add_rule(rule, egress)\\n        for active_rule in active:\\n            if active_rule not in requested:\\n                self._rm_rule(active_rule, egress)\\n        return True', 'def _rm_rule(self, ip_permissions, egress):\\n        \"\"\"\\n        :param ip_permissions: Dict of IP Permissions\\n        :param egress: Bool\\n        :return: Bool\\n        \"\"\"\\n        direction = \\'revoke-security-group-ingress\\'\\n        if egress:\\n            direction = \\'revoke-security-group-egress\\'\\n        command = [\\'ec2\\', direction,\\n                   \\'--region\\', self.region,\\n                   \\'--group-id\\', self.id,\\n                   \\'--ip-permissions\\', str(ip_permissions).replace(\"\\'\", \\'\"\\')\\n                   ]\\n        bin_aws(command)\\n        print(\\'Revoked: {0}\\'.format(ip_permissions))  # TODO: Log(...)\\n        self.changed = True\\n        return True', 'def _get(self):\\n        \"\"\"\\n        Get information about Security Group from AWS and update self\\n        :return: Bool\\n        \"\"\"\\n        command = [\\'ec2\\', \\'describe-security-groups\\', \\'--region\\', self.region, \\'--group-names\\', self.name]\\n        result = bin_aws(command, key=\\'SecurityGroups\\', max=1)  # will raise NotFound if empty\\n        me = result[0]\\n        self.id = me[\\'GroupId\\']\\n        self.owner = me[\\'OwnerId\\']\\n        self.IpPermissions = self._break_out(me[\\'IpPermissions\\'])\\n        self.IpPermissionsEgress = self._break_out(me[\\'IpPermissionsEgress\\'])\\n        print(\\'Got {0}\\'.format(command))  # TODO: Log(...)\\n        return True']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self):\\n        super(KanboardShell, self).__init__(\\n            description='Kanboard Command Line Client',\\n            version=app_version.VersionInfo('kanboard_cli').version_string(),\\n            command_manager=commandmanager.CommandManager('kanboard.cli'),\\n            deferred_help=True)\\n        self.client = None\\n        self.is_super_user = True\", \"def initialize_app(self, argv):\\n        client_manager = client.ClientManager(self.options)\\n        self.client = client_manager.get_client()\\n        self.is_super_user = client_manager.is_super_user()\\n\\n        self.command_manager.add_command('app version', application.ShowVersion)\\n        self.command_manager.add_command('app timezone', application.ShowTimezone)\\n        self.command_manager.add_command('project show', project.ShowProject)\\n        self.command_manager.add_command('project list', project.ListProjects)\\n        self.command_manager.add_command('task create', task.CreateTask)\\n        self.command_manager.add_command('task list', task.ListTasks)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp (self):\\n    utils.mktemp ()\\n    for filename in self.filenames:\\n      with open (os.path.join (utils.TEST_ROOT, filename), \"w\"):\\n        pass', 'def test_glob (self):\\n    import glob\\n    pattern = os.path.join (utils.TEST_ROOT, \"*\")\\n    self.assertEquals (list (fs.glob (pattern)), glob.glob (pattern))']}, {'features': [], 'snippets': ['def lbp_kernel(input, neighborhood, powers, h):\\n    i = cuda.grid(1)\\n    r = 0\\n    if i < input.shape[0] - 2 * neighborhood:\\n        i += neighborhood\\n        for j in range(i - neighborhood, i):\\n            if input[j] >= input[i]:\\n                r += powers[j - i + neighborhood]', \"def extract_1dlbp_gpu(input, neighborhood, d_powers):\\n    maxThread = 512\\n\\n    blockDim = maxThread\\n    d_input = cuda.to_device(input)\\n\\n    hist = np.zeros(2 ** (2 * neighborhood), dtype='int32')\\n    gridDim = (len(input) - 2 * neighborhood + blockDim) / blockDim\\n\\n    d_hist = cuda.to_device(hist)\\n\\n    lbp_kernel[gridDim, blockDim](d_input, neighborhood, d_powers, d_hist)\\n    d_hist.to_host()\\n    return hist\", 'def extract_1dlbp_cpu_jit(input, neighborhood, powers, res):\\n    maxThread = 512\\n    blockDim = maxThread\\n    gridDim = (len(input) - 2 * neighborhood + blockDim) / blockDim', 'def extract_1dlbp_cpu(input, neighborhood, p):\\n    \"\"\"\\n    Extract the 1d lbp pattern on CPU\\n    \"\"\"\\n    res = np.zeros(1 << (2 * neighborhood))\\n    for i in range(neighborhood, len(input) - neighborhood):\\n        left = input[i - neighborhood : i]\\n        right = input[i + 1 : i + neighborhood + 1]\\n        both = np.r_[left, right]\\n        res[np.sum(p [both >= input[i]])] += 1\\n    return res']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def canonical_url(configs, endpoint_type=PUBLIC):\\n    \\'\\'\\'\\n    Returns the correct HTTP URL to this host given the state of HTTPS\\n    configuration, hacluster and charm configuration.\\n\\n    :configs OSTemplateRenderer: A config tempating object to inspect for\\n        a complete https context.\\n    :endpoint_type str: The endpoint type to resolve.\\n\\n    :returns str: Base URL for services on the current service unit.\\n    \\'\\'\\'\\n    scheme = \\'http\\'\\n    if \\'https\\' in configs.complete_contexts():\\n        scheme = \\'https\\'\\n    address = resolve_address(endpoint_type)\\n    if is_ipv6(address):\\n        address = \"[{}]\".format(address)\\n    return \\'%s://%s\\' % (scheme, address)']}, {'features': [], 'snippets': ['def __init__(self, dsn, **defaults):\\n        \\'\\'\\' Parse a dsn to parts similar to urlparse.\\n        This is a nuts function that can serve as a good basis to parsing a custom dsn\\n\\n        :param dsn: the dsn to parse\\n        :type dsn: str\\n        :param defaults: any values you want to have defaults for if they aren\\'t in the dsn\\n        :type defaults: dict\\n        \\'\\'\\'\\n\\n        assert self.DSN_REGEXP.match(dsn), \\\\\\n            \"{} is invalid, only full dsn urls (scheme://host...) allowed\".format(dsn)\\n\\n        first_colon = dsn.find(\\':\\')\\n        scheme = dsn[0:first_colon]\\n        dsn_url = dsn[first_colon+1:]\\n        url = urlparse.urlparse(dsn_url)\\n\\n        options = {}\\n        if url.query:\\n            for k, kv in urlparse.parse_qs(url.query, True, True).iteritems():\\n                if len(kv) > 1:\\n                    options[k] = kv\\n                else:\\n                    options[k] = kv[0]\\n\\n        self.scheme = scheme\\n        self.hostname = url.hostname\\n        self.path = url.path\\n        self.params = url.params\\n        self.query = options\\n        self.fragment = url.fragment\\n        self.username = url.username\\n        self.password = url.password\\n        self.port = url.port\\n        self.query_str = url.query\\n\\n        for k, v in defaults.iteritems():\\n            self.set_default(k, v)', 'def __len__(self):\\n        return len(iter(self))', 'def __setitem__(self, field, value):\\n        setattr(self, field, value)', \"def schemes(self):\\n        '''the scheme, split by plus signs'''\\n        return self.scheme.split('+')\", 'def netloc(self):\\n        \\'\\'\\'return username:password@hostname:port\\'\\'\\'\\n        s = \\'\\'\\n        prefix = \\'\\'\\n        if self.username:\\n            s += self.username\\n            prefix = \\'@\\'\\n\\n        if self.password:\\n            s += \":{}\".format(self.password)\\n            prefix = \\'@\\'\\n\\n        s += \"{}{}\".format(prefix, self.hostloc)\\n        return s', \"def paths(self):\\n        '''the path attribute split by /'''\\n        return filter(None, self.path.split('/'))\", \"def host(self):\\n        '''the hostname, but I like host better'''\\n        return self.hostname\", \"def hostloc(self):\\n        '''return host:port'''\\n        hostloc = self.hostname\\n        if self.port:\\n            hostloc = '{}:{}'.format(hostloc, self.port)\\n\\n        return hostloc\", \"def get_url(self):\\n        '''return the dsn back into url form'''\\n        return urlparse.urlunparse((\\n            self.scheme,\\n            self.netloc,\\n            self.path,\\n            self.params,\\n            self.query_str,\\n            self.fragment,\\n        ))\"]}, {'features': [], 'snippets': ['def canonicalMachineName(machine=\\'\\'):\\n    aliases = {\\'nstxu\\': [\\'nstx\\', \\'nstxu\\', \\'nstx-u\\'],\\n               \\'diiid\\': [\\'diiid\\', \\'diii-d\\', \\'d3d\\'],\\n               \\'cmod\\': [\\'cmod\\', \\'c-mod\\']}\\n    for key, value in aliases.items():\\n        if machine.lower() in value:\\n            return key\\n    # invalid machine name\\n    raise FdpError(\\'\"{}\" is not a valid machine name\\\\n\\'.format(machine))']}, {'features': [], 'snippets': ['def _api_path(self):\\n        return \"/product/license\"', 'def _root_key(self):\\n        return \"LicenseInfo\"', 'def _root_key_m(self):\\n        return \"LicenseInfo\"', 'def _class_name(self):\\n        return \"LicenseInfo\"', 'def _create_resource_impl(self, obj, wrapped=False):\\n        Util.validate_type(wrapped, \"bool\")\\n        return LicenseInfo(self._client, obj, wrapped)', 'def offset(self, offset):\\n        Util.validate_type(offset, \"int\")\\n        return self._offset(offset)', 'def limit(self, count):\\n        Util.validate_type(count, \"int\")\\n        return self._limit(count)', 'def filter_by(self, key, value, multiple=False):\\n        Util.validate_type(key, \"str\")\\n        Util.validate_type(multiple, \"bool\")\\n        return self._filter_by(key, value, multiple)', 'def reset(self):\\n        return self._reset()', 'def get_by_id(self, id):\\n        Util.validate_type(id, \"str\")\\n        return self._get_by_id(id)', 'def find(self):\\n        return self._find()', 'def with_name_like(self, name):\\n        Util.validate_type(name, \"str\")\\n        return self._with_name_like(name)', 'def sort_by_name(self, reverse=False):\\n        Util.validate_type(reverse, \"bool\")\\n        return self._sort_by_name(reverse)', 'def __init__(self, client):\\n        super(Model_LicenseInfo, self).__init__(client)\\n        Util.validate_type(client, \"saklient.cloud.client.Client\")']}, {'features': [], 'snippets': [\"def __init__(self, manager, task_id, sub_ids=None, dry_run=False, resubmit=False):\\n\\t\\tself.__manager = manager\\n\\t\\tself.__task\\t= self.__manager.load_task(task_id)\\n\\t\\tself.__sub_ids = sub_ids\\n\\t\\tself.__dry_run = dry_run\\n\\t\\tself.__resubmit = resubmit\\n\\n\\t\\tself.__logger = logging.getLogger('JSUB')\\n\\t\\tif self.__sub_ids==None:\\n\\t\\t\\tself.__sub_ids=range(len(self.__task.data['jobvar']))\\n\\n\\t\\tself.__initialize_manager()\", \"def handle(self):\\n\\t\\trun_root = self.__backend_mgr.get_run_root(self.__task.data['backend'], self.__task.data['id'])\\n\\n\\t\\tmain_root = os.path.join(run_root, 'main')\\n\\n\\t\\tsafe_rmdir(main_root)\\n\\t\\tsafe_mkdir(main_root)\\n\\n\\t\\tself.__create_input(main_root)\\n\\t\\tself.__create_context(main_root)\\n\\t\\tself.__create_action(main_root)\\n\\t\\tself.__create_navigator(main_root)\\n\\t\\tself.__create_bootstrap(main_root)\\n\\n\\t\\tlauncher_param = self.__create_launcher(run_root)\\n\\n\\t\\tself.__submit(launcher_param)\", \"def __create_context(self, main_root):\\n\\t\\tcontext_dir = os.path.join(main_root, 'context')\\n\\t\\tsafe_mkdir(context_dir)\\n\\n\\t\\taction_default = {}\\n\\t\\tfor unit, param in self.__task.data['workflow'].items():\\n\\t\\t\\taction_default[unit] = self.__action_mgr.default_config(param['type'])\\n\\n\\t\\tnavigators = self.__config_mgr.navigator()\\n\\t\\tcontext_format = self.__navigator_mgr.context_format(navigators)\\n\\n\\t\\tself.__context_mgr.create_context_file(self.__task.data, action_default, context_format, context_dir)\", \"def __create_navigator(self, main_root):\\n\\t\\tnavigator_dir = os.path.join(main_root, 'navigator')\\n\\t\\tsafe_mkdir(navigator_dir)\\n\\n\\t\\tnavigators = self.__config_mgr.navigator()\\n\\t\\tself.__navigator_mgr.create_navigators(navigators, navigator_dir)\", \"def __create_launcher(self, run_root):\\n\\t\\tlauncher = self.__task.data['backend']['launcher']\\n\\t\\treturn self.__launcher_mgr.create_launcher(launcher, run_root)\"]}, {'features': [], 'snippets': ['def deploy_token(\\n    deploy_client: JSONRPCClient,\\n    contract_manager: ContractManager,\\n    initial_amount: typing.TokenAmount,\\n    decimals: int,\\n    token_name: str,\\n    token_symbol: str,', 'def deploy_tokens_and_fund_accounts(\\n    token_amount: int,\\n    number_of_tokens: int,\\n    deploy_service: BlockChainService,\\n    participants: typing.List[typing.Address],\\n    contract_manager: ContractManager,', 'def deploy_service_registry_and_set_urls(\\n    private_keys, web3, contract_manager, service_registry_address', 'def get_test_contract(name):\\n    contract_path = os.path.abspath(\\n        os.path.join(os.path.dirname(__file__), \"..\", \"smart_contracts\", name)\\n    )\\n    contracts = compile_files_cwd([contract_path])\\n\\n    return contract_path, contracts']}, {'features': [], 'snippets': [\"def _no_ssl_required_on_debug(app, **kwargs):\\n    if app.debug or app.testing:\\n        os.environ['AUTHLIB_INSECURE_TRANSPORT'] = '1'\"]}, {'features': [], 'snippets': ['def __init__(self, head):\\n        self.head = head\\n        self.lsize = 0\\n        while head.next:\\n            head = head.next\\n            self.lsize += 1\\n\\n        self.m1_idx = None\\n        self.m2_idx = None\\n        if self.lsize > self._largesize:\\n            self.m1_idx = self.lsize / 3   # start from 1/3\\n            self.m1 = self._getN(self.m1_idx)\\n            self.m2_idx = self.m1_idx * 2  # start from 2/3\\n            self.m2 = self._getN(self.m2_idx)']}, {'features': [], 'snippets': [\"def go(t):\\n    o = option.Option(**{'handle': t, 'type': t})\\n    o.validate()\\n    return o\", \"def setUpClass(cls):\\n        if os.path.exists(db):\\n            os.remove(db)\\n        cls.conn = sqlite3.connect(db, detect_types=sqlite3.PARSE_DECLTYPES)\\n        st0 = option.CsvStore(kid='/base/')\\n        st0.merge_file(c1)\\n        st0.validate()\\n        cls.desc = st0.desc\", \"def test_get_insert_cmd(self):\\n        print(get_insert_cmd(go('Integer'), base_col_def))\\n        print(get_insert_cmd(go('String'), base_col_def))\\n        print(get_insert_cmd(go('Point'), base_col_def))\\n        print(get_insert_cmd(go('Role'), base_col_def))\\n        print(get_insert_cmd(go('RoleIO'), base_col_def))\\n        print(get_insert_cmd(go('Log'), base_col_def))\\n        print(get_insert_cmd(go('Meta'), base_col_def))\", 'def test_write_desc(self):\\n        s = option.SqlStore()\\n        s.cursor = self.conn.cursor()\\n        s.write_desc(self.desc)\\n        print(\\'READING\\')\\n        r = s.read_tree()\\n        print(r)\\n        print(\\'print(tree\\\\n\\', print_tree(r))\\n        print(\\'WRITING AGAIN\\')\\n        s.write_tree(r)\\n        print(\"READING AGAIN\")\\n        r = s.read_tree()\\n        print(r)\\n        print(\\'print(tree2\\\\n\\', print_tree(r))', \"def test_tables(self):\\n        st0 = option.CsvStore(kid='ciao')\\n        st0.merge_file(c1)\\n        st = option.SqlStore(kid='ciao')\\n        st.desc = st0.desc\\n        k0 = set(st.desc.keys())\\n        cursor = self.conn.cursor()\\n        st.write_table(cursor, 'conf1')\\n        self.conn.commit()\\n        cursor.execute('select handle from conf1')\\n        r = cursor.fetchall()\\n        k1 = set([eval(k[0]) for k in r])\\n        self.assertEqual(k0, k1)\\n\\n        st2 = option.SqlStore(kid='ciao')\\n        st2.read_table(cursor, 'conf1')\\n        self.assertEqual(st.desc, st2.desc)\"]}, {'features': [], 'snippets': ['def __init__(self):\\n\\n        player.__init__(self)\\n        self.experiences = deque()', 'def act(self, state):\\n\\n        return self.calculate(state)', \"def calculate(self, state):\\n\\n        size = len( self.experiences )\\n\\n        if size < self.NUM_FRAMES:\\n            return self.create_random_action()\\n\\n        states = np.zeros( (self.NUM_FRAMES , self.obsv_shape[0], self.obsv_shape[1] ) )\\n\\n        for i , j in enumerate( range( size - self.NUM_FRAMES , size  ) ):\\n            states[i] = self.experiences[j][1]\\n\\n        states = np.expand_dims( states, 0 )\\n        output = np.squeeze( self.brain.run('Output', [['Observation', states]]) )\\n        action = np.random.choice( np.arange(len(output)), p=output )\\n\\n        return self.create_action(action)\", \"def operations(self):\\n\\n        # Action Placeholders\\n\\n        self.brain.addInput( shape = [ None , self.num_actions ] , name = 'Actions' )\\n        self.brain.addInput( shape = [ None                    ] , name = 'Target'  )\\n\\n\\n        # Operations\\n\\n        self.brain.addOperation( function = tb.ops.pgcost,\\n                                 input    = [ 'Output', 'Actions', 'Target' ],\\n                                 name     = 'Cost' )\\n\\n        # Optimizer\\n\\n        self.brain.addOperation( function      = tb.optims.adam,\\n                                 input         = 'Cost',\\n                                 learning_rate = self.LEARNING_RATE,\\n                                 name          = 'Optimizer' )\\n\\n        # TensorBoard\\n\\n        self.brain.addSummaryScalar( input = 'Cost' )\\n        self.brain.addSummaryHistogram( input = 'Target' )\\n        self.brain.addWriter( name = 'Writer' , dir = './' )\\n        self.brain.addSummary( name = 'Summary' )\\n        self.brain.initialize()\"]}, {'features': [], 'snippets': [\"def __init__(self, \\n                row_dict_I,\\n                ):\\n        self.analysis_id=row_dict_I['analysis_id'];\\n        self.experiment_id=row_dict_I['experiment_id'];\\n        self.sample_name_abbreviation=row_dict_I['sample_name_abbreviation'];\\n        self.sample_name=row_dict_I['sample_name'];\\n        self.time_point=row_dict_I['time_point'];\\n        self.analysis_type=row_dict_I['analysis_type'];\\n        self.used_=row_dict_I['used_'];\\n        self.comment_=row_dict_I['comment_'];\", \"def __repr__dict__(self):\\n        return {'id':self.id,\\n                'analysis_id':self.analysis_id,\\n            'experiment_id':self.experiment_id,\\n            'sample_name_abbreviation':self.sample_name_abbreviation,\\n            'sample_name':self.sample_name,\\n            'time_point':self.time_point,\\n            'analysis_type':self.analysis_type,\\n            'used_':self.used_,\\n            'comment_':self.comment_}\"]}, {'features': [], 'snippets': ['def normalized_cross(a, b):\\n    \"\"\"\\n    Returns the normalized cross product between vectors.\\n    Uses numpy.cross().', 'def general_plane_intersection(n_a, da, n_b, db):\\n    \"\"\"\\n    Returns a point and direction vector for the line of intersection\\n    of two planes in space, or None if planes are parallel.', 'def small_circle_intersection(axis_a, angle_a, axis_b, angle_b):\\n    \"\"\"\\n    Finds the intersection between two small-circles returning zero, one or two \\n    solutions as tuple.', 'def build_rotation_matrix(azim, plng, rake):\\n    \"\"\"\\n    Returns the rotation matrix that rotates the North vector to the line given \\n    by Azimuth and Plunge and East and Up vectors are rotate clock-wise by Rake\\n    around the rotated North vector.', 'def adjust_lines_to_planes(lines, planes):\\n    \"\"\"\\n    Project each given line to it\\'s respective plane. Returns the projected\\n    lines as a new LineSet and the angle (in radians) between each line and\\n    plane prior to projection.']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def graphviz_setup(gviz_path):\\n    os.environ[\\'PATH\\'] = gviz_path + \";\" + os.environ[\\'PATH\\']', 'def add_children(graph, parent_id, d, level=0):\\n\\n    blue = \"#6b6bd1\"\\n    white = \"#fdfefd\"\\n    green = \"#33a333\"\\n    colours = [blue, white, green] * 3\\n\\n    for class_, children in d.items():\\n        colour = colours[level]\\n        child_label = class_\\n        child_id = parent_id + \"_\" + class_\\n        add_child(graph, child_id, child_label, parent_id, colour)\\n        add_children(graph, child_id, children, level+1)', 'def main(gviz_path, layer_only=False):\\n\\n    graphviz_setup(gviz_path)\\n    graph = pydot.Dot(graph_type=\\'digraph\\', rankdir=\"TB\")\\n\\n    layer_children = {\\n            \\'CLASS\\': {\\n                \\'LABEL\\': {\\'STYLE\\': {}},\\n                \\'CONNECTIONOPTIONS\\': {},\\n                \\'LEADER\\': {\\'STYLE\\': {}},\\n                \\'STYLE\\': {},\\n                \\'VALIDATION\\': {}\\n            },\\n            \\'CLUSTER\\': {},\\n            \\'COMPOSITE\\': {},\\n            \\'FEATURE\\': {\\'POINTS\\': {}},\\n            \\'GRID\\': {},\\n            \\'JOIN\\': {},\\n            \\'METADATA\\': {},\\n            \\'PROJECTION\\': {},\\n            \\'SCALETOKEN\\': {\\'VALUES\\': {}},\\n            \\'VALIDATION\\': {}\\n     }\\n\\n    # pprint.pprint(layer_children)\\n\\n    classes = {\\n        \"MAP\": {\\n            \"LAYER\": layer_children,\\n            \\'LEGEND\\': {\\'LABEL\\': {}},\\n            \\'PROJECTION\\': {},\\n            \\'QUERYMAP\\': {},\\n            \\'REFERENCE\\': {},\\n            \\'SCALEBAR\\': {\\'LABEL\\': {}},\\n            \\'SYMBOL\\': {},\\n            \\'WEB\\': {\\'METADATA\\': {}, \\'VALIDATION\\': {}}\\n        }\\n     }\\n\\n    if layer_only:\\n        root = \"LAYER\"\\n        classes = classes[\"MAP\"]\\n        fn = \"layer_classes\"\\n    else:\\n        fn = \"map_classes\"\\n        root,  = classes.keys()\\n\\n    node = pydot.Node(root, style=\"filled\", fillcolor=\"#33a333\", label=root, fontname=FONT, shape=\"polygon\")\\n    graph.add_node(node)\\n    add_children(graph, root, classes[root])\\n    save_file(graph, fn)']}, {'features': [], 'snippets': ['def playlist_handler(playlist_name, playlist_description, playlist_tracks):\\n    # skip empty and no-name playlists\\n    if not playlist_name: return\\n    if len(playlist_tracks) == 0: return\\n\\n    # setup output files\\n    playlist_name = playlist_name.replace(\\'/\\', \\'\\')\\n    open_log(os.path.join(output_dir,playlist_name+u\\'.log\\'))\\n    outfile = codecs.open(os.path.join(output_dir,playlist_name+u\\'.csv\\'),\\n        encoding=\\'utf-8\\',mode=\\'w\\')\\n\\n    # keep track of stats\\n    stats = create_stats()\\n    export_skipped = 0\\n    # keep track of songids incase we need to skip duplicates\\n    song_ids = []\\n\\n    log(\\'\\')\\n    log(\\'============================================================\\')\\n    log(u\\'Exporting \\'+ unicode(len(playlist_tracks)) +u\\' tracks from \\'\\n        +playlist_name)\\n    log(\\'============================================================\\')\\n\\n    # add the playlist description as a \"comment\"\\n    if playlist_description:\\n        outfile.write(tsep)\\n        outfile.write(playlist_description)\\n        outfile.write(os.linesep)\\n\\n    for tnum, pl_track in enumerate(playlist_tracks):\\n        track = pl_track.get(\\'track\\')\\n\\n        # we need to look up these track in the library\\n        if not track:\\n            library_track = [\\n                item for item in library if item.get(\\'id\\')\\n                in pl_track.get(\\'trackId\\')]\\n            if len(library_track) == 0:\\n                log(u\\'!! \\'+str(tnum+1)+repr(pl_track))\\n                export_skipped += 1\\n                continue\\n            track = library_track[0]\\n\\n        result_details = create_result_details(track)\\n\\n        if not allow_duplicates and result_details[\\'songid\\'] in song_ids:\\n            log(\\'{D} \\'+str(tnum+1)+\\'. \\'+create_details_string(result_details,True))\\n            export_skipped += 1\\n            continue\\n\\n        # update the stats\\n        update_stats(track,stats)\\n\\n        # export the track\\n        song_ids.append(result_details[\\'songid\\'])\\n        outfile.write(create_details_string(result_details))\\n        outfile.write(os.linesep)\\n\\n    # calculate the stats\\n    stats_results = calculate_stats_results(stats,len(playlist_tracks))\\n\\n    # output the stats to the log\\n    log(\\'\\')\\n    log_stats(stats_results)\\n    log(u\\'export skipped: \\'+unicode(export_skipped))\\n\\n    # close the files\\n    close_log()\\n    outfile.close()']}, {'features': [], 'snippets': ['def __init__(\\n        self,\\n        plotly_name=\"showexponent\",\\n        parent_name=\"scatterpolar.marker.colorbar\",\\n        **kwargs']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, query, explanation):\\n        if isinstance(query, list):\\n            query = \" \".join(query)\\n        message = f\"\\'{query}\\': {explanation}\"\\n        super().__init__(message)', 'def __init__(self, what, expected, detail=None):\\n        message = f\"\\'{what}\\' is not {expected}\"\\n        if detail:\\n            message = f\"{message}: {detail}\"\\n        super().__init__(message)', 'def clause(self):\\n        \"\"\"Generate an SQLite expression implementing the query.\\n\\n        Return (clause, subvals) where clause is a valid sqlite\\n        WHERE clause implementing the query and subvals is a list of\\n        items to be substituted for ?s in the clause.\\n        \"\"\"\\n        return None, ()', 'def __repr__(self):\\n        return f\"{self.__class__.__name__}()\"', 'def __hash__(self):\\n        return 0', 'def __init__(self, field, pattern, fast=True):\\n        self.field = field\\n        self.pattern = pattern\\n        self.fast = fast', 'def clause(self):\\n        if self.fast:\\n            return self.col_clause()\\n        else:\\n            # Matching a flexattr. This is a slow query.\\n            return None, ()', 'def value_match(cls, pattern, value):\\n        \"\"\"Determine whether the value matches the pattern. Both\\n        arguments are strings.\\n        \"\"\"\\n        raise NotImplementedError()', 'def __repr__(self):\\n        return (\"{0.__class__.__name__}({0.field!r}, {0.pattern!r}, \"\\n                \"{0.fast})\".format(self))', 'def __hash__(self):\\n        return hash((self.field, hash(self.pattern)))', 'def col_clause(self):\\n        return self.field + \" = ?\", [self.pattern]', 'def value_match(cls, pattern, value):\\n        return pattern == value', 'def __init__(self, field, fast=True):\\n        super().__init__(field, None, fast)', 'def match(self, item):\\n        return item.get(self.field) is None', 'def value_match(cls, pattern, value):\\n        \"\"\"Determine whether the value matches the pattern. The value\\n        may have any type.\\n        \"\"\"\\n        return cls.string_match(pattern, util.as_string(value))', 'def string_match(cls, pattern, value):\\n        \"\"\"Determine whether the value matches the pattern. Both\\n        arguments are strings. Subclasses implement this method.\\n        \"\"\"\\n        raise NotImplementedError()', 'def col_clause(self):\\n        search = (self.pattern\\n                  .replace(\\'\\\\\\\\\\', \\'\\\\\\\\\\\\\\\\\\')\\n                  .replace(\\'%\\', \\'\\\\\\\\%\\')\\n                  .replace(\\'_\\', \\'\\\\\\\\_\\'))\\n        clause = self.field + \" like ? escape \\'\\\\\\\\\\'\"\\n        subvals = [search]\\n        return clause, subvals', 'def string_match(cls, pattern, value):\\n        return pattern.lower() == value.lower()', 'def col_clause(self):\\n        pattern = (self.pattern\\n                   .replace(\\'\\\\\\\\\\', \\'\\\\\\\\\\\\\\\\\\')\\n                   .replace(\\'%\\', \\'\\\\\\\\%\\')\\n                   .replace(\\'_\\', \\'\\\\\\\\_\\'))\\n        search = \\'%\\' + pattern + \\'%\\'\\n        clause = self.field + \" like ? escape \\'\\\\\\\\\\'\"\\n        subvals = [search]\\n        return clause, subvals', 'def string_match(cls, pattern, value):\\n        return pattern.lower() in value.lower()', 'def __init__(self, field, pattern, fast=True):\\n        super().__init__(field, pattern, fast)\\n        pattern = self._normalize(pattern)\\n        try:\\n            self.pattern = re.compile(self.pattern)\\n        except re.error as exc:\\n            # Invalid regular expression.\\n            raise InvalidQueryArgumentValueError(pattern,\\n                                                 \"a regular expression\",\\n                                                 format(exc))', 'def _normalize(s):\\n        \"\"\"Normalize a Unicode string\\'s representation (used on both\\n        patterns and matched values).\\n        \"\"\"\\n        return unicodedata.normalize(\\'NFC\\', s)', 'def string_match(cls, pattern, value):\\n        return pattern.search(cls._normalize(value)) is not None', 'def __init__(self, field, pattern, fast=True):\\n        super().__init__(field, pattern, fast)\\n        if isinstance(pattern, str):\\n            self.pattern = util.str2bool(pattern)\\n        self.pattern = int(self.pattern)', \"def __init__(self, field, pattern):\\n        super().__init__(field, pattern)\\n\\n        # Use a buffer/memoryview representation of the pattern for SQLite\\n        # matching. This instructs SQLite to treat the blob as binary\\n        # rather than encoded Unicode.\\n        if isinstance(self.pattern, (str, bytes)):\\n            if isinstance(self.pattern, str):\\n                self.pattern = self.pattern.encode('utf-8')\\n            self.buf_pattern = memoryview(self.pattern)\\n        elif isinstance(self.pattern, memoryview):\\n            self.buf_pattern = self.pattern\\n            self.pattern = bytes(self.pattern)\", 'def _convert(self, s):\\n        \"\"\"Convert a string to a numeric type (float or int).\\n\\n        Return None if `s` is empty.\\n        Raise an InvalidQueryError if the string cannot be converted.\\n        \"\"\"\\n        # This is really just a bit of fun premature optimization.\\n        if not s:\\n            return None\\n        try:\\n            return int(s)\\n        except ValueError:\\n            try:\\n                return float(s)\\n            except ValueError:\\n                raise InvalidQueryArgumentValueError(s, \"an int or a float\")', 'def match(self, item):\\n        if self.field not in item:\\n            return False\\n        value = item[self.field]\\n        if isinstance(value, str):\\n            value = self._convert(value)\\n\\n        if self.point is not None:\\n            return value == self.point\\n        else:\\n            if self.rangemin is not None and value < self.rangemin:\\n                return False\\n            if self.rangemax is not None and value > self.rangemax:\\n                return False\\n            return True', 'def __init__(self, subqueries=()):\\n        self.subqueries = subqueries', 'def __len__(self):\\n        return len(self.subqueries)', 'def __iter__(self):\\n        return iter(self.subqueries)', 'def clause_with_joiner(self, joiner):\\n        \"\"\"Return a clause created by joining together the clauses of\\n        all subqueries with the string joiner (padded by spaces).\\n        \"\"\"\\n        clause_parts = []\\n        subvals = []\\n        for subq in self.subqueries:\\n            subq_clause, subq_subvals = subq.clause()\\n            if not subq_clause:\\n                # Fall back to slow query.\\n                return None, ()\\n            clause_parts.append(\\'(\\' + subq_clause + \\')\\')\\n            subvals += subq_subvals\\n        clause = (\\' \\' + joiner + \\' \\').join(clause_parts)\\n        return clause, subvals', 'def __eq__(self, other):\\n        return super().__eq__(other) and \\\\\\n            self.subqueries == other.subqueries', 'def __init__(self, pattern, fields, cls):\\n        self.pattern = pattern\\n        self.fields = fields\\n        self.query_class = cls\\n\\n        subqueries = []\\n        for field in self.fields:\\n            subqueries.append(cls(field, pattern, True))\\n        super().__init__(subqueries)', 'def match(self, item):\\n        for subq in self.subqueries:\\n            if subq.match(item):\\n                return True\\n        return False', 'def __eq__(self, other):\\n        return super().__eq__(other) and \\\\\\n            self.query_class == other.query_class', 'def __setitem__(self, key, value):\\n        self.subqueries[key] = value', \"def clause(self):\\n        return self.clause_with_joiner('and')\", \"def clause(self):\\n        return self.clause_with_joiner('or')\", 'def __init__(self, subquery):\\n        self.subquery = subquery', 'def match(self, item):\\n        return not self.subquery.match(item)', 'def __eq__(self, other):\\n        return super().__eq__(other) and \\\\\\n            self.subquery == other.subquery', \"def clause(self):\\n        return '1', ()\", \"def clause(self):\\n        return '0', ()\", 'def _to_epoch_time(date):\\n    \"\"\"Convert a `datetime` object to an integer number of seconds since\\n    the (local) Unix epoch.\\n    \"\"\"\\n    if hasattr(date, \\'timestamp\\'):\\n        # The `timestamp` method exists on Python 3.3+.\\n        return int(date.timestamp())\\n    else:\\n        epoch = datetime.fromtimestamp(0)\\n        delta = date - epoch\\n        return int(delta.total_seconds())', 'def __init__(self, date, precision):\\n        \"\"\"Create a period with the given date (a `datetime` object) and\\n        precision (a string, one of \"year\", \"month\", \"day\", \"hour\", \"minute\",\\n        or \"second\").\\n        \"\"\"\\n        if precision not in Period.precisions:\\n            raise ValueError(f\\'Invalid precision {precision}\\')\\n        self.date = date\\n        self.precision = precision', 'def parse(cls, string):\\n        \"\"\"Parse a date and return a `Period` object or `None` if the\\n        string is empty, or raise an InvalidQueryArgumentValueError if\\n        the string cannot be parsed to a date.\\n\\n        The date may be absolute or relative. Absolute dates look like\\n        `YYYY`, or `YYYY-MM-DD`, or `YYYY-MM-DD HH:MM:SS`, etc. Relative\\n        dates have three parts:\\n\\n        - Optionally, a ``+`` or ``-`` sign indicating the future or the\\n          past. The default is the future.\\n        - A number: how much to add or subtract.\\n        - A letter indicating the unit: days, weeks, months or years\\n          (``d``, ``w``, ``m`` or ``y``). A \"month\" is exactly 30 days\\n          and a \"year\" is exactly 365 days.\\n        \"\"\"\\n\\n        def find_date_and_format(string):\\n            for ord, format in enumerate(cls.date_formats):\\n                for format_option in format:\\n                    try:\\n                        date = datetime.strptime(string, format_option)\\n                        return date, ord\\n                    except ValueError:\\n                        # Parsing failed.\\n                        pass\\n            return (None, None)\\n\\n        if not string:\\n            return None\\n\\n        # Check for a relative date.\\n        match_dq = re.match(cls.relative_re, string)\\n        if match_dq:\\n            sign = match_dq.group(\\'sign\\')\\n            quantity = match_dq.group(\\'quantity\\')\\n            timespan = match_dq.group(\\'timespan\\')\\n\\n            # Add or subtract the given amount of time from the current\\n            # date.\\n            multiplier = -1 if sign == \\'-\\' else 1\\n            days = cls.relative_units[timespan]\\n            date = datetime.now() + \\\\\\n                timedelta(days=int(quantity) * days) * multiplier\\n            return cls(date, cls.precisions[5])\\n\\n        # Check for an absolute date.\\n        date, ordinal = find_date_and_format(string)\\n        if date is None:\\n            raise InvalidQueryArgumentValueError(string,\\n                                                 \\'a valid date/time string\\')\\n        precision = cls.precisions[ordinal]\\n        return cls(date, precision)', 'def __init__(self, start, end):\\n        if start is not None and end is not None and not start < end:\\n            raise ValueError(\"start date {} is not before end date {}\"\\n                             .format(start, end))\\n        self.start = start\\n        self.end = end', 'def from_periods(cls, start, end):\\n        \"\"\"Create an interval with two Periods as the endpoints.\\n        \"\"\"\\n        end_date = end.open_right_endpoint() if end is not None else None\\n        start_date = start.date if start is not None else None\\n        return cls(start_date, end_date)', \"def __str__(self):\\n        return f'[{self.start}, {self.end})'\", 'def __init__(self, field, pattern, fast=True):\\n        super().__init__(field, pattern, fast)\\n        start, end = _parse_periods(pattern)\\n        self.interval = DateInterval.from_periods(start, end)', 'def col_clause(self):\\n        clause_parts = []\\n        subvals = []\\n\\n        if self.interval.start:\\n            clause_parts.append(self._clause_tmpl.format(self.field, \">=\"))\\n            subvals.append(_to_epoch_time(self.interval.start))\\n\\n        if self.interval.end:\\n            clause_parts.append(self._clause_tmpl.format(self.field, \"<\"))\\n            subvals.append(_to_epoch_time(self.interval.end))\\n\\n        if clause_parts:\\n            # One- or two-sided interval.\\n            clause = \\' AND \\'.join(clause_parts)\\n        else:\\n            # Match any date.\\n            clause = \\'1\\'\\n        return clause, subvals', 'def _convert(self, s):\\n        \"\"\"Convert a M:SS or numeric string to a float.\\n\\n        Return None if `s` is empty.\\n        Raise an InvalidQueryError if the string cannot be converted.\\n        \"\"\"\\n        if not s:\\n            return None\\n        try:\\n            return util.raw_seconds_short(s)\\n        except ValueError:\\n            try:\\n                return float(s)\\n            except ValueError:\\n                raise InvalidQueryArgumentValueError(\\n                    s,\\n                    \"a M:SS string or a float\")', 'def order_clause(self):\\n        \"\"\"Generates a SQL fragment to be used in a ORDER BY clause, or\\n        None if no fragment is used (i.e., this is a slow sort).\\n        \"\"\"\\n        return None', 'def is_slow(self):\\n        \"\"\"Indicate whether this query is *slow*, meaning that it cannot\\n        be executed in SQL and must be executed in Python.\\n        \"\"\"\\n        return False', 'def __eq__(self, other):\\n        return type(self) == type(other)', 'def __init__(self, sorts=None):\\n        self.sorts = sorts or []', 'def _sql_sorts(self):\\n        \"\"\"Return the list of sub-sorts for which we can be (at least\\n        partially) fast.\\n\\n        A contiguous suffix of fast (SQL-capable) sub-sorts are\\n        executable in SQL. The remaining, even if they are fast\\n        independently, must be executed slowly.\\n        \"\"\"\\n        sql_sorts = []\\n        for sort in reversed(self.sorts):\\n            if not sort.order_clause() is None:\\n                sql_sorts.append(sort)\\n            else:\\n                break\\n        sql_sorts.reverse()\\n        return sql_sorts', 'def is_slow(self):\\n        for sort in self.sorts:\\n            if sort.is_slow():\\n                return True\\n        return False', \"def __repr__(self):\\n        return f'MultipleSort({self.sorts!r})'\", 'def __eq__(self, other):\\n        return super().__eq__(other) and \\\\\\n            self.sorts == other.sorts', 'def __init__(self, field, ascending=True, case_insensitive=True):\\n        self.field = field\\n        self.ascending = ascending\\n        self.case_insensitive = case_insensitive', \"def key(item):\\n            field_val = item.get(self.field, '')\\n            if self.case_insensitive and isinstance(field_val, str):\\n                field_val = field_val.lower()\\n            return field_val\", \"def __repr__(self):\\n        return '<{}: {}{}>'.format(\\n            type(self).__name__,\\n            self.field,\\n            '+' if self.ascending else '-',\\n        )\", 'def __eq__(self, other):\\n        return super().__eq__(other) and \\\\\\n            self.field == other.field and \\\\\\n            self.ascending == other.ascending', 'def order_clause(self):\\n        order = \"ASC\" if self.ascending else \"DESC\"\\n        if self.case_insensitive:\\n            field = \\'(CASE \\' \\\\\\n                    \\'WHEN TYPEOF({0})=\"text\" THEN LOWER({0}) \\' \\\\\\n                    \\'WHEN TYPEOF({0})=\"blob\" THEN LOWER({0}) \\' \\\\\\n                    \\'ELSE {0} END)\\'.format(self.field)\\n        else:\\n            field = self.field\\n        return f\"{field} {order}\"', 'def is_slow(self):\\n        return True', 'def sort(self, items):\\n        return items', 'def __bool__(self):\\n        return False']}, {'features': [], 'snippets': ['def loginAndReturnRedditSession():\\n    config = ConfigParser()\\n    config.read(\"../reddit-password-credentials.cfg\")\\n    user = config.get(\"Reddit\", \"user\")\\n    password = config.get(\"Reddit\", \"password\")\\n    # TODO:  password auth is going away, and we will soon need to do oauth.\\n    redditSession = praw.Reddit(user_agent=\\'Test Script by /u/foobarbazblarg\\')\\n    redditSession.login(user, password, disable_warning=True)\\n    # submissions = redditSession.get_subreddit(\\'pornfree\\').get_hot(limit=5)\\n    # print [str(x) for x in submissions]\\n    return redditSession', 'def getSubmissionsForRedditSession(redditSession):\\n    # submissions = [redditSession.get_submission(submission_id=submissionId) for submissionId in signupPageSubmissionIds]\\n    submissions = [redditSession.submission(id=submissionId) for submissionId in signupPageSubmissionIds]\\n    for submission in submissions:\\n        submission.comments.replace_more(limit=None)\\n        # submission.replace_more_comments(limit=None, threshold=0)\\n    return submissions', 'def retireCommentHash(commentHash):\\n    with open(\"retiredcommenthashes.txt\", \"a\") as commentHashFile:\\n        commentHashFile.write(commentHash + \\'\\\\n\\')', 'def moderatesignups():\\n    global commentHashesAndComments\\n    commentHashesAndComments = {}\\n    stringio = StringIO()\\n    stringio.write(\\'<html>\\\\n<head>\\\\n</head>\\\\n\\\\n\\')\\n\\n    # redditSession = loginAndReturnRedditSession()\\n    redditSession = loginOAuthAndReturnRedditSession()\\n    submissions = getSubmissionsForRedditSession(redditSession)\\n    flat_comments = getCommentsForSubmissions(submissions)\\n    retiredHashes = retiredCommentHashes()\\n    i = 1\\n    stringio.write(\\'<iframe name=\"invisibleiframe\" style=\"display:none;\"></iframe>\\\\n\\')\\n    stringio.write(\"<h3>\")\\n    stringio.write(os.getcwd())\\n    stringio.write(\"<br>\\\\n\")\\n    for submission in submissions:\\n        stringio.write(submission.title)\\n        stringio.write(\"<br>\\\\n\")\\n    stringio.write(\"</h3>\\\\n\\\\n\")\\n    stringio.write(\\'<form action=\"copydisplayduringsignuptoclipboard.html\" method=\"post\" target=\"invisibleiframe\">\\')\\n    stringio.write(\\'<input type=\"submit\" value=\"Copy display-during-signup.py stdout to clipboard\">\\')\\n    stringio.write(\\'</form>\\')\\n    for comment in flat_comments:\\n        # print comment.is_root\\n        # print comment.score\\n        i += 1\\n        commentHash = sha1()\\n        commentHash.update(comment.fullname)\\n        commentHash.update(comment.body.encode(\\'utf-8\\'))\\n        commentHash = commentHash.hexdigest()\\n        if commentHash not in retiredHashes:\\n            commentHashesAndComments[commentHash] = comment\\n            authorName = str(comment.author)  # can be None if author was deleted.  So check for that and skip if it\\'s None.\\n            stringio.write(\"<hr>\\\\n\")\\n            stringio.write(\\'<font color=\"blue\"><b>\\')\\n            stringio.write(authorName)  # can be None if author was deleted.  So check for that and skip if it\\'s None.\\n            stringio.write(\\'</b></font><br>\\')\\n            if ParticipantCollection().hasParticipantNamed(authorName):\\n                stringio.write(\\' <small><font color=\"green\">(member)</font></small>\\')\\n                # if ParticipantCollection().participantNamed(authorName).isStillIn:\\n                #    stringio.write(\\' <small><font color=\"green\">(in)</font></small>\\')\\n                # else:\\n                #    stringio.write(\\' <small><font color=\"red\">(out)</font></small>\\')\\n            else:\\n                stringio.write(\\' <small><font color=\"red\">(not a member)</font></small>\\')\\n            stringio.write(\\'<form action=\"takeaction.html\" method=\"post\" target=\"invisibleiframe\">\\')\\n            stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Signup\" style=\"color:white;background-color:green\">\\')\\n            # stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Signup and checkin\">\\')\\n            # stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Relapse\">\\')\\n            # stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Reinstate\">\\')\\n            stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Skip comment\">\\')\\n            stringio.write(\\'<input type=\"submit\" name=\"actiontotake\" value=\"Skip comment and don\\\\\\'t upvote\">\\')\\n            stringio.write(\\'<input type=\"hidden\" name=\"username\" value=\"\\' + b64encode(authorName) + \\'\">\\')\\n            stringio.write(\\'<input type=\"hidden\" name=\"commenthash\" value=\"\\' + commentHash + \\'\">\\')\\n            # stringio.write(\\'<input type=\"hidden\" name=\"commentpermalink\" value=\"\\' + comment.permalink + \\'\">\\')\\n            stringio.write(\\'</form>\\')\\n\\n            stringio.write(bleach.clean(markdown.markdown(comment.body.encode(\\'utf-8\\')), tags=[\\'p\\']))\\n            stringio.write(\"\\\\n<br><br>\\\\n\\\\n\")\\n\\n    stringio.write(\\'</html>\\')\\n    pageString = stringio.getvalue()\\n    stringio.close()\\n    return Response(pageString, mimetype=\\'text/html\\')', 'def takeaction():\\n    username = b64decode(request.form[\"username\"])\\n    commentHash = str(request.form[\"commenthash\"])\\n    # commentPermalink = request.form[\"commentpermalink\"]\\n    actionToTake = request.form[\"actiontotake\"]\\n    # print commentHashesAndComments\\n    comment = commentHashesAndComments[commentHash]\\n    # print \"comment:  \" + str(comment)\\n    if actionToTake == \\'Signup\\':\\n        print \"signup - \" + username\\n        subprocess.call([\\'./signup.py\\', username])\\n        comment.upvote()\\n        retireCommentHash(commentHash)\\n    # if actionToTake == \\'Signup and checkin\\':\\n    #     print \"signup and checkin - \" + username\\n    #     subprocess.call([\\'./signup-and-checkin.sh\\', username])\\n    #     comment.upvote()\\n    #     retireCommentHash(commentHash)\\n    # elif actionToTake == \\'Relapse\\':\\n    #     print \"relapse - \" + username\\n    #     subprocess.call([\\'./relapse.py\\', username])\\n    #     comment.upvote()\\n    #     retireCommentHash(commentHash)\\n    # elif actionToTake == \\'Reinstate\\':\\n    #     print \"reinstate - \" + username\\n    #     subprocess.call([\\'./reinstate.py\\', username])\\n    #     comment.upvote()\\n    #     retireCommentHash(commentHash)\\n    elif actionToTake == \\'Skip comment\\':\\n        print \"Skip comment - \" + username\\n        comment.upvote()\\n        retireCommentHash(commentHash)\\n    elif actionToTake == \"Skip comment and don\\'t upvote\":\\n        print \"Skip comment and don\\'t upvote - \" + username\\n        retireCommentHash(commentHash)\\n    return Response(\"hello\", mimetype=\\'text/html\\')', 'def copydisplayduringsignuptoclipboard():\\n    print \"TODO: Copy display to clipboard\"\\n    subprocess.call([\\'./display-during-signup.py\\'])\\n    return Response(\"hello\", mimetype=\\'text/html\\')']}, {'features': [], 'snippets': ['def test_is_informational(self):\\n        self.assertFalse(status.is_informational(99))\\n        self.assertFalse(status.is_informational(200))\\n\\n        for i in range(100, 199):\\n            self.assertTrue(status.is_informational(i))', 'def test_is_redirect(self):\\n        self.assertFalse(status.is_redirect(299))\\n        self.assertFalse(status.is_redirect(400))\\n\\n        for i in range(300, 399):\\n            self.assertTrue(status.is_redirect(i))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def format_time(seconds):\\n    v = seconds\\n\\n    if v * 1000 * 1000 * 1000 < 1000:\\n        scale = u'ns'\\n        v = int(round(v*1000*1000*1000))\\n    elif v * 1000 * 1000 < 1000:\\n        scale = u'μs'\\n        v = int(round(v*1000*1000))\\n    elif v * 1000 < 1000:\\n        scale = u'ms'\\n        v = round(v*1000, 4)\\n    else:\\n        scale = u'sec'\\n        v = int(v)\\n\\n    return u'{} {}'.format(v, scale)\", 'def runit():\\n    pack_command.pack_command(\"ZADD\", \"foo\", 1369198341, 10000)']}, {'features': [], 'snippets': ['def inject_profiler():\\n        return dict(profiler_includes=templatetags.profiler_includes())']}, {'features': [], 'snippets': ['def runtests(*test_args):\\n    if not settings.configured:\\n        settings.configure(**DEFAULT_SETTINGS)\\n\\n    django.setup()\\n\\n    parent = os.path.dirname(os.path.abspath(__file__))\\n    sys.path.insert(0, parent)\\n\\n    try:\\n        from django.test.runner import DiscoverRunner\\n        runner_class = DiscoverRunner\\n        test_args = [\"pinax.pinax_hello.tests\"]\\n    except ImportError:\\n        from django.test.simple import DjangoTestSuiteRunner\\n        runner_class = DjangoTestSuiteRunner\\n        test_args = [\"tests\"]\\n\\n    failures = runner_class(verbosity=1, interactive=True, failfast=False).run_tests(test_args)\\n    sys.exit(failures)']}, {'features': [], 'snippets': ['def preprocess_image(screen_image):\\n\\n    # crop the top and bottom\\n    screen_image = screen_image[35:195]\\n\\n    # down sample by a factor of 2\\n    screen_image = screen_image[::2, ::2]\\n\\n    # convert to grey scale\\n    grey_image = np.zeros(screen_image.shape[0:2])\\n    for i in range(len(screen_image)):\\n        for j in range(len(screen_image[i])):\\n            grey_image[i][j] = np.mean(screen_image[i][j])\\n\\n    return np.array([grey_image.astype(np.float)])', 'def __init__(self):\\n\\n        #### Construct the model ####\\n        observation = cntk.ops.input_variable(STATE_DIMS, np.float32, name=\"s\")\\n        q_target = cntk.ops.input_variable(NUM_ACTIONS, np.float32, name=\"q\")\\n\\n        # Define the structure of the neural network\\n        self.model = self.create_convolutional_neural_network(observation, NUM_ACTIONS)\\n\\n        #### Define the trainer ####\\n        self.learning_rate = cntk.learner.training_parameter_schedule(0.0001, cntk.UnitType.sample)\\n        self.momentum = cntk.learner.momentum_as_time_constant_schedule(0.99)\\n\\n        self.loss =  cntk.ops.reduce_mean(cntk.ops.square(self.model - q_target), axis=0)\\n        mean_error = cntk.ops.reduce_mean(cntk.ops.square(self.model - q_target), axis=0)\\n\\n        learner = cntk.adam_sgd(self.model.parameters, self.learning_rate, momentum=self.momentum)\\n        self.trainer = cntk.Trainer(self.model, self.loss, mean_error, learner)', 'def predict(self, s):\\n        return self.model.eval([s])', 'def create_multi_layer_neural_network(input_vars, out_dims, num_hidden_layers):\\n\\n        num_hidden_neurons = 128\\n\\n        hidden_layer = lambda: Dense(num_hidden_neurons, activation=cntk.ops.relu)\\n        output_layer = Dense(out_dims, activation=None)\\n\\n        model = Sequential([LayerStack(num_hidden_layers, hidden_layer),\\n                            output_layer])(input_vars)\\n        return model', 'def create_convolutional_neural_network(input_vars, out_dims):\\n\\n        convolutional_layer_1 = Convolution((5, 5), 32, strides=1, activation=cntk.ops.relu, pad=True,\\n                                            init=glorot_normal(), init_bias=0.1)\\n        pooling_layer_1 = MaxPooling((2, 2), strides=(2, 2), pad=True)\\n\\n        convolutional_layer_2 = Convolution((5, 5), 64, strides=1, activation=cntk.ops.relu, pad=True,\\n                                            init=glorot_normal(), init_bias=0.1)\\n        pooling_layer_2 = MaxPooling((2, 2), strides=(2, 2), pad=True)\\n\\n        convolutional_layer_3 = Convolution((5, 5), 128, strides=1, activation=cntk.ops.relu, pad=True,\\n                                            init=glorot_normal(), init_bias=0.1)\\n        pooling_layer_3 = MaxPooling((2, 2), strides=(2, 2), pad=True)\\n\\n        fully_connected_layer = Dense(1024, activation=cntk.ops.relu, init=glorot_normal(), init_bias=0.1)\\n\\n        output_layer = Dense(out_dims, activation=None, init=glorot_normal(), init_bias=0.1)\\n\\n        model = Sequential([convolutional_layer_1, pooling_layer_1,\\n                            convolutional_layer_2, pooling_layer_2,\\n                            #convolutional_layer_3, pooling_layer_3,\\n                            fully_connected_layer,\\n                            output_layer])(input_vars)\\n        return model', 'def __init__(self, capacity):\\n        self.examplers = deque(maxlen=capacity)\\n        self.capacity = capacity', 'def get_random_samples(self, num_samples):\\n        num_samples = min(num_samples, len(self.examplers))\\n        return random.sample(tuple(self.examplers), num_samples)', 'def get_random_stacks(self, num_samples, stack_size):\\n\\n        start_indices = random.sample(range(len(self.examplers)), num_samples)\\n        return [self.get_stack(start_index, stack_size) for start_index in start_indices]', 'def __init__(self):\\n        self.explore_rate = self.MAX_EXPLORATION_RATE\\n        self.brain = Brain()\\n        self.memory = Memory(self.MEMORY_CAPACITY)\\n        self.steps = 0', 'def observe(self, sample):\\n        self.steps += 1\\n        self.memory.add(sample)\\n\\n        # Reduces exploration rate linearly\\n        self.explore_rate = self.MIN_EXPLORATION_RATE + (self.MAX_EXPLORATION_RATE - self.MIN_EXPLORATION_RATE) * math.exp(-self.DECAY_RATE * self.steps)', 'def action_from_output(cls, output_array):\\n        return np.argmax(output_array)', 'def test(model_path, num_episodes=10):\\n\\n    root = cntk.load_model(model_path)\\n    observation = env.reset()  # reset environment for new episode\\n    done = False\\n    for episode in range(num_episodes):\\n        while not done:\\n            try:\\n                env.render()\\n            except Exception:\\n                # this might fail on a VM without OpenGL\\n                pass\\n\\n            observation = preprocess_image(observation)\\n            action = np.argmax(root.eval(observation.astype(np.float32)))\\n            observation, reward, done, info = env.step(action)\\n        if done:\\n            observation = env.reset()  # reset environment for new episode']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def upgrade(engine_name):\\n    globals()[\"upgrade_%s\" % engine_name]()', \"def upgrade_validation():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column('field_type', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('field_type', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('file_columns', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('file_columns', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('file_type', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('file_type', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('multi_field_rule', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('multi_field_rule', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('multi_field_rule_type', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('multi_field_rule_type', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule_timing', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule_timing', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule_type', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('rule_type', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    op.add_column('tas_lookup', sa.Column('created_at', sa.DateTime(), nullable=True))\\n    op.add_column('tas_lookup', sa.Column('updated_at', sa.DateTime(), nullable=True))\\n    ### end Alembic commands ###\"]}, {'features': [], 'snippets': ['def open(cls, file, build, structure, environment):\\n\\t\\tif isinstance(file, basestring):\\n\\t\\t\\tfile = open(file, \"rb\")\\n\\n\\t\\tinstance = cls(file, build, environment)\\n\\t\\tinstance._readHeader()\\n\\t\\tinstance.setStructure(structure)\\n\\t\\tinstance._rowDynamicFields = 0 # Dynamic fields index, used when parsing a row\\n\\t\\tinstance._readAddresses()\\n\\n\\t\\treturn instance', 'def __repr__(self):\\n\\t\\treturn \"%s(file=%r, build=%r)\" % (self.__class__.__name__, self.file, self.build)', 'def __getitem__(self, item):\\n\\t\\tif isinstance(item, slice):\\n\\t\\t\\tkeys = sorted(self._addresses.keys())[item]\\n\\t\\t\\treturn [self[k] for k in keys]\\n\\n\\t\\tif item not in self._values:\\n\\t\\t\\tself._parse_row(item)\\n\\n\\t\\treturn self._values[item]', 'def __delitem__(self, item):\\n\\t\\tif item in self._values:\\n\\t\\t\\tdel self._values[item]\\n\\t\\tdel self._addresses[item]', 'def __len__(self):\\n\\t\\treturn len(self._addresses)', 'def _parse_field(self, data, field, row=None):\\n\\t\\t\"\"\"\\n\\t\\tParse a single field in stream.\\n\\t\\t\"\"\"\\n\\t\\tif field.dyn > self._rowDynamicFields:\\n\\t\\t\\treturn None # The column doesn\\'t exist in this row, we set it to None\\n\\n\\t\\tret = None\\n\\t\\ttry:\\n\\t\\t\\tif isinstance(field, fields.StringField):\\n\\t\\t\\t\\tret = self._parse_string(data)\\n\\n\\t\\t\\telif isinstance(field, fields.DataField): # wowcache.wdb\\n\\t\\t\\t\\tlength = getattr(row, field.master)\\n\\t\\t\\t\\tret = data.read(length)\\n\\n\\t\\t\\telif isinstance(field, fields.DynamicMaster):\\n\\t\\t\\t\\tret, = unpack(\"<I\", data.read(4))\\n\\t\\t\\t\\tself._rowDynamicFields = ret\\n\\n\\t\\t\\telse:\\n\\t\\t\\t\\tret, = unpack(\"<%s\" % (field.char), data.read(field.size))\\n\\t\\texcept StructError:\\n\\t\\t\\tlog.warning(\"Field %s could not be parsed properly\" % (field))\\n\\t\\t\\tret = None\\n\\n\\t\\treturn ret', 'def append(self, row):\\n\\t\\t\"\"\"\\n\\t\\tAppend a row at the end of the file.\\n\\t\\tIf the row does not have an id, one is automatically assigned.\\n\\t\\t\"\"\"\\n\\t\\ti = len(self) + 1 # FIXME this wont work properly in incomplete files\\n\\t\\tif \"_id\" not in row:\\n\\t\\t\\trow[\"_id\"] = i\\n\\t\\tself[i] = row', 'def keys(self):\\n\\t\\treturn self._addresses.keys()', 'def parse_row(self, data, reclen=0):\\n\\t\\t\"\"\"\\n\\t\\tAssign data to a DBRow instance\\n\\t\\t\"\"\"\\n\\t\\treturn DBRow(self, data=data, reclen=reclen)', 'def setRow(self, key, **values):\\n\\t\\tself.__setitem__(key, DBRow(self, columns=values))', 'def update(self, other):\\n\\t\\t\"\"\"\\n\\t\\tUpdate file from iterable other\\n\\t\\t\"\"\"\\n\\t\\tfor k in other:\\n\\t\\t\\tself[k] = other[k]', 'def __init__(self, parent, data=None, columns=None, reclen=0):\\n\\t\\tself._parent = parent\\n\\t\\tself._values = {} # Columns values storage\\n\\t\\tself.structure = parent.structure\\n\\n\\t\\tself.initialized = True # needed for __setattr__\\n\\n\\t\\tif columns:\\n\\t\\t\\tif type(columns) == list:\\n\\t\\t\\t\\tself.extend(columns)\\n\\n\\t\\t\\telif type(columns) == dict:\\n\\t\\t\\t\\tself._default()\\n\\t\\t\\t\\t_cols = [k.name for k in self.structure]\\n\\t\\t\\t\\tfor k in columns:\\n\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\tself[_cols.index(k)] = columns[k]\\n\\t\\t\\t\\t\\texcept ValueError:\\n\\t\\t\\t\\t\\t\\tlog.warning(\"Column %r not found\" % (k))\\n\\n\\t\\telif data:\\n\\t\\t\\tdynfields = 0\\n\\t\\t\\tdata = StringIO(data)\\n\\t\\t\\tfor field in self.structure:\\n\\t\\t\\t\\t_data = parent._parse_field(data, field, self)\\n\\t\\t\\t\\tself.append(_data)\\n\\n\\t\\t\\tif reclen:\\n\\t\\t\\t\\treal_reclen = reclen + self._parent.row_header_size\\n\\t\\t\\t\\tif data.tell() != real_reclen:\\n\\t\\t\\t\\t\\tlog.warning(\"Reclen not respected for row %r. Expected %i, read %i. (%+i)\" % (self.id, real_reclen, data.tell(), real_reclen-data.tell()))', 'def __getattr__(self, attr):\\n\\t\\tif attr in self.structure:\\n\\t\\t\\treturn self._get_value(attr)\\n\\n\\t\\tif attr in self.structure._abstractions: # Union abstractions etc\\n\\t\\t\\tfield, func = self.structure._abstractions[attr]\\n\\t\\t\\treturn func(field, self)\\n\\n\\t\\tif \"__\" in attr:\\n\\t\\t\\treturn self._query(attr)\\n\\n\\t\\treturn super(DBRow, self).__getattribute__(attr)', 'def __setattr__(self, attr, value):\\n\\t\\t# Do not preserve the value in DBRow! Use the save method to save.\\n\\t\\tif self.initialized and attr in self.structure:\\n\\t\\t\\tself._set_value(attr, value)\\n\\t\\treturn super(DBRow, self).__setattr__(attr, value)', 'def _get_reverse_relation(self, table, field):\\n\\t\\t\"\"\"\\n\\t\\tReturn a list of rows matching the reverse relation\\n\\t\\t\"\"\"\\n\\t\\tif not hasattr(self._parent, \"_reverse_relation_cache\"):\\n\\t\\t\\tself._parent._reverse_relation_cache = {}\\n\\t\\tcache = self._parent._reverse_relation_cache\\n\\n\\t\\ttfield = table + \"__\" + field\\n\\t\\tif tfield not in cache:\\n\\t\\t\\tcache[tfield] = {}\\n\\t\\t\\t# First time lookup, let\\'s build the cache\\n\\t\\t\\ttable = self._parent.environment.dbFile(table)\\n\\t\\t\\tfor row in table:\\n\\t\\t\\t\\trow = table[row]\\n\\t\\t\\t\\tid = row._raw(field)\\n\\t\\t\\t\\tif id not in cache[tfield]:\\n\\t\\t\\t\\t\\tcache[tfield][id] = []\\n\\t\\t\\t\\tcache[tfield][id].append(row)\\n\\n\\t\\treturn cache[tfield].get(self.id, None)', 'def _query(self, rel, value=None):\\n\\t\\t\"\"\"\\n\\t\\tParse a django-like multilevel relationship\\n\\t\\t\"\"\"\\n\\t\\trels = rel.split(\"__\")\\n\\t\\tif \"\" in rels: # empty string\\n\\t\\t\\traise ValueError(\"Invalid relation string\")\\n\\n\\t\\tfirst = rels[0]\\n\\t\\tif not hasattr(self, first):\\n\\t\\t\\tif self._parent.environment.hasDbFile(first):\\n\\t\\t\\t\\t# Handle reverse relations, eg spell__item for item table\\n\\t\\t\\t\\tremainder = rel[len(first + \"__\"):]\\n\\t\\t\\t\\treturn self._get_reverse_relation(first, remainder)\\n\\n\\t\\t\\traise ValueError(\"Invalid relation string\")\\n\\n\\t\\tret = self\\n\\t\\trels = rels[::-1]\\n\\n\\t\\tspecial = {\\n\\t\\t\\t\"contains\": lambda x, y: x in y,\\n\\t\\t\\t\"exact\": lambda x, y: x == y,\\n\\t\\t\\t\"icontains\": lambda x, y: x.lower() in y.lower(),\\n\\t\\t\\t\"iexact\": lambda x, y: x.lower() == y.lower(),\\n\\t\\t\\t\"gt\": lambda x, y: x > y,\\n\\t\\t\\t\"gte\": lambda x, y: x >= y,\\n\\t\\t\\t\"lt\": lambda x, y: x < y,\\n\\t\\t\\t\"lte\": lambda x, y: x <= y,\\n\\t\\t}\\n\\n\\t\\twhile rels:\\n\\t\\t\\tif rels[-1] in special:\\n\\t\\t\\t\\tif len(rels) != 1:\\n\\t\\t\\t\\t\\t# icontains always needs to be the last piece of the relation string\\n\\t\\t\\t\\t\\traise ValueError(\"Invalid relation string\")\\n\\n\\t\\t\\t\\treturn special[rels[-1]](value, ret)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tret = getattr(ret, rels.pop())\\n\\n\\t\\treturn ret', 'def _get_value(self, name):\\n\\t\\tif name not in self._values:\\n\\t\\t\\traw_value = self[self.structure.index(name)]\\n\\n\\t\\t\\tself._set_value(name, raw_value)\\n\\n\\t\\treturn self._values[name]', 'def _save(self):\\n\\t\\tfor name in self._values:\\n\\t\\t\\tindex = self.structure.index(name)\\n\\t\\t\\tcol = self.structure[index]\\n\\t\\t\\tself[index] = col.from_python(self._values[name])', 'def _default(self):\\n\\t\\t\"\"\"\\n\\t\\tChange all fields to their default values\\n\\t\\t\"\"\"\\n\\t\\tdel self[:]\\n\\t\\tself._values = {}\\n\\t\\tfor col in self.structure:\\n\\t\\t\\tchar = col.char\\n\\t\\t\\tif col.dyn:\\n\\t\\t\\t\\tself.append(None)\\n\\t\\t\\telif char == \"s\":\\n\\t\\t\\t\\tself.append(\"\")\\n\\t\\t\\telif char == \"f\":\\n\\t\\t\\t\\tself.append(0.0)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.append(0)', 'def update(self, other):\\n\\t\\tfor k in other:\\n\\t\\t\\tself[k] = other[k]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def getIcon(self):\\n        return  QtGui.QIcon(os.path.dirname(__file__) + \"/../images/saga.png\")']}, {'features': [], 'snippets': ['def run(context, UI):\\n    \"\"\"\\n    Run this plugin.\\n    \"\"\"\\n\\n    if len(context.graph.vertices) < 1:\\n        generate = True\\n    else:\\n        res = UI.prYesNo(\"Use current graph?\",\\n                         \"Would you like to apply the layout to the current graph? If not, a complete graph will be generated and the current graph cleared.\")\\n        if res:\\n            generate = False\\n            # Go through and eliminate any existing bend points\\n            from graph import DummyVertex\\n\\n            for v in [x for x in context.graph.vertices if isinstance(x, DummyVertex)]:\\n                context.graph.removeVertex(v)\\n        else:\\n            generate = True\\n\\n    if generate:\\n        N = UI.prType(\"Number of Vertices\", \"Input number of vertices to generate complete graph:\", int, 4)\\n        if N == None:\\n            return True\\n        while N < 0:\\n            N = UI.prType(\"Number of Vertices\",\\n                          \"Please input positive value.\\\\n\\\\nInput number of vertices to generate complete graph:\", int,\\n                          N)\\n            if N == None:\\n                return True\\n\\n        context.graph.clear()\\n\\n        # Generate a complete graph\\n        k_n(context, N)\\n\\n    res = UI.prYesNo(\"Use mod-p layout?\",\\n                     \"Would you like to use the mod-p compact layout (O(n^3) volume)? If not, the O(n^6) uncompacted layout will be used.\")\\n\\n    # Lay it out according to the 1bend layout\\n    moment(context, compact=res)\\n\\n    context.camera.lookAtGraph(context.graph, context.graph.centerOfMass(), offset=context.graph.viewpoint())\\n\\n    return True']}, {'features': [], 'snippets': [\"def __init__(self):\\n        self.priority = 1\\n        self.language = ['en']\\n        self.domains = ['coolmoviezone.online']\\n        self.base_link = 'https://coolmoviezone.online'\\n        self.scraper = cfscrape.create_scraper()\", 'def sources(self, url, hostDict, hostprDict):\\n        try:\\n            sources = []\\n            r = self.scraper.get(url).content\\n            match = re.compile(\\'<td align=\"center\"><strong><a href=\"(.+?)\"\\').findall(r)\\n            for url in match:\\n                host = url.split(\\'//\\')[1].replace(\\'www.\\', \\'\\')\\n                host = host.split(\\'/\\')[0].split(\\'.\\')[0].title()\\n                quality = source_utils.check_sd_url(url)\\n                sources.append({\\'source\\': host, \\'quality\\': quality, \\'language\\': \\'en\\', \\'url\\': url, \\'direct\\': False,\\n                                \\'debridonly\\': False})\\n        except Exception:\\n            return\\n        return sources']}, {'features': [], 'snippets': ['def __init__(self, resistance, voltage):\\n        self._resistance = resistance\\n        self.voltage = voltage', 'def resistance(self):\\n        return self._resistance', 'def current(self):\\n        return self.voltage / self.resistance', 'def current(self, value):\\n        self.voltage = self.resistance * value', 'def current_in_milliamps(self):\\n        return self.current * 1000', 'def current_in_milliamps(self, value):\\n        self.current = value / 1000']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, name, description, pluginClass,\\n\\t\\t\\t\\t pluginStartForm, pluginStartMethod,\\n\\t\\t\\t\\t pluginEditForm=None, pluginEditMethod=None):\\n\\t\\tself.name=name #No Spaces please...\\n\\t\\tself.description=description\\n\\t\\tself.plugin=pluginClass\\n\\t\\tself.manage_addForm=pluginStartForm\\n\\t\\tself.manage_addMethod=pluginStartMethod\\n\\t\\tself.manage_editForm=pluginEditForm\\n\\t\\tself.manage_editMethod=pluginEditMethod']}, {'features': [], 'snippets': ['def main():\\n    db = MySQLDatabase()\\n    fetcher = FeedFetcher()\\n\\n    feeds = db.get_feeds(offset=0, limit=10)\\n    read_count = 10\\n    while len(feeds) > 0:\\n        for feed in feeds:\\n            fid = feed[0]\\n            url = feed[1]\\n            title = feed[2]\\n            print \"fetching #{0}: {1}\".format(fid, url)\\n            entries = fetcher.fetch(url)\\n            for entry in entries:\\n                entry.feed_id = fid\\n                try:\\n                    print \"insert {0}\".format(entry.url)\\n                except UnicodeEncodeError:\\n                    print \"insert {0}\".format(entry.url.encode(\\'utf-8\\'))\\n                db.append_feed_content(entry)\\n        feeds = db.get_feeds(offset=read_count, limit=10)\\n        read_count += 10']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def make_input_key(c, control_key_state=None):\\r\\n    kc = win32console.PyINPUT_RECORDType (win32console.KEY_EVENT)\\r\\n    kc.KeyDown = True\\r\\n    kc.RepeatCount = 1\\r\\n    cnum = ord(c)\\r\\n    if cnum == 3:\\r\\n        pid_list = win32console.GetConsoleProcessList()\\r\\n        win32console.GenerateConsoleCtrlEvent(win32con.CTRL_C_EVENT, 0)\\r\\n        return \\r\\n    else:\\r\\n        kc.Char = unicode(c)\\r\\n        if str(cnum) in CONQUE_WINDOWS_VK:\\r\\n            kc.VirtualKeyCode = CONQUE_WINDOWS_VK[str(cnum)]\\r\\n        else:\\r\\n            kc.VirtualKeyCode = ctypes.windll.user32.VkKeyScanA(cnum)\\r\\n            #kc.VirtualKeyCode = ctypes.windll.user32.VkKeyScanA(cnum+96)\\r\\n            #kc.ControlKeyState = win32con.LEFT_CTRL_PRESSED']}, {'features': [], 'snippets': ['def __init__(self):\\n        self.id = 0\\n        self.name = \"\"\\n        self.prom = 0.0\\n        self.idle = \"\"\\n        self.drinks = 0', \"def get_listbox_items(drinkers):\\n    items = []\\n\\n    for drinker in drinkers:\\n        items.append(unicode('%s, %d drinks, %s' % (drinker.name, drinker.drinks, drinker.idle)))\\n\\n    return items\", 'def quit():\\n        app_lock.signal()', 'def handle_selection():\\n    selected_drinker = drinkers[lb.current()]\\n    urllib2.urlopen(\"http://192.168.11.5:8080/drinkcounter/add_drink/%d/\" % (selected_drinker.id))\\n    appuifw.note(u\"A drink has been added to \" + drinkers[lb.current()].name, \\'info\\')\\n\\n    new_drinkers = get_drinker_list()\\n    items = get_listbox_items(new_drinkers)\\n\\n    lb.set_list(items, lb.current())']}, {'features': [], 'snippets': ['def setInput(self, input):\\n\\t\\tINPUT = { \"ENCODER\": 0, \"SCART\": 1, \"AUX\": 2 }\\n\\t\\teAVSwitch.getInstance().setInput(INPUT[input])', 'def setAspectRatio(self, value):\\n\\t\\teAVSwitch.getInstance().setAspectRatio(value)', 'def getOutputAspect(self):\\n\\t\\tvalstr = config.av.aspectratio.value\\n\\t\\tif valstr in (\"4_3_letterbox\", \"4_3_panscan\"): # 4:3\\n\\t\\t\\treturn (4,3)\\n\\t\\telif valstr == \"16_9\": # auto ... 4:3 or 16:9\\n\\t\\t\\ttry:\\n\\t\\t\\t\\taspect_str = open(\"/proc/stb/vmpeg/0/aspect\", \"r\").read()\\n\\t\\t\\t\\tif aspect_str == \"1\": # 4:3\\n\\t\\t\\t\\t\\treturn (4,3)\\n\\t\\t\\texcept IOError:\\n\\t\\t\\t\\tpass\\n\\t\\telif valstr in (\"16_9_always\", \"16_9_letterbox\"): # 16:9\\n\\t\\t\\tpass\\n\\t\\telif valstr in (\"16_10_letterbox\", \"16_10_panscan\"): # 16:10\\n\\t\\t\\treturn (16,10)\\n\\t\\treturn (16,9)', 'def getAspectRatioSetting(self):\\n\\t\\tvalstr = config.av.aspectratio.value\\n\\t\\tif valstr == \"4_3_letterbox\":\\n\\t\\t\\tval = 0\\n\\t\\telif valstr == \"4_3_panscan\":\\n\\t\\t\\tval = 1\\n\\t\\telif valstr == \"16_9\":\\n\\t\\t\\tval = 2\\n\\t\\telif valstr == \"16_9_always\":\\n\\t\\t\\tval = 3\\n\\t\\telif valstr == \"16_10_letterbox\":\\n\\t\\t\\tval = 4\\n\\t\\telif valstr == \"16_10_panscan\":\\n\\t\\t\\tval = 5\\n\\t\\telif valstr == \"16_9_letterbox\":\\n\\t\\t\\tval = 6\\n\\t\\treturn val', 'def InitAVSwitch():\\n\\tconfig.av = ConfigSubsection()\\n\\tconfig.av.yuvenabled = ConfigBoolean(default=False)\\n\\tcolorformat_choices = {\"cvbs\": _(\"CVBS\"), \"rgb\": _(\"RGB\"), \"svideo\": _(\"S-Video\")}', 'def setColorFormat(configElement):\\n\\t\\tmap = {\"cvbs\": 0, \"rgb\": 1, \"svideo\": 2, \"yuv\": 3}\\n\\t\\tiAVSwitch.setColorFormat(map[configElement.value])', 'def setSystem(configElement):\\n\\t\\tmap = {\"pal\": 0, \"ntsc\": 1, \"multinorm\" : 2}\\n\\t\\tiAVSwitch.setSystem(map[configElement.value])', 'def setAC3Downmix(configElement):\\n\\t\\t\\topen(\"/proc/stb/audio/ac3\", \"w\").write(configElement.value and \"downmix\" or \"passthrough\")', 'def setAACDownmix(configElement):\\n\\t\\t\\topen(\"/proc/stb/audio/aac\", \"w\").write(configElement.value and \"downmix\" or \"passthrough\")', 'def setAlpha(config):\\n\\t\\topen(\"/proc/stb/video/alpha\", \"w\").write(str(config.value))', 'def setScaler_sharpness(config):\\n\\t\\t\\tmyval = int(config.value)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tprint \"--> setting scaler_sharpness to: %0.8X\" % myval\\n\\t\\t\\t\\topen(\"/proc/stb/vmpeg/0/pep_scaler_sharpness\", \"w\").write(\"%0.8X\" % myval)\\n\\t\\t\\t\\topen(\"/proc/stb/vmpeg/0/pep_apply\", \"w\").write(\"1\")\\n\\t\\t\\texcept IOError:\\n\\t\\t\\t\\tprint \"couldn\\'t write pep_scaler_sharpness\"']}, {'features': [], 'snippets': ['def broadcast_msg(server, ns_name, event, *args):\\n    pkt = dict(type=\"event\",\\n               name=event,\\n               args=args,\\n               endpoint=ns_name)\\n\\n    for sessid, socket in server.sockets.iteritems():\\n        socket.send_packet(pkt)', \"def __call__(self, environ, start_response):\\n        if environ['PATH_INFO'].startswith('/socket.io'):\\n            socketio_manage(environ, {'': QueueStatusHandler})\"]}, {'features': [], 'snippets': [\"def freq_month(obj):\\n    if obj is None or obj == []:\\n        return\\n    months = {1: 'jan',\\n              2: 'feb',\\n              3: 'mar',\\n              4: 'apr',\\n              5: 'may',\\n              6: 'jun',\\n              7: 'jul',\\n              8: 'aug',\\n              9: 'sep',\\n              10: 'oct',\\n              11: 'nov',\\n              12: 'dec',\\n             }\\n    frequencies = [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]\"]}, {'features': [], 'snippets': ['def __init__(self, cfg_file=None, strict=False):\\n        \"\"\" Constructor for :py:class:`pynag.Parsers.config` class\\n\\n        Args:\\n\\n            cfg_file (str): Full path to nagios.cfg. If None, try to\\n            auto-discover location\\n\\n            strict (bool): if True, use stricter parsing which is more prone to\\n            raising exceptions\\n        \"\"\"\\n\\n        self.cfg_file = cfg_file  # Main configuration file\\n        self.strict = strict  # Use strict parsing or not\\n\\n        # If nagios.cfg is not set, lets do some minor autodiscover.\\n        if self.cfg_file is None:\\n            self.cfg_file = self.guess_cfg_file()\\n\\n        self.data = {}\\n        self.maincfg_values = []\\n        self._is_dirty = False\\n        self.reset()  # Initilize misc member variables', 'def guess_nagios_binary(self):\\n        \"\"\" Returns a path to any nagios binary found on your system\\n\\n        Use this function if you don\\'t want specify path to the nagios binary\\n        in your code and you are confident that it is located in a common\\n        location\\n\\n        Checked locations are as follows:\\n\\n        * /usr/bin/nagios\\n        * /usr/sbin/nagios\\n        * /usr/local/nagios/bin/nagios\\n        * /nagios/bin/nagios\\n        * /usr/bin/icinga\\n        * /usr/sbin/icinga\\n        * /usr/bin/naemon\\n        * /usr/sbin/naemon\\n        * /usr/local/naemon/bin/naemon.cfg\\n        * /usr/bin/shinken\\n        * /usr/sbin/shinken\\n\\n        Returns:\\n\\n            str. Path to the nagios binary\\n\\n            None if could not find a binary in any of those locations\\n        \"\"\"\\n\\n        possible_files = (\\'/usr/bin/nagios\\',\\n                          \\'/usr/sbin/nagios\\',\\n                          \\'/usr/local/nagios/bin/nagios\\',\\n                          \\'/nagios/bin/nagios\\',\\n                          \\'/usr/bin/icinga\\',\\n                          \\'/usr/sbin/icinga\\',\\n                          \\'/usr/bin/naemon\\',\\n                          \\'/usr/sbin/naemon\\',\\n                          \\'/usr/local/naemon/bin/naemon.cfg\\',\\n                          \\'/usr/bin/shinken\\',\\n                          \\'/usr/sbin/shinken\\')\\n\\n        possible_binaries = (\\'nagios\\', \\'nagios3\\', \\'naemon\\', \\'icinga\\', \\'shinken\\')\\n        for i in possible_binaries:\\n            command = [\\'which\\', i]\\n            code, stdout, stderr = pynag.Utils.runCommand(command=command, shell=False)\\n            if code == 0:\\n                return stdout.splitlines()[0].strip()\\n\\n        return None', 'def reset(self):\\n        \"\"\" Reinitializes the data of a parser instance to its default values.\\n        \"\"\"\\n\\n        self.cfg_files = []  # List of other configuration files\\n        self.data = {}  # dict of every known object definition\\n        self.errors = []  # List of ParserErrors\\n        self.item_list = None\\n        self.item_cache = None\\n        self.maincfg_values = []  # The contents of main nagios.cfg\\n        self._resource_values = []  # The contents of any resource_files\\n        self.item_apply_cache = {}  # This is performance tweak used by _apply_template\\n\\n        # This is a pure listof all the key/values in the config files.  It\\n        # shouldn\\'t be useful until the items in it are parsed through with the proper\\n        # \\'use\\' relationships\\n        self.pre_object_list = []\\n        self.post_object_list = []\\n        self.object_type_keys = {\\n            \\'hostgroup\\': \\'hostgroup_name\\',\\n            \\'hostextinfo\\': \\'host_name\\',\\n            \\'host\\': \\'host_name\\',\\n            \\'service\\': \\'name\\',\\n            \\'servicegroup\\': \\'servicegroup_name\\',\\n            \\'contact\\': \\'contact_name\\',\\n            \\'contactgroup\\': \\'contactgroup_name\\',\\n            \\'timeperiod\\': \\'timeperiod_name\\',\\n            \\'command\\': \\'command_name\\',\\n            #\\'service\\':[\\'host_name\\',\\'description\\'],\\n        }', 'def _get_pid(self):\\n        \"\"\" Checks the lock_file var in nagios.cfg and returns the pid from the file\\n\\n        If the pid file does not exist, returns None.\\n        \"\"\"\\n        try:\\n            return self.open(self.get_cfg_value(\\'lock_file\\'), \"r\").readline().strip()\\n        except Exception:\\n            return None', 'def _get_key(self, object_type, user_key=None):\\n        \"\"\" Return the correct \\'key\\' for an item.\\n\\n        This is mainly a helper method for other methods in this class. It is\\n        used to shorten code repetition.\\n\\n        Args:\\n\\n            object_type: Object type from which to obtain the \\'key\\' (string)\\n\\n            user_key: User defined key. Default None. (string)\\n\\n        Returns:\\n            Correct \\'key\\' for the object type. (string)\\n        \"\"\"\\n\\n        if not user_key and not object_type in self.object_type_keys:\\n            raise ParserError(\"Unknown key for object type:  %s\\\\n\" % object_type)\\n\\n        # Use a default key\\n        if not user_key:\\n            user_key = self.object_type_keys[object_type]\\n\\n        return user_key', 'def _apply_template(self, original_item):\\n        \"\"\" Apply all attributes of item named parent_name to \"original_item\".\\n\\n        Applies all of the attributes of parents (from the \\'use\\' field) to item.\\n\\n        Args:\\n\\n            original_item: Item \\'use\\'-ing a parent item. The parent\\'s attributes\\n            will be concretely added to this item.\\n\\n        Returns:\\n\\n            original_item to which have been added all the attributes defined\\n            in parent items.\\n        \"\"\"\\n\\n        # TODO: There is space for more performance tweaks here\\n        # If item does not inherit from anyone else, lets just return item as is.\\n        if \\'use\\' not in original_item:\\n            return original_item\\n        object_type = original_item[\\'meta\\'][\\'object_type\\']\\n        raw_definition = original_item[\\'meta\\'][\\'raw_definition\\']\\n        my_cache = self.item_apply_cache.get(object_type, {})\\n\\n        # Performance tweak, if item has been parsed. Lets not do it again\\n        if raw_definition in my_cache:\\n            return my_cache[raw_definition]\\n\\n        parent_names = original_item[\\'use\\'].split(\\',\\')\\n        parent_items = []\\n        for parent_name in parent_names:\\n            parent_item = self._get_item(parent_name, object_type)\\n            if parent_item is None:\\n                error_string = \"Can not find any %s named %s\\\\n\" % (object_type, parent_name)\\n                self.errors.append(ParserError(error_string, item=original_item))\\n                continue\\n\\n            try:\\n                # Parent item probably has use flags on its own. So lets apply to parent first\\n                parent_item = self._apply_template(parent_item)\\n            except RuntimeError:\\n                t, e = sys.exc_info()[:2]\\n                self.errors.append(ParserError(\"Error while parsing item: %s (it might have circular use=)\" % str(e),\\n                                               item=original_item))\\n            parent_items.append(parent_item)\\n\\n        inherited_attributes = original_item[\\'meta\\'][\\'inherited_attributes\\']\\n        template_fields = original_item[\\'meta\\'][\\'template_fields\\']\\n        for parent_item in parent_items:\\n            for k, v in parent_item.iteritems():\\n                if k in (\\'use\\', \\'register\\', \\'meta\\', \\'name\\'):\\n                    continue\\n                if k not in inherited_attributes:\\n                    inherited_attributes[k] = v\\n                if k not in original_item:\\n                    original_item[k] = v\\n                    template_fields.append(k)\\n        if \\'name\\' in original_item:\\n            my_cache[raw_definition] = original_item\\n\\n        return original_item', 'def get_new_item(self, object_type, filename):\\n        \"\"\" Returns an empty item with all necessary metadata\\n\\n        Creates a new item dict and fills it with usual metadata:\\n\\n            * object_type : object_type (arg)\\n            * filename : filename (arg)\\n            * template_fields = []\\n            * needs_commit = None\\n            * delete_me = None\\n            * defined_attributes = {}\\n            * inherited_attributes = {}\\n            * raw_definition = \"define %s {\\\\\\\\n\\\\\\\\n} % object_type\"\\n\\n        Args:\\n\\n            object_type: type of the object to be created (string)\\n\\n            filename: Path to which the item will be saved (string)\\n\\n        Returns:\\n\\n            A new item with default metadata\\n\\n        \"\"\"\\n\\n        meta = {\\n            \\'object_type\\': object_type,\\n            \\'filename\\': filename,\\n            \\'template_fields\\': [],\\n            \\'needs_commit\\': None,\\n            \\'delete_me\\': None,\\n            \\'defined_attributes\\': {},\\n            \\'inherited_attributes\\': {},\\n            \\'raw_definition\\': \"define %s {\\\\n\\\\n}\" % object_type,\\n        }\\n        return {\\'meta\\': meta}', 'def parse_file(self, filename):\\n        \"\"\" Parses a nagios object configuration file and returns lists of dictionaries.\\n\\n        This is more or less a wrapper around :py:meth:`config.parse_string`,\\n        so reading documentation there is useful.\\n\\n        Args:\\n\\n            filename: Path to the file to parse (string)\\n\\n        Returns:\\n\\n            A list containing elements parsed by :py:meth:`parse_string`\\n        \"\"\"\\n        try:\\n            raw_string = self.open(filename, \\'rb\\').read()\\n            return self.parse_string(raw_string, filename=filename)\\n        except IOError:\\n            t, e = sys.exc_info()[:2]\\n            parser_error = ParserError(e.strerror)\\n            parser_error.filename = e.filename\\n            self.errors.append(parser_error)\\n            return []', 'def _locate_item(self, item):\\n        \"\"\" This is a helper function for anyone who wishes to modify objects.\\n\\n        It takes \"item\", locates the file which is configured in, and locates\\n        exactly the lines which contain that definition.\\n\\n        Returns: (tuple)\\n\\n            (everything_before, object_definition, everything_after, filename):\\n\\n                * everything_before (list of lines): Every line in filename before object was defined\\n                * everything_after (list of lines): Every line in \"filename\" after object was defined\\n                * object_definition (list of lines): Every line used to define our item in \"filename\"\\n                * filename (string): file in which the object was written to\\n\\n        Raises:\\n\\n            :py:class:`ValueError` if object was not found in \"filename\"\\n\\n        \"\"\"\\n        if \"filename\" in item[\\'meta\\']:\\n            filename = item[\\'meta\\'][\\'filename\\']\\n        else:\\n            raise ValueError(\"item does not have a filename\")\\n\\n        # Look for our item, store it as my_item\\n        for i in self.parse_file(filename):\\n            if self.compareObjects(item, i):\\n                my_item = i\\n                break\\n        else:\\n            raise ValueError(\"We could not find object in %s\\\\n%s\" % (filename, item))\\n\\n        # Caller of this method expects to be returned\\n        # several lists that describe the lines in our file.\\n        # The splitting logic starts here.\\n        my_file = self.open(filename)\\n        all_lines = my_file.readlines()\\n        my_file.close()\\n\\n        start = my_item[\\'meta\\'][\\'line_start\\'] - 1\\n        end = my_item[\\'meta\\'][\\'line_end\\']\\n        everything_before = all_lines[:start]\\n        object_definition = all_lines[start:end]\\n        everything_after = all_lines[end:]\\n\\n        # If there happen to be line continuations in the object we will edit\\n        # We will remove them from object_definition\\n        object_definition = self._clean_backslashes(object_definition)\\n        return everything_before, object_definition, everything_after, filename', 'def _modify_object(self, item, field_name=None, new_value=None, new_field_name=None, new_item=None,\\n                       make_comments=False):\\n        \"\"\" Locates \"item\" and changes the line which contains field_name.\\n\\n        Helper function for object_* functions. Locates \"item\" and changes the\\n        line which contains field_name. If new_value and new_field_name are both\\n        None, the attribute is removed.\\n\\n        Args:\\n\\n            item(dict): The item to be modified\\n\\n            field_name(str): The field_name to modify (if any)\\n\\n            new_field_name(str): If set, field_name will be renamed\\n\\n            new_value(str): If set the value of field_name will be changed\\n\\n            new_item(str): If set, whole object will be replaced with this\\n            string\\n\\n            make_comments: If set, put pynag-branded comments where changes\\n            have been made\\n\\n        Returns:\\n\\n            True on success\\n\\n        Raises:\\n\\n            :py:class:`ValueError` if object or field_name is not found\\n\\n            :py:class:`IOError` is save is unsuccessful.\\n\\n        \"\"\"\\n        if item is None:\\n            return\\n        if field_name is None and new_item is None:\\n            raise ValueError(\"either field_name or new_item must be set\")\\n        if \\'\\\\n\\' in str(new_value):\\n            raise ValueError(\"Invalid character \\\\\\\\n used as an attribute value.\")\\n        everything_before, object_definition, everything_after, filename = self._locate_item(item)\\n        if new_item is not None:\\n            # We have instruction on how to write new object, so we dont need to parse it\\n            object_definition = [new_item]\\n        else:\\n            change = None\\n            value = None\\n            i = 0\\n            for i in range(len(object_definition)):\\n                tmp = object_definition[i].split(None, 1)\\n                if len(tmp) == 0:\\n                    continue\\n                # Hack for timeperiods, they dont work like other objects\\n                elif item[\\'meta\\'][\\'object_type\\'] == \\'timeperiod\\' and field_name not in (\\'alias\\', \\'timeperiod_name\\'):\\n                    tmp = [object_definition[i]]\\n                    # we can\\'t change timeperiod, so we fake a field rename\\n                    if new_value is not None:\\n                        new_field_name = new_value\\n                        new_value = None\\n                        value = \\'\\'\\n                elif len(tmp) == 1:\\n                    value = \\'\\'\\n                else:\\n                    value = tmp[1]\\n                k = tmp[0].strip()\\n                if k == field_name:\\n                    # Attribute was found, lets change this line\\n                    if new_field_name is None and new_value is None:\\n                        # We take it that we are supposed to remove this attribute\\n                        change = object_definition.pop(i)\\n                        break\\n                    elif new_field_name:\\n                        # Field name has changed\\n                        k = new_field_name\\n                    if new_value is not None:\\n                        # value has changed\\n                        value = new_value\\n                        # Here we do the actual change\\n                    change = \"\\\\t%-30s%s\\\\n\" % (k, value)\\n                    if item[\\'meta\\'][\\'object_type\\'] == \\'timeperiod\\' and field_name not in (\\'alias\\', \\'timeperiod_name\\'):\\n                        change = \"\\\\t%s\\\\n\" % new_field_name\\n                    object_definition[i] = change\\n                    break\\n            if not change and new_value is not None:\\n                # Attribute was not found. Lets add it\\n                change = \"\\\\t%-30s%s\\\\n\" % (field_name, new_value)\\n                object_definition.insert(i, change)\\n            # Lets put a banner in front of our item\\n        if make_comments:\\n            comment = \\'# Edited by PyNag on %s\\\\n\\' % time.ctime()\\n            if len(everything_before) > 0:\\n                last_line_before = everything_before[-1]\\n                if last_line_before.startswith(\\'# Edited by PyNag on\\'):\\n                    everything_before.pop()  # remove this line\\n            object_definition.insert(0, comment)\\n            # Here we overwrite the config-file, hoping not to ruin anything\\n        str_buffer = \"%s%s%s\" % (\\'\\'.join(everything_before), \\'\\'.join(object_definition), \\'\\'.join(everything_after))\\n        self.write(filename, str_buffer)\\n        return True', 'def write(self, filename, string):\\n        \"\"\" Wrapper around open(filename).write()\\n\\n        Writes string to filename and closes the file handler. File handler is\\n        openned in `\\'w\\'` mode.\\n\\n        Args:\\n\\n            filename: File where *string* will be written. This is the path to\\n            the file. (string)\\n\\n            string: String to be written to file. (string)\\n\\n        Returns:\\n\\n            Return code as returned by :py:meth:`os.write`\\n\\n        \"\"\"\\n        fh = self.open(filename, \\'w\\')\\n        return_code = fh.write(string)\\n        fh.flush()\\n        # os.fsync(fh)\\n        fh.close()\\n        self._is_dirty = True\\n        return return_code', 'def item_remove(self, item):\\n        \"\"\" Delete one specific item from its configuration files\\n\\n        Args:\\n\\n            item: Item that is to be rewritten\\n\\n            str_new_item: string representation of the new item\\n\\n        ..\\n            In the following line, every \"\\\\\\\\n\" is actually a simple line break\\n            This is only a little patch for the generated documentation.\\n\\n        Examples::\\n            item_remove( item, \"define service {\\\\\\\\n name example-service \\\\\\\\n register 0 \\\\\\\\n }\\\\\\\\n\" )\\n\\n        Returns:\\n\\n            True on success\\n\\n        Raises:\\n\\n            :py:class:`ValueError` if object is not found\\n\\n            :py:class:`IOError` if save fails\\n        \"\"\"\\n        return self._modify_object(item=item, new_item=\"\")', 'def item_remove_field(self, item, field_name):\\n        \"\"\" Removes one field of a (currently existing) object.\\n\\n        Changes are immediate (i.e. there is no commit)\\n\\n        Args:\\n\\n            item: Item to remove field from.\\n\\n            field_name: Field to remove. (string)\\n\\n        Example usage::\\n            item_remove_field( item, field_name=\"contactgroups\" )\\n\\n        Returns:\\n            True on success\\n\\n        Raises:\\n\\n            :py:class:`ValueError` if object is not found\\n\\n            :py:class:`IOError` if save fails\\n        \"\"\"\\n        return self._modify_object(item=item, field_name=field_name, new_value=None, new_field_name=None)', 'def item_add(self, item, filename):\\n        \"\"\" Adds a new object to a specified config file.\\n\\n        Args:\\n\\n            item: Item to be created\\n\\n            filename: Filename that we are supposed to write the new item to.\\n            This is the path to the file. (string)\\n\\n        Returns:\\n\\n            True on success\\n\\n        Raises:\\n\\n            :py:class:`IOError` on failed save\\n        \"\"\"\\n        if not \\'meta\\' in item:\\n            item[\\'meta\\'] = {}\\n        item[\\'meta\\'][\\'filename\\'] = filename\\n\\n        # Create directory if it does not already exist\\n        dirname = os.path.dirname(filename)\\n        if not self.isdir(dirname):\\n            os.makedirs(dirname)\\n\\n        str_buffer = self.print_conf(item)\\n        fh = self.open(filename, \\'a\\')\\n        fh.write(str_buffer)\\n        fh.close()\\n        return True', 'def compareObjects(self, item1, item2):\\n        \"\"\" Compares two items. Returns true if they are equal\\n\\n        Compares every key: value pair for both items. If anything is different,\\n        the items will not be considered equal.\\n\\n        Args:\\n            item1, item2: Items to be compared.\\n\\n        Returns:\\n\\n            True -- Items are equal\\n\\n            False -- Items are not equal\\n        \"\"\"\\n        keys1 = item1[\\'meta\\'][\\'defined_attributes\\'].keys()\\n        keys2 = item2[\\'meta\\'][\\'defined_attributes\\'].keys()\\n        keys1.sort()\\n        keys2.sort()\\n        result = True\\n        if keys1 != keys2:\\n            return False\\n        for key in keys1:\\n            if key == \\'meta\\':\\n                continue\\n            key1 = item1[key]\\n            key2 = item2[key]\\n            # For our purpose, 30 is equal to 30.000\\n            if key == \\'check_interval\\':\\n                key1 = int(float(key1))\\n                key2 = int(float(key2))\\n            if str(key1) != str(key2):\\n                result = False\\n        if result is False:\\n            return False\\n        return True', 'def _get_list(self, item, key):\\n        \"\"\" Return a comma list from an item\\n\\n        Args:\\n\\n            item: Item from which to select value. (string)\\n\\n            key: Field name of the value to select and return as a list. (string)\\n\\n        Example::\\n\\n            _get_list(Foo_object, host_name)\\n\\n            define service {\\n                service_description Foo\\n                host_name            larry,curly,moe\\n            }\\n\\n            returns\\n            [\\'larry\\',\\'curly\\',\\'moe\\']\\n\\n        Returns:\\n\\n            A list of the item\\'s values of `key`\\n\\n        Raises:\\n\\n            :py:class:`ParserError` if item is not a dict\\n        \"\"\"\\n        if not isinstance(item, dict):\\n            raise ParserError(\"%s is not a dictionary\\\\n\" % item)\\n            # return []\\n        if not key in item:\\n            return []\\n\\n        return_list = []\\n\\n        if item[key].find(\",\") != -1:\\n            for name in item[key].split(\",\"):\\n                return_list.append(name)\\n        else:\\n            return_list.append(item[key])\\n\\n        # Alphabetize\\n        return_list.sort()\\n\\n        return return_list', 'def delete_service(self, service_description, host_name):\\n        \"\"\" Delete service from configuration files\\n\\n        Args:\\n\\n            service_description: service_description field value of the object\\n            to delete from configuration files.\\n\\n            host_name: host_name field value of the object to delete from\\n            configuration files.\\n\\n        Returns:\\n\\n            True on success.\\n\\n        \"\"\"\\n        item = self.get_service(host_name, service_description)\\n        return self.item_remove(item)', 'def delete_hostgroup(self, object_name, user_key=None):\\n        \"\"\" Delete a hostgroup from its configuration files\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            True on success.\\n\\n        \"\"\"\\n        return self.delete_object(\\'hostgroup\\', object_name, user_key=user_key)', 'def get_host(self, object_name, user_key=None):\\n        \"\"\" Return a host object\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            The item found to match all the criterias.\\n\\n        \"\"\"\\n\\n        return self.get_object(\\'host\\', object_name, user_key=user_key)', 'def get_contact(self, object_name, user_key=None):\\n        \"\"\" Return a Contact object\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            The item found to match all the criterias.\\n\\n        \"\"\"\\n        return self.get_object(\\'contact\\', object_name, user_key=user_key)', 'def get_timeperiod(self, object_name, user_key=None):\\n        \"\"\" Return a Timeperiod object\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            The item found to match all the criterias.\\n\\n        \"\"\"\\n        return self.get_object(\\'timeperiod\\', object_name, user_key=user_key)', 'def get_hostgroup(self, object_name, user_key=None):\\n        \"\"\" Return a hostgroup object\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            The item found to match all the criterias.\\n\\n        \"\"\"\\n        return self.get_object(\\'hostgroup\\', object_name, user_key=user_key)', 'def get_hostdependency(self, object_name, user_key=None):\\n        \"\"\" Return a hostdependency object\\n\\n        Args:\\n\\n            object_name: object_name field value of the object to delete from\\n            configuration files.\\n\\n            user_key: user_key to pass to :py:meth:`get_object`\\n\\n        Returns:\\n\\n            The item found to match all the criterias.\\n\\n        \"\"\"\\n        return self.get_object(\\'hostdependency\\', object_name, user_key=user_key)', 'def _append_use(self, source_item, name):\\n        \"\"\" Append attributes to source_item that are inherited via \\'use\\' attribute\\'\\n\\n        Args:\\n\\n            source_item: item (dict) to apply the inheritance upon\\n\\n            name: obsolete (discovered automatically via source_item[\\'use\\'].\\n            Here for compatibility.\\n\\n        Returns:\\n\\n            Source Item with appended attributes.\\n\\n        Raises:\\n\\n            :py:class:`ParserError` on recursion errors\\n\\n        \"\"\"\\n        # Remove the \\'use\\' key\\n        if \"use\" in source_item:\\n            del source_item[\\'use\\']\\n\\n        for possible_item in self.pre_object_list:\\n            if \"name\" in possible_item:\\n                # Start appending to the item\\n                for k, v in possible_item.iteritems():\\n\\n                    try:\\n                        if k == \\'use\\':\\n                            source_item = self._append_use(source_item, v)\\n                    except Exception:\\n                        raise ParserError(\"Recursion error on %s %s\" % (source_item, v))\\n\\n                    # Only add the item if it doesn\\'t already exist\\n                    if not k in source_item:\\n                        source_item[k] = v\\n        return source_item', 'def commit(self):\\n        \"\"\" Write any changes that have been made to it\\'s appropriate file \"\"\"\\n        # Loops through ALL items\\n        for k in self.data.keys():\\n            for item in self[k]:\\n\\n                # If the object needs committing, commit it!\\n                if item[\\'meta\\'][\\'needs_commit\\']:\\n                    # Create file contents as an empty string\\n                    file_contents = \"\"\\n\\n                    # find any other items that may share this config file\\n                    extra_items = self._get_items_in_file(item[\\'meta\\'][\\'filename\\'])\\n                    if len(extra_items) > 0:\\n                        for commit_item in extra_items:\\n                            # Ignore files that are already set to be deleted:w\\n                            if commit_item[\\'meta\\'][\\'delete_me\\']:\\n                                continue\\n                                # Make sure we aren\\'t adding this thing twice\\n                            if item != commit_item:\\n                                file_contents += self.print_conf(commit_item)\\n\\n                    # This is the actual item that needs commiting\\n                    if not item[\\'meta\\'][\\'delete_me\\']:\\n                        file_contents += self.print_conf(item)\\n\\n                    # Write the file\\n                    filename = item[\\'meta\\'][\\'filename\\']\\n                    self.write(filename, file_contents)\\n\\n                    # Recreate the item entry without the commit flag\\n                    self.data[k].remove(item)\\n                    item[\\'meta\\'][\\'needs_commit\\'] = None\\n                    self.data[k].append(item)', 'def print_conf(self, item):\\n        \"\"\" Return a string that can be used in a configuration file\\n\\n        Args:\\n\\n            item: Item to be dumped as a string.\\n\\n        Returns:\\n\\n            String representation of item.\\n        \"\"\"\\n        output = \"\"\\n        # Header, to go on all files\\n        output += \"# Configuration file %s\\\\n\" % item[\\'meta\\'][\\'filename\\']\\n        output += \"# Edited by PyNag on %s\\\\n\" % time.ctime()\\n\\n        # Some hostgroup information\\n        if \"hostgroup_list\" in item[\\'meta\\']:\\n            output += \"# Hostgroups: %s\\\\n\" % \",\".join(item[\\'meta\\'][\\'hostgroup_list\\'])\\n\\n        # Some hostgroup information\\n        if \"service_list\" in item[\\'meta\\']:\\n            output += \"# Services: %s\\\\n\" % \",\".join(item[\\'meta\\'][\\'service_list\\'])\\n\\n        # Some hostgroup information\\n        if \"service_members\" in item[\\'meta\\']:\\n            output += \"# Service Members: %s\\\\n\" % \",\".join(item[\\'meta\\'][\\'service_members\\'])\\n\\n        if len(item[\\'meta\\'][\\'template_fields\\']) != 0:\\n            output += \"# Values from templates:\\\\n\"\\n        for k in item[\\'meta\\'][\\'template_fields\\']:\\n            output += \"#\\\\t %-30s %-30s\\\\n\" % (k, item[k])\\n        output += \"\\\\n\"\\n        output += \"define %s {\\\\n\" % item[\\'meta\\'][\\'object_type\\']\\n        for k, v in item.iteritems():\\n            if v is None:\\n                # Skip entries with No value\\n                continue\\n            if k != \\'meta\\':\\n                if k not in item[\\'meta\\'][\\'template_fields\\']:\\n                    output += \"\\\\t %-30s %-30s\\\\n\" % (k, v)\\n\\n        output += \"}\\\\n\\\\n\"\\n        return output', 'def _edit_static_file(self, attribute, new_value, filename=None, old_value=None, append=False):\\n        \"\"\" Modify a general config file (like nagios.cfg) that has a key=value config file format.\\n\\n        Arguments:\\n\\n            filename: Name of config file that will be edited (i.e. nagios.cfg)\\n\\n            attribute: name of attribute to edit (i.e. check_external_commands)\\n\\n            new_value: new value for the said attribute (i.e. \"1\"). None deletes\\n            the line.\\n\\n            old_value: Useful if multiple attributes exist (i.e. cfg_dir) and\\n            you want to replace a specific one.\\n\\n            append: If true, do not overwrite current setting. Instead append\\n            this at the end. Use this with settings that are repeated like\\n            cfg_file.\\n\\n        Examples::\\n\\n            _edit_static_file(filename=\\'/etc/nagios/nagios.cfg\\', attribute=\\'check_external_commands\\', new_value=\\'1\\')\\n            _edit_static_file(filename=\\'/etc/nagios/nagios.cfg\\', attribute=\\'cfg_dir\\', new_value=\\'/etc/nagios/okconfig\\', append=True)\\n        \"\"\"\\n        if filename is None:\\n            filename = self.cfg_file\\n        # For some specific attributes, append should be implied\\n        if attribute in (\\'cfg_file\\', \\'cfg_dir\\', \\'broker_module\\'):\\n            append = True\\n\\n        # If/when we make a change, new_line is what will be written\\n        new_line = \\'%s=%s\\\\n\\' % (attribute, new_value)\\n\\n        # new_value=None means line should be removed\\n        if new_value is None:\\n            new_line = \\'\\'\\n\\n        write_buffer = self.open(filename).readlines()\\n        is_dirty = False  # dirty if we make any changes\\n        for i, line in enumerate(write_buffer):\\n            # Strip out new line characters\\n            line = line.strip()\\n\\n            # Skip blank lines\\n            if line == \"\":\\n                continue\\n\\n            # Skip comments\\n            if line[0] == \"#\" or line[0] == \\';\\':\\n                continue\\n            key, value = line.split(\"=\", 1)\\n            key = key.strip()\\n            value = value.strip()\\n\\n            # If key does not match, we are not interested in this line\\n            if key != attribute:\\n                continue\\n\\n            # If old_value was specified, and it matches, dont have to look any further\\n            elif value == old_value:\\n                write_buffer[i] = new_line\\n                is_dirty = True\\n                break\\n            # if current value is the same as new_value, no need to make changes\\n            elif value == new_value:\\n                return False\\n            # Special so cfg_dir matches despite double-slashes, etc\\n            elif attribute == \\'cfg_dir\\' and new_value and os.path.normpath(value) == os.path.normpath(new_value):\\n                return False\\n            # We are not appending, and no old value was specified:\\n            elif append is False and not old_value:\\n                write_buffer[i] = new_line\\n                is_dirty = True\\n                break\\n        if is_dirty is False and new_value is not None:\\n            # If we get here, it means we read the whole file,\\n            # and we have not yet made any changes, So we assume\\n            # We should append to the file\\n            write_buffer.append(new_line)\\n            is_dirty = True\\n            # When we get down here, it is time to write changes to file\\n        if is_dirty is True:\\n            str_buffer = \\'\\'.join(write_buffer)\\n            self.write(filename, str_buffer)\\n            return True\\n        else:\\n            return False', 'def needs_reparse(self):\\n        \"\"\" Checks if the Nagios configuration needs to be reparsed.\\n\\n        Returns:\\n\\n            True if any Nagios configuration file has changed since last parse()\\n\\n        \"\"\"\\n        # If Parse has never been run:\\n        if self.data == {}:\\n            return True\\n        # If previous save operation has forced a reparse\\n        if self._is_dirty is True:\\n            return True\\n\\n        # If we get here, we check the timestamps of the configs\\n        new_timestamps = self.get_timestamps()\\n        if len(new_timestamps) != len(self.timestamps):\\n            return True\\n        for k, v in new_timestamps.items():\\n            if self.timestamps.get(k, None) != v:\\n                return True\\n        return False', 'def parse_maincfg(self):\\n        \"\"\" Parses your main configuration (nagios.cfg) and stores it as key/value pairs in self.maincfg_values\\n\\n        This function is mainly used by config.parse() which also parses your\\n        whole configuration set.\\n\\n        Raises:\\n\\n            py:class:`ConfigFileNotFound`\\n\\n        \"\"\"\\n        # If nagios.cfg is not set, lets do some minor autodiscover.\\n        if self.cfg_file is None:\\n            raise ConfigFileNotFound(\\'Could not find nagios.cfg\\')\\n\\n        self.maincfg_values = self._load_static_file(self.cfg_file)', 'def parse(self):\\n        \"\"\" Parse all objects in your nagios configuration\\n\\n        This functions starts by loading up your nagios.cfg ( parse_maincfg() )\\n        then moving on to your object configuration files (as defined via\\n        cfg_file and cfg_dir) and and your resource_file as well.\\n\\n        Returns:\\n\\n          None\\n\\n        Raises:\\n\\n          :py:class:`IOError` if unable to read any file due to permission\\n          problems\\n        \"\"\"\\n\\n        # reset\\n        self.reset()\\n\\n        self.parse_maincfg()\\n\\n        self.cfg_files = self.get_cfg_files()\\n\\n        # When parsing config, we will softly fail if permission denied\\n        # comes on resource files. If later someone tries to get them via\\n        # get_resource, we will fail hard\\n        try:\\n            self._resource_values = self.get_resources()\\n        except IOError:\\n            t, e = sys.exc_info()[:2]\\n            self.errors.append(str(e))\\n\\n        self.timestamps = self.get_timestamps()\\n\\n        # This loads everything into\\n        for cfg_file in self.cfg_files:\\n            self._load_file(cfg_file)\\n\\n        self._post_parse()\\n\\n        self._is_dirty = False', 'def get_timestamps(self):\\n        \"\"\" Returns hash map of all nagios related files and their timestamps\"\"\"\\n        files = {}\\n        files[self.cfg_file] = None\\n        for k, v in self.maincfg_values:\\n            if k in (\\'resource_file\\', \\'lock_file\\', \\'object_cache_file\\'):\\n                files[v] = None\\n        for i in self.get_cfg_files():\\n            files[i] = None\\n        # Now lets lets get timestamp of every file\\n        for k, v in files.items():\\n            if not self.isfile(k):\\n                continue\\n            files[k] = self.stat(k).st_mtime\\n        return files', 'def isdir(self, *args, **kwargs):\\n        \"\"\" Wrapper around os.path.isdir \"\"\"\\n        return os.path.isdir(*args, **kwargs)', 'def readlink(selfself, *args, **kwargs):\\n        \"\"\" Wrapper around os.readlink \"\"\"\\n        return os.readlink(*args, **kwargs)', 'def remove(self, *args, **kwargs):\\n        \"\"\" Wrapper around os.remove \"\"\"\\n        return os.remove(*args, **kwargs)', 'def listdir(self, *args, **kwargs):\\n        \"\"\" Wrapper around os.listdir \"\"\"\\n\\n        return os.listdir(*args, **kwargs)', 'def get_resources(self):\\n        \"\"\"Returns a list of every private resources from nagios.cfg\"\"\"\\n        resources = []\\n        for config_object, config_value in self.maincfg_values:\\n            if config_object == \\'resource_file\\' and self.isfile(config_value):\\n                resources += self._load_static_file(config_value)\\n        return resources', 'def _get_active_hosts(self, item):\\n        \"\"\" Given an object, return a list of active hosts.\\n\\n        This will exclude hosts that are negated with a \"!\"\\n\\n        Args:\\n\\n            item: Item to obtain active hosts from.\\n\\n        Returns:\\n\\n            List of all the active hosts for `item`\\n        \"\"\"\\n        # First, generate the negation list\\n        negate_hosts = []\\n\\n        # Hostgroups\\n        if \"hostgroup_name\" in item:\\n            for hostgroup_name in self._get_list(item, \\'hostgroup_name\\'):\\n                if hostgroup_name[0] == \"!\":\\n                    hostgroup_obj = self.get_hostgroup(hostgroup_name[1:])\\n                    negate_hosts.extend(self._get_list(hostgroup_obj, \\'members\\'))\\n\\n        # Host Names\\n        if \"host_name\" in item:\\n            for host_name in self._get_list(item, \\'host_name\\'):\\n                if host_name[0] == \"!\":\\n                    negate_hosts.append(host_name[1:])\\n\\n        # Now get hosts that are actually listed\\n        active_hosts = []\\n\\n        # Hostgroups\\n        if \"hostgroup_name\" in item:\\n            for hostgroup_name in self._get_list(item, \\'hostgroup_name\\'):\\n                if hostgroup_name[0] != \"!\":\\n                    active_hosts.extend(self._get_list(self.get_hostgroup(hostgroup_name), \\'members\\'))\\n\\n        # Host Names\\n        if \"host_name\" in item:\\n            for host_name in self._get_list(item, \\'host_name\\'):\\n                if host_name[0] != \"!\":\\n                    active_hosts.append(host_name)\\n\\n        # Combine the lists\\n        return_hosts = []\\n        for active_host in active_hosts:\\n            if active_host not in negate_hosts:\\n                return_hosts.append(active_host)\\n\\n        return return_hosts', 'def get_cfg_files(self):\\n        \"\"\" Return a list of all cfg files used in this configuration\\n\\n        Filenames are normalised so that if nagios.cfg specifies relative\\n        filenames we will convert it to fully qualified filename before returning.\\n\\n        Returns:\\n\\n            List of all configurations files used in the configuration.\\n\\n        Example:\\n\\n            print(get_cfg_files())\\n            [\\'/etc/nagios/hosts/host1.cfg\\',\\'/etc/nagios/hosts/host2.cfg\\',...]\\n\\n        \"\"\"\\n        cfg_files = []\\n        for config_object, config_value in self.maincfg_values:\\n\\n            # Add cfg_file objects to cfg file list\\n            if config_object == \"cfg_file\":\\n                config_value = self.abspath(config_value)\\n                if self.isfile(config_value):\\n                    cfg_files.append(config_value)\\n\\n            # Parse all files in a cfg directory\\n            if config_object == \"cfg_dir\":\\n                config_value = self.abspath(config_value)\\n                directories = []\\n                raw_file_list = []\\n                directories.append(config_value)\\n                # Walk through every subdirectory and add to our list\\n                while directories:\\n                    current_directory = directories.pop(0)\\n                    # Nagios doesnt care if cfg_dir exists or not, so why should we ?\\n                    if not self.isdir(current_directory):\\n                        continue\\n                    for item in self.listdir(current_directory):\\n                        # Append full path to file\\n                        item = \"%s\" % (os.path.join(current_directory, item.strip()))\\n                        if self.islink(item):\\n                            item = os.readlink(item)\\n                        if self.isdir(item):\\n                            directories.append(item)\\n                        if raw_file_list.count(item) < 1:\\n                            raw_file_list.append(item)\\n                for raw_file in raw_file_list:\\n                    if raw_file.endswith(\\'.cfg\\'):\\n                        if self.exists(raw_file) and not self.isdir(raw_file):\\n                            # Nagios doesnt care if cfg_file exists or not, so we will not throws errors\\n                            cfg_files.append(raw_file)\\n\\n        return cfg_files', 'def get_cfg_value(self, key):\\n        \"\"\" Returns one specific value from your nagios.cfg file,\\n        None if value is not found.\\n\\n        Arguments:\\n\\n            key: what attribute to fetch from nagios.cfg (example: \"command_file\" )\\n\\n        Returns:\\n\\n            String of the first value found for\\n\\n        Example:\\n\\n            >>> c = Config() # doctest: +SKIP\\n            >>> log_file = c.get_cfg_value(\\'log_file\\') # doctest: +SKIP\\n            # Should return something like \"/var/log/nagios/nagios.log\"\\n        \"\"\"\\n        if not self.maincfg_values:\\n            self.parse_maincfg()\\n        for k, v in self.maincfg_values:\\n            if k == key:\\n                return v\\n        return None', 'def cleanup(self):\\n        \"\"\" Remove configuration files that have no configuration items \"\"\"\\n        for filename in self.cfg_files:\\n            if not self.parse_file(filename):  # parse_file returns empty list on empty files\\n                self.remove(filename)\\n                # If nagios.cfg specifies this file directly via cfg_file directive then...\\n                for k, v in self.maincfg_values:\\n                    if k == \\'cfg_file\\' and v == filename:\\n                        self._edit_static_file(k, old_value=v, new_value=None)', 'def __getitem__(self, key):\\n        return self.data[key]', 'def __init__(self, livestatus_socket_path=None, nagios_cfg_file=None, authuser=None):\\n        \"\"\" Initilize a new instance of Livestatus\\n\\n        Args:\\n\\n          livestatus_socket_path: Path to livestatus socket (if none specified,\\n          use one specified in nagios.cfg)\\n\\n          nagios_cfg_file: Path to your nagios.cfg. If None then try to\\n          auto-detect\\n\\n          authuser: If specified. Every data pulled is with the access rights\\n          of that contact.\\n\\n        \"\"\"\\n        self.nagios_cfg_file = nagios_cfg_file\\n        self.error = None\\n        if not livestatus_socket_path:\\n            c = config(cfg_file=nagios_cfg_file)\\n            c.parse_maincfg()\\n            self.nagios_cfg_file = c.cfg_file\\n            # Look for a broker_module line in the main config and parse its arguments\\n            # One of the arguments is path to the file socket created\\n            for k, v in c.maincfg_values:\\n                if k == \\'broker_module\\' and \"livestatus.o\" in v:\\n                    for arg in v.split()[1:]:\\n                        if arg.startswith(\\'/\\') or \\'=\\' not in arg:\\n                            livestatus_socket_path = arg\\n                            break\\n                    else:\\n                        # If we get here, then we could not locate a broker_module argument\\n                        # that looked like a filename\\n                        msg = \"No Livestatus socket defined. Make sure livestatus broker module is loaded.\"\\n                        raise ParserError(msg)\\n        self.livestatus_socket_path = livestatus_socket_path\\n        self.authuser = authuser', 'def _get_socket(self):\\n        \"\"\" Returns a socket.socket() instance to communicate with livestatus\\n\\n        Socket might be either unix filesocket or a tcp socket depenging in\\n        the content of :py:attr:`livestatus_socket_path`\\n\\n        Returns:\\n\\n            Socket to livestatus instance (socket.socket)\\n\\n        Raises:\\n\\n            :py:class:`LivestatusNotConfiguredException` on failed connection.\\n\\n            :py:class:`ParserError` If could not parse configured TCP address\\n            correctly.\\n\\n        \"\"\"\\n        if not self.livestatus_socket_path:\\n            msg = \"We could not find path to MK livestatus socket file. Make sure MK livestatus is installed and configured\"\\n            raise LivestatusNotConfiguredException(msg)\\n        try:\\n            # If livestatus_socket_path contains a colon, then we assume that it is tcp socket instead of a local filesocket\\n            if self.livestatus_socket_path.find(\\':\\') > 0:\\n                address, tcp_port = self.livestatus_socket_path.split(\\':\\', 1)\\n                if not tcp_port.isdigit():\\n                    msg = \\'Could not parse host:port \"%s\". %s  does not look like a valid port is not a valid tcp port.\\'\\n                    raise ParserError(msg % (self.livestatus_socket_path, tcp_port))\\n                tcp_port = int(tcp_port)\\n                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n                s.connect((address, tcp_port))\\n            else:\\n                s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\\n                s.connect(self.livestatus_socket_path)\\n            return s\\n        except IOError:\\n            t, e = sys.exc_info()[:2]\\n            msg = \"%s while connecting to \\'%s\\'. Make sure nagios is running and mk_livestatus loaded.\"\\n            raise ParserError(msg % (e, self.livestatus_socket_path))', 'def get(self, table, *args, **kwargs):\\n        \"\"\" Same as self.query(\\'GET %s\\' % (table,))\\n\\n        Extra arguments will be appended to the query.\\n\\n        Args:\\n\\n            table: Table from which the data will be retrieved\\n\\n            args, kwargs: These will be appendend to the end of the query to\\n            perform additionnal instructions.\\n\\n        Example::\\n\\n            get(\\'contacts\\', \\'Columns: name alias\\')\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET %s\\' % (table,), *args, **kwargs)', 'def get_service(self, host_name, service_description):\\n        \"\"\" Performs a GET query for a particular service\\n\\n        This performs::\\n\\n            \\'\\'\\'GET services\\n            Filter: host_name = %s\\n            Filter: service_description = %s\\'\\'\\' % (host_name, service_description)\\n\\n        Args:\\n\\n            host_name: name of the host the target service is attached to.\\n\\n            service_description: Description of the service to obtain livestatus\\n            data from.\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET services\\', \\'Filter: host_name = %s\\' % host_name,\\n                          \\'Filter: description = %s\\' % service_description)[0]', 'def get_services(self, *args, **kwargs):\\n        \"\"\" Performs a GET query for all services\\n\\n        This performs::\\n\\n            \\'\\'\\'GET services\\n            %s %s\\'\\'\\' % (*args, **kwargs)\\n\\n        Args:\\n\\n            args, kwargs: These will be appendend to the end of the query to\\n            perform additionnal instructions.\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET services\\', *args, **kwargs)', 'def get_servicegroups(self, *args, **kwargs):\\n        \"\"\" Performs a GET query for all servicegroups\\n\\n        This performs::\\n\\n            \\'\\'\\'GET servicegroups\\n            %s %s\\'\\'\\' % (*args, **kwargs)\\n\\n        Args:\\n\\n            args, kwargs: These will be appendend to the end of the query to\\n            perform additionnal instructions.\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET servicegroups\\', *args, **kwargs)', 'def get_contacts(self, *args, **kwargs):\\n        \"\"\" Performs a GET query for all contacts\\n\\n        This performs::\\n\\n            \\'\\'\\'GET contacts\\n            %s %s\\'\\'\\' % (*args, **kwargs)\\n\\n        Args:\\n\\n            args, kwargs: These will be appendend to the end of the query to\\n            perform additionnal instructions.\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET contacts\\', *args, **kwargs)', 'def get_servicegroup(self, name):\\n        \"\"\" Performs a GET query for a particular servicegroup\\n\\n        This performs::\\n\\n            \\'\\'\\'GET servicegroups\\n            Filter: servicegroup_name = %s\\'\\'\\' % servicegroup_name\\n\\n        Args:\\n\\n            servicegroup_name: name of the servicegroup to obtain livestatus data from\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET servicegroups\\', \\'Filter: name = %s\\' % name)[0]', 'def get_contactgroup(self, name):\\n        \"\"\" Performs a GET query for a particular contactgroup\\n\\n        This performs::\\n\\n            \\'\\'\\'GET contactgroups\\n            Filter: contactgroup_name = %s\\'\\'\\' % contactgroup_name\\n\\n        Args:\\n\\n            contactgroup_name: name of the contactgroup to obtain livestatus data from\\n\\n        Returns:\\n\\n            Answer from livestatus in python format.\\n\\n        \"\"\"\\n        return self.query(\\'GET contactgroups\\', \\'Filter: name = %s\\' % name)[0]', 'def __init__(self, filename=None, cfg_file=None):\\n        \"\"\" Initilize a new instance of retention.dat\\n\\n        Args (you only need to provide one of these):\\n\\n            filename: path to your retention.dat file\\n\\n            cfg_file: path to your nagios.cfg file, path to retention.dat will\\n            be looked up in this file\\n\\n        \"\"\"\\n        # If filename is not provided, lets try to discover it from\\n        # nagios.cfg\\n        if filename is None:\\n            c = config(cfg_file=cfg_file)\\n            for key, value in c._load_static_file():\\n                if key == \"state_retention_file\":\\n                    filename = value\\n\\n        self.filename = filename\\n        self.data = None', 'def __setitem__(self, key, item):\\n        self.data[key] = item', 'def __str__(self):\\n        if not self.data:\\n            self.parse()\\n        str_buffer = \"# Generated by pynag\"\\n        for datatype, datalist in self.data.items():\\n            for item in datalist:\\n                str_buffer += \"%s {\\\\n\" % datatype\\n                for attr, value in item.items():\\n                    str_buffer += \"%s=%s\\\\n\" % (attr, value)\\n                str_buffer += \"}\\\\n\"\\n        return str_buffer', 'def __init__(self, filename=None, cfg_file=None):\\n        \"\"\" Initilize a new instance of status\\n\\n        Args (you only need to provide one of these):\\n\\n            filename: path to your status.dat file\\n\\n            cfg_file: path to your nagios.cfg file, path to status.dat will be\\n            looked up in this file\\n\\n        \"\"\"\\n        # If filename is not provided, lets try to discover it from\\n        # nagios.cfg\\n        if filename is None:\\n            c = config(cfg_file=cfg_file)\\n            for key, value in c._load_static_file():\\n                if key == \"status_file\":\\n                    filename = value\\n\\n        self.filename = filename\\n        self.data = None', 'def get_hoststatus(self, host_name):\\n        \"\"\" Returns a dictionary derived from status.dat for one particular contact\\n\\n        Args:\\n\\n            host_name: `host_name` field of the host\\'s status.dat data\\n            to parse and return as a dict.\\n\\n        Returns:\\n\\n            dict derived from status.dat for the host.\\n\\n        Raises:\\n\\n            ValueError if object is not found\\n        \"\"\"\\n        if self.data is None:\\n            self.parse()\\n        for i in self.data[\\'hoststatus\\']:\\n            if i.get(\\'host_name\\') == host_name:\\n                return i\\n        raise ValueError(host_name)', \"def get_cfg_files(self):\\n        for k, v in self.maincfg_values:\\n            if k == 'object_cache_file':\\n                return [v]\", 'def __init__(self, message, item=None):\\n        \"\"\" Creates an instance of ParserError\\n\\n        Args:\\n\\n            message: Message to be printed by the error\\n\\n            item: Pynag item who caused the error\\n\\n        \"\"\"\\n        self.message = message\\n        if item is None:\\n            return\\n        self.item = item\\n        self.filename = item[\\'meta\\'][\\'filename\\']\\n        self.line_start = item[\\'meta\\'].get(\\'line_start\\')', \"def __init__(self, maincfg=None):\\n        self.config = config(maincfg)\\n\\n        self.log_file = self.config.get_cfg_value('log_file')\\n        self.log_archive_path = self.config.get_cfg_value('log_archive_path')\", 'def get_logfiles(self):\\n        \"\"\" Returns a list with the fullpath to every log file used by nagios.\\n\\n        Lists are sorted by modification times. Newest logfile is at the front\\n        of the list so usually nagios.log comes first, followed by archivelogs\\n\\n        Returns:\\n\\n            List of strings\\n\\n        \"\"\"\\n        logfiles = []\\n\\n        for filename in os.listdir(self.log_archive_path):\\n            full_path = \"%s/%s\" % (self.log_archive_path, filename)\\n            logfiles.append(full_path)\\n        logfiles.append(self.log_file)\\n\\n        # Sort the logfiles by modification time, newest file at the front\\n        compare_mtime = lambda a, b: os.stat(a).st_mtime < os.stat(b).st_mtime\\n        logfiles.sort(key=lambda x: int(os.stat(x).st_mtime))\\n\\n        # Newest logfiles go to the front of the list\\n        logfiles.reverse()\\n\\n        return logfiles', 'def get_notifications(self, **kwargs):\\n        \"\"\" Same as :py:meth:`get_log_entries`, except return only notifications.\\n        Takes same parameters.\\n        \"\"\"\\n        return self.get_log_entries(class_name=\"notification\", **kwargs)', 'def _parse_log_file(self, filename=None):\\n        \"\"\" Parses one particular nagios logfile into arrays of dicts.\\n\\n        Args:\\n\\n            filename: Log file to be parsed. If is None, then log_file from\\n            nagios.cfg is used.\\n\\n        Returns:\\n\\n            A list of dicts containing all data from the log file\\n        \"\"\"\\n        if filename is None:\\n            filename = self.log_file\\n        result = []\\n        for line in open(filename).readlines():\\n            parsed_entry = self._parse_log_line(line)\\n            if parsed_entry != {}:\\n                parsed_entry[\\'filename\\'] = filename\\n                result.append(parsed_entry)\\n        return result', 'def __init__(self, section_name=None, config_file=None):\\n        if not section_name:\\n            section_name = self.get_default_section_name()\\n        if not config_file:\\n            config_file = self.get_default_config_file()\\n        self.section_name = section_name\\n        self.config_file = config_file\\n        self._all_options = self.parse_file(filename=config_file) or {}', 'def get_default_section_name(self):\\n        \"\"\" According to extra-opts standard, the default should be filename of check script being run \"\"\"\\n        return os.path.basename(sys.argv[0])', 'def get(self, option_name, default=_sentinel):\\n        \"\"\" Return the value of one specific option\\n\\n        Args:\\n\\n            option_name: The value set to this option will be returned\\n\\n        Returns:\\n\\n            The value of `option_name`\\n\\n        Raises:\\n\\n            :py:class:`ValueError` when `option_name` cannot be found in options\\n\\n        \"\"\"\\n        result = self.getlist(option_name, default)\\n\\n        # If option was not found, raise error\\n        if result == _sentinel:\\n            raise ValueError(\"Option named %s was not found\" % (option_name))\\n        elif result == default:\\n            return result\\n        elif not result:\\n            # empty list\\n            return result\\n        else:\\n            return result[0]', 'def parse_file(self, filename):\\n        \"\"\" Parses an ini-file and returns a dict of the ini values.\\n\\n        The datatype returned is a list of sections where each section is a\\n        dict of values.\\n\\n        Args:\\n\\n            filename: Full path to the ini-file to be parsed.\\n\\n        Example the following the file::\\n\\n            [main]\\n            name = this is a name\\n            key = value\\n            key = value2\\n\\n        Would return::\\n\\n            [\\n              {\\'main\\':\\n                {\\n                  \\'name\\': [\\'this is a name\\'],\\n                  \\'key\\': [value, value2]\\n                }\\n              },\\n            ]\\n\\n        \"\"\"\\n        if filename is None:\\n            return {}\\n\\n        f = open(filename)\\n        try:\\n            data = f.read()\\n            return self.parse_string(data)\\n        finally:\\n            f.close()', 'def __init__(self, host, username, password=None, cfg_file=None):\\n        \"\"\" Creates a SshConfig instance\\n\\n        Args:\\n\\n            host: Host to connect to\\n\\n            username: User to connect with\\n\\n            password: Password for `username`\\n\\n            cfg_file: Nagios main cfg file\\n        \"\"\"\\n        import paramiko\\n        self.ssh = paramiko.SSHClient()\\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\\n        self.ssh.connect(host, username=username, password=password)\\n        self.ftp = self.ssh.open_sftp()\\n\\n        import cStringIO\\n        c = cStringIO.StringIO()\\n        self.tar = tarfile.open(mode=\\'w\\', fileobj=c)\\n\\n        self.cached_stats = {}\\n        super(SshConfig, self).__init__(cfg_file=cfg_file)', 'def add_to_tar(self, path):\\n        \"\"\"\\n        \"\"\"\\n        print \"Taring \", path\\n        command = \"find \\'{path}\\' -type f | tar -c -T - --to-stdout --absolute-names\"\\n        command = command.format(path=path)\\n        print command\\n        stdin, stdout, stderr = self.ssh.exec_command(command, bufsize=50000)\\n        tar = tarfile.open(fileobj=stdout, mode=\\'r|\\')\\n        if not self.tar:\\n            self.tar = tar\\n            # return\\n        else:\\n            for i in tar:\\n                self.tar.addfile(i)', 'def _get_file(self, filename):\\n        \"\"\" Download filename and return the TarInfo object \"\"\"\\n        if filename not in self.tar.getnames():\\n            self.add_to_tar(filename)\\n        return self.tar.getmember(filename)', 'def isfile(self, path):\\n        \"\"\" Behaves like os.path.isfile only, via ssh connection \"\"\"\\n        try:\\n            copy = self._get_file(path)\\n            return copy.isfile()\\n        except IOError:\\n            return False', 'def islink(self, path):\\n        \"\"\" Behaves like os.path.islink only, via ssh connection \"\"\"\\n        try:\\n            file_stat = self.stat(path)\\n            return stat.S_ISLNK(file_stat.st_mode)\\n        except IOError:\\n            return False', 'def stat(self, *args, **kwargs):\\n        \"\"\" Wrapper around os.stat only, via ssh connection \"\"\"\\n        path = args[0]\\n        if not self.is_cached(path):\\n            self.add_to_tar(path)\\n        if path not in self.tar.getnames():\\n            raise IOError(\"No such file or directory %s\" % path)\\n        member = self.tar.getmember(path)\\n        member.st_mode = member.mode\\n        member.st_mtime = member.mtime\\n        return member', 'def exists(self, path):\\n        \"\"\" Wrapper around os.path.exists only, via ssh connection \"\"\"\\n        try:\\n            self.ftp.stat(path)\\n            return True\\n        except IOError:\\n            return False', 'def __init__(self, *args, **kwargs):\\n        super(MultiSite, self).__init__(*args, **kwargs)\\n        self.backends = {}', 'def get_backends(self):\\n        \"\"\" Returns a list of mk_livestatus instances\\n\\n        Returns:\\n            list. List of mk_livestatus instances\\n        \"\"\"\\n        return self.backends', 'def query(self, query, *args, **kwargs):\\n        \"\"\" Behaves like mk_livestatus.query() except results are aggregated from multiple backends\\n\\n        Arguments:\\n            backend (str): If specified, fetch only data from this backend (see add_backend())\\n            *args:         Passed directly to mk_livestatus.query()\\n            **kwargs:      Passed directly to mk_livestatus.query()\\n        \"\"\"\\n        result = []\\n        backend = kwargs.pop(\\'backend\\', None)\\n\\n        # Special hack, if \\'Stats\\' argument was provided to livestatus\\n        # We have to maintain compatibility with old versions of livestatus\\n        # and return single list with all results instead of a list of dicts\\n        doing_stats = any(map(lambda x: x.startswith(\\'Stats:\\'), args + (query,)))\\n\\n        # Iterate though all backends and run the query\\n        # TODO: Make this multithreaded\\n        for name, backend_instance in self.backends.items():\\n            # Skip if a specific backend was requested and this is not it\\n            if backend and backend != name:\\n                continue\\n\\n            query_result = backend_instance.query(query, *args, **kwargs)\\n            if doing_stats:\\n                result = self._merge_statistics(result, query_result)\\n            else:\\n                for row in query_result:\\n                    row[\\'backend\\'] = name\\n                    result.append(row)\\n\\n        return result', 'def get_host(self, host_name, backend=None):\\n        \"\"\" Same as Livestatus.get_host() \"\"\"\\n        backend = self.get_backend(backend)\\n        return backend.get_host(host_name)', 'def get_contact(self, contact_name, backend=None):\\n        \"\"\" Same as Livestatus.get_contact() \"\"\"\\n        backend = self.get_backend(backend)\\n        return backend.get_contact(contact_name)', 'def get_servicegroup(self, servicegroup_name, backend=None):\\n        \"\"\" Same as Livestatus.get_servicegroup() \"\"\"\\n        backend = self.get_backend(backend)\\n        return backend.get_servicegroup(servicegroup_name)']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.ll = generate_ll()\\n        self.pl = self.ll.pl', \"def test_if_already_added_still_succeeds(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        t1.prioritize_before.append(t2)\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\\n\\n        # when\\n        results = self.ll.do_add_prioritize_before_to_task(t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\\n        self.assertIsNotNone(results)\\n        self.assertEqual([t1, t2], list(results))\", \"def test_null_user_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n\\n        # expect\\n        self.assertRaises(ValueError, self.ll.do_add_prioritize_before_to_task,\\n                          t1.id, t2.id, None)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\", \"def test_user_not_authorized_for_prioritize_before_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n\\n        # expect\\n        self.assertRaises(Forbidden, self.ll.do_add_prioritize_before_to_task,\\n                          t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\", \"def test_prioritize_before_not_found_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\\n\\n        # expect\\n        self.assertRaises(NotFound, self.ll.do_add_prioritize_before_to_task,\\n                          t1.id, t1.id + 1, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\", \"def test_if_prioritize_before_already_removed_still_succeeds(self):\\n\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n\\n        # when\\n        results = self.ll.do_remove_prioritize_before_from_task(t1.id, t2.id,\\n                                                                user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertIsNotNone(results)\\n        self.assertEqual([t1, t2], list(results))\", \"def test_remove_prioritize_before_with_null_user_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        t1.prioritize_before.append(t2)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\\n\\n        # expect\\n        self.assertRaises(ValueError,\\n                          self.ll.do_remove_prioritize_before_from_task,\\n                          t1.id, t2.id, None)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\", \"def test_remove_user_not_authd_for_prioritizebefore_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t1.prioritize_before.append(t2)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n        # note that this situation shouldn't happen anyways. a task shouldn't\\n        # be prioritized before another task unless both share a common set of\\n        # one or more authorized users\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\\n\\n        # expect\\n        self.assertRaises(Forbidden,\\n                          self.ll.do_remove_prioritize_before_from_task,\\n                          t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t2.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertTrue(t2 in t1.prioritize_before)\\n        self.assertTrue(t1 in t2.prioritize_after)\", \"def test_remove_prioritize_before_when_not_found_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\\n\\n        # expect\\n        self.assertRaises(NotFound,\\n                          self.ll.do_remove_prioritize_before_from_task,\\n                          t1.id, t1.id + 1, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\", 'def setUp(self):\\n        self.ll = generate_ll()\\n        self.pl = self.ll.pl', \"def test_if_already_added_still_succeeds(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        t1.prioritize_after.append(t2)\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\\n\\n        # when\\n        results = self.ll.do_add_prioritize_after_to_task(t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\\n        self.assertIsNotNone(results)\\n        self.assertEqual([t1, t2], list(results))\", \"def test_null_user_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n\\n        # expect\\n        self.assertRaises(ValueError, self.ll.do_add_prioritize_after_to_task,\\n                          t1.id, t2.id, None)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\", \"def test_user_not_authorized_for_prioritize_after_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n\\n        # expect\\n        self.assertRaises(Forbidden, self.ll.do_add_prioritize_after_to_task,\\n                          t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\", \"def test_prioritize_after_not_found_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\\n\\n        # expect\\n        self.assertRaises(NotFound, self.ll.do_add_prioritize_after_to_task,\\n                          t1.id, t1.id + 1, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertIsNone(self.pl.get_task(t1.id + 1))\", \"def test_if_prioritize_after_already_removed_still_succeeds(self):\\n\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n\\n        # when\\n        results = self.ll.do_remove_prioritize_after_from_task(t1.id, t2.id,\\n                                                               user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(0, len(t1.prioritize_after))\\n        self.assertEqual(0, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertIsNotNone(results)\\n        self.assertEqual([t1, t2], list(results))\", \"def test_remove_prioritize_after_with_null_user_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t2.users.append(user)\\n        t1.prioritize_after.append(t2)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\\n\\n        # expect\\n        self.assertRaises(ValueError,\\n                          self.ll.do_remove_prioritize_after_from_task,\\n                          t1.id, t2.id, None)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\", \"def test_remove_user_not_authd_for_prioritize_after_raises_exception(self):\\n        # given\\n        t1 = self.pl.create_task('t1')\\n        t2 = self.pl.create_task('t2')\\n        user = self.pl.create_user('name@example.com')\\n        t1.users.append(user)\\n        t1.prioritize_after.append(t2)\\n        self.pl.add(t1)\\n        self.pl.add(t2)\\n        self.pl.add(user)\\n        self.pl.commit()\\n        # note that this situation shouldn't happen anyways. a task shouldn't\\n        # be prioritized before another task unless both share a common set of\\n        # one or more authorized users\\n\\n        # precondition\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\\n\\n        # expect\\n        self.assertRaises(Forbidden,\\n                          self.ll.do_remove_prioritize_after_from_task,\\n                          t1.id, t2.id, user)\\n\\n        # then\\n        self.assertEqual(0, len(t1.prioritize_before))\\n        self.assertEqual(1, len(t1.prioritize_after))\\n        self.assertEqual(1, len(t2.prioritize_before))\\n        self.assertEqual(0, len(t2.prioritize_after))\\n        self.assertTrue(t2 in t1.prioritize_after)\\n        self.assertTrue(t1 in t2.prioritize_before)\"]}, {'features': [], 'snippets': ['def __init__(self, tableName, tableColumns=[], coreInfo={}):\\n        self.tableName = tableName\\n        self.columnsDict = OrderedDict(tableColumns)\\n        self.dbFile = os.path.join(os.getcwd().replace(\"python\", \"metadata\"), \"libretro.sqlite\")\\n        self.dbFileExists = os.path.isfile(self.dbFile)\\n        self.coreInfo = coreInfo\\n\\n        # self.filterUnusedCores()', 'def updateColumns(self, database, additionalStatement: str = \"\"):\\n\\n        if not self.dbFileExists:\\n            database.createTable(self.tableName, self.columnsDict, additionalStatement)\\n        else:\\n            try:\\n                database.deleteTable(self.tableName)\\n            except:\\n                database.createTable(self.tableName, self.columnsDict, additionalStatement)', 'def libretroSystemList(self):\\n        systems = []\\n        for k, v in self.coreInfo[\\'cores\\'].items():\\n\\n            if \"categories\" not in v or v[\"categories\"] != \"Emulator\":\\n                continue\\n\\n            if \"database\" in v:\\n                name = v[\"database\"].split(\"|\")\\n\\n                for n in name:\\n                    systems.append(n)\\n\\n                    # Split console and manufacturer names\\n                    # Not really necessary for Libretro identifiers\\n                    #tup = n.split(\" - \")\\n                    #\\n                    ## \"MAME\"\\n                    #if len(tup) == 1:\\n                    #    systems.append(tup[0])\\n                    #\\n                    ## Nearly every one\\n                    #elif len(tup) == 2:\\n                    #    systems.append(tup[1])\\n                    #\\n                    ## Sega - Master System - Mark III\\n                    ## Sega - Mega Drive - Genesis\\n                    #elif len(tup) == 3:\\n                    #    systems.append(tup[1])\\n\\n            # There are some cores that do not have \"database\" defined\\n            elif \"systemname\" in v:\\n                systems.append(v[\"systemname\"])\\n\\n        systems = list(set(systems))\\n        systems.sort()\\n        return systems', 'def phoenixSystems(self):\\n        return OrderedDict(sorted(self.phoenixSystemDatabase.items(), key=lambda t: t[0]))', 'def phoenixToOpenVGDB(self, phoenixID):\\n        ret = \"\"\\n        try:\\n            ret = self.phoenixToOpenVGDBMap[phoenixID]\\n        except KeyError:\\n            ret = \"\"\\n        return ret', 'def getOpenVGDBToPhoenixMap(self):\\n        return OrderedDict(sorted(self.openVGDBToPhoenixMap.items(), key=lambda t: t[0]))']}, {'features': [], 'snippets': [\"def authenticate(self, assertion):\\n        logging.warning('entering authenticate function')\\n        response = requests.post(\\n            PERSONA_VERIFY_URL,\\n            data = {'assertion': assertion, 'audience': settings.DOMAIN}\\n        )\\n        logging.warning('got response from persona')\\n        logging.warning(response.content.decode())\\n        if response.ok and response.json()['status'] == 'okay':\\n            email = response.json()['email']\\n            try:\\n                return User.objects.get(email=email)\\n            except User.DoesNotExist:\\n                return User.objects.create(email=email)\\n        else:\\n            logger.warning(\\n                'Persona says no. Json was: {}'.format(response.json())\\n                )\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def clean_setup_provider(request, provider):\\n    BaseProvider.clear_providers()\\n    setup_or_skip(request, provider)\\n    yield\\n    BaseProvider.clear_providers()', 'def host_count(appliance, metrics_tbl, mgmt_system_id):\\n    return bool(appliance.db.client.session.query(metrics_tbl).filter(\\n        metrics_tbl.parent_ems_id == mgmt_system_id).filter(\\n        metrics_tbl.resource_type == \"Host\").count()\\n    )', 'def metrics_collection(appliance, clean_setup_provider, provider, enable_candu):\\n    \"\"\"Check the db is gathering collection data for the given provider.\\n\\n    Metadata:\\n        test_flag: metrics_collection\\n    \"\"\"\\n    metrics_tbl = appliance.db.client[\\'metrics\\']\\n    mgmt_systems_tbl = appliance.db.client[\\'ext_management_systems\\']\\n\\n    logger.info(\"Fetching provider ID for %s\", provider.key)\\n    mgmt_system_id = appliance.db.client.session.query(mgmt_systems_tbl).filter(\\n        mgmt_systems_tbl.name == conf.cfme_data.get(\\'management_systems\\', {})[provider.key][\\'name\\']\\n    ).first().id\\n\\n    logger.info(\"ID fetched; testing metrics collection now\")\\n\\n    # vms for both infa and cloud provider\\n    wait_for(\\n        vm_count, [appliance, metrics_tbl, mgmt_system_id],\\n        delay=20,\\n        timeout=1500,\\n        fail_condition=False,\\n        message=\"wait for VMs\")\\n\\n    # host only for infa\\n    if provider.category == \"infra\":\\n        wait_for(\\n            vm_count, [appliance, metrics_tbl, mgmt_system_id],\\n            delay=20,\\n            timeout=1500,\\n            fail_condition=False,\\n            message=\"wait for hosts.\")', \"def query_metric_db(appliance, provider, metric, vm_name=None, host_name=None):\\n    metrics_tbl = appliance.db.client['metrics']\\n    ems = appliance.db.client['ext_management_systems']\\n    if vm_name is None:\\n        if host_name is not None:\\n            object_name = host_name\\n    elif vm_name is not None:\\n        object_name = vm_name\\n\\n    with appliance.db.client.transaction:\\n        provs = (\\n            appliance.db.client.session.query(metrics_tbl.id)\\n            .join(ems, metrics_tbl.parent_ems_id == ems.id)\\n            .filter(metrics_tbl.resource_name == object_name,\\n            ems.name == provider.name)\\n        )\\n    return appliance.db.client.session.query(metrics_tbl).filter(\\n        metrics_tbl.id.in_(provs.subquery()))\", 'def test_raw_metric_vm_cpu(metrics_collection, appliance, provider):\\n    vm_name = provider.data[\\'cap_and_util\\'][\\'capandu_vm\\']\\n    if provider.category == \"infra\":\\n        query = query_metric_db(appliance, provider, \\'cpu_usagemhz_rate_average\\',\\n            vm_name)\\n        average_rate = attrgetter(\\'cpu_usagemhz_rate_average\\')\\n    elif provider.category == \"cloud\":\\n        query = query_metric_db(appliance, provider, \\'cpu_usage_rate_average\\',\\n            vm_name)\\n        average_rate = attrgetter(\\'cpu_usage_rate_average\\')\\n\\n    for record in query:\\n        if average_rate(record) is not None:\\n            assert average_rate(record) > 0, \\'Zero VM CPU Usage\\'\\n            break', \"def test_raw_metric_vm_memory(metrics_collection, appliance, provider):\\n    vm_name = provider.data['cap_and_util']['capandu_vm']\\n\\n    if provider.type == 'azure':\\n        query = query_metric_db(appliance, provider, 'mem_usage_absolute_average',\\n            vm_name)\\n        average_rate = attrgetter('mem_usage_absolute_average')\\n    else:\\n        query = query_metric_db(appliance, provider, 'derived_memory_used',\\n            vm_name)\\n        average_rate = attrgetter('derived_memory_used')\\n\\n    for record in query:\\n        if average_rate(record) is not None:\\n            assert average_rate(record) > 0, 'Zero VM Memory Usage'\\n            break\", \"def test_raw_metric_vm_network(metrics_collection, appliance, provider):\\n    vm_name = provider.data['cap_and_util']['capandu_vm']\\n    query = query_metric_db(appliance, provider, 'net_usage_rate_average',\\n        vm_name)\\n\\n    for record in query:\\n        if record.net_usage_rate_average is not None:\\n            assert record.net_usage_rate_average > 0, 'Zero VM Network IO'\\n            break\", \"def test_raw_metric_vm_disk(metrics_collection, appliance, provider):\\n    vm_name = provider.data['cap_and_util']['capandu_vm']\\n    query = query_metric_db(appliance, provider, 'disk_usage_rate_average',\\n        vm_name)\\n\\n    for record in query:\\n        if record.disk_usage_rate_average is not None:\\n            assert record.disk_usage_rate_average > 0, 'Zero VM Disk IO'\\n            break\", \"def test_raw_metric_host_cpu(metrics_collection, appliance, provider):\\n    host_name = get_host_name(provider)\\n    query = query_metric_db(appliance, provider, 'cpu_usagemhz_rate_average',\\n        host_name)\\n\\n    for record in query:\\n        if record.cpu_usagemhz_rate_average is not None:\\n            assert record.cpu_usagemhz_rate_average > 0, 'Zero Host CPU Usage'\\n            break\", \"def test_raw_metric_host_memory(metrics_collection, appliance, provider):\\n    host_name = get_host_name(provider)\\n    query = query_metric_db(appliance, provider, 'derived_memory_used',\\n        host_name)\\n\\n    for record in query:\\n        if record.derived_memory_used is not None:\\n            assert record.derived_memory_used > 0, 'Zero Host Memory Usage'\\n            break\", \"def test_raw_metric_host_network(metrics_collection, appliance, provider):\\n    host_name = get_host_name(provider)\\n    query = query_metric_db(appliance, provider, 'net_usage_rate_average',\\n        host_name)\\n\\n    for record in query:\\n        if record.net_usage_rate_average is not None:\\n            assert record.net_usage_rate_average > 0, 'Zero Host Network IO'\\n            break\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def read_dataset(datafile):\\n    f=open(datafile,'r')\\n    ds=[]\\n    for line in f:\\n        ds.append(line.decode('utf-8').split())\\n    return ds\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def main():\\r\\n    pass']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_input_var_names(self):\\n\\n        #--------------------------------------------------------\\n        # Note: These are currently variables needed from other\\n        #       components vs. those read from files or GUI.\\n        #--------------------------------------------------------   \\n        return self._input_var_names', 'def get_output_var_names(self):', 'def get_var_name(self, long_var_name):', 'def get_var_units(self, long_var_name):\\n\\n        return self._var_units_map[ long_var_name ]', \"def set_constants(self):\\n\\n        #------------------------\\n        # Define some constants\\n        #------------------------\\n        self.g          = np.float64(9.81)    # (gravitation const.)\\n        self.aval       = np.float64(0.476)   # (integration const.)\\n        self.kappa      = np.float64(0.408)   # (von Karman's const.)\\n        self.law_const  = np.sqrt(self.g) / self.kappa\\n        self.one_third  = np.float64(1.0) / 3.0        \\n        self.two_thirds = np.float64(2.0) / 3.0\\n        self.deg_to_rad = np.pi / 180.0\", 'def initialize(self, cfg_file=None, mode=\"nondriver\", SILENT=False): \\n\\n        if not(SILENT):\\n            print \\' \\'\\n            print \\'Channels component: Initializing...\\'', \"def update(self, dt=-1.0):\\n\\n        #---------------------------------------------\\n        # Note that u and d from previous time step\\n        # must be used on RHS of the equations here.\\n        #---------------------------------------------\\n        self.status = 'updating'  # (OpenMI 2.0 convention)\\n\\n        #-------------------------------------------------------\\n        # There may be times where we want to call this method\\n        # even if component is not the driver.  But note that\\n        # the TopoFlow driver also makes this same call.\\n        #-------------------------------------------------------\\n        if (self.mode == 'driver'):\\n            self.print_time_and_value(self.Q_outlet, 'Q_out', '[m^3/s]')\\n                                      ### interval=0.5)  # [seconds]\\n\\n        # For testing (5/19/12)\\n        # self.print_time_and_value(self.Q_outlet, 'Q_out', '[m^3/s]  CHANNEL')\", \"def finalize(self):\\n\\n        #---------------------------------------------------\\n        # We can compute mins and maxes in the final grids\\n        # here, but the framework will not then pass them\\n        # to any component (e.g. topoflow_driver) that may\\n        # need them.\\n        #---------------------------------------------------\\n        REPORT = True\\n        self.update_mins_and_maxes( REPORT=REPORT )  ## (2/6/13)\\n        self.print_final_report(comp_name='Channels component')\", 'def set_computed_input_vars(self):\\n\\n        #---------------------------------------------------------------    \\n        # Note: The initialize() method calls initialize_config_vars()\\n        #       (in BMI_base.py), which calls this method at the end.\\n        #--------------------------------------------------------------\\n        cfg_extension = self.get_attribute( \\'cfg_extension\\' ).lower()\\n        # cfg_extension = self.get_cfg_extension().lower()\\n        self.KINEMATIC_WAVE = (\"kinematic\" in cfg_extension)\\n        self.DIFFUSIVE_WAVE = (\"diffusive\" in cfg_extension)\\n        self.DYNAMIC_WAVE   = (\"dynamic\"   in cfg_extension)\\n\\n        ##########################################################\\n        # (5/17/12) If MANNING, we need to set z0vals to -1 so\\n        # they are always defined for use with new framework.\\n        ##########################################################\\n        if (self.MANNING):\\n            if (self.nval != None):\\n                self.nval = np.float64( self.nval )  #### 10/9/10, NEED\\n                self.nval_min = self.nval.min()\\n                self.nval_max = self.nval.max()\\n            #-----------------------------------\\n            self.z0val     = np.float64(-1)\\n            self.z0val_min = np.float64(-1)\\n            self.z0val_max = np.float64(-1)', 'def initialize_d8_vars(self):\\n\\n        #---------------------------------------------\\n        # Compute and store a variety of (static) D8\\n        # flow grid variables.  Embed structure into\\n        # the \"channel_base\" component.\\n        #---------------------------------------------\\n        self.d8 = d8_base.d8_component()\\n        ###############################################\\n        # (5/13/10)  Do next line here for now, until\\n        # the d8 cfg_file includes static prefix.\\n        # Same is done in GW_base.py.\\n        ###############################################\\n        # tf_d8_base.read_grid_info() also needs\\n        # in_directory to be set. (10/27/11)\\n        ###############################################', 'def initialize_computed_vars(self):', 'def initialize_diversion_vars(self):\\n\\n        #-----------------------------------------\\n        # Compute source IDs from xy coordinates\\n        #-----------------------------------------\\n        source_rows     = np.int32( self.sources_y / self.ny )\\n        source_cols     = np.int32( self.sources_x / self.nx )\\n        self.source_IDs = (source_rows, source_cols)\\n        ## self.source_IDs = (source_rows * self.nx) + source_cols', \"def initialize_outlet_values(self):\\n\\n        #---------------------------------------------------\\n        # Note:  These are retrieved and used by TopoFlow\\n        #        for the stopping condition.  TopoFlow\\n        #        receives a reference to these, but in\\n        #        order to see the values change they need\\n        #        to be stored as mutable, 1D numpy arrays.\\n        #---------------------------------------------------\\n        # Note:  Q_last is internal to TopoFlow.\\n        #---------------------------------------------------        \\n        # self.Q_outlet = self.Q[ self.outlet_ID ]\\n        self.Q_outlet = self.initialize_scalar(0, dtype='float64')\\n        self.u_outlet = self.initialize_scalar(0, dtype='float64')\\n        self.d_outlet = self.initialize_scalar(0, dtype='float64')\\n        self.f_outlet = self.initialize_scalar(0, dtype='float64')\", \"def initialize_peak_values(self):\\n\\n        #-------------------------\\n        # Initialize peak values\\n        #-------------------------\\n        self.Q_peak  = self.initialize_scalar(0, dtype='float64')\\n        self.T_peak  = self.initialize_scalar(0, dtype='float64')\\n        self.u_peak  = self.initialize_scalar(0, dtype='float64')\\n        self.Tu_peak = self.initialize_scalar(0, dtype='float64') \\n        self.d_peak  = self.initialize_scalar(0, dtype='float64')\\n        self.Td_peak = self.initialize_scalar(0, dtype='float64')\", \"def initialize_min_and_max_values(self):\\n\\n        #-------------------------------\\n        # Initialize min & max values\\n        # (2/3/13), for new framework.\\n        #-------------------------------\\n        v = 1e6\\n        self.Q_min = self.initialize_scalar(v,  dtype='float64')\\n        self.Q_max = self.initialize_scalar(-v, dtype='float64')\\n        self.u_min = self.initialize_scalar(v,  dtype='float64')\\n        self.u_max = self.initialize_scalar(-v, dtype='float64')\\n        self.d_min = self.initialize_scalar(v,  dtype='float64')\\n        self.d_max = self.initialize_scalar(-v, dtype='float64')\", 'def update_R(self):\\n\\n        #----------------------------------------\\n        # Compute the \"excess rainrate\", R.\\n        # Each term must have same units: [m/s]\\n        # Sum = net gain/loss rate over pixel.\\n        #----------------------------------------------------\\n        # R can be positive or negative.  If negative, then\\n        # water is removed from the surface at rate R until\\n        # surface water is consumed.\\n        #--------------------------------------------------------------\\n        # P  = precip_rate   [m/s]  (converted by read_input_data()).\\n        # SM = snowmelt rate [m/s]\\n        # GW = seep rate     [m/s]  (water_table intersects surface)\\n        # ET = evap rate     [m/s]\\n        # IN = infil rate    [m/s]\\n        # MR = icemelt rate  [m/s]\\n\\n        #------------------------------------------------------------\\n        # Use refs to other comp vars from new framework. (5/18/12)\\n        #------------------------------------------------------------         \\n        P  = self.P_rain  # (This is now liquid-only precip. 9/14/14)\\n        SM = self.SM\\n        GW = self.GW\\n        ET = self.ET\\n        IN = self.IN\\n        MR = self.MR', 'def update_R_integral(self):\\n\\n        #-----------------------------------------------\\n        # Update mass total for R, sum over all pixels\\n        #-----------------------------------------------   \\n        volume = np.double(self.R * self.da * self.dt)  # [m^3]\\n        if (np.size(volume) == 1):\\n            self.vol_R += (volume * self.rti.n_pixels)\\n        else:\\n            self.vol_R += np.sum(volume)', \"def update_discharge(self):\\n\\n        #---------------------------------------------------------\\n        # The discharge grid, Q, gives the flux of water _out_\\n        # of each grid cell.  This entire amount then flows\\n        # into one of the 8 neighbor grid cells, as indicated\\n        # by the D8 flow code. The update_flow_volume() function\\n        # is called right after this one in update() and uses\\n        # the Q grid.\\n        #---------------------------------------------------------\\n        # 7/15/05.  The cross-sectional area of a trapezoid is\\n        # given by:    Ac = d * (w + (d * tan(theta))),\\n        # where w is the bottom width.  If we were to\\n        # use: Ac = w * d, then we'd have Ac=0 when w=0.\\n        # We also need angle units to be radians.\\n        #---------------------------------------------------------\\n\\n        #-----------------------------\\n        # Compute the discharge grid\\n        #------------------------------------------------------ \\n        # A_wet is initialized in initialize_computed_vars().\\n        # A_wet is updated in update_trapezoid_Rh().\\n        #------------------------------------------------------     \\n        ### self.Q = np.float64(self.u * A_wet)\\n        self.Q[:] = self.u * self.A_wet   ## (2/19/13, in place)\\n\\n        #--------------\\n        # For testing\\n        #--------------\", 'def update_diversions(self):\\n\\n        #--------------------------------------------------------------    \\n        # Note: The Channel component requests the following input\\n        #       vars from the Diversions component by including\\n        #       them in its \"get_input_vars()\":\\n        #       (1) Q_sources, Q_sources_x, Q_sources_y\\n        #       (2) Q_sinks,   Q_sinks_x, Q_sinks_y\\n        #       (3) Q_canals_out, Q_canals_out_x, Q_canals_out_y\\n        #       (4) Q_canals_fraction, Q_canals_in_x, Q_canals_in_y.', 'def update_flow_volume(self):\\n\\n        #-----------------------------------------------------------\\n        # Notes: This function must be called after\\n        #        update_discharge() and update_diversions().\\n        #-----------------------------------------------------------        \\n        # Notes: Q   = surface discharge  [m^3/s]\\n        #        R   = excess precip. rate  [m/s]\\n        #        da  = pixel area  [m^2]\\n        #        dt  = channel flow timestep  [s]\\n        #        vol = total volume of water in pixel [m^3]\\n        #        v2  = temp version of vol\\n        #        w1  = IDs of pixels that...\\n        #        p1  = IDs of parent pixels that...\\n        #-----------------------------------------------------------\\n        dt = self.dt  # [seconds]\\n\\n        #----------------------------------------------------\\n        # Add contribution (or loss ?) from excess rainrate\\n        #----------------------------------------------------\\n        # Contributions over entire grid cell from rainfall,\\n        # snowmelt, icemelt and baseflow (minus losses from\\n        # evaporation and infiltration) are assumed to flow\\n        # into the channel within the grid cell.\\n        # Note that R is allowed to be negative.\\n        #----------------------------------------------------        \\n        self.vol += (self.R * self.da) * dt   # (in place)', 'def update_flow_depth(self):\\n\\n        #-----------------------------------------------------------\\n        # Notes: 7/18/05.  Modified to use the equation for volume\\n        #        of a trapezoidal channel:  vol = Ac * ds, where\\n        #        Ac=d*[w + d*tan(t)], and to solve the resulting\\n        #        quadratic (discarding neg. root) for new depth, d.\\n\\n        #        8/29/05.  Now original ds is used for subsurface\\n        #        flow and there is a ds_chan which can include a\\n        #        sinuosity greater than 1.  This may be especially\\n        #        important for larger pixel sizes.\\n\\n        #        Removed (ds > 1) here which was only meant to\\n        #        avoid a \"divide by zero\" error at pixels where\\n        #        (ds eq 0).  This isn\\'t necessary since the\\n        #        Flow_Lengths function in utils_TF.pro never\\n        #        returns a value of zero.\\n        #----------------------------------------------------------\\n        #        Modified to avoid double where calls, which\\n        #        reduced cProfile run time for this method from\\n        #        1.391 to 0.644.  (9/23/14)\\n        #----------------------------------------------------------\\n        # Commented this out on (2/18/10) because it doesn\\'t\\n        #           seem to be used anywhere now.  Checked all\\n        #           of the Channels components.\\n        #----------------------------------------------------------        \\n        # self.d_last = self.d.copy()\\n\\n        #-----------------------------------        \\n        # Make some local aliases and vars\\n        #-----------------------------------------------------------\\n        # Note: angles were read as degrees & converted to radians\\n        #-----------------------------------------------------------\\n        d = self.d\\n        width = self.width  ###\\n        angle = self.angle\\n        SCALAR_ANGLES = (np.size(angle) == 1)', \"def update_free_surface_slope(self):\\n\\n        #-----------------------------------------------------------\\n        # Notes:  It is assumed that the flow directions don't\\n        #         change even though the free surface is changing.\\n        #-----------------------------------------------------------\\n        delta_d     = (self.d - self.d[self.d8.parent_IDs])\\n        self.S_free[:] = self.S_bed + (delta_d / self.d8.ds)\", 'def update_shear_stress(self):\\n\\n        #--------------------------------------------------------\\n        # Notes: 9/9/14.  Added so shear stress could be shared.\\n        #        This uses the depth-slope product.\\n        #--------------------------------------------------------\\n        if (self.KINEMATIC_WAVE):\\n        \\tslope = self.S_bed\\n        else:\\n            slope = self.S_free\\n        self.tau[:] = self.rho_H2O * self.g * self.d * slope', 'def update_shear_speed(self):\\n\\n        #--------------------------------------------------------\\n        # Notes: 9/9/14.  Added so shear speed could be shared.\\n        #--------------------------------------------------------\\n        self.u_star[:] = np.sqrt( self.tau / self.rho_H2O )', 'def update_trapezoid_Rh(self):\\n\\n        #-------------------------------------------------------------\\n        # Notes: Compute the hydraulic radius of a trapezoid that:\\n        #          (1) has a bed width of wb >= 0 (0 for triangular)\\n        #          (2) has a bank angle of theta (0 for rectangular)\\n        #          (3) is filled with water to a depth of d.\\n        #        The units of wb and d are meters.  The units of\\n        #        theta are assumed to be degrees and are converted.\\n        #-------------------------------------------------------------\\n        # NB!    wb should never be zero, so P_wet can never be 0,\\n        #        which would produce a NaN (divide by zero).\\n        #-------------------------------------------------------------\\n        #        See Notes for TF_Tan function in utils_TF.pro\\n        #            AW = d * (wb + (d * TF_Tan(theta_rad)) )\\n        #-------------------------------------------------------------\\n        # 9/9/14.  Bug fix.  Angles were already in radians but\\n        #          were converted to radians again.\\n        #--------------------------------------------------------------\\n\\n        #---------------------------------------------------------\\n        # Compute hydraulic radius grid for trapezoidal channels\\n        #-----------------------------------------------------------\\n        # Note: angles were read as degrees & converted to radians\\n        #-----------------------------------------------------------\\n        d     = self.d        # (local synonyms)\\n        wb    = self.width    # (trapezoid bottom width)\\n        L2    = d * np.tan( self.angle )          \\n        A_wet = d * (wb + L2)      \\n        P_wet = wb + (np.float64(2) * d / np.cos(self.angle) )\\n\\n        #---------------------------------------------------\\n        # At noflow_IDs (e.g. edges) P_wet may be zero\\n        # so do this to avoid \"divide by zero\". (10/29/11)\\n        #---------------------------------------------------\\n        P_wet[ self.d8.noflow_IDs ] = np.float64(1)\\n        Rh = (A_wet / P_wet)\\n        #--------------------------------\\n        # w = np.where(P_wet == 0)\\n        # print \\'In update_trapezoid_Rh():\\'\\n        # print \\'   P_wet= 0 at\\', w[0].size, \\'cells\\'\\n\\n        #------------------------------------\\n        # Force edge pixels to have Rh = 0.\\n        # This will make u = 0 there also.\\n        #------------------------------------\\n        Rh[ self.d8.noflow_IDs ] = np.float64(0)', \"def update_friction_factor(self):    \\n\\n        #----------------------------------------    \\n        # Note:  Added on 9/9/14 to streamline.\\n        #----------------------------------------------------------\\n        # Note:  f  = half of the Fanning friction factor\\n        #        d  = flow depth [m]\\n        #        z0 = roughness length\\n        #        S  = bed slope (assumed equal to friction slope)\\n        #        g  = 9.81 = gravitation constant [m/s^2]\\n        #---------------------------------------------------------       \\n        #        For law of the wall:\\n        #        kappa = 0.41 = von Karman's constant\\n        #        aval  = 0.48 = integration constant\\n\\n        #        law_const  = sqrt(g)/kappa = 7.6393d\\n        #        smoothness = (aval / z0) * d\\n        #        f = (kappa / alog(smoothness))^2d\\n        #        tau_bed = rho_w * f * u^2 = rho_w * g * d * S\\n\\n        #        d, S, and z0 can be arrays.\\n\\n        #        To make default z0 correspond to default\\n        #        Manning's n, can use this approximation:\\n        #        z0 = a * (2.34 * sqrt(9.81) * n / kappa)^6d\\n        #        For n=0.03, this gives: z0 = 0.011417\\n        #########################################################\\n        #        However, for n=0.3, it gives: z0 = 11417.413\\n        #        which is 11.4 km!  So the approximation only\\n        #        holds within some range of values.\\n        #--------------------------------------------------------\\n\\n        ###############################################################\\n        # cProfile:  This method took: 0.369 secs for topoflow_test()\\n        ###############################################################            \\n        #--------------------------------------\\n        # Find where (d <= 0).  g=good, b=bad\\n        #-------------------------------------- \\n        wg = self.d_is_pos\\n        wb = self.d_is_neg\", 'def update_velocity(self):\\n\\n        #---------------------------------------------------------\\n        # Note: Do nothing now unless this method is overridden\\n        #       by a particular method of computing velocity.\\n        #---------------------------------------------------------\\n        print \"Warning: update_velocity() method is inactive.\"', 'def update_velocity_on_edges(self):\\n\\n        #---------------------------------\\n        # Force edge pixels to have u=0.\\n        #----------------------------------------\\n        # Large slope around 1 flows into small\\n        # slope & leads to a negative velocity.\\n        #----------------------------------------\\n        self.u[ self.d8.noflow_IDs ] = np.float64(0)', 'def update_froude_number(self):\\n\\n        #----------------------------------------------------------\\n        # Notes: 9/9/14.  Added so Froude number could be shared.\\n        # This use of wg & wb reduced cProfile time from:\\n        # 0.644 sec to: 0.121.  (9/23/14)\\n        #----------------------------------------------------------\\n        # g = good, b = bad\\n        #-------------------- \\n        wg = self.d_is_pos\\n        wb = self.d_is_neg\\n\\n        self.froude[ wg ] = self.u[wg] / np.sqrt( self.g * self.d[wg] )       \\n        self.froude[ wb ] = np.float64(0)', 'def update_outlet_values(self):', 'def update_peak_values(self):\\n\\n        if (self.Q_outlet > self.Q_peak):    \\n            self.Q_peak.fill( self.Q_outlet )\\n            self.T_peak.fill( self.time_min )      # (time to peak)\\n        #---------------------------------------\\n        if (self.u_outlet > self.u_peak):\\n            self.u_peak.fill( self.u_outlet )\\n            self.Tu_peak.fill( self.time_min )\\n        #---------------------------------------\\n        if (self.d_outlet > self.d_peak):    \\n            self.d_peak.fill(  self.d_outlet )\\n            self.Td_peak.fill( self.time_min )', 'def update_Q_out_integral(self):\\n\\n        #--------------------------------------------------------\\n        # Note: Renamed \"volume_out\" to \"vol_Q\" for consistency\\n        # with vol_P, vol_SM, vol_IN, vol_ET, etc. (5/18/12)\\n        #--------------------------------------------------------\\n        self.vol_Q += (self.Q_outlet * self.dt)  ## Experiment: 5/19/12.\\n        ## self.vol_Q += (self.Q[self.outlet_ID] * self.dt)', 'def update_mins_and_maxes(self, REPORT=False):\\n\\n        #--------------------------------------\\n        # Get mins and max over entire domain\\n        #--------------------------------------', 'def check_flow_depth(self):\\n\\n        OK = True\\n        d  = self.d\\n        dt = self.dt\\n        nx = self.nx   #################', 'def check_flow_velocity(self):\\n\\n        OK = True\\n        u  = self.u\\n        dt = self.dt\\n        nx = self.nx', \"def open_input_files(self):\\n\\n        # This doesn't work, because file_unit doesn't get full path. (10/28/11)\\n        # start_dir = os.getcwd()\\n        # os.chdir( self.in_directory )\\n\\n        # print '### start_dir =', start_dir\\n        # print '### in_directory =', self.in_directory\\n\\n        in_files = ['slope_file', 'nval_file', 'z0val_file',\\n                    'width_file', 'angle_file', 'sinu_file', 'd0_file']\\n        self.prepend_directory( in_files, INPUT=True )\\n\\n        # self.slope_file = self.in_directory + self.slope_file\\n        # self.nval_file  = self.in_directory + self.nval_file\\n        # self.z0val_file = self.in_directory + self.z0val_file\\n        # self.width_file = self.in_directory + self.width_file\\n        # self.angle_file = self.in_directory + self.angle_file\\n        # self.sinu_file  = self.in_directory + self.sinu_file\\n        # self.d0_file    = self.in_directory + self.d0_file\\n\\n        #self.code_unit = model_input.open_file(self.code_type,  self.code_file)\\n        self.slope_unit = model_input.open_file(self.slope_type, self.slope_file)\\n        if (self.MANNING):\\n            self.nval_unit  = model_input.open_file(self.nval_type,  self.nval_file)\\n        if (self.LAW_OF_WALL):\\n            self.z0val_unit = model_input.open_file(self.z0val_type, self.z0val_file)\\n        self.width_unit = model_input.open_file(self.width_type, self.width_file)\\n        self.angle_unit = model_input.open_file(self.angle_type, self.angle_file)\\n        self.sinu_unit  = model_input.open_file(self.sinu_type,  self.sinu_file)\\n        self.d0_unit    = model_input.open_file(self.d0_type,    self.d0_file)\\n\\n        # os.chdir( start_dir )\", 'def read_input_files(self):\\n\\n        #---------------------------------------------------\\n        # The flow codes are always a grid, size of DEM.\\n        #---------------------------------------------------\\n        # NB! model_input.py also has a read_grid() function.\\n        #---------------------------------------------------        \\n        rti = self.rti', \"def close_input_files(self):\\n\\n        # if not(self.slope_unit.closed):\\n        # if (self.slope_unit != None):\\n\\n        #-------------------------------------------------\\n        # NB!  self.code_unit was never defined as read.\\n        #-------------------------------------------------\\n        # if (self.code_type != 'scalar'): self.code_unit.close()\\n\\n        if (self.slope_type != 'Scalar'): self.slope_unit.close()\\n        if (self.MANNING):\\n            if (self.nval_type != 'Scalar'): self.nval_unit.close()\\n        if (self.LAW_OF_WALL):\\n           if (self.z0val_type != 'Scalar'): self.z0val_unit.close()\\n        if (self.width_type != 'Scalar'): self.width_unit.close()\\n        if (self.angle_type != 'Scalar'): self.angle_unit.close()\\n        if (self.sinu_type  != 'Scalar'): self.sinu_unit.close()\\n        if (self.d0_type    != 'Scalar'): self.d0_unit.close()\", 'def update_outfile_names(self):\\n\\n        #-------------------------------------------------\\n        # Notes:  Append out_directory to outfile names.\\n        #-------------------------------------------------\\n        self.Q_gs_file = (self.out_directory + self.Q_gs_file)\\n        self.u_gs_file = (self.out_directory + self.u_gs_file)\\n        self.d_gs_file = (self.out_directory + self.d_gs_file) \\n        self.f_gs_file = (self.out_directory + self.f_gs_file) \\n        #--------------------------------------------------------\\n        self.Q_ts_file = (self.out_directory + self.Q_ts_file)\\n        self.u_ts_file = (self.out_directory + self.u_ts_file) \\n        self.d_ts_file = (self.out_directory + self.d_ts_file) \\n        self.f_ts_file = (self.out_directory + self.f_ts_file)', 'def bundle_output_files(self):    \\n\\n        ###################################################\\n        # NOT READY YET. Need \"get_long_name()\" and a new\\n        # version of \"get_var_units\".  (9/21/14)\\n        ###################################################', 'def open_output_files(self):\\n\\n        model_output.check_netcdf()\\n        self.update_outfile_names()\\n        ## self.bundle_output_files()', 'def write_output_files(self, time_seconds=None):\\n\\n        #---------------------------------------------------------\\n        # Notes:  This function was written to use only model\\n        #         time (maybe from a caller) in seconds, and\\n        #         the save_grid_dt and save_pixels_dt parameters\\n        #         read by read_cfg_file().\\n        #\\n        #         read_cfg_file() makes sure that all of\\n        #         the \"save_dts\" are larger than or equal to the\\n        #         process dt.\\n        #---------------------------------------------------------', \"def close_output_files(self):\\n\\n        if (self.SAVE_Q_GRIDS):  model_output.close_gs_file( self, 'Q')   \\n        if (self.SAVE_U_GRIDS):  model_output.close_gs_file( self, 'u')  \\n        if (self.SAVE_D_GRIDS):  model_output.close_gs_file( self, 'd')   \\n        if (self.SAVE_F_GRIDS):  model_output.close_gs_file( self, 'f')\\n        #---------------------------------------------------------------\\n        if (self.SAVE_Q_PIXELS): model_output.close_ts_file( self, 'Q')   \\n        if (self.SAVE_U_PIXELS): model_output.close_ts_file( self, 'u')    \\n        if (self.SAVE_D_PIXELS): model_output.close_ts_file( self, 'd')    \\n        if (self.SAVE_F_PIXELS): model_output.close_ts_file( self, 'f')\", 'def save_grids(self):', 'def save_pixel_values(self):   ##### save_time_series_data(self)  #######', \"def manning_formula(self):\\n\\n        #---------------------------------------------------------\\n        # Notes: R = (A/P) = hydraulic radius [m]\\n        #        N = Manning's roughness coefficient\\n        #            (usually in the range 0.012 to 0.035)\\n        #        S = bed slope or free slope\\n\\n        #        R,S, and N may be 2D arrays.\\n\\n        #        If length units are all *feet*, then an extra\\n        #        factor of 1.49 must be applied.  If units are\\n        #        meters, no such factor is needed.\\n\\n        #        Note that Q = Ac * u, where Ac is cross-section\\n        #        area.  For a trapezoid, Ac does not equal w*d.\\n        #---------------------------------------------------------\\n        if (self.KINEMATIC_WAVE):\\n            S = self.S_bed\\n        else:\\n        \\tS = self.S_free\\n\\n        u = (self.Rh ** self.two_thirds) * np.sqrt(S) / self.nval\", \"def law_of_the_wall(self):\\n\\n        #---------------------------------------------------------\\n        # Notes: u  = flow velocity  [m/s]\\n        #        d  = flow depth [m]\\n        #        z0 = roughness length\\n        #        S  = bed slope or free slope\\n\\n        #        g     = 9.81 = gravitation constant [m/s^2]\\n        #        kappa = 0.41 = von Karman's constant\\n        #        aval  = 0.48 = integration constant\\n\\n        #        law_const  = sqrt(g)/kappa = 7.6393d\\n        #        smoothness = (aval / z0) * d\\n        #        f = (kappa / alog(smoothness))^2d\\n        #        tau_bed = rho_w * f * u^2 = rho_w * g * d * S\\n\\n        #        d, S, and z0 can be arrays.\\n\\n        #        To make default z0 correspond to default\\n        #        Manning's n, can use this approximation:\\n        #        z0 = a * (2.34 * sqrt(9.81) * n / kappa)^6d\\n        #        For n=0.03, this gives: z0 = 0.011417\\n        #########################################################\\n        #        However, for n=0.3, it gives: z0 = 11417.413\\n        #        which is 11.4 km!  So the approximation only\\n        #        holds within some range of values.\\n        #--------------------------------------------------------\\n        if (self.KINEMATIC_WAVE):\\n            S = self.S_bed\\n        else:\\n        \\tS = self.S_free\\n\\n        smoothness = (self.aval / self.z0val) * self.d\", 'def print_status_report(self): \\n\\n        #----------------------------------------------------\\n        # Wherever depth is less than z0, assume that water\\n        # is not flowing and set u and Q to zero.\\n        # However, we also need (d gt 0) to avoid a divide\\n        # by zero problem, even when numerators are zero.\\n        #----------------------------------------------------\\n        # FLOWING = (d > (z0/aval))\\n        #*** FLOWING[noflow_IDs] = False    ;******', 'def remove_bad_slopes(self, FLOAT=False):\\n\\n        #------------------------------------------------------------\\n        # Notes: The main purpose of this routine is to find\\n        #        pixels that have nonpositive slopes and replace\\n        #        then with the smallest value that occurs anywhere\\n        #        in the input slope grid.  For example, pixels on\\n        #        the edges of the DEM will have a slope of zero.\\n\\n        #        With the Kinematic Wave option, flow cannot leave\\n        #        a pixel that has a slope of zero and the depth\\n        #        increases in an unrealistic manner to create a\\n        #        spike in the depth grid.\\n\\n        #        It would be better, of course, if there were\\n        #        no zero-slope pixels in the DEM.  We could use\\n        #        an \"Imposed gradient DEM\" to get slopes or some\\n        #        method of \"profile smoothing\".\\n\\n        #        It is possible for the flow code to be nonzero\\n        #        at a pixel that has NaN for its slope. For these\\n        #        pixels, we also set the slope to our min value.\\n\\n        #        7/18/05. Broke this out into separate procedure.\\n        #------------------------------------------------------------\\n\\n        #-----------------------------------\\n        # Are there any \"bad\" pixels ?\\n        # If not, return with no messages.\\n        #-----------------------------------  \\n        wb = np.where(np.logical_or((self.slope <= 0.0), \\\\\\n                              np.logical_not(np.isfinite(self.slope))))\\n        nbad = np.size(wb[0])\\n        print \\'size(slope) =\\', np.size(self.slope)\\n        print \\'size(wb) =\\', nbad', 'def Trapezoid_Rh(d, wb, theta):\\n\\n    #-------------------------------------------------------------\\n    # Notes: Compute the hydraulic radius of a trapezoid that:\\n    #          (1) has a bed width of wb >= 0 (0 for triangular)\\n    #          (2) has a bank angle of theta (0 for rectangular)\\n    #          (3) is filled with water to a depth of d.\\n    #        The units of wb and d are meters.  The units of\\n    #        theta are assumed to be degrees and are converted.\\n    #-------------------------------------------------------------\\n    # NB!    wb should never be zero, so PW can never be 0,\\n    #        which would produce a NaN (divide by zero).\\n    #-------------------------------------------------------------\\n    #        See Notes for TF_Tan function in utils_TF.pro\\n    #            AW = d * (wb + (d * TF_Tan(theta_rad)) )\\n    #-------------------------------------------------------------    \\n    theta_rad = (theta * np.pi / 180.0)', \"def Manning_Formula(Rh, S, nval):\\n\\n    #---------------------------------------------------------\\n    # Notes: R = (A/P) = hydraulic radius [m]\\n    #        N = Manning's roughness coefficient\\n    #            (usually in the range 0.012 to 0.035)\\n    #        S = bed slope (assumed equal to friction slope)\\n\\n    #        R,S, and N may be 2D arrays.\\n\\n    #        If length units are all *feet*, then an extra\\n    #        factor of 1.49 must be applied.  If units are\\n    #        meters, no such factor is needed.\\n\\n    #        Note that Q = Ac * u, where Ac is cross-section\\n    #        area.  For a trapezoid, Ac does not equal w*d.\\n    #---------------------------------------------------------\\n    ##  if (N == None): N = np.float64(0.03)\\n\\n    two_thirds = np.float64(2) / 3.0\", \"def Law_of_the_Wall(d, Rh, S, z0val):\\n\\n    #---------------------------------------------------------\\n    # Notes: u  = flow velocity  [m/s]\\n    #        d  = flow depth [m]\\n    #        z0 = roughness height\\n    #        S  = bed slope (assumed equal to friction slope)\\n\\n    #        g     = 9.81 = gravitation constant [m/s^2]\\n    #        kappa = 0.41 = von Karman's constant\\n    #        aval  = 0.48 = integration constant\\n\\n    #        sqrt(g)/kappa = 7.6393d\\n    #        smoothness = (aval / z0) * d\\n    #        f = (kappa / alog(smoothness))^2d\\n    #        tau_bed = rho_w * f * u^2 = rho_w * g * d * S\\n\\n    #        d, S, and z0 can be arrays.\\n\\n    #        To make default z0 correspond to default\\n    #        Manning's n, can use this approximation:\\n    #        z0 = a * (2.34 * sqrt(9.81) * n / kappa)^6d\\n    #        For n=0.03, this gives: z0 = 0.011417\\n    #        However, for n=0.3, it gives: z0 = 11417.413\\n    #        which is 11.4 km!  So the approximation only\\n    #        holds within some range of values.\\n    #--------------------------------------------------------\"]}, {'features': [], 'snippets': ['def __unicode__(self):\\n        return self.type_desc', \"def __unicode__(self):\\n        return self.street_address + ',' + self.city\", 'def __unicode__(self):\\n        return self.title', 'def __unicode__(self, arg):']}, {'features': [], 'snippets': ['def setup(self):\\n        self.add_copy_specs([\\n            \"/proc/sysvipc/msg\",\\n            \"/proc/sysvipc/sem\",\\n            \"/proc/sysvipc/shm\"\\n        ])\\n        self.add_cmd_output(\"ipcs\")']}, {'features': [], 'snippets': ['def __init__(self,\\n            default_action=False,\\n            auth=None,\\n            **kwargs):\\n        self.auth = auth\\n        if auth:\\n            assert IAuth.providedBy(auth)\\n\\n        self.config = dict( (a, default_action) for a in self.knownActions )\\n        for act in self.knownActions:\\n            if act in kwargs:\\n                self.config[act] = kwargs[act]\\n                del kwargs[act]\\n\\n        if kwargs:\\n            raise ValueError(\"unknown authorization action(s) \" + \", \".join(kwargs.keys()))', 'def needAuthForm(self, action):\\n        \"\"\"Does this action require an authentication form?\"\"\"\\n        if action not in self.knownActions:\\n            raise KeyError(\"unknown action\")\\n        cfg = self.config.get(action, False)\\n        if cfg == \\'auth\\' or callable(cfg):\\n            return True\\n        return False']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def test(b):\\n    if not b:\\n        raise RuntimeError('test assertion failed')\", 'def run(args, communicator):\\n    ports = []\\n    for arg in args[1:]:\\n        if arg[0] == \\'-\\':\\n            sys.stderr.write(args[0] + \": unknown option `\" + arg + \"\\'\\\\n\")\\n            usage(args[0])\\n            return False\\n\\n        ports.append(int(arg))\\n\\n    if len(ports) == 0:\\n        sys.stderr.write(args[0] + \": no ports specified\\\\n\")\\n        usage(args[0])\\n        return False\\n\\n    try:\\n        AllTests.allTests(communicator, ports)\\n    except:\\n        traceback.print_exc()\\n        test(False)\\n\\n    return True']}, {'features': [], 'snippets': ['def get_external_vms(uri, username, password, vm_names=None):\\n    if vm_names is not None:\\n        if not vm_names:\\n            vm_names = None\\n        else:\\n            vm_names = frozenset(vm_names)\\n\\n    try:\\n        conn = libvirtconnection.open_connection(uri=uri,\\n                                                 username=username,\\n                                                 passwd=password)\\n    except libvirt.libvirtError as e:\\n        logging.exception(\\'error connecting to hypervisor\\')\\n        return {\\'status\\': {\\'code\\': errCode[\\'V2VConnection\\'][\\'status\\'][\\'code\\'],\\n                           \\'message\\': str(e)}}\\n\\n    with closing(conn):\\n        vms = []\\n        for vm in _list_domains(conn):\\n            if vm_names is not None and vm.name() not in vm_names:\\n                # Skip this VM.\\n                continue\\n            elif conn.getType() == \"ESX\" and _vm_has_snapshot(vm):\\n                logging.error(\"vm %r has snapshots and therefore can not be \"\\n                              \"imported since snapshot conversion is not \"\\n                              \"supported for VMware\", vm.name())\\n                continue\\n            _add_vm(conn, vms, vm)\\n        return {\\'status\\': doneCode, \\'vmList\\': vms}', \"def convert_external_vm(uri, username, password, vminfo, job_id, irs):\\n    if uri.startswith(_XEN_SSH_PROTOCOL):\\n        command = XenCommand(uri, vminfo, job_id, irs)\\n    elif uri.startswith(_VMWARE_PROTOCOL):\\n        command = LibvirtCommand(uri, username, password, vminfo, job_id,\\n                                 irs)\\n    elif uri.startswith(_KVM_PROTOCOL):\\n        if ovirt_imageio_common is None:\\n            raise V2VError('Unsupported protocol KVM, ovirt_imageio_common'\\n                           'package is needed for importing KVM images')\\n        command = KVMCommand(uri, username, password, vminfo, job_id, irs)\\n    else:\\n        raise ClientError('Unknown protocol for Libvirt uri: %s', uri)\\n    job = ImportVm(job_id, command)\\n    job.start()\\n    _add_job(job_id, job)\\n    return {'status': doneCode}\", \"def get_ova_info(ova_path):\\n    ns = {'ovf': _OVF_NS, 'rasd': _RASD_NS}\\n\\n    try:\\n        root = ET.fromstring(_read_ovf_from_ova(ova_path))\\n    except ET.ParseError as e:\\n        raise V2VError('Error reading ovf from ova, position: %r' % e.position)\\n\\n    vm = {}\\n    _add_general_ovf_info(vm, root, ns, ova_path)\\n    _add_disks_ovf_info(vm, root, ns)\\n    _add_networks_ovf_info(vm, root, ns)\\n\\n    return response.success(vmList=vm)\", \"def delete_job(job_id):\\n    try:\\n        job = _get_job(job_id)\\n        _validate_job_finished(job)\\n        _remove_job(job_id)\\n    except ClientError as e:\\n        logging.info('Cannot delete job, error: %s', e)\\n        return errCode[e.err_name]\\n    return {'status': doneCode}\", \"def get_jobs_status():\\n    ret = {}\\n    with _lock:\\n        items = tuple(_jobs.items())\\n    for job_id, job in items:\\n        ret[job_id] = {\\n            'status': job.status,\\n            'description': job.description,\\n            'progress': job.progress\\n        }\\n    return ret\", 'def _get_job(job_id):\\n    with _lock:\\n        if job_id not in _jobs:\\n            raise NoSuchJob(\"No such job %r\" % job_id)\\n        return _jobs[job_id]', 'def _validate_job_done(job):\\n    if job.status != STATUS.DONE:\\n        raise JobNotDone(\"Job %r is %s\" % (job.id, job.status))', 'def _read_ovf(job_id):\\n    file_name = os.path.join(_V2V_DIR, \"%s.ovf\" % job_id)\\n    try:\\n        with open(file_name, \\'r\\') as f:\\n            return f.read()\\n    except IOError as e:\\n        if e.errno != errno.ENOENT:\\n            raise\\n        raise NoSuchOvf(\"No such ovf %r\" % file_name)', 'def __init__(self):\\n        self._auth = None\\n        self._agent_pid = None\\n        self._ssh_auth_re = re.compile(_SSH_AUTH_RE)', \"def __exit__(self, *args):\\n        rc, out, err = execCmd([_SSH_ADD.cmd, '-d'], env=self._auth)\\n        if rc != 0:\\n            logging.error('Error deleting ssh-add, exit code: %r'\\n                          ', out: %r, err: %r' %\\n                          (rc, out, err))\\n\\n        self._kill_agent()\", 'def auth(self):\\n        return self._auth', 'def __init__(self, vminfo, vmid, irs):\\n        self._vminfo = vminfo\\n        self._vmid = vmid\\n        self._irs = irs\\n        self._prepared_volumes = []\\n        self._passwd_file = os.path.join(_V2V_DIR, \"%s.tmp\" % vmid)\\n        self._password = password.ProtectedPassword(\\'\\')\\n        self._base_command = [_VIRT_V2V.cmd, \\'-v\\', \\'-x\\']\\n        self._query_v2v_caps()\\n        if \\'qcow2_compat\\' in vminfo:\\n            qcow2_compat = vminfo[\\'qcow2_compat\\']\\n            if qcow2_compat not in _QCOW2_COMPAT_SUPPORTED:\\n                logging.error(\\'Invalid QCOW2 compat version %r\\' %\\n                              qcow2_compat)\\n                raise ValueError(\\'Invalid QCOW2 compat version %r\\' %\\n                                 qcow2_compat)\\n            if \\'vdsm-compat-option\\' in self._v2v_caps:\\n                self._base_command.extend([\\'--vdsm-compat\\', qcow2_compat])\\n            elif qcow2_compat != \\'0.10\\':\\n                # Note: qcow2 is only a suggestion from the engine\\n                # if virt-v2v doesn\\'t support it we fall back to default\\n                logging.info(\\'virt-v2v not supporting qcow2 compat version: \\'\\n                             \\'%r\\', qcow2_compat)', 'def _command(self):\\n        raise NotImplementedError(\"Subclass must implement this\")', 'def _get_disk_format(self):\\n        fmt = self._vminfo.get(\\'format\\', \\'raw\\').lower()\\n        return \"qcow2\" if fmt == \"cow\" else fmt', 'def _volumes(self):\\n        self._prepare_volumes()\\n        try:\\n            yield\\n        finally:\\n            self._teardown_volumes()', \"def _teardown_volumes(self):\\n        for drive in self._prepared_volumes:\\n            try:\\n                self._irs.teardownImage(drive['domainID'],\\n                                        drive['poolID'],\\n                                        drive['imageID'])\\n            except Exception as e:\\n                logging.error('Job %r error tearing down drive: %s',\\n                              self._vmid, e)\", \"def _environment(self):\\n        # Provide some sane environment\\n        env = os.environ.copy()\\n\\n        # virt-v2v specific variables\\n        env['LIBGUESTFS_BACKEND'] = 'direct'\\n        if 'virtio_iso_path' in self._vminfo:\\n            env['VIRTIO_WIN'] = self._vminfo['virtio_iso_path']\\n        return env\", 'def _password_file(self):\\n        fd = os.open(self._passwd_file, os.O_WRONLY | os.O_CREAT, 0o600)\\n        try:\\n            if self._password.value is None:\\n                os.write(fd, \"\")\\n            else:\\n                os.write(fd, self._password.value)\\n        finally:\\n            os.close(fd)\\n        try:\\n            yield\\n        finally:\\n            try:\\n                os.remove(self._passwd_file)\\n            except Exception:\\n                logging.exception(\"Job %r error removing passwd file: %s\",\\n                                  self._vmid, self._passwd_file)', 'def __init__(self, uri, username, password, vminfo, vmid, irs):\\n        super(LibvirtCommand, self).__init__(vminfo, vmid, irs)\\n        self._uri = uri\\n        self._username = username\\n        self._password = password', 'def execute(self):\\n        with self._volumes(), self._password_file():\\n            yield self._start_helper()', 'def __init__(self, ova_path, vminfo, vmid, irs):\\n        super(OvaCommand, self).__init__(vminfo, vmid, irs)\\n        self._ova_path = ova_path', 'def execute(self):\\n        with self._volumes():\\n            yield self._start_helper()', 'def __init__(self, uri, vminfo, job_id, irs):\\n        super(XenCommand, self).__init__(vminfo, job_id, irs)\\n        self._uri = uri\\n        self._ssh_agent = SSHAgent()', 'def execute(self):\\n        with self._volumes(), self._ssh_agent:\\n            yield self._start_helper()', 'def __init__(self, uri, username, password, vminfo, vmid, irs):\\n        super(KVMCommand, self).__init__(vminfo, vmid, irs)\\n        self._uri = uri\\n        self._username = username\\n        self._password = password', 'def execute(self):\\n        with self._volumes(), self._password_file():\\n            yield self._start_helper()', \"def _dest_images(self):\\n        ret = []\\n        for vol in self._prepared_volumes:\\n            ret.append(vol['path'])\\n        return ret\", 'def __init__(self, proc1, proc2):\\n        self._procs = (proc1, proc2)\\n        self._stdout = proc2.stdout', 'def pids(self):\\n        return [p.pid for p in self._procs]', 'def returncode(self):\\n        \"\"\"\\n        Returns None if any of the processes is still running. Returns 0 if all\\n        processes have finished with a zero exit code, otherwise return first\\n        nonzero exit code.\\n        \"\"\"\\n        ret = 0\\n        for p in self._procs:\\n            p.poll()\\n            if p.returncode is None:\\n                return None\\n            if p.returncode != 0 and ret == 0:\\n                # One of the processes has failed\\n                ret = p.returncode\\n\\n        # All processes have finished\\n        return ret', 'def stdout(self):\\n        return self._stdout', \"def __init__(self, job_id, command):\\n        self._id = job_id\\n        self._command = command\\n        self._thread = None\\n\\n        self._status = STATUS.STARTING\\n        self._description = ''\\n        self._disk_progress = 0\\n        self._disk_count = 1\\n        self._current_disk = 1\\n        self._aborted = False\\n        self._proc = None\", 'def wait(self):\\n        if self._thread is not None and self._thread.is_alive():\\n            self._thread.join()', 'def id(self):\\n        return self._id', 'def status(self):\\n        return self._status', 'def description(self):\\n        return self._description', \"def progress(self):\\n        '''\\n        progress is part of multiple disk_progress its\\n        flat and not 100% accurate - each disk take its\\n        portion ie if we have 2 disks the first will take\\n        0-50 and the second 50-100\\n        '''\\n        completed = (self._disk_count - 1) * 100\\n        return (completed + self._disk_progress) / self._disk_count\", 'def _run(self):\\n        try:\\n            self._import()\\n        except Exception as ex:\\n            if self._aborted:\\n                logging.debug(\"Job %r was aborted\", self._id)\\n            else:\\n                logging.exception(\"Job %r failed\", self._id)\\n                self._status = STATUS.FAILED\\n                self._description = str(ex)\\n                try:\\n                    if self._proc is not None:\\n                        self._abort()\\n                except Exception as e:\\n                    logging.exception(\\'Job %r, error trying to abort: %r\\',\\n                                      self._id, e)', 'def _wait_for_process(self):\\n        if self._proc.returncode is not None:\\n            return\\n        logging.debug(\"Job %r waiting for virt-v2v process\", self._id)\\n        if not self._proc.wait(timeout=self.PROC_WAIT_TIMEOUT):\\n            raise V2VProcessError(\"Job %r timeout waiting for process pid=%s\",\\n                                  self._id, self._proc.pids)', \"def abort(self):\\n        self._status = STATUS.ABORTED\\n        logging.info('Job %r aborting...', self._id)\\n        self._abort()\", \"def parse(self, stream):\\n        for line in stream:\\n            if 'Copying disk' in line:\\n                description, current_disk, disk_count = self._parse_line(line)\\n                yield ImportProgress(int(current_disk), int(disk_count),\\n                                     description)\\n                for chunk in self._iter_progress(stream):\\n                    progress = self._parse_progress(chunk)\\n                    if progress is not None:\\n                        yield DiskProgress(progress)\\n                    if progress == 100:\\n                        break\", \"def _iter_progress(self, stream):\\n        chunk = ''\\n        while True:\\n            c = stream.read(1)\\n            if not c:\\n                raise OutputParserError('copy-disk stream closed unexpectedly')\\n            chunk += c\\n            if c == '\\\\r':\\n                yield chunk\\n                chunk = ''\", 'def _mem_to_mib(size, unit):\\n    lunit = unit.lower()\\n    if lunit in (\\'bytes\\', \\'b\\'):\\n        return size / 1024 / 1024\\n    elif lunit in (\\'kib\\', \\'k\\'):\\n        return size / 1024\\n    elif lunit in (\\'mib\\', \\'m\\'):\\n        return size\\n    elif lunit in (\\'gib\\', \\'g\\'):\\n        return size * 1024\\n    elif lunit in (\\'tib\\', \\'t\\'):\\n        return size * 1024 * 1024\\n    else:\\n        raise InvalidVMConfiguration(\"Invalid currentMemory unit attribute:\"\\n                                     \" %r\" % unit)', 'def _add_vm(conn, vms, vm):\\n    params = {}\\n    try:\\n        _add_vm_info(vm, params)\\n    except libvirt.libvirtError as e:\\n        logging.error(\"error getting domain information: %s\", e)\\n        return\\n    try:\\n        xml = vm.XMLDesc(0)\\n    except libvirt.libvirtError as e:\\n        logging.error(\"error getting domain xml for vm %r: %s\",\\n                      vm.name(), e)\\n        return\\n    try:\\n        root = ET.fromstring(xml)\\n    except ET.ParseError as e:\\n        logging.error(\\'error parsing domain xml: %s\\', e)\\n        return\\n    if not _block_disk_supported(conn, root):\\n        return\\n    try:\\n        _add_general_info(root, params)\\n    except InvalidVMConfiguration as e:\\n        logging.error(\"error adding general info: %s\", e)\\n        return\\n    _add_snapshot_info(conn, vm, params)\\n    _add_networks(root, params)\\n    _add_disks(root, params)\\n    _add_graphics(root, params)\\n    _add_video(root, params)\\n\\n    disk_info = None\\n    for disk in params[\\'disks\\']:\\n        disk_info = _get_disk_info(conn, disk, vm)\\n        if disk_info is None:\\n            break\\n        disk.update(disk_info)\\n    if disk_info is not None:\\n        vms.append(params)\\n    else:\\n        logging.warning(\\'Cannot add VM %s due to disk storage error\\',\\n                        vm.name())', 'def _add_vm_info(vm, params):\\n    params[\\'vmName\\'] = vm.name()\\n    # TODO: use new API: vm.state()[0] == libvirt.VIR_DOMAIN_SHUTOFF\\n    #       when supported in Xen under RHEL 5.x\\n    if vm.isActive():\\n        params[\\'status\\'] = \"Up\"\\n    else:\\n        params[\\'status\\'] = \"Down\"', 'def _get_disk_info(conn, disk, vm):\\n    if \\'alias\\' in disk.keys():\\n        try:\\n            if disk[\\'disktype\\'] == \\'file\\':\\n                vol = conn.storageVolLookupByPath(disk[\\'alias\\'])\\n                _, capacity, alloc = vol.info()\\n            elif disk[\\'disktype\\'] == \\'block\\':\\n                vol = vm.blockInfo(disk[\\'alias\\'])\\n                # We use the physical for allocation\\n                # in blockInfo can report 0\\n                capacity, _, alloc = vol\\n            else:\\n                logging.error(\\'Unsupported disk type: %r\\', disk[\\'disktype\\'])\\n\\n        except libvirt.libvirtError:\\n            logging.exception(\"Error getting disk size\")\\n            return None\\n        else:\\n            return {\\'capacity\\': str(capacity), \\'allocation\\': str(alloc)}\\n    return {}', 'def _add_disks(root, params):\\n    params[\\'disks\\'] = []\\n    disks = root.findall(\\'.//disk[@type=\"file\"]\\')\\n    disks = disks + root.findall(\\'.//disk[@type=\"block\"]\\')\\n    for disk in disks:\\n        d = {}\\n        disktype = disk.get(\\'type\\')\\n        device = disk.get(\\'device\\')\\n        if device is not None:\\n            if device == \\'cdrom\\':\\n                # Skip CD-ROM drives\\n                continue\\n            d[\\'type\\'] = device\\n        target = disk.find(\\'./target/[@dev]\\')\\n        if target is not None:\\n            d[\\'dev\\'] = target.get(\\'dev\\')\\n        if disktype == \\'file\\':\\n            d[\\'disktype\\'] = \\'file\\'\\n            source = disk.find(\\'./source/[@file]\\')\\n            if source is not None:\\n                d[\\'alias\\'] = source.get(\\'file\\')\\n        elif disktype == \\'block\\':\\n            d[\\'disktype\\'] = \\'block\\'\\n            source = disk.find(\\'./source/[@dev]\\')\\n            if source is not None:\\n                d[\\'alias\\'] = source.get(\\'dev\\')\\n        else:\\n            logging.error(\\'Unsupported disk type: %r\\', type)\\n\\n        driver = disk.find(\\'./driver/[@type]\\')\\n        if driver is not None:\\n            try:\\n                d[\"format\"] = _convert_disk_format(driver.get(\\'type\\'))\\n            except KeyError:\\n                logging.warning(\"Disk %s has unsupported format: %r\", d,\\n                                format)\\n        params[\\'disks\\'].append(d)', \"def _add_video(root, params):\\n    e = root.find('./devices/video/model/[@type]')\\n    if e is not None:\\n        params['video'] = e.get('type')\", \"def _add_snapshot_info(conn, vm, params):\\n    # Snapshot related API is not yet implemented in the libvirt's Xen driver\\n    if conn.getType() == 'Xen':\\n        return\\n\\n    try:\\n        ret = vm.hasCurrentSnapshot()\\n    except libvirt.libvirtError:\\n        logging.exception('Error checking for existing snapshots.')\\n    else:\\n        params['has_snapshots'] = ret > 0\", 'def _read_ovf_from_ova(ova_path):\\n    \"\"\"\\n       virt-v2v support ova in tar, zip formats as well as\\n       extracted directory\\n    \"\"\"\\n    if os.path.isdir(ova_path):\\n        return _read_ovf_from_ova_dir(ova_path)\\n    elif zipfile.is_zipfile(ova_path):\\n        return _read_ovf_from_zip_ova(ova_path)\\n    elif tarfile.is_tarfile(ova_path):\\n        return _read_ovf_from_tar_ova(ova_path)\\n    raise ClientError(\\'Unknown ova format, supported formats:\\'\\n                      \\' tar, zip or a directory\\')', \"def _read_ovf_from_ova_dir(ova_path):\\n    files = os.listdir(ova_path)\\n    name = _find_ovf(files)\\n    if name is not None:\\n        with open(os.path.join(ova_path, name), 'r') as ovf_file:\\n            return ovf_file.read()\\n    raise ClientError('OVA directory %s does not contain ovf file' % ova_path)\", \"def _read_ovf_from_tar_ova(ova_path):\\n    with tarfile.open(ova_path) as tar:\\n        for member in tar:\\n            if member.name.endswith('.ovf'):\\n                with closing(tar.extractfile(member)) as ovf:\\n                    return ovf.read()\\n        raise ClientError('OVA does not contains file with .ovf suffix')\", 'def _get_max_disk_size(populated_size, size):\\n    if populated_size is None:\\n        return size\\n    if size is None:\\n        return populated_size\\n    return str(max(int(populated_size), int(size)))', 'def _add_disks_ovf_info(vm, node, ns):\\n    vm[\\'disks\\'] = []\\n    for d in node.findall(\".//ovf:DiskSection/ovf:Disk\", ns):\\n        disk = {\\'type\\': \\'disk\\'}\\n        capacity = int(d.attrib.get(\\'{%s}capacity\\' % _OVF_NS))\\n        if \\'{%s}capacityAllocationUnits\\' % _OVF_NS in d.attrib:\\n            units = d.attrib.get(\\'{%s}capacityAllocationUnits\\' % _OVF_NS)\\n            capacity *= _parse_allocation_units(units)\\n        disk[\\'capacity\\'] = str(capacity)\\n        fileref = d.attrib.get(\\'{%s}fileRef\\' % _OVF_NS)\\n        alias = node.find(\\'.//ovf:References/ovf:File[@ovf:id=\"%s\"]\\' %\\n                          fileref, ns)\\n        if alias is not None:\\n            disk[\\'alias\\'] = alias.attrib.get(\\'{%s}href\\' % _OVF_NS)\\n            populated_size = d.attrib.get(\\'{%s}populatedSize\\' % _OVF_NS, None)\\n            size = alias.attrib.get(\\'{%s}size\\' % _OVF_NS)\\n            disk[\\'allocation\\'] = _get_max_disk_size(populated_size, size)\\n        else:\\n            raise V2VError(\\'Error parsing ovf information: disk href info\\')\\n        vm[\\'disks\\'].append(disk)']}, {'features': [], 'snippets': ['def profiles():\\n    \"\"\"Gets all profiles for all elements for user application to display and manipulate elements\"\"\"\\n    return jsonify(home_services.get_profiles())', 'def update_element():\\n    \"\"\"Updates single element with all new values received from the user application\"\"\"\\n    received_element = request.get_json()\\n    home_services.update_element(received_element)\\n    return \\'OK\\'', 'def update_elements():\\n    \"\"\"Updates all elements with all new values received from the user application\"\"\"\\n    received_elements = request.get_json()\\n    home_services.update_elements(received_elements)\\n    return \\'OK\\'', 'def delete_element():\\n    \"\"\"Deletes a single element with given hid\"\"\"\\n    element = request.get_json()\\n    home_services.delete_element(element[\\'hid\\'])\\n    return \\'OK\\'', 'def timerules():\\n    \"\"\"Adds, Updates or deletes time rule for the given element\"\"\"\\n    rules = request.get_json()\\n\\n    if len(rules) == 0:\\n        raise Exception(\"No elements in the list\")\\n\\n    for rule in rules:\\n        if \\'id\\' not in rule:\\n            rule[\\'id\\'] = None\\n\\n    home_services.save_time_rules(rules)\\n    return \\'OK\\'']}, {'features': [], 'snippets': ['def __init__(self, file_path=None, author=\\'pympi\\'):\\n        \"\"\"Construct either a new Eaf file or read on from a file/stream.\\n\\n        :param str file_path: Path to read from, - for stdin. If ``None`` an\\n                              empty Eaf file will be created.\\n        :param str author: Author of the file.\\n        \"\"\"\\n        self.naive_gen_ann, self.naive_gen_ts = False, False\\n        self.annotation_document = {\\n            \\'AUTHOR\\': author,\\n            \\'DATE\\': time.strftime(\"%Y-%m-%dT%H:%M:%S%z\"),\\n            \\'VERSION\\': \\'2.8\\',\\n            \\'FORMAT\\': \\'2.8\\',\\n            \\'xmlns:xsi\\': \\'http://www.w3.org/2001/XMLSchema-instance\\',\\n            \\'xsi:noNamespaceSchemaLocation\\':\\n                \\'http://www.mpi.nl/tools/elan/EAFv2.8.xsd\\'}\\n        self.constraints = {}\\n        self.controlled_vocabularies = {}\\n        self.header = {}\\n        self.licences = {}\\n        self.linguistic_types = {}\\n        self.tiers = {}\\n        self.timeslots = {}\\n        self.external_refs = []\\n        self.lexicon_refs = []\\n        self.linked_file_descriptors = []\\n        self.locales = []\\n        self.media_descriptors = []\\n        self.properties = []\\n        self.new_time, self.new_ann = 0, 0\\n\\n        if file_path is None:\\n            self.add_linguistic_type(\\'default-lt\\', None)\\n            self.constraints = {\\'Time_Subdivision\\': \\'Time subdivision of paren\\'\\n                                \\'t annotation\\\\\\'s time interval, no time gaps a\\'\\n                                \\'llowed within this interval\\',\\n                                \\'Symbolic_Subdivision\\': \\'Symbolic subdivision \\'\\n                                \\'of a parent annotation. Annotations refering \\'\\n                                \\'to the same parent are ordered\\',\\n                                \\'Symbolic_Association\\': \\'1-1 association with \\'\\n                                \\'a parent annotation\\',\\n                                \\'Included_In\\': \\'Time alignable annotations wit\\'\\n                                \\'hin the parent annotation\\\\\\'s time interval, g\\'\\n                                \\'aps are allowed\\'}\\n            self.properties.append((\\'0\\', {\\'NAME\\': \\'lastUsedAnnotation\\'}))\\n            self.add_tier(\\'default\\')\\n        else:\\n            EafIO.parse_eaf(file_path, self)', 'def to_textgrid(self, excluded_tiers=[], included_tiers=[]):\\n        \"\"\"Convert the object to a :class:`pympi.Praat.TextGrid` object.\\n\\n        :param list excluded_tiers: Specifically exclude these tiers.\\n        :param list included_tiers: Only include this tiers, when empty all are\\n                                    included.\\n        :returns: :class:`pympi.Praat.TextGrid` object\\n        :raises ImportError: If the pympi.Praat module can\\'t be loaded.\\n        \"\"\"\\n        from Praat import TextGrid\\n        tgout = TextGrid()\\n        tiers = [a for a in self.tiers if a not in excluded_tiers]\\n        if included_tiers:\\n            tiers = [a for a in tiers if a in included_tiers]\\n        for tier in tiers:\\n            currentTier = tgout.add_tier(tier)\\n            for interval in self.get_annotation_data_for_tier(tier):\\n                if interval[0] == interval[1]:\\n                    continue\\n                currentTier.add_interval(interval[0]/1000.0,\\n                                         interval[1]/1000.0, interval[2])\\n        return tgout', 'def get_linked_files(self):\\n        \"\"\"Give all linked files.\"\"\"\\n        return self.media_descriptors', 'def copy_tier(self, eaf_obj, tier_name):\\n        \"\"\"Copies a tier to another :class:`pympi.Elan.Eaf` object.\\n\\n        :param pympi.Elan.Eaf eaf_obj: Target Eaf object.\\n        :param str tier_name: Name of the tier.\\n        :raises KeyError: If the tier doesn\\'t exist.\\n        \"\"\"\\n        eaf_obj.remove_tier(tier_name)\\n        eaf_obj.add_tier(tier_name, tier_dict=self.tiers[tier_name][3])\\n        for ann in self.get_annotation_data_for_tier(tier_name):\\n            eaf_obj.insert_annotation(tier_name, ann[0], ann[1], ann[2])', 'def remove_tiers(self, tiers):\\n        \"\"\"Remove multiple tiers, note that this is a lot faster then removing\\n        them individually because of the delayed cleaning of timeslots.\\n\\n        :param list tiers: Names of the tier to remove.\\n        :raises KeyError: If a tier is non existent.\\n        \"\"\"\\n        for a in tiers:\\n            self.remove_tier(a, check=False, clean=False)\\n        self.clean_time_slots()', 'def get_tier_names(self):\\n        \"\"\"List all the tier names.\\n\\n        :returns: List of all tier names\\n        \"\"\"\\n        return self.tiers.keys()', 'def child_tiers_for(self, id_tier):\\n        \"\"\"Give all child tiers for a tier.\\n\\n        :param str id_tier: Name of the tier.\\n        :returns: List of all children\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        return [m for m in self.tiers if \\'PARENT_REF\\' in self.tiers[m][2] and\\n                self.tiers[m][2][\\'PARENT_REF\\'] == id_tier]', 'def get_annotation_data_at_time(self, id_tier, time):\\n        \"\"\"Give the annotations at the given time.\\n\\n        :param str id_tier: Name of the tier.\\n        :param int time: Time of the annotation.\\n        :returns: List of annotations at that time.\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        anns = self.tiers[id_tier][0]\\n        return sorted(\\n            [(self.timeslots[m[0]], self.timeslots[m[1]], m[2])\\n                for m in anns.itervalues() if\\n                self.timeslots[m[0]] <= time and\\n                self.timeslots[m[1]] >= time])', 'def remove_all_annotations_from_tier(self, id_tier):\\n        \"\"\"remove all annotations from a tier\\n\\n        :param str id_tier: Name of the tier.\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        self.tiers[id_tier][0], self.tiers[id_tier][1] = {}, {}\\n        self.clean_time_slots()', 'def remove_annotation(self, id_tier, time, clean=True):\\n        \"\"\"Remove an annotation in a tier, if you need speed the best thing is\\n        to clean the timeslots after the last removal.\\n\\n        :param str id_tier: Name of the tier.\\n        :param int time: Timepoint within the annotation.\\n        :param bool clean: Flag to clean the timeslots afterwards.\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        for b in [a for a in self.tiers[id_tier][0].iteritems() if\\n                  a[1][0] >= time and a[1][1] <= time]:\\n            del(self.tiers[id_tier][0][b[0]])\\n        if clean:\\n            self.clean_time_slots()', 'def get_ref_annotation_data_for_tier(self, id_tier):\\n        \"\"\"\"Give a list of all reference annotations of the form:\\n        ``[{id -> (ref, value, previous, svg_ref}]``\\n\\n        :param str id_tier: Name of the tier.\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        return self.tiers[id_tier][1]', 'def generate_annotation_id(self):\\n        \"\"\"Generate the next annotation id, this function is mainly used\\n        internally.\\n        \"\"\"\\n        if self.naive_gen_ann:\\n            new = self.last_ann+1\\n            self.last_ann = new\\n        else:\\n            new = 1\\n            anns = {int(ann[1:]) for tier in self.tiers.itervalues()\\n                    for ann in tier[0]}\\n            if len(anns) > 0:\\n                newann = set(xrange(1, max(anns))).difference(anns)\\n                if len(newann) == 0:\\n                    new = max(anns)+1\\n                    self.naive_gen_ann = True\\n                    self.last_ann = new\\n                else:\\n                    new = sorted(newann)[0]\\n        return \\'a%d\\' % new', 'def clean_time_slots(self):\\n        \"\"\"Clean up all unused timeslots.\\n        .. warning:: This can and will take time for larger tiers. When you\\n                     want to do a lot of operations on a lot of tiers please\\n                     unset the flags for cleaning in the functions so that the\\n                     cleaning is only performed afterwards.\\n        \"\"\"\\n        ts_in_tier = set(sum([a[0:2] for tier in self.tiers.itervalues()\\n                              for a in tier[0].itervalues()], ()))\\n        ts_avail = set(self.timeslots)\\n        for a in ts_in_tier.symmetric_difference(ts_avail):\\n            del(self.timeslots[a])\\n        self.naive_gen_ts = False\\n        self.naive_gen_ann = False', 'def merge_tiers(self, tiers, tiernew=None, gaptresh=1):\\n        \"\"\"Merge tiers into a new tier and when the gap is lower then the\\n        threshhold glue the annotations together.\\n\\n        :param list tiers: List of tier names.\\n        :param str tiernew: Name for the new tier, if ``None`` the name will be\\n                            generated.\\n        :param int gapthresh: Threshhold for the gaps.\\n        :raises KeyError: If a tier is non existent.\\n        :raises TypeError: If there are no annotations within the tiers.\\n        \"\"\"\\n        if tiernew is None:\\n            tiernew = \\'%s_Merged\\' % \\'_\\'.join(tiers)\\n        self.remove_tier(tiernew)\\n        self.add_tier(tiernew)\\n        timepts = sorted(set.union(\\n            *[set(j for j in xrange(d[0], d[1])) for d in\\n                [ann for tier in tiers for ann in\\n                 self.get_annotation_data_for_tier(tier)]]))\\n        if len(timepts) > 1:\\n            start = timepts[0]\\n            for i in xrange(1, len(timepts)):\\n                if timepts[i]-timepts[i-1] > gaptresh:\\n                    self.insert_annotation(\\n                        tiernew, start, timepts[i-1],\\n                        self.generate_annotation_concat(tiers, start,\\n                                                        timepts[i-1]))\\n                    start = timepts[i]\\n            self.insert_annotation(\\n                tiernew, start, timepts[i-1],\\n                self.generate_annotation_concat(tiers, start, timepts[i-1]))', 'def filterAnnotations(self, tier, tier_name=None, filtin=None,\\n                          filtex=None):\\n        \"\"\"Filter annotations in a tier\\n\\n        :param str tier: Name of the tier:\\n        :param str tier_name: Name of the new tier, when ``None`` the name will\\n                              be generated.\\n        :param list filtin: List of strings to be included, if None all\\n                            annotations all is included.\\n        :param list filtex: List of strings to be excluded, if None no strings\\n                            are excluded.\\n        :raises KeyError: If the tier is non existent.\\n        \"\"\"\\n        if tier_name is None:\\n            tier_name = \\'%s_filter\\' % tier\\n        self.remove_tier(tier_name)\\n        self.add_tier(tier_name)\\n        for a in [b for b in self.get_annotation_data_for_tier(tier)\\n                  if (filtex is None or b[2] not in filtex) and\\n                  (filtin is None or b[2] in filtin)]:\\n            self.insert_annotation(tier_name, a[0], a[1], a[2])', 'def get_full_time_interval(self):\\n        \"\"\"Give the full time interval of the file.\\n\\n        :returns: Tuple of the form: ``(min_time, max_time``.\\n        \"\"\"\\n        return (min(self.timeslots.itervalues()),\\n                max(self.timeslots.itervalues()))', 'def get_gaps_and_overlaps_duration(self, tier1, tier2, maxlen=-1,\\n                                       progressbar=False):\\n        \"\"\"Give gaps and overlaps. The return types are shown in the table\\n        below. The string will be of the format: ``id_tiername_tiername``.\\n\\n        For example when a gap occurs between tier1 and tier2 and they are\\n        called ``speakerA`` and ``speakerB`` the annotation value of that gap\\n        will be ``G12_speakerA_speakerB``.\\n\\n        | The gaps and overlaps are calculated using Heldner and Edlunds\\n          method found in:\\n        | *Heldner, M., & Edlund, J. (2010). Pauses, gaps and overlaps in\\n         conversations. Journal of Phonetics, 38(4), 555–568.\\n         doi:10.1016/j.wocn.2010.08.002*\\n\\n        +-----+--------------------------------------------+\\n        | id  | Description                                |\\n        +=====+============================================+\\n        | O12 | Overlap from tier1 to tier2                |\\n        +-----+--------------------------------------------+\\n        | O21 | Overlap from tier2 to tier1                |\\n        +-----+--------------------------------------------+\\n        | G12 | Gap from tier1 to tier2                    |\\n        +-----+--------------------------------------------+\\n        | G21 | Gap from tier2 to tier1                    |\\n        +-----+--------------------------------------------+\\n        | P1  | Pause for tier1                            |\\n        +-----+--------------------------------------------+\\n        | P2  | Pause for tier2                            |\\n        +-----+--------------------------------------------+\\n        | B12 | Within speaker overlap from tier1 to tier2 |\\n        +-----+--------------------------------------------+\\n        | B21 | Within speaker overlap from tier2 to tier1 |\\n        +-----+--------------------------------------------+\\n\\n        :param str tier1: Name of the first tier.\\n        :param str tier2: Name of the second tier.\\n        :param int maxlen: Maximum length of gaps (skip longer ones), if ``-1``\\n                           no maximum will be used.\\n        :param bool progressbar: Flag for debugging purposes that shows the\\n                                 progress during the process.\\n        :returns: List of gaps and overlaps of the form:\\n                  ``[(type, start, end)]``.\\n        :raises KeyError: If a tier is non existent.\\n        :raises IndexError: If no annotations are available in the tiers.\\n        \"\"\"\\n        spkr1anns = sorted((self.timeslots[a[0]], self.timeslots[a[1]])\\n                           for a in self.tiers[tier1][0].values())\\n        spkr2anns = sorted((self.timeslots[a[0]], self.timeslots[a[1]])\\n                           for a in self.tiers[tier2][0].values())\\n        line1 = []\\n        isin = lambda x, lst: False if\\\\\\n            len([i for i in lst if i[0] <= x and i[1] >= x]) == 0 else True\\n        minmax = (min(spkr1anns[0][0], spkr2anns[0][0]),\\n                  max(spkr1anns[-1][1], spkr2anns[-1][1]))\\n        last = (1, minmax[0])\\n        lastP = 0\\n        for ts in xrange(*minmax):\\n            in1, in2 = isin(ts, spkr1anns), isin(ts, spkr2anns)\\n            if in1 and in2:      # Both speaking\\n                if last[0] == \\'B\\':\\n                    continue\\n                ty = \\'B\\'\\n            elif in1:            # Only 1 speaking\\n                if last[0] == \\'1\\':\\n                    continue\\n                ty = \\'1\\'\\n            elif in2:            # Only 2 speaking\\n                if last[0] == \\'2\\':\\n                    continue\\n                ty = \\'2\\'\\n            else:                # None speaking\\n                if last[0] == \\'N\\':\\n                    continue\\n                ty = \\'N\\'\\n            line1.append((last[0], last[1], ts))\\n            last = (ty, ts)\\n            if progressbar and int((ts*1.0/minmax[1])*100) > lastP:\\n                lastP = int((ts*1.0/minmax[1])*100)\\n                print \\'%d%%\\' % lastP\\n        line1.append((last[0], last[1], minmax[1]))\\n        ftos = []\\n        for i in xrange(len(line1)):\\n            if line1[i][0] == \\'N\\':\\n                if i != 0 and i < len(line1) - 1 and\\\\\\n                        line1[i-1][0] != line1[i+1][0]:\\n                    ftos.append((\\'G12_%s_%s\\' % (tier1, tier2)\\n                                if line1[i-1][0] == \\'1\\' else \\'G21_%s_%s\\' %\\n                                (tier2, tier1), line1[i][1], line1[i][2]))\\n                else:\\n                    ftos.append((\\'P_%s\\' %\\n                                (tier1 if line1[i-1][0] == \\'1\\' else tier2),\\n                                line1[i][1], line1[i][2]))\\n            elif line1[i][0] == \\'B\\':\\n                if i != 0 and i < len(line1) - 1 and\\\\\\n                        line1[i-1][0] != line1[i+1][0]:\\n                    ftos.append((\\'O12_%s_%s\\' % ((tier1, tier2)\\n                                if line1[i-1][0] else \\'O21_%s_%s\\' %\\n                                (tier2, tier1)), line1[i][1], line1[i][2]))\\n                else:\\n                    ftos.append((\\'B_%s_%s\\' % ((tier1, tier2)\\n                                if line1[i-1][0] == \\'1\\' else\\n                                (tier2, tier1)), line1[i][1], line1[i][2]))\\n        return [f for f in ftos if maxlen == -1 or abs(f[2] - f[1]) < maxlen]', 'def get_tier_ids_for_linguistic_type(self, ling_type, parent=None):\\n        \"\"\"Give a list of all tiers matching a linguistic type.\\n\\n        :param str ling_type: Name of the linguistic type.\\n        :param str parent: Only match tiers from this parent, when ``None``\\n                           this option will be ignored.\\n        :returns: List of tiernames.\\n        :raises KeyError: If a tier or linguistic type is non existent.\\n        \"\"\"\\n        return [t for t in self.tiers if\\n                self.tiers[t][2][\\'LINGUISTIC_TYPE_REF\\'] == ling_type and\\n                (parent is None or self.tiers[t][2][\\'PARENT_REF\\'] == parent)]', 'def add_linguistic_type(self, lingtype, constraints=None,\\n                            timealignable=True, graphicreferences=False,\\n                            extref=None):\\n        \"\"\"Add a linguistic type.\\n\\n        :param str lingtype: Name of the linguistic type.\\n        :param list constraints: Constraint names.\\n        :param bool timealignable: Flag for time alignable.\\n        :param bool graphicreferences: Flag for graphic references.\\n        :param str extref: External reference.\\n        \"\"\"\\n        self.linguistic_types[lingtype] = {\\n            \\'LINGUISTIC_TYPE_ID\\': lingtype,\\n            \\'TIME_ALIGNABLE\\': str(timealignable).lower(),\\n            \\'GRAPHIC_REFERENCES\\': str(graphicreferences).lower(),\\n            \\'CONSTRAINTS\\': constraints}\\n        if extref is not None:\\n            self.linguistic_types[lingtype][\\'EXT_REF\\'] = extref']}, {'features': [], 'snippets': ['def __init__(self, config_path=CONFIG_PATH):\\n        self.config = configparser.ConfigParser(allow_no_value=True)\\n        self.config.read(config_path)']}, {'features': [], 'snippets': [\"def plot(files, fac=1.0):\\n    for f in files:\\n        if f.split('.')[-1] == 'xy':\\n            td = np.loadtxt(f)\\n            plt.plot(td[:, 0], np.log(1. / td[:, 1]) * fac, label=f)\\n        elif f.split('.')[-1] == 'spc':\\n            td = SPC(f)\\n            plt.plot(td.xdata, np.log(1. / np.array(td.ydata)), label=f)\\n    plt.legend()\\n    plt.show()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, pages=None, *args, **kwargs):\\n        super(GrantsSpider, self).__init__(*args, **kwargs)\\n\\n        if pages is not None:\\n            self.pages = pages\\n            self.start_urls = [ self.start_url_str % str(page) for page in xrange(1,int(self.pages)+1)]']}, {'features': [], 'snippets': ['def print_all(f):\\n    print f.read()', 'def print_a_line(line_count, f):\\n    print line_count, f.readline()']}, {'features': [], 'snippets': [\"def __init__(self,username=None,password=None,filter=None):\\n\\t\\tContentProvider.__init__(self,'hejbejse.tv','http://www.kynychova-tv.cz/',username,password,filter)\\n\\t\\topener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookielib.LWPCookieJar()))\\n\\t\\turllib2.install_opener(opener)\", \"def categories(self):\\n\\t\\tpage = util.parse_html('http://www.kynychova-tv.cz/index.php?id=5')\\n\\t\\tresult = []\\n\\t\\tfor title,uri in [(x.h3.text,x.h3.a['href']) for x in page.select('div.entry5') if x.h3]:\\n\\t\\t\\titem = self.dir_item()\\n\\t\\t\\titem['title'] = title\\n\\t\\t\\titem['url'] = uri\\n\\t\\t\\tresult.append(item)\\n\\t\\treturn result\", 'def resolve(self,item,captcha_cb=None,select_cb=None):\\n\\t\\titem = item.copy()\\n\\t\\turl = self._url(item[\\'url\\'])\\n\\t\\tpage = util.parse_html(url)\\n\\t\\tresult = []\\n\\t\\tdata=str(page.select(\\'div.entry3 > center\\')[0])\\n\\t\\tresolved = resolver.findstreams(data,[\\'<iframe(.+?)src=[\\\\\"\\\\\\'](?P<url>.+?)[\\\\\\'\\\\\"]\\'])\\n\\t\\ttry:\\n\\t\\t\\tfor i in resolved:\\n\\t\\t\\t\\titem = self.video_item()\\n\\t\\t\\t\\titem[\\'title\\'] = i[\\'name\\']\\n\\t\\t\\t\\titem[\\'url\\'] = i[\\'url\\']\\n\\t\\t\\t\\titem[\\'quality\\'] = i[\\'quality\\']\\n\\t\\t\\t\\titem[\\'surl\\'] = i[\\'surl\\']\\n\\t\\t\\t\\tresult.append(item)\\t \\n\\t\\texcept:\\n\\t\\t\\tprint \\'===Unknown resolver===\\'']}, {'features': [], 'snippets': [\"def tcplink(sock, addr):\\r\\n    print 'Accept new connection from %s:%s...' % addr\\r\\n    sock.send('Welcome!')\\r\\n    while True:\\r\\n        data = sock.recv(1024)\\r\\n        time.sleep(1)\\r\\n        if data == 'exit' or not data:\\r\\n            break\\r\\n        sock.send('Hello, %s!' % data)\\r\\n    sock.close()\\r\\n    print 'Connection from %s:%s closed.' % addr\"]}, {'features': [], 'snippets': ['def main():\\n    # first make all the appropriate directories\\n    print(\"Making directories...\")\\n    for d in LAUNCHER_PATH, DATA_PATH:\\n        print(\"Creating\", d, end=\" \", flush=True)\\n        os.mkdir(d)\\n        print(\"Success!\")\\n\\n    print(\"Generating random password file...\", end=\" \", flush=True)\\n    # then generate the password\\n    password = binascii.b2a_hex(os.urandom(32))\\n    passfile = open(PASSFILE, \"w\")\\n    passfile.write(password.decode(\\'ascii\\'))\\n    passfile.close()\\n    print(\"Success!\")\\n\\n    # Then copy \".exe\"s to the launcher path\\n    exes = GETH_EXE, LAUNCHER_EXE\\n    results = []\\n    for exe in exes:\\n        print(\"Copying\", os.path.basename(exe), \"to\", LAUNCHER_PATH, \"...\", end=\" \", flush=True)\\n        results.append(shutil.copy(exe, LAUNCHER_PATH))\\n        print(\"Sucess!\")\\n\\n    print(\"Creating node account...\", end=\" \", flush=True)\\n    # create account on node\\n    p = subprocess.Popen([results[0],\\n                      \"--password\", PASSFILE,\\n                      \"account\", \"new\"])\\n    p.wait()\\n    print(\"Success!\")\\n\\n    print(\"Creating shortcut...\", end=\" \", flush=True)\\n    desktop = os.path.join(os.path.expanduser(\\'~\\'), \\'Desktop\\')\\n    shortcut_path = os.path.join(desktop, \"Augur Launcher.lnk\")\\n    wDir = LAUNCHER_PATH\\n\\n    shell = Dispatch(\\'WScript.Shell\\')\\n    shortcut = shell.CreateShortCut(shortcut_path)\\n    shortcut.Targetpath = results[1]\\n    shortcut.WorkingDirectory = wDir\\n    shortcut.IconLocation = results[1]\\n    shortcut.save()\\n    print(\"Success!\")']}, {'features': [], 'snippets': ['def test_buildPaths(self):\\n\\n        recPaths, repPaths, rouPaths, corePaths = buildPaths()\\n        findTxt = lambda x, y: x.find(y) > -1\\n\\n        assert findTxt(recPaths[\"Task\"][0], \"base\")\\n        assert findTxt(recPaths[\"Department\"][0], \"StdPy\")\\n        assert findTxt(recPaths[\"Department\"][1], \"standard\")\\n\\n        assert findTxt(repPaths[\"ListWindowReport\"][0], \"base\")\\n        assert findTxt(repPaths[\"ExpensesList\"][0], \"StdPy\")\\n        assert findTxt(repPaths[\"ExpensesList\"][1], \"standard\")\\n\\n        assert findTxt(rouPaths[\"GenNLT\"][0], \"StdPy\")\\n        assert findTxt(rouPaths[\"GenNLT\"][1], \"standard\")\\n        assert findTxt(corePaths[\"Field\"][0], \"embedded\")\\n\\n        self.assertFalse([k for (k, v) in rouPaths.iteritems() if findTxt(v[0], \"base\")]) #no routines in base', 'def test_recordsInfo(self):\\n        recf, recd = getRecordsInfo(\"Department\", RECORD)\\n        assert recf[\"Department\"][\"AutoCashCancel\"] == \"integer\" #From StdPy\\n        assert recf[\"Department\"][\"DeptName\"]       == \"string\" #From standard\\n        assert recf[\"Department\"][\"Closed\"]         == \"Boolean\" #From Master\\n        assert recf[\"Department\"][\"internalId\"]     == \"internalid\" #From Record\\n        assert recd[\"Department\"][\"OfficePayModes\"] == \"DepartmentOfficePayModeRow\" #Recordname from detail\\n\\n        repf, repd = getRecordsInfo(\"Balance\", REPORT)\\n        assert repf[\"Balance\"][\"LabelType\"]         == \"string\" #StdPy\\n        assert repf[\"Balance\"][\"ExplodeByLabel\"]    == \"boolean\" #Standard\\n        assert repf[\"Balance\"][\"internalId\"]        == \"internalid\" #Record\\n        assert not repd[\"Balance\"] #Empty dict, no detail\\n\\n        rouf, roud = getRecordsInfo(\"GenNLT\", ROUTINE)\\n        assert rouf[\"GenNLT\"][\"ExcludeInvalid\"]     == \"boolean\"\\n        assert rouf[\"GenNLT\"][\"Table\"]              == \"string\"\\n        assert not roud[\"GenNLT\"]\\n\\n        rouf, roud = getRecordsInfo(\"LoginDialog\", RECORD)\\n        assert rouf[\"LoginDialog\"][\"Password\"]      == \"string\" #embedded\\n        assert not roud[\"LoginDialog\"]', 'def test_suite():\\n    suite = unittest.TestSuite()\\n    suite.addTest(unittest.makeSuite(TestFuncs))\\n    return suite']}, {'features': [], 'snippets': ['def outfit():\\n    collection = []\\n\\n    for _ in range(0, 5):\\n        collection.append(\"Item{}\".format(_))\\n\\n    return {\\n        \"data\": collection,\\n    }']}, {'features': [], 'snippets': ['def setUp(self):\\n        import time\\n        self.repo_path = tempfile.mkdtemp(prefix=\\'selftest-buildhistory\\',\\n            dir=get_bb_var(\\'TOPDIR\\'))\\n\\n        self.repo = Repo.init(self.repo_path)\\n        self.test_file = \"test\"\\n        self.var_map = {}', 'def commit_vars(self, to_add={}, to_remove = [], msg=\"A commit message\"):\\n        if len(to_add) == 0 and len(to_remove) == 0:\\n            return\\n\\n        for k in to_remove:\\n            self.var_map.pop(x,None)\\n        for k in to_add:\\n            self.var_map[k] = to_add[k]\\n\\n        with open(os.path.join(self.repo_path, self.test_file), \\'w\\') as repo_file:\\n            for k in self.var_map:\\n                repo_file.write(\"%s = %s\\\\n\" % (k, self.var_map[k]))\\n\\n        self.repo.git.add(\"--all\")\\n        self.repo.git.commit(message=msg)', 'def test_compare_dict_blobs(self):\\n        \"\"\"\\n        Test comparisson of dictionaries extracted from git blobs\\n        \"\"\"\\n        changesmap = { \"foo-2\" : (\"2\", \"8\"), \"bar\" : (\"\",\"4\"), \"bar-2\" : (\"\",\"5\")}\\n\\n        self.commit_vars(to_add = { \"foo\" : \"1\", \"foo-2\" : \"2\", \"foo-3\" : \"3\" })\\n        blob1 = self.repo.heads.master.commit.tree.blobs[0]\\n\\n        self.commit_vars(to_add = { \"foo-2\" : \"8\", \"bar\" : \"4\", \"bar-2\" : \"5\" })\\n        blob2 = self.repo.heads.master.commit.tree.blobs[0]\\n\\n        change_records = compare_dict_blobs(os.path.join(self.repo_path, self.test_file),\\n            blob1, blob2, False, False)\\n\\n        var_changes = { x.fieldname : (x.oldvalue, x.newvalue) for x in change_records}\\n        self.assertEqual(changesmap, var_changes, \"Changes not reported correctly\")']}, {'features': [], 'snippets': ['def __init__(self, title, info):\\n        self.title = title\\n        self.info = info', 'def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        self.setStyleSheet(self.STYLESHEET)', 'def check(self):\\n        pass', 'def save(self):\\n        pass', 'def display_error(self, error):\\n        dialog = QtWidgets.QMessageBox(QtWidgets.QMessageBox.Warning, error.title, error.info, QtWidgets.QMessageBox.Ok, self)\\n        dialog.exec_()', 'def check():\\n            try:\\n                re.compile(regex_edit.text())\\n            except re.error as e:\\n                raise OptionsCheckError(_(\"Regex Error\"), string_(e))']}, {'features': [], 'snippets': ['def load():\\n    global loaded_with_language\\n    if loaded_with_language == current_language:\\n        return\\n\\n    config.declare_permission_section(\"general\", _(\\'General Permissions\\'), 10)\\n\\n    config.declare_permission(\"general.use\",\\n         _(\"Use Multisite at all\"),\\n         _(\"Users without this permission are not let in at all\"),\\n         [ \"admin\", \"user\", \"guest\" ])\\n\\n    config.declare_permission(\"general.see_all\",\\n         _(\"See all Nagios objects\"),\\n         _(\"See all objects regardless of contacts and contact groups. \"\\n           \"If combined with \\'perform commands\\' then commands may be done on all objects.\"),\\n         [ \"admin\", \"guest\" ])\\n\\n    declare_visual_permissions(\\'views\\', _(\"views\"))\\n    declare_visual_permissions(\\'dashboards\\', _(\"dashboards\"))\\n\\n    config.declare_permission(\"general.view_option_columns\",\\n         _(\"Change view display columns\"),\\n         _(\"Interactively change the number of columns being displayed by a view (does not edit or customize the view)\"),\\n         [ \"admin\", \"user\", \"guest\" ])\\n\\n    config.declare_permission(\"general.view_option_refresh\",\\n         _(\"Change view display refresh\"),\\n         _(\"Interactively change the automatic browser reload of a view being displayed (does not edit or customize the view)\"),\\n         [ \"admin\", \"user\" ])\\n\\n    config.declare_permission(\"general.painter_options\",\\n         _(\"Change column display options\"),\\n         _(\"Some of the display columns offer options for customizing their output. \"\\n         \"For example time stamp columns can be displayed absolute, relative or \"\\n         \"in a mixed style. This permission allows the user to modify display options\"),\\n         [ \"admin\", \"user\", \"guest\" ])\\n\\n    config.declare_permission(\"general.act\",\\n         _(\"Perform commands\"),\\n         _(\"Allows users to perform Nagios commands. If no further permissions \"\\n           \"are granted, actions can only be done on objects one is a contact for\"),\\n         [ \"admin\", \"user\" ])\\n\\n    config.declare_permission(\"general.see_sidebar\",\\n         _(\"Use Check_MK sidebar\"),\\n         _(\"Without this permission the Check_MK sidebar will be invisible\"),\\n         [ \"admin\", \"user\", \"guest\" ])\\n\\n    config.declare_permission(\"general.configure_sidebar\",\\n         _(\"Configure sidebar\"),\\n         _(\"This allows the user to add, move and remove sidebar snapins.\"),\\n         [ \"admin\", \"user\" ])\\n\\n    config.declare_permission(\\'general.edit_profile\\',\\n        _(\\'Edit the user profile\\'),\\n        _(\\'Permits the user to change the user profile settings.\\'),\\n        [ \\'admin\\', \\'user\\' ]\\n    )\\n\\n    config.declare_permission(\\'general.edit_notifications\\',\\n        _(\\'Edit personal notification settings\\'),\\n        _(\\'This allows a user to edit his personal notification settings. You also need the permission \\'\\n          \\'<i>Edit the user profile</i> in order to do this.\\'),\\n        [ \\'admin\\', \\'user\\' ]\\n    )\\n\\n    config.declare_permission(\\'general.disable_notifications\\',\\n        _(\\'Disable all personal notifications\\'),\\n        _(\\'This permissions provides a checkbox in the personal settings of the user that \\'\\n          \\'allows him to completely disable all of his notifications. Use with caution.\\'),\\n        [ \\'admin\\', ]\\n    )\\n\\n    config.declare_permission(\\'general.edit_user_attributes\\',\\n        _(\\'Edit personal user attributes\\'),\\n        _(\\'This allows a user to edit his personal user attributes. You also need the permission \\'\\n          \\'<i>Edit the user profile</i> in order to do this.\\'),\\n        [ \\'admin\\', \\'user\\' ]\\n    )\\n\\n    config.declare_permission(\\'general.change_password\\',\\n        _(\\'Edit the user password\\'),\\n        _(\\'Permits the user to change the password.\\'),\\n        [ \\'admin\\', \\'user\\' ]\\n    )\\n\\n    config.declare_permission(\\'general.logout\\',\\n        _(\\'Logout\\'),\\n        _(\\'Permits the user to logout.\\'),\\n        [ \\'admin\\', \\'user\\', \\'guest\\' ]\\n    )\\n\\n    config.declare_permission(\"general.ignore_soft_limit\",\\n         _(\"Ignore soft query limit\"),\\n         _(\"Allows to ignore the soft query limit imposed upon the number of datasets returned by a query\"),\\n         [ \"admin\", \"user\" ])\\n\\n    config.declare_permission(\"general.ignore_hard_limit\",\\n         _(\"Ignore hard query limit\"),\\n         _(\"Allows to ignore the hard query limit imposed upon the number of datasets returned by a query\"),\\n         [ \"admin\" ])\\n\\n    loaded_with_language = current_language']}, {'features': [], 'snippets': [\"def __init__(self, root=None, main_frame=None):\\n        param = self.params()\\n        if root is None:\\n            # standalone\\n            self.__root = tkinter.Tk()\\n            self.__root.title(param['title'])\\n            self.__root.geometry('%sx%s+%s+%s' % (param['w'],\\n                                                  param['h'],\\n                                                  param['x'],\\n                                                  param['y']\\n                                                  ))\\n        else:\\n            # inside\\n            self.__root = root\\n\\n        self.bd = param['bd']\\n\\n        if main_frame is None:\\n            # standalone\\n            main_f = tkinter.Frame(master=self.__root, bg='black', bd=self.bd)\\n            main_f.pack(fill='both', expand=True)\\n        else:\\n            # inside\\n            main_f = main_frame\\n\\n        self.make_widgets(main_f)\", 'def root(self):\\n        return self.__root', 'def make_widgets(self, main_frame):\\n        pass', \"def params(self):\\n        param = {\\n            'x': 0,\\n            'y': 0,\\n            'w': 500,\\n            'h': 500,\\n            'title': '% Type Prog Title Here %',\\n        }\\n        return param\", \"def mk_listbox(frame, side='top', sbars='y', sel_mode=tkinter.EXTENDED):\\n    BORDER = 0\\n    COLOR = 'grey'\\n\\n    listbox_frame = tkinter.Frame(frame, bg=COLOR, bd=BORDER)\\n    listbox_frame.pack(side=side, fill='both', expand=True)\\n\\n    listbox = tkinter.Listbox(listbox_frame, selectmode=sel_mode)\\n    mk_scrollable_area(listbox, listbox_frame, sbars)\\n    return listbox\"]}, {'features': [], 'snippets': ['def defineBrowserAgent(uiname, uiversion):\\n    class AppURLopener(urllib.FancyURLopener):\\n        version = \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\"\\n        #version = uiname + \" \" + uiversion + \" / \" + sys.platform\\n    urllib._urlopener = AppURLopener()']}, {'features': [], 'snippets': ['def message(self, *args, **kwargs): # real signature unknown\\n        pass']}, {'features': [], 'snippets': ['def __call__(self, parser, namespace, values, option_string=None):\\n\\t\\tprint \"Official Repository\" # Name\\n\\t\\tprint \"web\" # Type (maybe web for web, or anything else for usb)\\n\\t\\tprint \"http://www.gcw-zero.com/files/upload/opk/\" #URL\\n\\t\\tprint \"official.py --update\" #Call for updating the list\\n\\t\\tprint \"O\" #letter to show', 'def __call__(self, parser, namespace, values, option_string=None):\\n\\t\\tprocess = subprocess.Popen(\\'wget --timeout=\\'+str(values[0])+\\' -qO- http://ziz.gp2x.de/gcw-repos/count.php\\',stdout=subprocess.PIPE,shell=True)\\n\\t\\tprocess = subprocess.Popen(\\'wget --timeout=\\'+str(values[0])+\\' -qO- http://www.gcw-zero.com/downloads\\',stdout=subprocess.PIPE,shell=True)\\n\\t\\t#process = subprocess.Popen(\\'wget --timeout=\\'+str(values[0])+\\' -qO- http://ziz.gp2x.de/temp/test.htm\\',stdout=subprocess.PIPE,shell=True)\\n\\t\\t#process = subprocess.Popen(\\'cat downloads\\',stdout=subprocess.PIPE,shell=True)\\n\\t\\toutput = process.stdout.read().split(\\'<div class=\"downloads_overview\">\\')\\n\\t\\tfor output_part in output:\\n\\t\\t\\tpart = output_part.split(\\'\\\\n\\')\\n\\t\\t\\tline_number = 0;\\n\\t\\t\\tnot_found = 1;\\n\\t\\t\\twhile (line_number < len(part)):\\n\\t\\t\\t\\tif (part[line_number].strip().startswith(\\'<span class=\"downloads_title\">\\')):\\n\\t\\t\\t\\t\\tnot_found = 0;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tline_number += 1;\\n\\t\\t\\tif not_found:\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tprogram_name_description = part[line_number];\\n\\t\\t\\tname = program_name_description.split(\\'>\\')[1].split(\\'<\\')[0];\\n\\t\\t\\tif (name == \"\"):\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tline_number = 0;\\n\\t\\t\\tnot_found = 1;\\n\\t\\t\\twhile (line_number < len(part)):\\n\\t\\t\\t\\tif (part[line_number].strip().startswith(\\'<a class=\"downloads_link\"\\')):\\n\\t\\t\\t\\t\\tnot_found = 0;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tline_number += 1;\\n\\t\\t\\tif not_found:\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tfilename = part[line_number].split(\\'href=\"file.php?file=\\')[1].split(\\'\">\\')[0];\\n\\t\\t\\tprint \"[\"+name+\"]\"\\n\\t\\t\\tdescription = program_name_description.split(\\'>\\')[3];\\n\\t\\t\\tprint \"description: \"+description\\n\\t\\t\\tprint \"filename: \" + filename\\n\\t\\t\\tl = len(part)\\n\\t\\t\\tfound_version = 0\\n\\t\\t\\tfound_image = 0\\n\\t\\t\\tfound_long = 0;\\n\\t\\t\\tfor i in range(0,l-1):\\n\\t\\t\\t\\tif string.find(part[i],\\'Publication Date\\') != -1:\\n\\t\\t\\t\\t\\tversion = part[i+1]\\n\\t\\t\\t\\t\\tversion = version.split(\\'>\\')[1]\\n\\t\\t\\t\\t\\tversion = version.split(\\'<\\')[0]\\n\\t\\t\\t\\t\\tt = time.strptime(version,\"%A, %d %b %Y\")\\n\\t\\t\\t\\t\\tprint \"version: \" + str(calendar.timegm(t)) #NEEDED!\\n\\t\\t\\t\\t\\tfound_version = 1\\n\\t\\t\\t\\tif string.find(part[i],\\'<div class=\"downloads_preview\"\\') != -1:\\n\\t\\t\\t\\t\\timage = part[i];\\n\\t\\t\\t\\t\\timage = image.split(\"background-image: url(\\'\")[1].split(\"\\');\")[0];\\n\\t\\t\\t\\t\\tprint \"image_url: http://www.gcw-zero.com/\" + image\\n\\t\\t\\t\\tif string.find(part[i],\\'<p class=\"more fade\">\\') != -1:\\n\\t\\t\\t\\t\\tlong_description = part[i];\\n\\t\\t\\t\\t\\tlong_description = long_description.split(\\'<p class=\"more fade\">\\')[1].split(\"</p>\")[0];\\n\\t\\t\\t\\t\\tlong_description = long_description.replace(\\'<br /> \\',\\'\\\\\\\\n\\')\\n\\t\\t\\t\\t\\tlong_description = long_description.split(\\'>\\')\\n\\t\\t\\t\\t\\tsys.stdout.write(\"long_description: \")\\n\\t\\t\\t\\t\\tfor long_description_part in long_description:\\n\\t\\t\\t\\t\\t\\tsys.stdout.write(long_description_part.split(\\'<\\')[0])\\n\\t\\t\\t\\t\\tsys.stdout.write(\\'\\\\n\\')\\n\\t\\t\\t\\t\\tfound_long = 1\\n\\t\\t\\t\\tif (found_version and found_image and found_long):\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tprint \"\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"data\", String(100)),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"data\": \"collate data1\"},\\n                {\"id\": 2, \"data\": \"collate data2\"},\\n            ],\\n        )', 'def test_collate_order_by(self):\\n        collation = testing.requires.get_order_by_collation(testing.config)\\n\\n        self._assert_result(\\n            select([self.tables.some_table]).order_by(\\n                self.tables.some_table.c.data.collate(collation).asc()\\n            ),\\n            [(1, \"collate data1\"), (2, \"collate data2\")],\\n        )', 'def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"x\", Integer),\\n            Column(\"y\", Integer),\\n            Column(\"q\", String(50)),\\n            Column(\"p\", String(50)),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"x\": 1, \"y\": 2, \"q\": \"q1\", \"p\": \"p3\"},\\n                {\"id\": 2, \"x\": 2, \"y\": 3, \"q\": \"q2\", \"p\": \"p2\"},\\n                {\"id\": 3, \"x\": 3, \"y\": 4, \"q\": \"q3\", \"p\": \"p1\"},\\n            ],\\n        )', 'def test_plain(self):\\n        table = self.tables.some_table\\n        lx = table.c.x.label(\"lx\")\\n        self._assert_result(select([lx]).order_by(lx), [(1,), (2,), (3,)])', 'def test_composed_multiple(self):\\n        table = self.tables.some_table\\n        lx = (table.c.x + table.c.y).label(\"lx\")\\n        ly = (func.lower(table.c.q) + table.c.p).label(\"ly\")\\n        self._assert_result(\\n            select([lx, ly]).order_by(lx, ly.desc()),\\n            [(3, util.u(\"q1p3\")), (5, util.u(\"q2p2\")), (7, util.u(\"q3p1\"))],\\n        )', 'def test_composed_int_desc(self):\\n        table = self.tables.some_table\\n        lx = (table.c.x + table.c.y).label(\"lx\")\\n        self._assert_result(\\n            select([lx]).order_by(lx.desc()), [(7,), (5,), (3,)]\\n        )', 'def test_group_by_composed(self):\\n        table = self.tables.some_table\\n        expr = (table.c.x + table.c.y).label(\"lx\")\\n        stmt = (\\n            select([func.count(table.c.id), expr])\\n            .group_by(expr)\\n            .order_by(expr)\\n        )\\n        self._assert_result(stmt, [(1, 3), (1, 5), (1, 7)])', 'def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"x\", Integer),\\n            Column(\"y\", Integer),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"x\": 1, \"y\": 2},\\n                {\"id\": 2, \"x\": 2, \"y\": 3},\\n                {\"id\": 3, \"x\": 3, \"y\": 4},\\n                {\"id\": 4, \"x\": 4, \"y\": 5},\\n            ],\\n        )', 'def test_simple_limit(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table]).order_by(table.c.id).limit(2),\\n            [(1, 1, 2), (2, 2, 3)],\\n        )', 'def test_simple_offset(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table]).order_by(table.c.id).offset(2),\\n            [(3, 3, 4), (4, 4, 5)],\\n        )', 'def test_simple_limit_offset(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table]).order_by(table.c.id).limit(2).offset(1),\\n            [(2, 2, 3), (3, 3, 4)],\\n        )', 'def test_limit_offset_nobinds(self):\\n        \"\"\"test that \\'literal binds\\' mode works - no bound params.\"\"\"\\n\\n        table = self.tables.some_table\\n        stmt = select([table]).order_by(table.c.id).limit(2).offset(1)\\n        sql = stmt.compile(\\n            dialect=config.db.dialect, compile_kwargs={\"literal_binds\": True}\\n        )\\n        sql = str(sql)\\n\\n        self._assert_result(sql, [(2, 2, 3), (3, 3, 4)])', 'def test_bound_limit(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table]).order_by(table.c.id).limit(bindparam(\"l\")),\\n            [(1, 1, 2), (2, 2, 3)],\\n            params={\"l\": 2},\\n        )', 'def test_bound_offset(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table]).order_by(table.c.id).offset(bindparam(\"o\")),\\n            [(3, 3, 4), (4, 4, 5)],\\n            params={\"o\": 2},\\n        )', 'def test_bound_limit_offset(self):\\n        table = self.tables.some_table\\n        self._assert_result(\\n            select([table])\\n            .order_by(table.c.id)\\n            .limit(bindparam(\"l\"))\\n            .offset(bindparam(\"o\")),\\n            [(2, 2, 3), (3, 3, 4)],\\n            params={\"l\": 2, \"o\": 1},\\n        )', 'def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"x\", Integer),\\n            Column(\"y\", Integer),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"x\": 1, \"y\": 2},\\n                {\"id\": 2, \"x\": 2, \"y\": 3},\\n                {\"id\": 3, \"x\": 3, \"y\": 4},\\n                {\"id\": 4, \"x\": 4, \"y\": 5},\\n            ],\\n        )', 'def test_plain_union(self):\\n        table = self.tables.some_table\\n        s1 = select([table]).where(table.c.id == 2)\\n        s2 = select([table]).where(table.c.id == 3)\\n\\n        u1 = union(s1, s2)\\n        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])', 'def test_limit_offset_selectable_in_unions(self):\\n        table = self.tables.some_table\\n        s1 = (\\n            select([table])\\n            .where(table.c.id == 2)\\n            .limit(1)\\n            .order_by(table.c.id)\\n        )\\n        s2 = (\\n            select([table])\\n            .where(table.c.id == 3)\\n            .limit(1)\\n            .order_by(table.c.id)\\n        )\\n\\n        u1 = union(s1, s2).limit(2)\\n        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])', 'def test_order_by_selectable_in_unions(self):\\n        table = self.tables.some_table\\n        s1 = select([table]).where(table.c.id == 2).order_by(table.c.id)\\n        s2 = select([table]).where(table.c.id == 3).order_by(table.c.id)\\n\\n        u1 = union(s1, s2).limit(2)\\n        self._assert_result(u1.order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)])', 'def test_limit_offset_in_unions_from_alias(self):\\n        table = self.tables.some_table\\n        s1 = (\\n            select([table])\\n            .where(table.c.id == 2)\\n            .limit(1)\\n            .order_by(table.c.id)\\n        )\\n        s2 = (\\n            select([table])\\n            .where(table.c.id == 3)\\n            .limit(1)\\n            .order_by(table.c.id)\\n        )\\n\\n        # this necessarily has double parens\\n        u1 = union(s1, s2).alias()\\n        self._assert_result(\\n            u1.select().limit(2).order_by(u1.c.id), [(2, 2, 3), (3, 3, 4)]\\n        )', 'def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"x\", Integer),\\n            Column(\"y\", Integer),\\n            Column(\"z\", String(50)),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"x\": 1, \"y\": 2, \"z\": \"z1\"},\\n                {\"id\": 2, \"x\": 2, \"y\": 3, \"z\": \"z2\"},\\n                {\"id\": 3, \"x\": 3, \"y\": 4, \"z\": \"z3\"},\\n                {\"id\": 4, \"x\": 4, \"y\": 5, \"z\": \"z4\"},\\n            ],\\n        )', 'def test_multiple_empty_sets(self):\\n        # test that any anonymous aliasing used by the dialect\\n        # is fine with duplicates\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(table.c.x.in_(bindparam(\"q\", expanding=True)))\\n            .where(table.c.y.in_(bindparam(\"p\", expanding=True)))\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(stmt, [], params={\"q\": [], \"p\": []})', 'def test_empty_heterogeneous_tuples(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(\\n                tuple_(table.c.x, table.c.z).in_(\\n                    bindparam(\"q\", expanding=True)\\n                )\\n            )\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(stmt, [], params={\"q\": []})', 'def test_empty_homogeneous_tuples(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(\\n                tuple_(table.c.x, table.c.y).in_(\\n                    bindparam(\"q\", expanding=True)\\n                )\\n            )\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(stmt, [], params={\"q\": []})', 'def test_bound_in_two_tuple(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(\\n                tuple_(table.c.x, table.c.y).in_(\\n                    bindparam(\"q\", expanding=True)\\n                )\\n            )\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(\\n            stmt, [(2,), (3,), (4,)], params={\"q\": [(2, 3), (3, 4), (4, 5)]}\\n        )', 'def test_bound_in_heterogeneous_two_tuple(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(\\n                tuple_(table.c.x, table.c.z).in_(\\n                    bindparam(\"q\", expanding=True)\\n                )\\n            )\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(\\n            stmt,\\n            [(2,), (3,), (4,)],\\n            params={\"q\": [(2, \"z2\"), (3, \"z3\"), (4, \"z4\")]},\\n        )', 'def test_empty_set_against_integer_negation(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(table.c.x.notin_(bindparam(\"q\", expanding=True)))\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={\"q\": []})', 'def test_empty_set_against_string_negation(self):\\n        table = self.tables.some_table\\n\\n        stmt = (\\n            select([table.c.id])\\n            .where(table.c.z.notin_(bindparam(\"q\", expanding=True)))\\n            .order_by(table.c.id)\\n        )\\n\\n        self._assert_result(stmt, [(1,), (2,), (3,), (4,)], params={\"q\": []})', 'def define_tables(cls, metadata):\\n        Table(\\n            \"some_table\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"data\", String(50)),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.some_table.insert(),\\n            [\\n                {\"id\": 1, \"data\": \"abcdefg\"},\\n                {\"id\": 2, \"data\": \"ab/cdefg\"},\\n                {\"id\": 3, \"data\": \"ab%cdefg\"},\\n                {\"id\": 4, \"data\": \"ab_cdefg\"},\\n                {\"id\": 5, \"data\": \"abcde/fg\"},\\n                {\"id\": 6, \"data\": \"abcde%fg\"},\\n                {\"id\": 7, \"data\": \"ab#cdefg\"},\\n                {\"id\": 8, \"data\": \"ab9cdefg\"},\\n                {\"id\": 9, \"data\": \"abcde#fg\"},\\n                {\"id\": 10, \"data\": \"abcd9fg\"},\\n            ],\\n        )', 'def test_startswith_unescaped(self):\\n        col = self.tables.some_table.c.data\\n        self._test(col.startswith(\"ab%c\"), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})', 'def test_startswith_sqlexpr(self):\\n        col = self.tables.some_table.c.data\\n        self._test(\\n            col.startswith(literal_column(\"\\'ab%c\\'\")),\\n            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\\n        )', 'def test_startswith_autoescape_escape(self):\\n        col = self.tables.some_table.c.data\\n        self._test(col.startswith(\"ab%c\", autoescape=True, escape=\"#\"), {3})\\n        self._test(col.startswith(\"ab#c\", autoescape=True, escape=\"#\"), {7})', 'def test_endswith_sqlexpr(self):\\n        col = self.tables.some_table.c.data\\n        self._test(\\n            col.endswith(literal_column(\"\\'e%fg\\'\")), {1, 2, 3, 4, 5, 6, 7, 8, 9}\\n        )', 'def test_endswith_escape(self):\\n        col = self.tables.some_table.c.data\\n        self._test(col.endswith(\"e##fg\", escape=\"#\"), {9})', 'def test_contains_unescaped(self):\\n        col = self.tables.some_table.c.data\\n        self._test(col.contains(\"b%cde\"), {1, 2, 3, 4, 5, 6, 7, 8, 9})', 'def test_contains_escape(self):\\n        col = self.tables.some_table.c.data\\n        self._test(col.contains(\"b##cde\", escape=\"#\"), {7})', 'def define_tables(cls, metadata):\\n        Table(\\n            \"square\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"side\", Integer),\\n            Column(\"area\", Integer, Computed(\"side * side\")),\\n            Column(\"perimeter\", Integer, Computed(\"4 * side\")),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.square.insert(),\\n            [{\"id\": 1, \"side\": 10}, {\"id\": 10, \"side\": 42}],\\n        )', 'def test_select_columns(self):\\n        with config.db.connect() as conn:\\n            res = conn.execute(\\n                select(\\n                    [self.tables.square.c.area, self.tables.square.c.perimeter]\\n                )\\n                .select_from(self.tables.square)\\n                .order_by(self.tables.square.c.id)\\n            ).fetchall()\\n            eq_(res, [(100, 40), (1764, 168)])', 'def define_tables(cls, metadata):\\n        Table(\\n            \"stuff\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"data\", String(50)),\\n        )', 'def insert_data(cls, connection):\\n        connection.execute(\\n            cls.tables.stuff.insert(),\\n            [\\n                {\"id\": 1, \"data\": \"some data\"},\\n                {\"id\": 2, \"data\": \"some data\"},\\n                {\"id\": 3, \"data\": \"some data\"},\\n                {\"id\": 4, \"data\": \"some other data\"},\\n            ],\\n        )', 'def test_select_exists_false(self, connection):\\n        stuff = self.tables.stuff\\n        eq_(\\n            connection.execute(\\n                select([literal(1)]).where(\\n                    exists().where(stuff.c.data == \"no data\")\\n                )\\n            ).fetchall(),\\n            [],\\n        )', 'def define_tables(cls, metadata):\\n        Table(\\n            \"is_distinct_test\",\\n            metadata,\\n            Column(\"id\", Integer, primary_key=True),\\n            Column(\"col_a\", Integer, nullable=True),\\n            Column(\"col_b\", Integer, nullable=True),\\n        )', 'def test_is_or_isnot_distinct_from(\\n        self, col_a_value, col_b_value, expected_row_count_for_is, connection']}, {'features': [], 'snippets': ['def __call__(cls, verb, *args):\\n        if verb not in cls.verbs:\\n            cls.verbs[verb] = super(_VerbSingleton, cls).__call__(verb, *args)\\n        return cls.verbs[verb]', 'def __init__(self, verb, description):\\n        \"\"\" Initialize EventType.\\n\\n        :verb: Scim verb\\n        :description: HR description text\\n        \"\"\"\\n        self.verb = verb\\n        self.description = description', 'def __eq__(self, other):\\n        \"\"\"Equality.\"\"\"\\n        return isinstance(other, EventType) and other.verb == self.verb', 'def __init__(self, entity_id, entity_type, ident):\\n        self.entity_id = int(entity_id)\\n        self.entity_type = entity_type\\n        self.ident = ident', 'def __eq__(self, other):\\n        return (isinstance(other, EntityRef) and\\n                self.entity_id == other.entity_id)', 'def __init__(self, slot):\\n        \"\"\" Creates a new datetime descriptor.\\n\\n        :param str slot:\\n            The attribute name where the actual value is stored.\\n        \"\"\"\\n        self.slot = slot', 'def __get__(self, obj, cls=None):\\n        if not obj:\\n            return self\\n        return getattr(obj, self.slot, None)', 'def __delete__(self, obj):\\n        if hasattr(obj, self.slot):\\n            delattr(obj, self.slot)', 'def __init__(self, event_type,\\n                 subject=None,\\n                 objects=None,\\n                 context=None,\\n                 attributes=None,\\n                 timestamp=None,\\n                 scheduled=None):\\n        \"\"\"\\n        :param EventType event: the type of event\\n        :param EntityRef subject: reference to the affected entity\\n        :param list objects: sequence of other affected objects (EntityRef)\\n        :param list context: sequence of affected systems (str)\\n        :param list attributes: sequence of affected attributes (str)\\n        :param datetime timestamp: when the event originated\\n        :param datetime schedule: when the event should be issued\\n        \"\"\"\\n        self.event_type = event_type\\n        self.subject = subject\\n        self.timestamp = timestamp\\n        self.scheduled = scheduled\\n        self.objects = set(objects or [])\\n        self.context = set(context or [])\\n        self.attributes = set(attributes or [])', 'def mergeable(self, other):\\n        \"\"\"Can this event be merged with other.\"\"\"\\n\\n        if self.scheduled is not None:\\n            return False\\n        if self.subject != other.subject:\\n            return False\\n        if self.event_type == CREATE:\\n            return other.event_type not in (DEACTIVATE, REMOVE)\\n        if self.event_type == DELETE:\\n            return other.event_type in (REMOVE, DEACTIVATE, ADD, ACTIVATE,\\n                                        MODIFY, PASSWORD)\\n        if (self.event_type == other.event_type and\\n                self.event_type in (ADD, REMOVE, ACTIVATE, DEACTIVATE)):\\n            return True\\n        if self.context != other.context:\\n            return False\\n        return True', 'def ret_self():\\n            self.objects.update(other.objects)\\n            return [self]']}, {'features': [], 'snippets': [\"def to_seconds(time, unit):\\r\\n    if unit == 's':\\r\\n        return float(time)\\r\\n    elif unit == 'm':\\r\\n        return float(time) * 60\\r\\n    elif unit == 'h':\\r\\n        return float(time) * 60 * 60\\r\\n    elif unit == 'd':\\r\\n        return float(time) * 60 * 60 * 24\", \"def run(msg):\\r\\n    input = get_input(msg['text'])\"]}, {'features': [], 'snippets': ['def setUp(self):\\n        self.parser = Parser()']}, {'features': [], 'snippets': [\"def get_path():\\n    return addon.getAddonInfo('path').decode('utf-8')\", \"def translate_path(path):\\n    return xbmc.translatePath(path).decode('utf-8')\", \"def get_version():\\n    return addon.getAddonInfo('version')\", \"def get_name():\\n    return addon.getAddonInfo('name')\", 'def end_of_directory(cache_to_disc=True):\\n    xbmcplugin.endOfDirectory(int(sys.argv[1]), cacheToDisc=cache_to_disc)', \"def create_item(queries, label, thumb='', fanart='', is_folder=None, is_playable=None, total_items=0, menu_items=None, replace_menu=False):\\n    list_item = xbmcgui.ListItem(label, iconImage=thumb, thumbnailImage=thumb)\\n    add_item(queries, list_item, fanart, is_folder, is_playable, total_items, menu_items, replace_menu)\", \"def parse_query(query):\\n    q = {'mode': 'main'}\\n    if query.startswith('?'): query = query[1:]\\n    queries = urlparse.parse_qs(query)\\n    for key in queries:\\n        if len(queries[key]) == 1:\\n            q[key] = queries[key][0]\\n        else:\\n            q[key] = queries[key]\\n    return q\"]}, {'features': [], 'snippets': ['def __init__(self, options):\\n        MasterRule.__init__(self, options)\\n        self.short_name=\"php\"\\n        self.long_name=\"Checks security problems on php config file\"\\n        self.type=\"config\"\\n        self.required_files = [\\'/etc/php5/apache2/php.ini\\', \\'/etc/php5/cli/php.ini\\', \\'/etc/php.ini\\']']}, {'features': [], 'snippets': ['def __init__(self):\\n         #Método constructor, asociando los widgets\\n        self.glade_file = \"progreso.glade\"\\n        self.glade = gtk.Builder()\\n        self.glade.add_from_file(self.glade_file)\\n        self.window1 = self.glade.get_object(\\'window1\\')\\n        self.togglebutton1 = self.glade.get_object(\\'togglebutton1\\')\\n        self.button1 = self.glade.get_object(\\'button1\\')\\n        self.progressbar1 = self.glade.get_object(\\'progressbar1\\')\\n        self.new_val = 0.0\\n        self.rango =60\\n        #Definiendo el valor inicial de la barra de proceso, definiendo los     saltos en 0.1\\n        self.progressbar1.set_fraction(self.new_val)\\n        self.progressbar1.set_pulse_step(0.1)\\n        self.window1.connect(\"destroy\",self.on_window1_destroy)\\n        self.button1.connect(\\'clicked\\', self.on_button1_clicked)\\n        self.togglebutton1.connect(\\'toggled\\',self.on_togglebutton1_toggled)\\n        #Iniciando el hilo en el constructor\\n        threading.Thread.__init__(self)\\n        self.window1.show_all()']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.cg = CharacterGeneratorMock(width = 9, height = 14)\\n        self.mda = MonochromeDisplayAdapter(self.cg)', 'def reset_testable(self):\\n        self.reset_count += 1', 'def test_ports_list(self):\\n        self.assertEqual(self.mda.get_ports_list(), [0x03B0, 0x03B1, 0x03B2, 0x03B3,\\n                                                     0x03B4, 0x03B5, 0x03B6, 0x03B7,\\n                                                     0x03B8, 0x03B9, 0x03BA, 0x03BB])', 'def test_get_memory_size(self):\\n        self.assertEqual(self.mda.get_memory_size(), 4096)', 'def test_initial_state(self):\\n        self.assertEqual(self.mda.control_reg, 0x00)\\n        self.assertEqual(self.mda.control_reg, 0x00)\\n        self.assertEqual(self.mda.screen, None)\\n        self.assertEqual(self.mda.char_generator, self.cg)\\n        self.assertEqual(len(self.mda.video_ram), 4096)', 'def test_mem_write_byte_updates_video_ram(self):\\n        self.mda.mem_write_byte(0x0000, 0x41)\\n        self.assertEqual(self.mda.video_ram[0x0000], 0x41)', 'def test_mem_write_byte_calls_char_generator_top_left(self):\\n        self.mda.mem_write_byte(0x0000, 0x41)\\n        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_GREEN, MDA_BLACK))', 'def test_mem_write_byte_calls_char_generator_bottom_right(self):\\n        self.mda.mem_write_byte(3998, 0xFF)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))', 'def test_mem_write_byte_char_before_attribute(self):\\n        self.mda.mem_write_byte(3998, 0xFF)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_GREEN, MDA_BLACK))\\n        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))', 'def test_mem_write_byte_attribute_before_char(self):\\n        self.mda.mem_write_byte(3999, MDA_ATTR_INTENSITY)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))\\n        self.mda.mem_write_byte(3998, 0xFF)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0xFF, MDA_BRIGHT_GREEN, MDA_BLACK))', 'def test_mem_write_byte_write_off_screen(self):\\n        self.mda.mem_write_byte(4000, 0xFF)\\n        self.assertEqual(self.cg.last_blit, None)', 'def test_mem_read_byte(self):\\n        self.mda.video_ram[77] = 0xA5\\n        self.assertEqual(self.mda.mem_read_byte(77), 0xA5)', 'def test_mem_read_byte_off_screen(self):\\n        self.assertEqual(self.mda.mem_read_byte(4000), 0x00)', 'def test_reset_on_high_resolution_enable(self):\\n        self.assertEqual(self.reset_count, 0)', \"def test_mem_write_word_at_top_left(self):\\n        self.mda.mem_write_word(0x0000, 0x0841) # 'A' with intensity.\\n        self.assertEqual(self.mda.video_ram[0x0000], 0x41)\\n        self.assertEqual(self.mda.video_ram[0x0001], 0x08)\\n        self.assertEqual(self.cg.last_blit, (None, (0, 0), 0x41, MDA_BRIGHT_GREEN, MDA_BLACK))\", \"def test_mem_write_word_at_bottom_right(self):\\n        self.mda.mem_write_word(3998, 0x085A) # 'Z' with intensity.\\n        self.assertEqual(self.mda.video_ram[3998], 0x5A)\\n        self.assertEqual(self.mda.video_ram[3999], 0x08)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x5A, MDA_BRIGHT_GREEN, MDA_BLACK))\", \"def test_mem_write_word_at_bottom_right_just_past(self):\\n        self.mda.mem_write_word(3999, 0xFF08) # 'Z' with intensity.\\n        self.assertEqual(self.mda.video_ram[3998], 0x00) # Should be unmodified.\\n        self.assertEqual(self.mda.video_ram[3999], 0x08)\\n        self.assertEqual(self.cg.last_blit, (None, (711, 336), 0x00, MDA_BRIGHT_GREEN, MDA_BLACK))\", 'def test_mem_read_word(self):\\n        self.mda.video_ram[0x0000] = 0x41\\n        self.mda.video_ram[0x0001] = 0x08\\n        self.assertEqual(self.mda.mem_read_word(0x0000), 0x0841)', 'def test_mem_read_word_just_past_the_end(self):\\n        self.mda.video_ram[3998] = 0x12\\n        self.mda.video_ram[3999] = 0x34\\n        self.assertEqual(self.mda.mem_read_word(3999), 0x0034)', 'def test_horizontal_retrace_toggles(self):\\n        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)\\n        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF1)\\n        self.assertEqual(self.mda.io_read_byte(0x3BA), 0xF0)', 'def test_current_pixel_updates_on_status_read(self):\\n        self.assertEqual(self.mda.current_pixel, [0, 0])\\n        self.mda.io_read_byte(0x3BA)\\n        self.assertEqual(self.mda.current_pixel, [1, 0])', 'def test_current_pixel_wraps_right(self):\\n        self.mda.current_pixel = [719, 0]\\n        self.mda.io_read_byte(0x3BA)\\n        self.assertEqual(self.mda.current_pixel, [0, 1])', 'def test_current_pixel_wraps_bottom(self):\\n        self.mda.current_pixel = [719, 349]\\n        self.mda.io_read_byte(0x3BA)\\n        self.assertEqual(self.mda.current_pixel, [0, 0])']}, {'features': [], 'snippets': ['def sumaDos():\\n    print 10*20', 'def areatriangulo(base,altura):\\n    result2=(base*altura)/2\\n    print result2', 'def __init__(self, nombre, edad):\\n        self.nombre=nombre\\n        self.edad=edad', 'def esMayor(self):\\n    if self.edad>=18:\\n        return true\\n    else:\\n        return false', 'def main():\\n    e=Estudiante(\"Diego\",22)\\n    print\"Hola %s\" % e.hola()\\n    if e.esMayor():\\n        print\"Es mayor de edad\"\\n        else:\\n            print\"Es menor de edad\"\\n\\n    contador = 0\\n    while contador <=10:\\n        print contador\\n        contador +=1', 'def getWeb():\\n    try:\\n        web=urllib2.urlopen(\"http://itjiquilpan.edu.mx/\")\\n        print web.read()\\n        web.close()\\n        except urllib2.HTTPError, e:\\n            print e\\n\\n        except urllib2.URLError as e:\\n            print e']}, {'features': [], 'snippets': ['def _getSmartIndenter(indenterName, qpart, indenter):\\n    \"\"\"Get indenter by name.\\n    Available indenters are none, normal, cstyle, haskell, lilypond, lisp, python, ruby, xml\\n    Indenter name is not case sensitive\\n    Raise KeyError if not found\\n    indentText is indentation, which shall be used. i.e. \\'\\\\t\\' for tabs, \\'    \\' for 4 space symbols\\n    \"\"\"\\n    indenterName = indenterName.lower()\\n\\n    if indenterName in (\\'haskell\\', \\'lilypond\\'):  # not supported yet\\n        logger.warning(\\'Smart indentation for %s not supported yet. But you could be a hero who implemented it\\' % indenterName)\\n        from qutepart.indenter.base import IndentAlgNormal as indenterClass\\n    elif \\'none\\' == indenterName:\\n        from qutepart.indenter.base import IndentAlgBase as indenterClass\\n    elif \\'normal\\' == indenterName:\\n        from qutepart.indenter.base import IndentAlgNormal as indenterClass\\n    elif \\'cstyle\\' == indenterName:\\n        from qutepart.indenter.cstyle import IndentAlgCStyle as indenterClass\\n    elif \\'python\\' == indenterName:\\n        from qutepart.indenter.python import IndentAlgPython as indenterClass\\n    elif \\'ruby\\' == indenterName:\\n        from qutepart.indenter.ruby import IndentAlgRuby as indenterClass\\n    elif \\'xml\\' == indenterName:\\n        from qutepart.indenter.xmlindent import IndentAlgXml as indenterClass\\n    elif \\'haskell\\' == indenterName:\\n        from qutepart.indenter.haskell import IndenterHaskell as indenterClass\\n    elif \\'lilypond\\' == indenterName:\\n        from qutepart.indenter.lilypond import IndenterLilypond as indenterClass\\n    elif \\'lisp\\' == indenterName:\\n        from qutepart.indenter.lisp import IndentAlgLisp as indenterClass\\n    elif \\'scheme\\' == indenterName:\\n        from qutepart.indenter.scheme import IndentAlgScheme as indenterClass\\n    else:\\n        raise KeyError(\"Indenter %s not found\" % indenterName)\\n\\n    return indenterClass(qpart, indenter)', \"def __init__(self, qpart):\\n        self._qpart = qpart\\n\\n        self.width = self._DEFAULT_INDENT_WIDTH\\n        self.useTabs = self._DEFAULT_INDENT_USE_TABS\\n\\n        self._smartIndenter = _getSmartIndenter('normal', self._qpart, self)\", 'def text(self):\\n        \"\"\"Get indent text as \\\\t or string of spaces\\n        \"\"\"\\n        if self.useTabs:\\n            return \\'\\\\t\\'\\n        else:\\n            return \\' \\' * self.width', 'def autoIndentBlock(self, block, char = \\'\\\\n\\'):\\n        \"\"\"Indent block after Enter pressed or trigger character typed\\n        \"\"\"\\n        cursor = QTextCursor(block)\\n        currentText = block.text()\\n        spaceAtStartLen = len(currentText) - len(currentText.lstrip())\\n        currentIndent = currentText[:spaceAtStartLen]\\n        indent = self._smartIndenter.computeIndent(block, char)\\n        if indent is not None and indent != currentIndent:\\n            self._qpart.replaceText(block.position(), spaceAtStartLen, indent)', 'def blockIndentation(block):\\n            text = block.text()\\n            return text[:len(text) - len(text.lstrip())]', \"def indentBlock(block):\\n            cursor = cursorAtSpaceEnd(block)\\n            cursor.insertText(' ' if withSpace else self.text())\", \"def unIndentBlock(block):\\n            currentIndent = blockIndentation(block)\\n\\n            if currentIndent.endswith('\\\\t'):\\n                charsToRemove = 1\\n            elif withSpace:\\n                charsToRemove = 1 if currentIndent else 0\\n            else:\\n                if self.useTabs:\\n                    charsToRemove = min(spacesCount(currentIndent), self.width)\\n                else:  # spaces\\n                    if currentIndent.endswith(self.text()):  # remove indent level\\n                        charsToRemove = self.width\\n                    else:  # remove all spaces\\n                        charsToRemove = min(spacesCount(currentIndent), self.width)\\n\\n            if charsToRemove:\\n                cursor = cursorAtSpaceEnd(block)\\n                cursor.setPosition(cursor.position() - charsToRemove, QTextCursor.KeepAnchor)\\n                cursor.removeSelectedText()\", 'def onShortcutIndentAfterCursor(self):\\n        \"\"\"Tab pressed and no selection. Insert text after cursor\\n        \"\"\"\\n        cursor = self._qpart.textCursor()\\n\\n        def insertIndent():\\n            if self.useTabs:\\n                cursor.insertText(\\'\\\\t\\')\\n            else:  # indent to integer count of indents from line start\\n                charsToInsert = self.width - (len(self._qpart.textBeforeCursor()) % self.width)\\n                cursor.insertText(\\' \\' * charsToInsert)\\n\\n        if cursor.positionInBlock() == 0:  # if no any indent - indent smartly\\n            block = cursor.block()\\n            self.autoIndentBlock(block, \\'\\')\\n\\n            # if no smart indentation - just insert one indent\\n            if self._qpart.textBeforeCursor() == \\'\\':\\n                insertIndent()\\n        else:\\n            insertIndent()', 'def onAutoIndentTriggered(self):\\n        \"\"\"Indent current line or selected lines\\n        \"\"\"\\n        cursor = self._qpart.textCursor()\\n\\n        startBlock = self._qpart.document().findBlock(cursor.selectionStart())\\n        endBlock = self._qpart.document().findBlock(cursor.selectionEnd())\\n\\n        if startBlock != endBlock:  # indent multiply lines\\n            stopBlock = endBlock.next()\\n\\n            block = startBlock\\n\\n            with self._qpart:\\n                while block != stopBlock:\\n                    self.autoIndentBlock(block, \\'\\')\\n                    block = block.next()\\n        else:  # indent 1 line\\n            self.autoIndentBlock(startBlock, \\'\\')']}, {'features': [], 'snippets': ['def setup_keyring(keyring_name):\\n    \\'\\'\\'Setup the keyring\\'\\'\\'\\n    keyring_path = os.path.join(\"test\", \"outputdata\", keyring_name)\\n    # Delete the entire keyring\\n    shutil.rmtree(keyring_path, ignore_errors=True)\\n    os.makedirs(keyring_path)\\n    gpg = GPG(gnupghome=keyring_path, gpgbinary=\"gpg\")\\n    for key_name in [\"key1_private\", \"key1_public\"]:\\n        with open(os.path.join(\"test\", \"inputdata\", key_name + \".txt\"), \"r\") as keyfile:\\n            key_str = \"\".join(keyfile.readlines())\\n        import_result = gpg.import_keys(key_str)\\n        print(\"Import result:\", type(import_result))\\n        print(import_result.__dict__)\\n        if import_result.count == 1 and len(set(import_result.fingerprints)) == 1:\\n            print(\"Got one import result\")\\n    return gpg']}, {'features': [], 'snippets': ['def count(func):\\n    def wrapper(*args, **kw):\\n        global call_count\\n        call_count += 1\\n        return func(*args, **kw)\\n    return wrapper', 'def decorator(func_to_decorate):\\n\\tdef wrapper(*args, **kwargs):\\n\\t\\t# do something before invocation\\n\\t\\tresult = func_to_decorate(*args,**kwargs)\\n\\t\\t# do something after invocation\\n\\t\\treturn result\\n\\t#update wrapper.__doc__ and .func_name\\n\\t# or functools.wraps\\t\\n\\treturn wrapper', 'def __init__(self, function):\\n\\t\\tself.function = function', 'def __init__(self, function):\\n\\t\\tself.function = function', 'def wrapper(*args, **kw):\\n\\t\\t\\t# do something before invocation\\n\\t\\t\\tresult = self.function(*args, **kw)\\n\\t\\t\\t# do something after\\n\\t\\t\\treturn result', 'def limit(length):\\n\\tdef decorator(function):\\n\\t\\tdef wrapper(*args, **kw):\\n\\t\\t\\tresult = function(*args, **kw)\\n\\t\\t\\tresult = result[:length]\\n\\t\\t\\treturn result\\n\\t\\treturn wrapper\\n\\treturn decorator', 'def limit(length):\\n\\tdef decorator(function):\\n\\t\\tdef wrapper(*args, **kw):\\n\\t\\t\\tresult = function(*args, **kw)\\n\\t\\t\\tresult = result[:length]\\n\\t\\t\\treturn result\\n\\t\\twrapper.__doc__ = function.__doc__\\n\\t\\twrapper.func_name = function.func_name\\t\\n\\t\\treturn wrapper\\n\\treturn decorator', 'def limit(length):\\n\\tdef decorator(function):\\n\\t\\t@functools.wraps(function)\\n\\t\\tdef wrapper(*args, **kw):\\n\\t\\t\\tresult = function(*args, **kw)\\n\\t\\t\\tresult = result[:length]\\n\\t\\t\\treturn result\\n\\t\\t#wrapper.__doc__ = function.__doc__\\n\\t\\t#wrapper.func_name = function.func_name\\t\\n\\t\\treturn wrapper\\n\\treturn decorator', 'def cwd_decorator(func):\\n\\t\"\"\"\\n\\tdecorator to change cwd to directory containing rst for this function\\n\\t\"\"\"\\n\\tdef wrapper(*args, **kw):\\n\\t\\tcur_dir = os.getcwd()\\n\\t\\tfound = False\\n\\t\\tfor arg in sys.argv:\\n\\t\\t\\tif arg.endswith(\".rst\"):\\n\\t\\t\\t\\tfound = arg\\n\\t\\t\\t\\tbreak\\n\\t\\tif found:\\n\\t\\t\\tdirectory = os.path.dirname(arg)\\n\\t\\t\\tif directory:\\n\\t\\t\\t\\tos.chdir(directory)\\n\\t\\tdata = func(*args, **kw)\\n\\t\\tos.chdir(cur_dir)\\n\\t\\treturn data\\n\\treturn wrapper', 'def getx(self):\\n\\t\\treturn self._x', 'def delx(self):\\n\\t\\tdel self._x']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, execute = False, ip = '127.0.0.1', port = 5000, npc = False):\\n\\t\\tself.ip = ip\\n\\t\\tself.port = port\\n\\t\\tself.npc = npc\\n\\t\\tif (execute):\\n\\t\\t\\tself.iniciaBatalha()\", 'def iniciaBatalha(self):\\n\\t\\tpkmn = pokemon.Pokemon()\\n\\t\\txml = self.writeXML(pkmn)\\n\\t\\ttry:\\n\\t\\t\\tself.battle_state = requests.post(\\'http://{}:{}/battle/\\'.format(self.ip, self.port), data = xml).text\\n\\t\\texcept requests.exceptions.ConnectionError:\\n\\t\\t\\tprint(\"Não foi possível conectar ao servidor.\")\\n\\t\\t\\treturn None\\n\\t\\tpkmn2 = pokemon.lePokemonXML(1, self.battle_state)\\n\\t\\tself.batalha = batalha.Batalha([pkmn, pkmn2])\\n\\t\\tif (self.npc): \\n\\t\\t\\tself.batalha.pkmn[0].npc = True\\n\\t\\t\\tprint(\"Eu sou um NPC\")\\n\\t\\tself.batalha.turno = 0\\n\\t\\tself.batalha.display.showPokemon(self.batalha.pkmn[0])\\n\\t\\tself.batalha.display.showPokemon(self.batalha.pkmn[1])\\n\\t\\treturn self.atualizaBatalha()', \"def sendShutdownSignal(self):\\n\\t\\trequests.post('http://{}:{}/shutdown'.format(self.ip, self.port))\"]}, {'features': [], 'snippets': ['def setup(*args, **kwargs):\\n    try:\\n        ctypes.cdll.LoadLibrary(leptlib)\\n    except Exception as e:\\n        raise NidabaPluginException(e.message)', 'def sauvola(doc, method=u\\'sauvola\\', whsize=10, factor=0.35):\\n    \"\"\"\\n    Binarizes an input document utilizing Sauvola thresholding as described in\\n    [0]. Expects 8bpp grayscale images as input.\\n\\n    [0] Sauvola, Jaakko, and Matti Pietikäinen. \"Adaptive document image\\n    binarization.\" Pattern recognition 33.2 (2000): 225-236.\\n\\n    Args:\\n        doc (unicode): The input document tuple.\\n        method (unicode): The suffix string appended to all output files\\n        whsize (int): The window width and height that local statistics are\\n                      calculated on are twice the value of whsize. The minimal\\n                      value is 2.\\n        factor (float): The threshold reduction factor due to variance. 0 =<\\n                        factor < 1.\\n\\n    Returns:\\n        (unicode, unicode): Storage tuple of the output file\\n\\n    Raises:\\n        NidabaInvalidParameterException: Input parameters are outside the valid\\n                                         range.\\n    \"\"\"\\n    input_path = storage.get_abs_path(*doc)\\n    output_path = storage.insert_suffix(input_path, method, unicode(whsize),\\n                                        unicode(factor))\\n    lept_sauvola(input_path, output_path, whsize, factor)\\n    return storage.get_storage_path(output_path)', 'def dewarp(doc, method=u\\'dewarp\\'):\\n    \"\"\"\\n    Removes perspective distortion (as commonly exhibited by overhead scans)\\n    from an 1bpp input image.\\n\\n    Args:\\n        doc (unicode, unicode): The input document tuple.\\n        method (unicode): The suffix string appended to all output files.\\n\\n    Returns:\\n        (unicode, unicode): Storage tuple of the output file\\n    \"\"\"\\n    input_path = storage.get_abs_path(*doc)\\n    output_path = storage.insert_suffix(input_path, method)\\n    lept_dewarp(input_path, output_path)\\n    return storage.get_storage_path(output_path)', 'def deskew(doc, method=u\\'deskew\\'):\\n    \"\"\"\\n    Removes skew (rotational distortion) from an 1bpp input image.\\n\\n    Args:\\n        doc (unicode, unicode): The input document tuple.\\n        method (unicode): The suffix string appended to all output files.\\n\\n    Returns:\\n        (unicode, unicode): Storage tuple of the output file\\n    \"\"\"\\n    input_path = storage.get_abs_path(*doc)\\n    output_path = storage.insert_suffix(input_path, method)\\n    lept_deskew(input_path, output_path)\\n    return storage.get_storage_path(output_path)']}, {'features': [], 'snippets': ['def create(self, request, *args, **kwargs):\\n        raise MethodNotAllowed(self.action)', 'def destroy(self, request, *args, **kwargs):\\n        raise MethodNotAllowed(self.action)', 'def perform_create(self, serializer):\\n        instance = serializer.save()\\n        instance.open(applicant=self.request.user)', 'def open(self, request, *args, **kwargs):\\n        return super().create(request, *args, **kwargs)', 'def approve(self, request, *args, **kwargs):\\n        response = super().update(request, *args, **kwargs)\\n        instance = self.get_object()\\n        instance.approve(processor=self.request.user)\\n        return response', 'def reject(self, request, *args, **kwargs):\\n        instance = self.get_object()\\n        serializer = self.get_serializer(instance)\\n        instance.reject(processor=request.user)\\n        return Response(serializer.data)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, ics):\\n        InstallWindow.__init__(self, ics)\\n        self.parent = ics.getICW().window', 'def getNext(self):\\n        # forcing lba32 can be a bad idea.. make sure they really want to\\n        if (self.forceLBA.get_active() and not self.bl.forceLBA32):\\n            rc = self.intf.messageWindow(_(\"Warning\"),\\n                    _(\"Forcing the use of LBA32 for your bootloader when \"\\n                      \"not supported by the BIOS can cause your machine \"\\n                      \"to be unable to boot.\\\\n\\\\n\"\\n                      \"Would you like to continue and force LBA32 mode?\"),\\n                                         type = \"custom\",\\n                                         custom_buttons = [_(\"Cancel\"),\\n                                                           _(\"Force LBA32\")])\\n            if rc != 1:\\n                raise gui.StayOnScreen\\n\\n        # set forcelba\\n        self.bl.setForceLBA(self.forceLBA.get_active())\\n        # set kernel args\\n        self.bl.args.set(self.appendEntry.get_text())\\n\\n        # set the boot device\\n        self.bl.setDevice(self.blloc.getBootDevice())\\n\\n        # set the drive order\\n        self.bl.drivelist = self.blloc.getDriveOrder()', 'def setupOptionsVbox(self):\\n        self.options_vbox = gtk.VBox(False, 5)\\n        self.options_vbox.set_border_width(5)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, context, sock):\\n        self._context = context\\n        self._sock = sock\\n        self._connection = OpenSSL.SSL.Connection(context, sock)\\n        self._makefile_refs = 0', 'def __iowait(self, io_func, *args, **kwargs):\\n        timeout = self._sock.gettimeout() or 0.1\\n        fd = self._sock.fileno()\\n        time_start = time.time()\\n        while True:\\n            try:\\n                return io_func(*args, **kwargs)\\n            except (OpenSSL.SSL.WantReadError, OpenSSL.SSL.WantX509LookupError):\\n                sys.exc_clear()\\n                _, _, errors = select.select([fd], [], [fd], timeout)\\n                if errors:\\n                    break\\n                time_now = time.time()\\n                if time_now - time_start > timeout:\\n                    break\\n            except OpenSSL.SSL.WantWriteError:\\n                sys.exc_clear()\\n                _, _, errors = select.select([], [fd], [fd], timeout)\\n                if errors:\\n                    break\\n                time_now = time.time()\\n                if time_now - time_start > timeout:\\n                    break', 'def do_handshake(self):\\n        self.__iowait(self._connection.do_handshake)', 'def __send(self, data, flags=0):\\n        try:\\n            return self.__iowait(self._connection.send, data, flags)\\n        except OpenSSL.SSL.SysCallError as e:\\n            if e[0] == -1 and not data:\\n                # errors when writing empty strings are expected and can be ignored\\n                return 0\\n            raise', \"def recv(self, bufsiz, flags=0):\\n        pending = self._connection.pending()\\n        if pending:\\n            return self._connection.recv(min(pending, bufsiz))\\n        try:\\n            return self.__iowait(self._connection.recv, bufsiz, flags)\\n        except OpenSSL.SSL.ZeroReturnError:\\n            return ''\\n        except OpenSSL.SSL.SysCallError as e:\\n            if e[0] == -1 and 'Unexpected EOF' in e[1]:\\n                # errors when reading empty strings are expected and can be ignored\\n                return ''\\n            raise\", 'def write(self, buf, flags=0):\\n        return self.sendall(buf, flags)', \"def makefile(self, mode='r', bufsize=-1):\\n        self._makefile_refs += 1\\n        return socket._fileobject(self, mode, bufsize, close=True)\"]}, {'features': [], 'snippets': ['def route(method, path):\\n    def decorator(f):\\n        f.http_route = path\\n        f.http_method = method\\n        return f\\n    return decorator', 'def __init__(self, ndb, config):\\n        self.ndb = ndb\\n        self.config = config', \"def sources_list(self, mode='short'):\\n        ret = {}\\n        mode = bottle.request.query.mode or mode\\n        for name, spec in self.ndb.sources.items():\\n            ret[name] = {'class': spec.nl.__class__.__name__,\\n                         'status': spec.status}\\n            if mode == 'full':\\n                ret[name]['config'] = spec.nl_kwarg\\n        return bottle.template('{{!ret}}', ret=json.dumps(ret))\", \"def sources_restart(self):\\n        node = bottle.request.body.getvalue().decode('utf-8')\\n        self.ndb.sources[node].start()\", \"def sources_add(self):\\n        data = bottle.request.body.getvalue().decode('utf-8')\\n        node, spec = make_spec(data, self.config)\\n        self.config['sources'].append(node)\\n        self.ndb.connect_source(node, spec)\", \"def sources_del(self):\\n        node = bottle.request.body.getvalue().decode('utf-8')\\n        self.config['sources'].remove(node)\\n        self.ndb.disconnect_source(node)\", \"def config_get(self):\\n        return bottle.template('{{!ret}}',\\n                               ret=json.dumps(self.config))\", \"def config_dump(self):\\n        path = bottle.request.body.getvalue().decode('utf-8')\\n        self.config.dump(path)\", \"def view(self, name):\\n        ret = []\\n        obj = getattr(self.ndb, name)\\n        for line in obj.dump():\\n            ret.append(line)\\n        return bottle.template('{{!ret}}', ret=json.dumps(ret))\"]}, {'features': [], 'snippets': [\"def __init__(self):\\n        super(Level0, self).__init__()\\n\\n        self.activeSprites = sprite.RenderClear()\\n        self.drawnSprites = []\\n        self.npc = GameObject(image.load('User.png'), 100,50)\\n        self.activeSprites.add(self.npc)\", 'def update(self, dT):\\n        #print \"Running level0\"\\n        #Character info\\n        for gobject in self.activeSprites:\\n            if gobject is not self.npc:\\n                if not gobject.rect.colliderect(self.npc.rect):\\n                    #if self.npc.vy < 0.3 and (gobject.rect.y >= self.npc.rect.y + self.npc.rect.height):\\n                    if self.npc.vy < 0.3:\\n                        self.npc.vy += 0.1 \\n                else:\\n                    self.npc.vy = 0\\n\\n            gobject.update(dT)', 'def processKeyDown(self,key):\\n        print \"You hit the key \" + str(key) + \"!\"\\n        if key == pygame.K_RIGHT:\\n            self.npc.vx = 0.1', 'def processMouseButtonDown(self, pos):\\n        print \"Ya clicked at \" + str(pos[0]) + \" \" + str(pos[1]) + \" ya goof!\"\\n        self.drawing = True']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_queryset(self):\\n        return CommandExecution.objects.filter(\\n            user_id=str(self.request.user.id)\\n        )', 'def check_permissions(self, request):\\n        if not settings.SECURITY_COMMAND_EXECUTION and request.user.is_common_user:\\n            return self.permission_denied(request, \"Command execution disabled\")\\n        return super().check_permissions(request)']}, {'features': [], 'snippets': [\"def mayaxes(title_string='VOID', xlabel='VOID', ylabel='VOID', zlabel='VOID', handle='VOID', \\\\\\n    title_size=25, ticks=7, font_scaling=0.7, background='w'):\", 'def test_mayaxes():\\n\\n    from mayaxes import mayaxes\\n    from scipy import sqrt,sin,meshgrid,linspace,pi\\n    import mayavi.mlab as mlab']}, {'features': [], 'snippets': ['def __init__(self, skill=None):\\n\\t\\t\"\"\"Creates a picross game.\\n\\n\\t\\tParameters:\\n\\t\\t\\tskill\\t\\t- Desired skill level (None == random)\\n\\t\\t\"\"\"\\n\\t\\tself.__level = None\\n\\t\\tself.__name = None\\n\\t\\tself.__skill = None\\n\\t\\tself.__fieldsToOpen = 0\\n\\t\\tself.__fieldsOpened = 0\\n\\t\\tself.load(skill=skill)', \"def _debug_print(self):\\n\\t\\tprint self.getInfo()\\n\\t\\tprint 'go: %s' % (self.__gameOver)\\n\\t\\tfor row in self.__level:\\n\\t\\t\\tprint row\", 'def getInfo(self):\\n\\t\\t\"\"\"Returns the name, skill and size of the level\\n\\t\\t\"\"\"\\n\\t\\treturn self.__name,self.__skill,len(self.__level)', 'def getColumnHint(self,col):\\n\\t\\t\"\"\"Returns the hint for a specific column.\\n\\t\\t\"\"\"\\n\\t\\thint,count = [],0\\n\\t\\tfor row in self.__level:\\n\\t\\t\\tif row[col] == FIELD_VALID:\\n\\t\\t\\t\\tcount += 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif count > 0:\\n\\t\\t\\t\\t\\thint.append(count)\\n\\t\\t\\t\\t\\tcount = 0\\n\\t\\tif count > 0:\\n\\t\\t\\thint.append(count)\\n\\t\\tif not hint: \\n\\t\\t\\thint.append(0)\\n\\t\\treturn hint', 'def isGameWon(self):\\n\\t\\treturn self.__fieldsOpened == self.__fieldsToOpen', 'def restart(self):\\n\\t\\t\"\"\"Reinitializes the current game \\n\\t\\t\"\"\"\\n\\t\\tfor i, row in enumerate(self.__level):\\n\\t\\t\\tfor j, field in enumerate(row):\\n\\t\\t\\t\\tif field == FIELD_OPEN or field == FIELD_MARKED_VALID:\\n\\t\\t\\t\\t\\tself.__level[i][j] = FIELD_VALID\\n\\t\\t\\t\\telif field == FIELD_MARKED_INVALID:\\n\\t\\t\\t\\t\\tself.__level[i][j] = FIELD_INVALID\\n\\t\\tself.__gameOver = False\\n\\t\\tself.__fieldsOpened = 0', 'def markField(self,col,row):\\n\\t\\tfield = self.__level[row][col]\\n\\t\\tif field == FIELD_VALID:\\n\\t\\t\\tself.__level[row][col] = FIELD_MARKED_VALID\\n\\t\\telif field == FIELD_MARKED_VALID:\\n\\t\\t\\tself.__level[row][col] = FIELD_VALID\\n\\t\\telif field == FIELD_INVALID:\\n\\t\\t\\tself.__level[row][col] = FIELD_MARKED_INVALID\\n\\t\\telif field == FIELD_MARKED_INVALID:\\n\\t\\t\\tself.__level[row][col] = FIELD_INVALID\\n\\t\\treturn self.__level[row][col]', 'def load(self,file=DEFAULT_LEVELPACK,skill=None):\\n\\t\\t\"\"\"Loads a level either from a zipped levelpack or from a textfile.\\n\\n\\t\\tParameters:\\n\\t\\t\\tfile\\t- Can be a file path or zipped levelpack\\n\\t\\t\\tskill\\t- Desired level skill (None == random)\\n\\t\\t\"\"\"\\n\\t\\tif file.endswith(\\'.lvl\\'):\\n\\t\\t\\t# Set the skill variable\\n\\t\\t\\tif file.startswith(\\'easy\\'):\\t\\tself.__skill = SKILL_EASY\\n\\t\\t\\telif file.startswith(\\'medium\\'):\\tself.__skill = SKILL_MEDIUM\\n\\t\\t\\telif file.startswith(\\'hard\\'):\\tself.__skill = SKILL_HARD\\n\\n\\t\\t\\tself.__loadFileContent(open(file,\\'r\\'))\\n\\n\\t\\telif file.endswith(\\'.zip\\'):\\n\\t\\t\\tzip = ZipFile(file)\\n\\n\\t\\t\\t# We have to select from which files in the zipfile we \\n\\t\\t\\t# want to choose randomly based on the level\\'s skill\\n\\t\\t\\tcandidates = []\\n\\n\\t\\t\\tif skill == SKILL_EASY:\\n\\t\\t\\t\\tfor file in zip.namelist():\\n\\t\\t\\t\\t\\tif file.startswith(\\'easy\\'):\\n\\t\\t\\t\\t\\t\\tcandidates.append(file)\\t\\t\\t\\t\\n\\t\\t\\telif skill == SKILL_MEDIUM:\\n\\t\\t\\t\\tfor file in zip.namelist():\\n\\t\\t\\t\\t\\tif file.startswith(\\'medium\\'):\\n\\t\\t\\t\\t\\t\\tcandidates.append(file)\\t\\t\\t\\t\\n\\t\\t\\telif skill == SKILL_HARD:\\n\\t\\t\\t\\tfor file in zip.namelist():\\n\\t\\t\\t\\t\\tif file.startswith(\\'hard\\'):\\n\\t\\t\\t\\t\\t\\tcandidates.append(file)\\t\\t\\t\\t\\n\\n\\t\\t\\t# This should never happen in a good levelpack, but if it\\n\\t\\t\\t# is malformed, just pick something!\\n\\t\\t\\tif not candidates:\\n\\t\\t\\t\\tcandidates = zip.namelist()\\n\\n\\t\\t\\t# Select one candidate randomly\\n\\t\\t\\twhich = candidates[randrange(len(candidates))]\\n\\t\\t\\t# Set the skill variable\\n\\t\\t\\tif which.startswith(\\'easy\\'):\\tself.__skill = SKILL_EASY\\n\\t\\t\\telif which.startswith(\\'medium\\'):self.__skill = SKILL_MEDIUM\\n\\t\\t\\telif which.startswith(\\'hard\\'):\\tself.__skill = SKILL_HARD\\n\\t\\t\\t# Read from zipfile and load file content\\n\\t\\t\\tbuf = zip.read(which)\\n\\t\\t\\tself.__loadFileContent(StringIO(buf))']}, {'features': [], 'snippets': ['def on_key_press(symbol, modifiers):\\n        # ... handle this event ...', 'def on_draw():\\n        # ... drawing code ...', 'def draw(self, x, y):\\n        \"\"\"Abstract render method.\\n\\n        The cursor should be drawn with the \"hot\" spot at the given\\n        coordinates.  The projection is set to the pyglet default (i.e., \\n        orthographic in window-space), however no other aspects of the \\n        state can be assumed.\\n\\n        :Parameters:\\n            `x` : int\\n                X coordinate of the mouse pointer\\'s hot spot.\\n            `y` : int\\n                Y coordinate of the mouse pointer\\'s hot spot.\\n\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def __init__(self, image, hot_x=0, hot_y=0):\\n        \"\"\"Create a mouse cursor from an image.\\n\\n        :Parameters:\\n            `image` : `pyglet.image.AbstractImage`\\n                Image to use for the mouse cursor.  It must have a\\n                valid ``texture`` attribute.\\n            `hot_x` : int\\n                X coordinate of the \"hot\" spot in the image relative to the\\n                image\\'s anchor.\\n            `hot_y` : int\\n                Y coordinate of the \"hot\" spot in the image, relative to the\\n                image\\'s anchor.\\n        \"\"\"\\n        self.texture = image.get_texture()\\n        self.hot_x = hot_x\\n        self.hot_y = hot_y', 'def _PlatformEventHandler(data):\\n    \"\"\"Decorator for platform event handlers.', \"def _event_wrapper(f):\\n        f._platform_event = True\\n        if not hasattr(f, '_platform_event_data'):\\n            f._platform_event_data = []\\n        f._platform_event_data.append(data)\\n        return f\", 'def _ViewEventHandler(f):\\n    f._view = True\\n    return f', \"def __init__(cls, name, bases, dict):\\n        cls._platform_event_names = set()\\n        for base in bases:\\n            if hasattr(base, '_platform_event_names'):\\n                cls._platform_event_names.update(base._platform_event_names)\\n        for name, func in dict.items():\\n            if hasattr(func, '_platform_event'):\\n                cls._platform_event_names.add(name)\\n        super(_WindowMetaclass, cls).__init__(name, bases, dict)\", 'def __init__(self, \\n                 width=None,\\n                 height=None,\\n                 caption=None,\\n                 resizable=False,\\n                 style=WINDOW_STYLE_DEFAULT,\\n                 fullscreen=False,\\n                 visible=True,\\n                 vsync=True,\\n                 display=None,\\n                 screen=None,\\n                 config=None,\\n                 context=None,\\n                 mode=None):\\n        \"\"\"Create a window.\\n\\n        All parameters are optional, and reasonable defaults are assumed\\n        where they are not specified.\\n\\n        The `display`, `screen`, `config` and `context` parameters form\\n        a hierarchy of control: there is no need to specify more than \\n        one of these.  For example, if you specify `screen` the `display`\\n        will be inferred, and a default `config` and `context` will be\\n        created.\\n\\n        `config` is a special case; it can be a template created by the\\n        user specifying the attributes desired, or it can be a complete\\n        `config` as returned from `Screen.get_matching_configs` or similar.\\n\\n        The context will be active as soon as the window is created, as if\\n        `switch_to` was just called.\\n\\n        :Parameters:\\n            `width` : int\\n                Width of the window, in pixels.  Defaults to 640, or the\\n                screen width if `fullscreen` is True.\\n            `height` : int\\n                Height of the window, in pixels.  Defaults to 480, or the\\n                screen height if `fullscreen` is True.\\n            `caption` : str or unicode\\n                Initial caption (title) of the window.  Defaults to\\n                ``sys.argv[0]``.\\n            `resizable` : bool\\n                If True, the window will be resizable.  Defaults to False.\\n            `style` : int\\n                One of the ``WINDOW_STYLE_*`` constants specifying the\\n                border style of the window.\\n            `fullscreen` : bool\\n                If True, the window will cover the entire screen rather\\n                than floating.  Defaults to False.\\n            `visible` : bool\\n                Determines if the window is visible immediately after\\n                creation.  Defaults to True.  Set this to False if you\\n                would like to change attributes of the window before\\n                having it appear to the user.\\n            `vsync` : bool\\n                If True, buffer flips are synchronised to the primary screen\\'s\\n                vertical retrace, eliminating flicker.\\n            `display` : `Display`\\n                The display device to use.  Useful only under X11.\\n            `screen` : `Screen`\\n                The screen to use, if in fullscreen.\\n            `config` : `pyglet.gl.Config`\\n                Either a template from which to create a complete config,\\n                or a complete config.\\n            `context` : `pyglet.gl.Context`\\n                The context to attach to this window.  The context must\\n                not already be attached to another window.\\n            `mode` : `ScreenMode`\\n                The screen will be switched to this mode if `fullscreen` is\\n                True.  If None, an appropriate mode is selected to accomodate\\n                `width` and `height.`\\n\\n        \"\"\"\\n        EventDispatcher.__init__(self)\\n        self._event_queue = []\\n\\n        if not display:\\n            display = get_platform().get_default_display()\\n\\n        if not screen:\\n            screen = display.get_default_screen()\\n\\n        if not config:\\n            for template_config in [\\n                gl.Config(double_buffer=True, depth_size=24),\\n                gl.Config(double_buffer=True, depth_size=16),\\n                None]:\\n                try:\\n                    config = screen.get_best_config(template_config)\\n                    break\\n                except NoSuchConfigException:\\n                    pass\\n            if not config:\\n                raise NoSuchConfigException(\\'No standard config is available.\\')\\n\\n        if not config.is_complete():\\n            config = screen.get_best_config(config)\\n\\n        if not context:\\n            context = config.create_context(gl.current_context)\\n\\n        # Set these in reverse order to above, to ensure we get user\\n        # preference\\n        self._context = context\\n        self._config = self._context.config\\n        # XXX deprecate config\\'s being screen-specific\\n        if hasattr(self._config, \\'screen\\'):\\n            self._screen = self._config.screen\\n        else:\\n            display = self._config.canvas.display\\n            self._screen = display.get_default_screen()\\n        self._display = self._screen.display\\n\\n        if fullscreen:\\n            if width is None and height is None:\\n                self._windowed_size = self._default_width, self._default_height\\n            width, height = self._set_fullscreen_mode(mode, width, height)\\n            if not self._windowed_size:\\n                self._windowed_size = width, height\\n        else:\\n            if width is None:\\n                width = self._default_width\\n            if height is None:\\n                height = self._default_height\\n\\n        self._width = width\\n        self._height = height\\n        self._resizable = resizable\\n        self._fullscreen = fullscreen\\n        self._style = style\\n        if pyglet.options[\\'vsync\\'] is not None:\\n            self._vsync = pyglet.options[\\'vsync\\']\\n        else:\\n            self._vsync = vsync\\n\\n        if caption is None:\\n            caption = sys.argv[0]\\n            # Decode hack for Python2 unicode support:\\n            if hasattr(caption, \"decode\"):\\n                try:\\n                    caption = caption.decode(\"utf8\")\\n                except UnicodeDecodeError:\\n                    caption = \"pyglet\"\\n        self._caption = caption\\n\\n        from pyglet import app\\n        app.windows.add(self)\\n        self._create()\\n\\n        self.switch_to()\\n        if visible:\\n            self.set_visible(True)\\n            self.activate()', \"def __repr__(self):\\n        return '%s(width=%d, height=%d)' % \\\\\\n            (self.__class__.__name__, self.width, self.height)\", 'def _recreate(self, changes):\\n        \"\"\"Recreate the window with current attributes.\\n\\n        :Parameters:\\n            `changes` : list of str\\n                List of attribute names that were changed since the last\\n                `_create` or `_recreate`.  For example, ``[\\'fullscreen\\']``\\n                is given if the window is to be toggled to or from fullscreen. \\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def switch_to(self):\\n        \"\"\"Make this window the current OpenGL rendering context.\\n\\n        Only one OpenGL context can be active at a time.  This method sets\\n        the current window\\'s context to be current.  You should use this\\n        method in preference to `pyglet.gl.Context.set_current`, as it may\\n        perform additional initialisation functions.\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', \"def _set_fullscreen_mode(self, mode, width, height):\\n        if mode is not None:\\n            self.screen.set_mode(mode)\\n            if width is None:\\n                width = self.screen.width\\n            if height is None:\\n                height = self.screen.height\\n        elif width is not None or height is not None:\\n            if width is None:\\n                width = 0\\n            if height is None:\\n                height = 0\\n            mode = self.screen.get_closest_mode(width, height)\\n            if mode is not None:\\n                self.screen.set_mode(mode)\\n            elif self.screen.get_modes():\\n                # Only raise exception if mode switching is at all possible.\\n                raise NoSuchScreenModeException(\\n                    'No mode matching %dx%d' % (width, height))\\n        else:\\n            width = self.screen.width\\n            height = self.screen.height\\n        return width, height\", 'def on_close(self):\\n        \"\"\"Default on_close handler.\"\"\"\\n        self.has_exit = True\\n        from pyglet import app\\n        if app.event_loop.is_running:\\n            self.close()', 'def close(self):\\n        \"\"\"Close the window.\\n\\n        After closing the window, the GL context will be invalid.  The\\n        window instance cannot be reused once closed (see also `set_visible`).\\n\\n        The `pyglet.app.EventLoop.on_window_close` event is dispatched on\\n        `pyglet.app.event_loop` when this method is called.\\n        \"\"\"\\n        from pyglet import app\\n        if not self._context:\\n            return\\n        app.windows.remove(self)\\n        self._context.destroy()\\n        self._config = None\\n        self._context = None\\n        if app.event_loop:\\n            app.event_loop.dispatch_event(\\'on_window_close\\', self)\\n        self._event_queue = []', 'def caption(self):\\n        \"\"\"The window caption (title).  Read-only.\\n\\n        :type: str\\n        \"\"\"\\n        return self._caption', 'def resizeable(self):\\n        \"\"\"True if the window is resizable.  Read-only.\\n\\n        :type: bool\\n        \"\"\"\\n        return self._resizable', 'def style(self):\\n        \"\"\"The window style; one of the ``WINDOW_STYLE_*`` constants.\\n        Read-only.\\n\\n        :type: int\\n        \"\"\"\\n        return self._style', 'def fullscreen(self):\\n        \"\"\"True if the window is currently fullscreen.  Read-only.\\n\\n        :type: bool\\n        \"\"\"\\n        return self._fullscreen', 'def visible(self):\\n        \"\"\"True if the window is currently visible.  Read-only.\\n\\n        :type: bool\\n        \"\"\"\\n        return self._visible', 'def vsync(self):\\n        \"\"\"True if buffer flips are synchronised to the screen\\'s vertical\\n        retrace.  Read-only.\\n\\n        :type: bool\\n        \"\"\"\\n        return self._vsync', 'def display(self):\\n        \"\"\"The display this window belongs to.  Read-only.\\n\\n        :type: `Display`\\n        \"\"\"\\n        return self._display', 'def screen(self):\\n        \"\"\"The screen this window is fullscreen in.  Read-only.\\n\\n        :type: `Screen`\\n        \"\"\"\\n        return self._screen', 'def config(self):\\n        \"\"\"A GL config describing the context of this window.  Read-only.\\n\\n        :type: `pyglet.gl.Config`\\n        \"\"\"\\n        return self._config', 'def context(self):\\n        \"\"\"The OpenGL context attached to this window.  Read-only.\\n\\n        :type: `pyglet.gl.Context`\\n        \"\"\"\\n        return self._context', 'def width(self):\\n        \"\"\"The width of the window, in pixels.  Read-write.\\n\\n        :type: int\\n        \"\"\"\\n        return self.get_size()[0]', 'def width(self, new_width):\\n        self.set_size(new_width, self.height)', 'def height(self):\\n        \"\"\"The height of the window, in pixels.  Read-write.\\n\\n        :type: int\\n        \"\"\"\\n        return self.get_size()[1]', 'def height(self, new_height):\\n        self.set_size(self.width, new_height)', 'def set_minimum_size(self, width, height):\\n        \"\"\"Set the minimum size of the window.\\n\\n        Once set, the user will not be able to resize the window smaller\\n        than the given dimensions.  There is no way to remove the\\n        minimum size constraint on a window (but you could set it to 0,0).\\n\\n        The behaviour is undefined if the minimum size is set larger than\\n        the current size of the window.\\n\\n        The window size does not include the border or title bar.\\n\\n        :Parameters:\\n            `width` : int\\n                Minimum width of the window, in pixels.\\n            `height` : int\\n                Minimum height of the window, in pixels.\\n\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def set_size(self, width, height):\\n        \"\"\"Resize the window.', 'def get_size(self):\\n        \"\"\"Return the current size of the window.\\n\\n        The window size does not include the border or title bar.\\n\\n        :rtype: (int, int)\\n        :return: The width and height of the window, in pixels.\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def get_location(self):\\n        \"\"\"Return the current position of the window.\\n\\n        :rtype: (int, int)\\n        :return: The distances of the left and top edges from their respective\\n            edges on the virtual desktop, in pixels.\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def set_visible(self, visible=True):    \\n        \"\"\"Show or hide the window.\\n\\n        :Parameters:\\n            `visible` : bool\\n                If True, the window will be shown; otherwise it will be\\n                hidden.\\n\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def maximize(self):\\n        \"\"\"Maximize the window.\\n\\n        The behaviour of this method is somewhat dependent on the user\\'s\\n        display setup.  On a multi-monitor system, the window may maximize\\n        to either a single screen or the entire virtual desktop.\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def set_mouse_visible(self, visible=True):\\n        \"\"\"Show or hide the mouse cursor.\\n\\n        The mouse cursor will only be hidden while it is positioned within\\n        this window.  Mouse events will still be processed as usual.\\n\\n        :Parameters:\\n            `visible` : bool\\n                If True, the mouse cursor will be visible, otherwise it\\n                will be hidden.\\n\\n        \"\"\"\\n        self._mouse_visible = visible\\n        self.set_mouse_platform_visible()', 'def set_mouse_cursor(self, cursor=None):\\n        \"\"\"Change the appearance of the mouse cursor.\\n\\n        The appearance of the mouse cursor is only changed while it is\\n        within this window.\\n\\n        :Parameters:\\n            `cursor` : `MouseCursor`\\n                The cursor to set, or None to restore the default cursor.\\n\\n        \"\"\"\\n        if cursor is None:\\n            cursor = DefaultMouseCursor()\\n        self._mouse_cursor = cursor\\n        self.set_mouse_platform_visible()', 'def set_exclusive_keyboard(self, exclusive=True):\\n        \"\"\"Prevent the user from switching away from this window using\\n        keyboard accelerators.\\n\\n        When enabled, this feature disables certain operating-system specific\\n        key combinations such as Alt+Tab (Command+Tab on OS X).  This can be\\n        useful in certain kiosk applications, it should be avoided in general\\n        applications or games.\\n\\n        :Parameters:\\n            `exclusive` : bool\\n                If True, exclusive keyboard is enabled, otherwise it is\\n                disabled.\\n\\n        \"\"\"\\n        raise NotImplementedError(\\'abstract\\')', 'def set_icon(self, *images):\\n        \"\"\"Set the window icon.\\n\\n        If multiple images are provided, one with an appropriate size \\n        will be selected (if the correct size is not provided, the image\\n        will be scaled).\\n\\n        Useful sizes to provide are 16x16, 32x32, 64x64 (Mac only) and\\n        128x128 (Mac only).\\n\\n        :Parameters:\\n            `images` : sequence of `pyglet.image.AbstractImage`\\n                List of images to use for the window icon.', 'def clear(self):\\n        \"\"\"Clear the window.\\n\\n        This is a convenience method for clearing the color and depth\\n        buffer.  The window must be the active context (see `switch_to`).\\n        \"\"\"\\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)', 'def dispatch_event(self, *args):\\n        if not self._enable_event_queue or self._allow_dispatch_event:\\n            if EventDispatcher.dispatch_event(self, *args) != False:\\n                self._legacy_invalid = True\\n        else:\\n            self._event_queue.append(args)', 'def on_key_press(symbol, modifiers):\\n            \"\"\"A key on the keyboard was pressed (and held down).\\n\\n            In pyglet 1.0 the default handler sets `has_exit` to ``True`` if\\n            the ``ESC`` key is pressed.\\n\\n            In pyglet 1.1 the default handler dispatches the `on_close`\\n            event if the ``ESC`` key is pressed.\\n\\n            :Parameters:\\n                `symbol` : int\\n                    The key symbol pressed.\\n                `modifiers` : int\\n                    Bitwise combination of the key modifiers active.', 'def on_key_release(symbol, modifiers):\\n            \"\"\"A key on the keyboard was released.\\n\\n            :Parameters:\\n                `symbol` : int\\n                    The key symbol pressed.\\n                `modifiers` : int\\n                    Bitwise combination of the key modifiers active.\\n\\n            :event:\\n            \"\"\"', 'def on_text_motion(motion):\\n            \"\"\"The user moved the text input cursor.\\n\\n            Typically this is called after `on_key_press` and before\\n            `on_key_release`, but may also be called multiple times if the key\\n            is help down (key repeating).\\n\\n            You should always use this method for moving the text input cursor\\n            (caret), as different platforms have different default keyboard\\n            mappings, and key repeats are handled correctly.\\n\\n            The values that `motion` can take are defined in\\n            `pyglet.window.key`:\\n\\n            * MOTION_UP\\n            * MOTION_RIGHT\\n            * MOTION_DOWN\\n            * MOTION_LEFT\\n            * MOTION_NEXT_WORD\\n            * MOTION_PREVIOUS_WORD\\n            * MOTION_BEGINNING_OF_LINE\\n            * MOTION_END_OF_LINE\\n            * MOTION_NEXT_PAGE\\n            * MOTION_PREVIOUS_PAGE\\n            * MOTION_BEGINNING_OF_FILE\\n            * MOTION_END_OF_FILE\\n            * MOTION_BACKSPACE\\n            * MOTION_DELETE\\n\\n            :Parameters:\\n                `motion` : int\\n                    The direction of motion; see remarks.\\n\\n            :event:\\n            \"\"\"', 'def on_mouse_motion(x, y, dx, dy):\\n            \"\"\"The mouse was moved with no buttons held down.\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance in pixels from the left edge of the window.\\n                `y` : int\\n                    Distance in pixels from the bottom edge of the window.\\n                `dx` : int\\n                    Relative X position from the previous mouse position.\\n                `dy` : int\\n                    Relative Y position from the previous mouse position.\\n\\n            :event:\\n            \"\"\"', 'def on_mouse_press(x, y, button, modifiers):\\n            \"\"\"A mouse button was pressed (and held down).\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance in pixels from the left edge of the window.\\n                `y` : int\\n                    Distance in pixels from the bottom edge of the window.\\n                `button` : int\\n                    The mouse button that was pressed.\\n                `modifiers` : int\\n                    Bitwise combination of any keyboard modifiers currently\\n                    active.', 'def on_mouse_release(x, y, button, modifiers):\\n            \"\"\"A mouse button was released.\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance in pixels from the left edge of the window.\\n                `y` : int\\n                    Distance in pixels from the bottom edge of the window.\\n                `button` : int\\n                    The mouse button that was released.\\n                `modifiers` : int\\n                    Bitwise combination of any keyboard modifiers currently\\n                    active.\\n\\n            :event:\\n            \"\"\"', 'def on_mouse_scroll(x, y, scroll_x, scroll_y):\\n            \"\"\"The mouse wheel was scrolled.\\n\\n            Note that most mice have only a vertical scroll wheel, so\\n            `scroll_x` is usually 0.  An exception to this is the Apple Mighty\\n            Mouse, which has a mouse ball in place of the wheel which allows\\n            both `scroll_x` and `scroll_y` movement.\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance in pixels from the left edge of the window.\\n                `y` : int\\n                    Distance in pixels from the bottom edge of the window.\\n                `scroll_x` : int\\n                    Number of \"clicks\" towards the right (left if negative).\\n                `scroll_y` : int\\n                    Number of \"clicks\" upwards (downwards if negative).\\n\\n            :event:\\n            \"\"\"', 'def on_mouse_enter(x, y):\\n            \"\"\"The mouse was moved into the window.\\n\\n            This event will not be trigged if the mouse is currently being\\n            dragged.\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance in pixels from the left edge of the window.\\n                `y` : int\\n                    Distance in pixels from the bottom edge of the window.\\n\\n            :event:\\n            \"\"\"', 'def on_expose():\\n            \"\"\"A portion of the window needs to be redrawn.\\n\\n            This event is triggered when the window first appears, and any time\\n            the contents of the window is invalidated due to another window\\n            obscuring it.\\n\\n            There is no way to determine which portion of the window needs\\n            redrawing.  Note that the use of this method is becoming\\n            increasingly uncommon, as newer window managers composite windows\\n            automatically and keep a backing store of the window contents.\\n\\n            :event:\\n            \"\"\"', 'def on_move(x, y):\\n            \"\"\"The window was moved.\\n\\n            :Parameters:\\n                `x` : int\\n                    Distance from the left edge of the screen to the left edge\\n                    of the window.\\n                `y` : int\\n                    Distance from the top edge of the screen to the top edge of\\n                    the window.  Note that this is one of few methods in pyglet\\n                    which use a Y-down coordinate system.\\n\\n            :event:\\n            \"\"\"', 'def on_deactivate():\\n            \"\"\"The window was deactivated.\\n\\n            This event can be triggered by clicking on another application\\n            window.  When a window is deactivated it no longer has the\\n            keyboard focus.\\n\\n            :event:\\n            \"\"\"', 'def on_hide():\\n            \"\"\"The window was hidden.\\n\\n            This event is triggered when a window is minimised or (on Mac OS X)\\n            hidden by the user.\\n\\n            :event:\\n            \"\"\"', 'def on_context_state_lost():\\n            \"\"\"The state of the window\\'s GL context was lost.\\n\\n            pyglet may sometimes need to recreate the window\\'s GL context if\\n            the window is moved to another video device, or between fullscreen\\n            or windowed mode.  In this case it will try to share the objects\\n            (display lists, texture objects, shaders) between the old and new\\n            contexts.  If this is possible, only the current state of the GL\\n            context is lost, and the application should simply restore state.\\n\\n            :event:\\n            \"\"\"', 'def on_draw():\\n            # ... perform ordinary window drawing operations ...\\n\\n            fps_display.draw()', \"def __init__(self, window):\\n        from time import time\\n        from pyglet.text import Label\\n        self.label = Label('', x=10, y=10, \\n                           font_size=24, bold=True,\\n                           color=(127, 127, 127, 127))\\n\\n        self.window = window\\n        self._window_flip = window.flip\\n        window.flip = self._hook_flip\\n\\n        self.time = 0.0\\n        self.last_time = time()\\n        self.count = 0\", 'def set_fps(self, fps):\\n        \"\"\"Set the label text for the given FPS estimation.\\n\\n        Called by `update` every `update_period` seconds.\\n\\n        :Parameters:\\n            `fps` : float\\n                Estimated framerate of the window.\\n\\n        \"\"\"\\n        self.label.text = \\'%.2f\\' % fps', 'def _hook_flip(self):\\n        self.update()\\n        self._window_flip()', 'def get_platform():\\n    \"\"\"Get an instance of the Platform most appropriate for this\\n    system.\\n\\n    :deprecated: Use `pyglet.canvas.Display`.\\n\\n    :rtype: `Platform`\\n    :return: The platform instance.\\n    \"\"\"\\n    return Platform()', 'def get_display(self, name):\\n        \"\"\"Get a display device by name.\\n\\n        This is meaningful only under X11, where the `name` is a\\n        string including the host name and display number; for example\\n        ``\"localhost:1\"``.\\n\\n        On platforms other than X11, `name` is ignored and the default\\n        display is returned.  pyglet does not support multiple multiple\\n        video devices on Windows or OS X.  If more than one device is\\n        attached, they will appear as a single virtual device comprising\\n        all the attached screens.\\n\\n        :deprecated: Use `pyglet.canvas.get_display`.\\n\\n        :Parameters:\\n            `name` : str\\n                The name of the display to connect to.\\n\\n        :rtype: `Display`\\n        \"\"\"\\n        for display in pyglet.app.displays:\\n            if display.name == name:\\n                return display\\n        return pyglet.canvas.Display(name)', \"def __init__(self):\\n            raise NotImplementedError('deprecated')\", 'def get_default_screen(self):\\n            \"\"\"Get the default screen as specified by the user\\'s operating system\\n            preferences.\\n\\n            :rtype: `Screen`\\n            \"\"\"\\n            raise NotImplementedError(\\'deprecated\\')']}, {'features': [], 'snippets': ['def init(): pass', 'def loadPlugin(plugin_name): \\n    \"\"\"\\n    @type plugin_name: str\\n    \"\"\"\\n    pass', 'def unLoadPlugin(plugin_name): \\n    \"\"\"\\n    @type plugin_name: str\\n    \"\"\"\\n    pass', 'def registerPlugin(plugin_instance): \\n    \"\"\"\\n    @type plugin_instance: L{amsn2.plugins.developers.aMSNPlugin}\\n    \"\"\"\\n    pass', 'def getPlugins(): pass', 'def getPluginsWithStatus(): pass', 'def getLoadedPlugins(): pass', 'def findPlugin(plugin_name): \\n    \"\"\"\\n    @type plugin_name: str\\n    \"\"\"\\n    pass', 'def saveConfig(plugin_name, data): \\n    \"\"\"\\n    @type plugin_name: str\\n    @type data: object\\n    \"\"\"\\n    pass']}, {'features': [], 'snippets': ['def removef(filename):\\n    try:\\n        os.remove(filename)\\n    except OSError as e:\\n        if e.errno != errno.ENOENT:\\n            raise', 'def setUp(self):\\n        self.db = self.opendb()\\n        self.sample_data()', 'def sample_data(self):\\n        pass', 'def opendb(self):\\n        return smadata2.db.mock.MockDatabase()', 'def prepare_sqlite(self):\\n        self.dbname = \"__testdb__smadata2_%s_.sqlite\" % self.__class__.__name__\\n        self.bakname = self.dbname + \".bak\"\\n\\n        # Start with a blank slate\\n        removef(self.dbname)\\n        removef(self.bakname)\\n\\n        self.prepopulate()\\n\\n        if os.path.exists(self.dbname):\\n            self.original = open(self.dbname).read()\\n        else:\\n            self.original = None', 'def opendb(self):\\n        self.prepare_sqlite()\\n        return smadata2.db.sqlite.create_or_update(self.dbname)', 'def test_trivial(self):\\n        assert isinstance(self.db, smadata2.db.base.BaseDatabase)', 'def test_get_last_historic_missing(self):\\n        serial = \"__TEST__\"\\n\\n        last = self.db.get_last_historic(serial)\\n        assert last is None', 'def sample_data(self):\\n        super(AggregateChecks, self).sample_data()\\n\\n        self.serial1 = \"__TEST__1\"\\n        self.serial2 = \"__TEST__2\"\\n\\n        self.dawn = 8*3600\\n        self.dusk = 20*3600\\n\\n        sampledata = check.generate_linear(0, self.dawn, self.dusk, 24*3600,\\n                                           0, 1)\\n\\n        for ts, y in sampledata:\\n            self.db.add_historic(self.serial1, ts, y)\\n            self.db.add_historic(self.serial2, ts, 2*y)', 'def test_aggregate_one(self):\\n        val = self.db.get_aggregate_one_historic(self.dusk,\\n                                                 (self.serial1, self.serial2))\\n        assert_equals(val, 3*((self.dusk - self.dawn - 2) / 300))', 'def test_aggregate(self):\\n        yield self.check_aggregate_range, 0, 24*3600\\n        yield self.check_aggregate_range, 8*3600, 20*3600\\n        yield self.check_aggregate_range, 13*3600, 14*3600', 'def test_backup(self):\\n        assert os.path.exists(self.bakname)\\n        backup = open(self.bakname).read()\\n        assert_equals(self.original, backup)', 'def prepopulate(self):\\n        DB_MAGIC = 0x71534d41\\n        DB_VERSION = 0\\n\\n        conn = sqlite3.connect(self.dbname)\\n        conn.executescript(\"\"\"', 'def prepopulate(self):\\n        DB_MAGIC = 0x71534d41\\n        DB_VERSION = 0\\n\\n        conn = sqlite3.connect(self.dbname)\\n        conn.executescript(\"\"\"', 'def setUp(self):\\n        self.prepare_sqlite()', 'def test_open(self):\\n        self.db = smadata2.db.SQLiteDatabase(self.dbname)', 'def test_is_empty(self):\\n        assert not os.path.exists(self.dbname)', 'def prepopulate(self):\\n        conn = sqlite3.connect(self.dbname)\\n        conn.execute(\"CREATE TABLE unrelated (random STRING, data INTEGER)\")\\n        conn.commit()\\n        del conn']}, {'features': [], 'snippets': ['def GET(self):\\n\\n        return render.hello_form()']}, {'features': [], 'snippets': ['def wait_for_and_do(browser, query, callback):\\n    not_filled = True\\n    n = 0\\n\\n    while not_filled:\\n        try:\\n            callback(browser.find_by_css(query).first)\\n            not_filled = False\\n        except ElementNotVisibleException, e:\\n            if n > 20:\\n                raise e\\n            n += 1', \"def setUp(self):\\n        with db.conn(settings['database']) as conn:\\n            assets = assets_helper.read(conn)\\n            for asset in assets:\\n                assets_helper.delete(conn, asset['asset_id'])\", 'def test_add_asset_url(self):\\n        with Browser() as browser:\\n            browser.visit(main_page_url)\\n\\n            wait_for_and_do(browser, \\'#add-asset-button\\', lambda btn: btn.click())\\n            sleep(1)\\n\\n            wait_for_and_do(browser, \\'input[name=\"uri\"]\\', lambda field: field.fill(\\'http://example.com\\'))\\n            sleep(1)\\n\\n            wait_for_and_do(browser, \\'#add-form\\', lambda form: form.click())\\n            sleep(1)\\n\\n            wait_for_and_do(browser, \\'#save-asset\\', lambda btn: btn.click())\\n            sleep(3)  # backend need time to process request\\n\\n        with db.conn(settings[\\'database\\']) as conn:\\n            assets = assets_helper.read(conn)\\n\\n            self.assertEqual(len(assets), 1)\\n            asset = assets[0]\\n\\n            self.assertEqual(asset[\\'name\\'], u\\'http://example.com\\')\\n            self.assertEqual(asset[\\'uri\\'], u\\'http://example.com\\')\\n            self.assertEqual(asset[\\'mimetype\\'], u\\'webpage\\')\\n            self.assertEqual(asset[\\'duration\\'], settings[\\'default_duration\\'])', 'def test_add_asset_image_upload(self):\\n        image_file = \\'/tmp/image.png\\'\\n\\n        with Browser() as browser:\\n            browser.visit(main_page_url)\\n\\n            browser.find_by_id(\\'add-asset-button\\').click()\\n            sleep(1)\\n\\n            wait_for_and_do(browser, \\'a[href=\"#tab-file_upload\"]\\', lambda tab: tab.click())\\n            wait_for_and_do(browser, \\'input[name=\"file_upload\"]\\', lambda input: input.fill(image_file))\\n            sleep(1)  # wait for new-asset panel animation\\n\\n            sleep(3)  # backend need time to process request\\n\\n        with db.conn(settings[\\'database\\']) as conn:\\n            assets = assets_helper.read(conn)\\n\\n            self.assertEqual(len(assets), 1)\\n            asset = assets[0]\\n\\n            self.assertEqual(asset[\\'name\\'], u\\'image.png\\')\\n            self.assertEqual(asset[\\'mimetype\\'], u\\'image\\')\\n            self.assertEqual(asset[\\'duration\\'], settings[\\'default_duration\\'])', 'def test_add_two_assets_upload(self):\\n        video_file = \\'/tmp/video.flv\\'\\n        image_file = \\'/tmp/image.png\\'\\n\\n        with Browser() as browser:\\n            browser.visit(main_page_url)\\n\\n            browser.find_by_id(\\'add-asset-button\\').click()\\n            sleep(1)\\n\\n            wait_for_and_do(browser, \\'a[href=\"#tab-file_upload\"]\\', lambda tab: tab.click())\\n            wait_for_and_do(browser, \\'input[name=\"file_upload\"]\\', lambda input: input.fill(image_file))\\n            wait_for_and_do(browser, \\'input[name=\"file_upload\"]\\', lambda input: input.fill(video_file))\\n\\n            sleep(3)  # backend need time to process request\\n\\n        with db.conn(settings[\\'database\\']) as conn:\\n            assets = assets_helper.read(conn)\\n\\n            self.assertEqual(len(assets), 2)\\n\\n            self.assertEqual(assets[0][\\'name\\'], u\\'image.png\\')\\n            self.assertEqual(assets[0][\\'mimetype\\'], u\\'image\\')\\n            self.assertEqual(assets[0][\\'duration\\'], settings[\\'default_duration\\'])\\n\\n            self.assertEqual(assets[1][\\'name\\'], u\\'video.flv\\')\\n            self.assertEqual(assets[1][\\'mimetype\\'], u\\'video\\')\\n            self.assertEqual(assets[1][\\'duration\\'], u\\'54\\')', \"def test_rm_asset(self):\\n        with db.conn(settings['database']) as conn:\\n            assets_helper.create(conn, asset_x)\\n\\n        with Browser() as browser:\\n            browser.visit(main_page_url)\\n\\n            wait_for_and_do(browser, '.delete-asset-button', lambda btn: btn.click())\\n            wait_for_and_do(browser, '.confirm-delete', lambda btn: btn.click())\\n            sleep(3)  # backend need time to process request\\n\\n        with db.conn(settings['database']) as conn:\\n            assets = assets_helper.read(conn)\\n            self.assertEqual(len(assets), 0)\", 'def test_disable_asset(self):\\n        with db.conn(settings[\\'database\\']) as conn:\\n            _asset_x = asset_x.copy()\\n            _asset_x[\\'is_enabled\\'] = 1\\n            assets_helper.create(conn, _asset_x)\\n\\n        with Browser() as browser:\\n            browser.visit(main_page_url)\\n\\n            wait_for_and_do(browser, \\'span[class=\"off\"]\\', lambda btn: btn.click())\\n            sleep(3)  # backend need time to process request\\n\\n        with db.conn(settings[\\'database\\']) as conn:\\n            assets = assets_helper.read(conn)\\n            self.assertEqual(len(assets), 1)\\n\\n            asset = assets[0]\\n            self.assertEqual(asset[\\'is_enabled\\'], 0)', \"def test_settings_page_should_work(self):\\n        with Browser() as browser:\\n            browser.visit(settings_url)\\n            self.assertEqual(browser.is_text_present('Error: 500 Internal Server Error'), False,\\n                             '500: internal server error not expected')\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def PlotFit(data,BaseName):\\n    fig = Example_Data.PlotHistograms(data)\\n    fig.savefig(BaseName + \"_Histogram.png\")\\n    fig = Example_Data.PlotLifetimesAndFit(data)\\n    fig.savefig(BaseName + \"_Lifetimes.png\")', 'def run():\\n    \"\"\"\\n\\n    \"\"\"\\n    # figure 1 from dudko 2008\\n    data = Example_Data.Dudko2008Fig1_Probabilities()\\n    PlotFit(data,\"../Out/Dudko2008_Fig1\")\\n    # figure 2 frm dudko 2008\\n    data = Example_Data.Dudko2008Fig2_Probabilities()\\n    PlotFit(data,\"../Out/Dudko2008_Fig2\")']}, {'features': [], 'snippets': [\"def login_page(request):\\n    return render(request, 'pages/login.html')\", 'def login_auth(request):\\n    postdata = request.POST\\n    print(postdata)\\n    if \\'username\\' and \\'password\\' in postdata:\\n        print(postdata[\\'username\\'])\\n        login_username = postdata[\\'username\\']\\n        print(postdata[\\'password\\'])\\n        if ACL.objects.filter(loginID=postdata[\\'username\\'][-9:]).exists():\\n            login_username = login_username[-9:]\\n        else:\\n            login_username = login_username\\n        user = authenticate(username=login_username, password=postdata[\\'password\\'])\\n        if user is not None:\\n            if user.is_active:\\n                login(request, user)\\n                request.session[\\'user\\'] = login_username\\n                if user.is_superuser:\\n                    res = redirect(\\'/admin\\')\\n                else:\\n                    res = redirect(\\'/\\')\\n            else:\\n                res = render(request, \\'pages/login.html\\',\\n                             {\\'wrong\\': True,\\n                              \\'text\\': \\'The password is valid, but the account has been disabled!\\'})\\n        else:\\n            res = render(request, \\'pages/login.html\\',\\n                         {\\'wrong\\': True,\\n                          \\'text\\': \\'The username and password you have entered is not correct. Please retry\\'})\\n    else:\\n        res = render(request, \\'pages/login.html\\', {\\'wrong\\': False})\\n\\n    res[\\'Access-Control-Allow-Origin\\'] = \"*\"\\n    res[\\'Access-Control-Allow-Headers\\'] = \"Origin, X-Requested-With, Content-Type, Accept\"\\n    res[\\'Access-Control-Allow-Methods\\'] = \"PUT, GET, POST, DELETE, OPTIONS\"\\n    return res', \"def home(request):\\n    transcriber_name = request.session['user']\\n    print request.session['user']\\n    if ACL.objects.filter(loginID=transcriber_name).exists():\\n        login_user = ACL.objects.get(loginID=transcriber_name)\\n        print(login_user.loginUser.name)\\n        transcriber_name = login_user.loginUser.name\\n        if login_user.loginUser.type.type_name == 'Distributor':\\n            if login_user.loginUser.number_of_child == 'CHANGED !!!':\\n                return render(request, 'pages/Distributor/index.html', {'transcriber_name': transcriber_name})\\n            else:\\n                return redirect('/change_password/')\\n        elif login_user.loginUser.type.type_name == 'SR':\\n            if login_user.loginUser.number_of_child == 'CHANGED !!!':\\n                return render(request, 'pages/SR/index.html', {'transcriber_name': transcriber_name})\\n            else:\\n                return redirect('/change_password/')\\n        elif login_user.loginUser.type.type_name == 'Seller':\\n            if login_user.loginUser.number_of_child == 'CHANGED !!!':\\n                return render(request, 'pages/Shop/index.html', {'transcriber_name': transcriber_name})\\n            else:\\n                return redirect('/change_password/')\\n        elif login_user.loginUser.type.type_name == 'Buyer':\\n            if login_user.loginUser.number_of_child == 'CHANGED !!!':\\n                return render(request, 'pages/Consumer/index.html', {'transcriber_name': transcriber_name})\\n            else:\\n                return redirect('/change_password/')\\n    else:\\n        number_of_reg_calls = VoiceReg.objects.filter().count()\\n        number_of_transaction_calls = VoiceRecord.objects.filter().count()\\n        total = number_of_reg_calls + number_of_transaction_calls\\n        if total > 0:\\n            reg_call_percentage = (number_of_reg_calls / float(total)) * 100\\n            transaction_call_percentage = (number_of_transaction_calls / float(total)) * 100\\n        else:\\n            transaction_call_percentage = 0\\n            reg_call_percentage = 0\\n        today_month = datetime.date.today().month\\n        today_year = datetime.date.today().year\\n        count = 1\\n        data_2 = ''\\n        data_3 = ''\\n        data_4 = ''\\n        data_5 = ''\\n        data_6 = ''\\n        max = 0\\n        max_table_2 = 0\\n        total_sell = VoiceRecord.objects.filter(purpose='sell').count()\\n        total_buy = VoiceRecord.objects.filter(purpose='buy').count()\\n        total_money_transaction = SMSPayment.objects.filter().count()\\n        total_for_chart2 = number_of_reg_calls + number_of_transaction_calls\\n        if total_for_chart2 > 0:\\n            sell_percentage = (total_sell / float(total_for_chart2)) * 100\\n            buy_percentage = (total_buy / float(total_for_chart2)) * 100\\n            money_transaction_percentage = (total_money_transaction / float(total_for_chart2)) * 100\\n        else:\\n            sell_percentage = 0\\n            buy_percentage = 0\\n            money_transaction_percentage = 0\\n        while count < 32:\\n            total_call_that_day = VoiceRecord.objects.filter(DateAdded__month=today_month,\\n                                                             DateAdded__year=today_year, DateAdded__day=count).count()\\n            total_reg_that_day = VoiceReg.objects.filter(DateAdded__month=today_month,\\n                                                         DateAdded__year=today_year, DateAdded__day=count).count()\\n            if max < total_call_that_day:\\n                max = total_call_that_day + 2\\n            if max < total_reg_that_day:\\n                max = total_reg_that_day + 2\\n\\n            data_2 += '[gd(%s, %s, %s), %s],' % (today_year, today_month, count, total_call_that_day)\\n            data_3 += '[gd(%s, %s, %s), %s],' % (today_year, today_month, count, total_reg_that_day)\\n            total_buy_that_day = VoiceRecord.objects.filter(DateAdded__month=today_month,\\n                                                            DateAdded__year=today_year,\\n                                                            DateAdded__day=count,\\n                                                            purpose='buy').count()\\n            total_sell_that_day = VoiceRecord.objects.filter(DateAdded__month=today_month,\\n                                                             DateAdded__year=today_year,\\n                                                             DateAdded__day=count,\\n                                                             purpose='sell').count()\\n            total_payment_that_day = SMSPayment.objects.filter(DateAdded__month=today_month,\\n                                                               DateAdded__year=today_year,\\n                                                               DateAdded__day=count).count()\\n            if max_table_2 < total_buy_that_day:\\n                max_table_2 = total_buy_that_day + 2\\n            if max_table_2 < total_sell_that_day:\\n                max_table_2 = total_sell_that_day + 2\\n            if max_table_2 < total_payment_that_day:\\n                max_table_2 = total_payment_that_day + 2\\n            data_4 += '[gd(%s, %s, %s), %s],' % (today_year, today_month, count, total_buy_that_day)\\n            data_5 += '[gd(%s, %s, %s), %s],' % (today_year, today_month, count, total_sell_that_day)\\n            data_6 += '[gd(%s, %s, %s), %s],' % (today_year, today_month, count, total_payment_that_day)\\n\\n            count += 1\\n        data_2 = data_2[:-1]\\n        data_3 = data_3[:-1]\\n        data_4 = data_4[:-1]\\n        data_5 = data_5[:-1]\\n        data_6 = data_6[:-1]\\n        number_of_transactions = Transaction.objects.filter().count()\\n        number_of_transactions_with_due = Transaction.objects.filter(total_due__gt=0).count()\\n        number_of_transactions_without_due = Transaction.objects.filter(total_due__lte=0).count()\\n        shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n        all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n        all_user_for_base = Consumer.objects.all()\\n        shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n        all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n        print(all_consumer_for_base.count)\\n        return render(request, 'pages/index.html', {'shop_list_base': all_shop_for_base,\\n                                                    'number_of_reg_calls': number_of_reg_calls,\\n                                                    'transcriber_name': transcriber_name,\\n                                                    'number_of_transaction_calls': number_of_transaction_calls,\\n                                                    'all_consumer_for_base' :all_consumer_for_base,\\n                                                    'reg_call_percentage': reg_call_percentage,\\n                                                    'transaction_call_percentage': transaction_call_percentage,\\n                                                    'data_2': data_2,\\n                                                    'data_3': data_3,\\n                                                    'data_4': data_4,\\n                                                    'data_5': data_5,\\n                                                    'data_6': data_6,\\n                                                    'max': max,\\n                                                    'number_of_transactions': number_of_transactions,\\n                                                    'number_of_transactions_with_due': number_of_transactions_with_due,\\n                                                    'number_of_transactions_without_due': number_of_transactions_without_due,\\n                                                    'max_table_2': max_table_2,\\n                                                    'total_sell': total_sell,\\n                                                    'total_buy': total_buy,\\n                                                    'total_money_transaction': total_money_transaction,\\n                                                    'sell_percentage': sell_percentage,\\n                                                    'buy_percentage': buy_percentage,\\n                                                    'money_transaction_percentage': money_transaction_percentage,\\n                                                    'all_user_for_base': all_user_for_base})\", \"def translator_page(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/translator.html', {'shop_list_base': all_shop_for_base,\\n                                                     'all_consumer_for_base' :all_consumer_for_base,\\n                                                     'transcriber_name': transcriber_name,\\n                                                     'all_user_for_base': all_user_for_base})\", \"def report_monthly_shop(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_id = shop_object.id\\n    total_sell = 0\\n    total_sell_due = 0\\n    total_sell_paid = 0\\n    total_purchase = 0\\n    total_purchase_due = 0\\n    total_purchase_paid = 0\\n    for month_sell in BuyerSellerAccount.objects.filter(seller=shop_object):\\n        total_sell += month_sell.total_amount_of_transaction\\n        total_sell_due += month_sell.total_due\\n        total_sell_paid += month_sell.total_paid\\n    for month_purchase in BuyerSellerAccount.objects.filter(buyer=shop_object):\\n        total_purchase += month_purchase.total_amount_of_transaction\\n        total_purchase_due += month_purchase.total_due\\n        total_purchase_paid += month_purchase.total_paid\\n\\n\\n\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_monthly_shop.html', {'shop_list_base': all_shop_for_base,\\n                                                              'shop_name': shop_name,\\n                                                              'shop_id': shop_id,\\n                                                              'all_consumer_for_base' :all_consumer_for_base,\\n                                                              'total_sell': total_sell,\\n                                                              'transcriber_name': transcriber_name,\\n                                                              'total_sell_due': total_sell_due,\\n                                                              'total_sell_paid': total_sell_paid,\\n                                                              'bangla': bangla,\\n                                                              'total_purchase': total_purchase,\\n                                                              'total_purchase_due': total_purchase_due,\\n                                                              'total_purchase_paid': total_purchase_paid,\\n                                                              'all_user_for_base': all_user_for_base})\", 'def report_monthly_shop_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n\\n    shop_inventory = BuySellProfitInventoryIndividual.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    output = \\'{\"data\": [ \\'\\n\\n    if get_data[\\'t\\'] == \\'1\\':\\n        rank = 1\\n        this_year = datetime.date.today().year\\n        # this_month = 1\\n        this_day = 1\\n        for this_month in range(1, 13, 1):\\n            count = 0\\n            for this_day in range(1, 32, 1):\\n                for a_product in Product.objects.all():\\n\\n                    product_price = 0\\n                    product_name = a_product.name\\n                    total_sell = 0\\n                    total_due = 0\\n                    total_paid = 0\\n                    for this_day_sell_transaction in Transaction.objects.filter(seller=shop_object,\\n                                                                                DateAdded__year=this_year,\\n                                                                                DateAdded__month=this_month,\\n                                                                                DateAdded__day=this_day):\\n                        total_sell += this_day_sell_transaction.total_amount\\n                        total_due += this_day_sell_transaction.total_due\\n                        total_paid += this_day_sell_transaction.total_paid\\n                        count += 1\\n\\n                    total_purchase = 0\\n                    total_purchase_due = 0\\n                    total_purchase_paid = 0\\n                    for this_day_purchase_transaction in Transaction.objects.filter(buyer=shop_object,\\n                                                                                    DateAdded__year=this_year,\\n                                                                                    DateAdded__month=this_month,\\n                                                                                    DateAdded__day=this_day):\\n                        total_purchase += this_day_purchase_transaction.total_amount\\n                        total_purchase_due += this_day_purchase_transaction.total_due\\n                        total_purchase_paid += this_day_purchase_transaction.total_paid\\n                        count += 1\\n\\n                if count > 0:\\n                    output += \\'[\"%s/%s/%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (this_day, this_month, this_year,\\n                                                                                total_sell, total_paid, total_due,\\n                                                                                total_purchase, total_purchase_paid,\\n                                                                                total_purchase_due)\\n                    count = 0\\n                    # this_day += 1\\n                    # this_month = this_month + 1\\n    if get_data[\\'t\\'] == \\'2\\':\\n        for this_day_transaction in Transaction.objects.filter(Q(seller=shop_object) | Q(buyer=shop_object)):\\n            # start counting for this product\\n            id = this_day_transaction.pk\\n            date = this_day_transaction.DateAdded\\n            if this_day_transaction.seller == shop_object:\\n                with_trade = this_day_transaction.buyer\\n                trade_type = \\'Sell\\'\\n            elif this_day_transaction.buyer == shop_object:\\n                with_trade = this_day_transaction.seller\\n                trade_type = \\'Buy\\'\\n            number_of_items = ProductsInTransaction.objects.filter(TID=this_day_transaction).count()\\n            total_amount = this_day_transaction.total_amount\\n            total_paid = this_day_transaction.total_paid\\n            total_due = this_day_transaction.total_due\\n\\n            output += \\'[\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (id, date, with_trade, trade_type,\\n                                                                       number_of_items, total_amount,\\n                                                                       total_paid, total_due)\\n\\n    output = output[:-1]\\n    output += \\']}\\'\\n    return HttpResponse(output, content_type=\"text/plain\")', \"def report_sales_analysis(request):\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_id = shop_object.id\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_sales_analysis.html', {'shop_list_base': all_shop_for_base,\\n                                                                'shop_name': shop_name,\\n                                                                'all_consumer_for_base' :all_consumer_for_base,\\n                                                                'shop_id': shop_id,\\n                                                                'bangla': bangla,\\n                                                                'transcriber_name': transcriber_name,\\n                                                                'all_user_for_base': all_user_for_base})\", 'def report_sales_analysis_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n\\n    shop_inventory = BuySellProfitInventoryIndividual.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    output = \\'{\"data\": [ \\'\\n\\n    if get_data[\\'t\\'] == \\'1\\':\\n        rank = 1\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_price = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(seller=shop_object):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                            if a_product.bulk_to_retail_unit == 0:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                        else:\\n                            count = count + product_in_this_transaction.quantity\\n                            product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name, str(count) + \\' \\' + a_product.retail_unit)\\n                rank += 1\\n    if get_data[\\'t\\'] == \\'2\\':\\n        rank = 1\\n        for a_product in Product.objects.all():\\n            count = 0\\n\\n            product_price = 0\\n            previous_product_price = 0\\n            change = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(seller=shop_object):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if count == 0:\\n                            previous_product_price = product_in_this_transaction.price_per_unit\\n                        product_price = product_in_this_transaction.price_per_unit\\n                        change += abs(previous_product_price - product_price)\\n                        count += 1\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name, count,\\n                                                       change/count)\\n                rank += 1\\n    if get_data[\\'t\\'] == \\'3\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        day = 1\\n\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while day < 32:\\n            day_string = True\\n            rank = 1\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n\\n                for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__year=this_year,\\n                                                                       DateAdded__month=this_month, DateAdded__day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                                if a_product.bulk_to_retail_unit == 0:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                                else:\\n                                    count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n                if count > 0:\\n                    if day_string:\\n                        output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name,\\n                                                              str(count) + \\' \\' + a_product.retail_unit,\\n                                                              float(product_price / count))\\n                    rank += 1\\n            day += 1\\n            # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n    if get_data[\\'t\\'] == \\'4\\':\\n        day = 1\\n\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while day < 8:\\n            day_string = True\\n            rank = 1\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n\\n                for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__week_day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                                if a_product.bulk_to_retail_unit == 0:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                                else:\\n                                    count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n                if count > 0:\\n                    if day_string:\\n                        if day == 1:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Sunday\\'\\n                        elif day == 2:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Monday\\'\\n                        elif day == 3:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Tuesday\\'\\n                        elif day == 4:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Wednesday\\'\\n                        elif day == 5:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Thursday\\'\\n                        elif day == 6:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Friday\\'\\n                        elif day == 7:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Saturday\\'\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name,\\n                                                              str(count) + \\' \\' + a_product.retail_unit,\\n                                                              float(product_price / count))\\n                    rank += 1\\n            day += 1\\n    if get_data[\\'t\\'] == \\'5\\':\\n        this_year = datetime.date.today().year\\n        day_string = True\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_profit = 0\\n            product_name = a_product.name\\n            for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object):\\n                # start counting for this product\\n                if this_day_transaction.product == a_product:\\n                    product_profit += this_day_transaction.profit\\n                    count += 1\\n            output += \\'[\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n    output = output[:-1]\\n    output += \\']}\\'\\n    return HttpResponse(output, content_type=\"text/plain\")', \"def report_payment(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    sell_transaction_with_due = Transaction.objects.filter(seller_id=shop_object, total_due__lte=0)\\n    buy_transaction_with_due = Transaction.objects.filter(buyer_id=shop_object, total_due__lte=0)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    buyer_account = BuyerSellerAccount.objects.filter(seller=shop_object,  total_due__lte=0)\\n    seller_account = BuyerSellerAccount.objects.filter(buyer=shop_object,  total_due__lte=0)\\n    all_user_for_base = Consumer.objects.all()\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    transcriber_name = request.session['user']\\n    return render(request, 'pages/report_payment.html', {'shop_list_base': all_shop_for_base,\\n                                                         'sell_transaction_with_due': sell_transaction_with_due,\\n                                                         'buy_transaction_with_due': buy_transaction_with_due,\\n                                                         'all_consumer_for_base' :all_consumer_for_base,\\n                                                         'buyer_account': buyer_account,\\n                                                         'transcriber_name': transcriber_name,\\n                                                         'seller_account': seller_account,\\n                                                         'shop_name': shop_name,\\n                                                         'bangla': bangla,\\n                                                         'all_user_for_base': all_user_for_base})\", \"def report_due(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    sell_transaction_with_due = Transaction.objects.filter(seller_id=shop_object, total_due__gt=0)\\n    buy_transaction_with_due = Transaction.objects.filter(buyer_id=shop_object, total_due__gt=0)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    buyer_account = SMSPayment.objects.filter(seller=shop_object)\\n    seller_account = SMSPayment.objects.filter(buyer=shop_object)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_due.html', {'shop_list_base': all_shop_for_base,\\n                                                     'sell_transaction_with_due': sell_transaction_with_due,\\n                                                     'buy_transaction_with_due': buy_transaction_with_due,\\n                                                     'buyer_account': buyer_account,\\n                                                     'all_consumer_for_base' :all_consumer_for_base,\\n                                                     'bangla': bangla,\\n                                                     'seller_account': seller_account,\\n                                                     'transcriber_name': transcriber_name,\\n                                                     'shop_name': shop_name,\\n                                                     'all_user_for_base': all_user_for_base})\", \"def report_profit(request):\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_id = shop_object.id\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_profit.html', {'shop_list_base': all_shop_for_base,\\n                                                        'shop_name': shop_name,\\n                                                        'shop_id': shop_id,\\n                                                        'all_consumer_for_base' :all_consumer_for_base,\\n                                                        'bangla': bangla,\\n                                                        'transcriber_name': transcriber_name,\\n                                                        'all_user_for_base': all_user_for_base})\", 'def report_profit_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n    shop_inventory = BuySellProfitInventoryIndividual.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    output = \\'{\"data\": [ \\'\\n\\n    if get_data[\\'t\\'] == \\'1\\':\\n        this_year = datetime.date.today().year\\n        this_month = 1\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while this_month < 13:\\n            day_string = True\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_profit = 0\\n                product_name = a_product.name\\n                for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object,\\n                                                                                            DateAdded__year=this_year,\\n                                                                                            DateAdded__month=this_month):\\n                    # start counting for this product\\n                    if this_day_transaction.product == a_product:\\n                        product_profit += this_day_transaction.profit\\n                        count += 1\\n\\n                if count > 0:\\n                    if day_string:\\n                        if this_month == 1:\\n                            output += \\'[\"January\",\"\",\"\"], \\'\\n                        elif this_month == 2:\\n                            output += \\'[\"February\",\"\",\"\"], \\'\\n                        elif this_month == 3:\\n                            output += \\'[\"March\",\"\",\"\"], \\'\\n                        elif this_month == 4:\\n                            output += \\'[\"April\",\"\",\"\"], \\'\\n                        elif this_month == 5:\\n                            output += \\'[\"May\",\"\",\"\"], \\'\\n                        elif this_month == 6:\\n                            output += \\'[\"June\",\"\",\"\"], \\'\\n                        elif this_month == 7:\\n                            output += \\'[\"July\",\"\",\"\"], \\'\\n                        elif this_month == 8:\\n                            output += \\'[\"August\",\"\",\"\"], \\'\\n                        elif this_month == 9:\\n                            output += \\'[\"September\",\"\",\"\"], \\'\\n                        elif this_month == 10:\\n                            output += \\'[\"October\",\"\",\"\"], \\'\\n                        elif this_month == 11:\\n                            output += \\'[\"November\",\"\",\"\"], \\'\\n                        elif this_month == 12:\\n                            output += \\'[\"December\",\"\",\"\"], \\'\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n            # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n            this_month += 1\\n    if get_data[\\'t\\'] == \\'2\\':\\n        this_year = datetime.date.today().year\\n        this_month = 1\\n\\n        while this_month < 13:\\n            day = 1\\n            while day < 32:\\n                day_string = True\\n                for a_product in Product.objects.all():\\n                    count = 0\\n                    product_profit = 0\\n                    product_name = a_product.name\\n                    for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object,\\n                                                                                                DateAdded__year=this_year,\\n                                                                                                DateAdded__month=this_month,\\n                                                                                                DateAdded__day=day):\\n                        # start counting for this product\\n                        if this_day_transaction.product == a_product:\\n                            product_profit += this_day_transaction.profit\\n                            count += 1\\n\\n                    if count > 0:\\n                        if day_string:\\n                            output += \\'[\"%s/%s/%s\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n                            day_string = False\\n                        output += \\'[\"\",\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n                day += 1\\n            this_month += 1\\n    if get_data[\\'t\\'] == \\'3\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        day_string = True\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_profit = 0\\n            product_name = a_product.name\\n            for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object,\\n                                                                                        DateAdded__year=this_year,\\n                                                                                        DateAdded__month=this_month):\\n                # start counting for this product\\n                if this_day_transaction.product == a_product:\\n                    product_profit += this_day_transaction.profit\\n                    count += 1\\n            output += \\'[\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n            # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n    if get_data[\\'t\\'] == \\'4\\':\\n        this_year = datetime.date.today().year\\n        day_string = True\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_profit = 0\\n            product_name = a_product.name\\n            for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object,\\n                                                                                        DateAdded__year=this_year):\\n                # start counting for this product\\n                if this_day_transaction.product == a_product:\\n                    product_profit += this_day_transaction.profit\\n                    count += 1\\n            output += \\'[\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n    if get_data[\\'t\\'] == \\'5\\':\\n        this_year = datetime.date.today().year\\n        day_string = True\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_profit = 0\\n            product_name = a_product.name\\n            for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object):\\n                # start counting for this product\\n                if this_day_transaction.product == a_product:\\n                    product_profit += this_day_transaction.profit\\n                    count += 1\\n            output += \\'[\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n    output = output[:-1]\\n    output += \\']}\\'\\n    return HttpResponse(output, content_type=\"text/plain\")', \"def report_product(request):\\n\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_id = shop_object.id\\n    shop_inventory = Inventory.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    selected_products = ProductsInTransaction.objects.filter(TID=Transaction.objects.filter(seller=shop_object))\\n\\n    selected_products_buy = ProductsInTransaction.objects.filter(TID=Transaction.objects.filter(buyer=shop_object))\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_product.html', {'shop_list_base': all_shop_for_base,\\n                                                         'shop_inventory': shop_inventory,\\n                                                         'shop_name': shop_name,\\n                                                         'shop_id': shop_id,\\n                                                         'bangla': bangla,\\n                                                         'all_consumer_for_base' :all_consumer_for_base,\\n                                                         'transcriber_name': transcriber_name,\\n                                                         'selected_products_buy': selected_products_buy,\\n                                                         'selected_products': selected_products,\\n                                                         'all_user_for_base': all_user_for_base})\", 'def report_product_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n    shop_inventory = Inventory.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    output = \\'{\"data\": [ \\'\\n\\n    if get_data[\\'t\\'] == \\'1\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        day = 1\\n\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while day < 32:\\n            day_string = True\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n                for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__year=this_year,\\n                                                                       DateAdded__month=this_month, DateAdded__day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            # if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                            #     if a_product.bulk_to_retail_unit == 0:\\n                            #         count = count + product_in_this_transaction.quantity\\n                            #         product_price = product_price + product_in_this_transaction.price_per_unit\\n                            #     else:\\n                            #         count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                            #         product_price = product_price + product_in_this_transaction.price_per_unit\\n                            # else:\\n                            count = count + product_in_this_transaction.quantity\\n                            product_price = product_price + product_in_this_transaction.price_per_unit * product_in_this_transaction.quantity\\n\\n                if count > 0:\\n                    if day_string:\\n                        output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (product_name, count,\\n                                                              a_product.retail_unit,\\n                                                              float(product_price / count))\\n            day += 1\\n            # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n    if get_data[\\'t\\'] == \\'2\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        day = 1\\n        while day < 32:\\n            day_string = True\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n                for this_day_transaction in Transaction.objects.filter(buyer=shop_object, DateAdded__year=this_year,\\n                                                                       DateAdded__month=this_month, DateAdded__day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                                if a_product.bulk_to_retail_unit == 0:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                                else:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n                if count > 0:\\n                    if day_string:\\n                        output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (product_name, count,\\n                                                              a_product.bulk_wholesale_unit,\\n                                                              float(product_price / count))\\n            day += 1\\n    if get_data[\\'t\\'] == \\'3\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_price = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__year=this_year, DateAdded__month=this_month):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                            if a_product.bulk_to_retail_unit == 0:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                        else:\\n                            count = count + product_in_this_transaction.quantity\\n                            product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (product_name, count,\\n                                                       a_product.retail_unit,\\n                                                       float(product_price / count))\\n    if get_data[\\'t\\'] == \\'4\\':\\n        this_year = datetime.date.today().year\\n        this_month = datetime.date.today().month\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_price = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(buyer=shop_object, DateAdded__year=this_year, DateAdded__month=this_month):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                            if a_product.bulk_to_retail_unit == 0:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                        else:\\n                            count = count + product_in_this_transaction.quantity\\n                            product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (product_name, count,\\n                                                       a_product.retail_unit,\\n                                                       float(product_price / count))\\n    output = output[:-1]\\n    output += \\']}\\'\\n    selected_products_buy = ProductsInTransaction.objects.filter(TID=Transaction.objects.filter(buyer=shop_object))\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    return HttpResponse(output, content_type=\"text/plain\")', 'def report_analytical(request):\\n    all_product = Product.objects.all()\\n    final_output = \\'\\'\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_id = shop_object.id\\n    for product in all_product:\\n        print(product.name)\\n        if ProductsInTransaction.objects.filter(product=product).exists():\\n            product_output = \"[%s, \" % product.name\\n            sold_amount = 0\\n            for product_details in ProductsInTransaction.objects.filter(product=product):\\n                sold_amount = sold_amount + product_details.quantity\\n            product_output += str(sold_amount)\\n            final_output += product_output\\n        final_output += \"] ,\"\\n    print(final_output)\\n    final_output = final_output[:-1]\\n    print(final_output)\\n    add_notification = False\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session[\\'user\\']\\n    shop_consumer2 = ConsumerType.objects.get(type_name=\\'Buyer\\')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, \\'pages/reports_analytical.html\\',\\n                  {\\'all_product\\': all_product, \\'add_notification\\': add_notification,\\n                   \\'shop_list_base\\': all_shop_for_base, \\'product_sell\\': final_output,\\n                   \\'all_consumer_for_base\\' :all_consumer_for_base,\\n                   \\'transcriber_name\\': transcriber_name,\\n                   \\'shop_name\\': shop_name,\\n                   \\'shop_id\\': shop_id,\\n                   \\'all_user_for_base\\': all_user_for_base})', 'def report_analytical_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n    if get_data[\\'t\\'] == \\'1\\':\\n        all_product = Product.objects.all()\\n        final_output = \\'{\"cols\": [ { \"id\": \"\", \"label\": \"Topping\", \"pattern\": \"\", \"type\": \"string\" }, \\' \\\\\\n                       \\'{ \"id\": \"\", \"label\": \"Units\", \"pattern\": \"\", \"type\": \"number\" } ], \"rows\": [ \\'\\n        for product in all_product:\\n            print(product.name)\\n            if ProductsInTransaction.objects.filter(product=product).exists():\\n                product_name = product.name\\n                sold_amount = 0\\n                for transaction_id in Transaction.objects.filter(seller=shop_object):\\n                    for product_details in ProductsInTransaction.objects.filter(product=product, TID=transaction_id):\\n                        sold_amount = sold_amount + product_details.quantity\\n                final_output += \\'{\"c\": [{\"v\": \"%s\",\"f\": null},{\"v\": %s,\"f\": null}]},\\' % (product_name,\\n                                                                                         sold_amount)\\n        final_output = final_output[:-1]\\n        print(final_output)\\n    if get_data[\\'t\\'] == \\'2\\':\\n        all_product = BuySellProfitInventory.objects.filter(shop=shop_object)\\n        final_output = \\'{\"cols\": [ { \"id\": \"\", \"label\": \"Topping\", \"pattern\": \"\", \"type\": \"string\" }, \\' \\\\\\n                       \\'{ \"id\": \"\", \"label\": \"Profit\", \"pattern\": \"\", \"type\": \"number\" } ], \"rows\": [ \\'\\n        for product in all_product:\\n            final_output += \\'{\"c\": [{\"v\": \"%s\",\"f\": null},{\"v\": %s,\"f\": null}]},\\' % (product.product,\\n                                                                                     product.profit)\\n        final_output = final_output[:-1]\\n        print(final_output)\\n    final_output += \\']}\\'\\n    print(final_output)\\n    return HttpResponse(final_output, content_type=\"text/plain\")', \"def report_recharge(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_recharge.html', {'shop_list_base': all_shop_for_base,\\n                                                          'all_consumer_for_base' :all_consumer_for_base,\\n                                                          'transcriber_name': transcriber_name,\\n                                                          'all_user_for_base': all_user_for_base})\", \"def report_callduration(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_callduration_graph.html', {'shop_list_base': all_shop_for_base,\\n                                                                    'all_consumer_for_base' :all_consumer_for_base,\\n                                                                    'transcriber_name': transcriber_name,\\n                                                                    'all_user_for_base': all_user_for_base})\", \"def report_transaction(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_transaction.html', {'shop_list_base': all_shop_for_base,\\n                                                             'all_consumer_for_base' :all_consumer_for_base,\\n                                                             'transcriber_name': transcriber_name,\\n                                                             'all_user_for_base': all_user_for_base})\", \"def report_calltranscription(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_transcription.html', {'shop_list_base': all_shop_for_base,\\n                                                               'all_consumer_for_base' :all_consumer_for_base,\\n                                                               'transcriber_name': transcriber_name,\\n                                                               'all_user_for_base': all_user_for_base})\", \"def report_usercall(request):\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_user_call_recharge.html', {'shop_list_base': all_shop_for_base,\\n                                                                    'transcriber_name': transcriber_name,\\n                                                                    'all_consumer_for_base' :all_consumer_for_base,\\n                                                                    'all_user_for_base': all_user_for_base})\", \"def transcription_page(request):\\n    print(request.POST)\\n    number_of_pending_calls = VoiceRecord.objects.filter(transcribed=False).count()\\n    number_of_pending_reg_calls = VoiceReg.objects.filter(completed=False).count()\\n\\n    type_of_subscriber = ConsumerType.objects.all()\\n    number_of_fail_calls = VoiceRecord.objects.filter(with_error=True).count()\\n    number_of_completed_calls = VoiceRecord.objects.filter(with_error=False, transcribed=True).count()\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    return render(request, 'pages/transcription.html',\\n                  dict(pending_calls=number_of_pending_calls, types=type_of_subscriber,\\n                       pending_calls_reg=number_of_pending_reg_calls, number_of_fail_calls=str(number_of_fail_calls),\\n                       number_of_completed_calls=number_of_completed_calls, transcriber_name=transcriber_name,\\n                       shop_list_base=all_shop_for_base,all_consumer_for_base=all_consumer_for_base,\\n                       all_user_for_base=all_user_for_base))\", \"def add_subscriber_page(request):\\n    all_subscriber = Consumer.objects.all()\\n    type_of_subscriber = ConsumerType.objects.all()\\n    add_notification = False\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n    notification = ''\\n    if 'delete' in request.GET:\\n        get_data = request.GET\\n        add_notification = True\\n        delID = get_data['delete']\\n        if Consumer.objects.filter(id=delID).exists():\\n            item_for_delete = Consumer.objects.get(id=delID)\\n            notification = 'Daily statement for the user : ' + item_for_delete.name + ' is sent successfully.'\\n            # item_for_delete.delete()\\n            sales_statement = ''\\n            purchase_statement = ''\\n            today_date = datetime.date.today()\\n            today_day = today_date.day\\n            today_month = today_date.month\\n            today_year = today_date.year\\n            # for selling\\n            sell_transactions = Transaction.objects.filter(seller=item_for_delete, DateAdded__day=today_day,\\n                                                           DateAdded__month=today_month, DateAdded__year=today_year)\\n            total_sales = 0\\n            total_due = 0\\n            total_paid = 0\\n            for sell_transaction in sell_transactions:\\n                total_sales += sell_transaction.total_amount\\n                total_paid += sell_transaction.total_paid\\n                total_due += sell_transaction.total_due\\n            if total_sales > 0:\\n                sales_statement = ' bikroy korechen mot: ' + str(total_sales) + ' takar. nogod peyechen : ' + \\\\\\n                                  str(total_paid) + ' taka ebong baki royeche ' + str(total_due) + ' taka.'\\n            buy_transactions = Transaction.objects.filter(buyer=item_for_delete, DateAdded__day=today_day,\\n                                                          DateAdded__month=today_month, DateAdded__year=today_year)\\n            total_purchase = 0\\n            total_due = 0\\n            total_paid = 0\\n            for buy_transaction in buy_transactions:\\n                total_purchase += buy_transaction.total_amount\\n                total_paid += buy_transaction.total_paid\\n                total_due += buy_transaction.total_due\\n            if total_purchase > 0:\\n                purchase_statement = ' kinechen mot: ' + str(total_purchase) + ' takar. Nogod diyechen : ' + \\\\\\n                                     str(total_paid) + ' taka ebong baki royeche ' + str(total_due) + ' taka.'\\n            final_text = 'Aj apni' + sales_statement + purchase_statement + ' Dhonnobad'\\n\\n            if total_purchase > 0 or total_sales > 0:\\n                print(final_text)\\n                send_sms(final_text, item_for_delete.phone)\\n\\n        else:\\n            notification = 'Item not found'\\n\\n    return render(request, 'pages/add_subscriber.html',\\n                  {'subscribers': all_subscriber, 'types': type_of_subscriber, 'add_notification': add_notification,\\n                   'shop_list_base': all_shop_for_base,\\n                   'all_consumer_for_base' :all_consumer_for_base,\\n                   'transcriber_name': transcriber_name,\\n                   'notification':notification,\\n                   'all_user_for_base': all_user_for_base})\", \"def add_product_page(request):\\n    all_product = Product.objects.all()\\n    add_notification = False\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n    notification = ''\\n    if 'delete' in request.GET:\\n        get_data = request.GET\\n        add_notification = True\\n        delID = get_data['delete']\\n        if Product.objects.filter(id=delID).exists():\\n            item_for_delete = Product.objects.get(id=delID)\\n            notification = 'The product : ' + item_for_delete.name + ' is deleted successfully.'\\n            item_for_delete.delete()\\n        else:\\n            notification = 'Item not found'\\n\\n    return render(request, 'pages/add_product.html',\\n                  {'all_product': all_product, 'add_notification': add_notification,\\n                   'all_consumer_for_base' :all_consumer_for_base,\\n                   'transcriber_name': transcriber_name,'notification': notification,\\n                   'shop_list_base': all_shop_for_base, 'all_user_for_base': all_user_for_base})\", \"def report_transcriber_performance(request):\\n    all_product = Product.objects.all()\\n    add_notification = False\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_transcriber_performance.html',\\n                  {'all_product': all_product, 'add_notification': add_notification,\\n                   'transcriber_name': transcriber_name,\\n                   'all_consumer_for_base' :all_consumer_for_base,\\n                   'shop_list_base': all_shop_for_base, 'all_user_for_base': all_user_for_base})\", 'def report_transcriber_performance_json(request):\\n    final_output = \\'{\"data\": [ \\'\\n    for transcriber in Transcriber.objects.all():\\n        number_of_transcriptions = TranscriberInTranscription.objects.filter(name=transcriber).count()\\n        total_time_taken = 0\\n        total_product_trancribed = 0\\n        for transcriber_in_transaction in TranscriberInTranscription.objects.filter(name=transcriber):\\n            total_time_taken += float(transcriber_in_transaction.time_taken)\\n            total_product_trancribed += transcriber_in_transaction.number_of_products\\n        if number_of_transcriptions > 0:\\n            avg_time = total_time_taken / number_of_transcriptions\\n            avg_product = total_product_trancribed / number_of_transcriptions\\n            final_output += \\'[\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (transcriber.id, transcriber.name,\\n                                                              number_of_transcriptions, avg_time, avg_product)\\n    final_output = final_output[:-1]\\n    final_output += \\']}\\'\\n    return HttpResponse(final_output, content_type=\"text/plain\")', \"def user_balance_recharge(request):\\n    post_data = request.POST\\n    notification = ''\\n    for all_consumers in Consumer.objects.all():\\n        if Recharge.objects.filter(user=all_consumers).exists():\\n            print('Already_Added')\\n        else:\\n            new_added = Recharge(user=all_consumers)\\n            new_added.save()\\n\\n        if TotalRecharge.objects.filter(user=all_consumers).exists():\\n            print('Already_Added')\\n        else:\\n            new_added = TotalRecharge(user=all_consumers)\\n            new_added.save()\\n    add_notification = False\\n    if 'user' in post_data and 'recharge_amount' in post_data:\\n        user_name = post_data['user']\\n        user_object = Consumer.objects.get(name=user_name)\\n        if is_number(post_data['recharge_amount']) or is_float(post_data['recharge_amount']):\\n            new_recharge_added = Recharge(user=user_object, amount=float(post_data['recharge_amount']))\\n            new_recharge_added.save()\\n            new_recharge_update = TotalRecharge.objects.get(user=user_object)\\n            new_recharge_update.amount += float(post_data['recharge_amount'])\\n            new_recharge_update.save()\\n            add_notification = True\\n            notification = 'Amount %s has been added to the number %s' %(post_data['recharge_amount'],\\n                                                                         user_object.phone)\\n        else:\\n            notification = 'Something went wrong. Please try again.'\\n    recharge_all = TotalRecharge.objects.all()\\n    today_date = datetime.date.today()\\n    today_day = today_date.day\\n    today_month = today_date.month\\n    today_year = today_date.year\\n    recharge_today = Recharge.objects.filter(DateAdded__day=today_day,\\n                                             DateAdded__month=today_month, DateAdded__year=today_year, amount__gt=0)\\n    all_product = Product.objects.all()\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'pages/report_user_call_recharge.html',\\n                  {'all_product': all_product, 'add_notification': add_notification,\\n                   'all_consumer_for_base' :all_consumer_for_base,\\n                   'shop_list_base': all_shop_for_base, 'recharge_all': recharge_all,\\n                   'transcriber_name': transcriber_name,\\n                   'recharge_today': recharge_today, 'all_user_for_base': all_user_for_base,\\n                   'notification': notification})\", \"def report_monthly_shop_print(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    total_sell = 0\\n    total_sell_due = 0\\n    total_sell_paid = 0\\n    total_purchase = 0\\n    total_purchase_due = 0\\n    total_purchase_paid = 0\\n    for month_sell in BuyerSellerAccount.objects.filter(seller=shop_object):\\n        total_sell += month_sell.total_amount_of_transaction\\n        total_sell_due += month_sell.total_due\\n        total_sell_paid += month_sell.total_paid\\n    for month_purchase in BuyerSellerAccount.objects.filter(buyer=shop_object):\\n        total_purchase += month_purchase.total_amount_of_transaction\\n        total_purchase_due += month_purchase.total_due\\n        total_purchase_paid += month_purchase.total_paid\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    transcriber_name = request.session['user']\\n    return render(request, 'print/report_monthly_shop.html', {'shop_list_base': all_shop_for_base,\\n                                                              'shop_name': shop_name,\\n                                                              'all_consumer_for_base' :all_consumer_for_base,\\n                                                              'total_sell': total_sell,\\n                                                              'transcriber_name': transcriber_name,\\n                                                              'total_sell_due': total_sell_due,\\n                                                              'total_sell_paid': total_sell_paid,\\n                                                              'bangla': bangla,\\n                                                              'total_purchase': total_purchase,\\n                                                              'total_purchase_due': total_purchase_due,\\n                                                              'total_purchase_paid': total_purchase_paid,\\n                                                              'all_user_for_base': all_user_for_base})\", \"def report_due_print(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    sell_transaction_with_due = Transaction.objects.filter(seller_id=shop_object, total_due__gt=0)\\n    buy_transaction_with_due = Transaction.objects.filter(buyer_id=shop_object, total_due__gt=0)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    buyer_account = BuyerSellerAccount.objects.filter(seller=shop_object,  total_due__gt=0)\\n    seller_account = BuyerSellerAccount.objects.filter(buyer=shop_object,  total_due__gt=0)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'print/report_due.html', {'shop_list_base': all_shop_for_base,\\n                                                     'sell_transaction_with_due': sell_transaction_with_due,\\n                                                     'buy_transaction_with_due': buy_transaction_with_due,\\n                                                     'buyer_account': buyer_account,\\n                                                     'all_consumer_for_base' :all_consumer_for_base,\\n                                                     'bangla': bangla,\\n                                                     'seller_account': seller_account,\\n                                                     'transcriber_name': transcriber_name,\\n                                                     'shop_name': shop_name,\\n                                                     'all_user_for_base': all_user_for_base})\", \"def report_payment_print(request):\\n    get_data = request.GET\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n\\n    shop_name = get_data['shop']\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    sell_transaction_with_due = Transaction.objects.filter(seller_id=shop_object, total_due__lte=0)\\n    buy_transaction_with_due = Transaction.objects.filter(buyer_id=shop_object, total_due__lte=0)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    buyer_account = BuyerSellerAccount.objects.filter(seller=shop_object,  total_due__lte=0)\\n    seller_account = BuyerSellerAccount.objects.filter(buyer=shop_object,  total_due__lte=0)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'print/report_payment.html', {'shop_list_base': all_shop_for_base,\\n                                                         'sell_transaction_with_due': sell_transaction_with_due,\\n                                                         'buy_transaction_with_due': buy_transaction_with_due,\\n                                                         'all_consumer_for_base' :all_consumer_for_base,\\n                                                         'buyer_account': buyer_account,\\n                                                         'transcriber_name': transcriber_name,\\n                                                         'seller_account': seller_account,\\n                                                         'shop_name': shop_name,\\n                                                         'bangla': bangla,\\n                                                         'all_user_for_base': all_user_for_base})\", \"def report_product_print(request):\\n\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    if 'ban' in get_data:\\n        bangla = True\\n    else:\\n        bangla = False\\n    shop_object = Consumer.objects.get(name=shop_name)\\n    shop_inventory = Inventory.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    selected_products = ProductsInTransaction.objects.filter(TID=Transaction.objects.filter(seller=shop_object))\\n\\n    selected_products_buy = ProductsInTransaction.objects.filter(TID=Transaction.objects.filter(buyer=shop_object))\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'print/report_product.html', {'shop_list_base': all_shop_for_base,\\n                                                         'shop_inventory': shop_inventory,\\n                                                         'shop_name': shop_name,\\n                                                         'bangla': bangla,\\n                                                         'all_consumer_for_base' :all_consumer_for_base,\\n                                                         'transcriber_name': transcriber_name,\\n                                                         'selected_products_buy': selected_products_buy,\\n                                                         'selected_products': selected_products,\\n                                                         'all_user_for_base': all_user_for_base})\", \"def report_sales_analysis_print(request):\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    transcriber_name = request.session['user']\\n    return render(request, 'print/report_sales_analysis.html', {'shop_list_base': all_shop_for_base,\\n                                                                'all_consumer_for_base' :all_consumer_for_base,\\n                                                                'shop_name': shop_name,\\n                                                                'transcriber_name': transcriber_name,\\n                                                                'all_user_for_base': all_user_for_base})\", \"def report_profit_print(request):\\n    get_data = request.GET\\n    shop_name = get_data['shop']\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'print/report_profit.html', {'shop_list_base': all_shop_for_base,\\n                                                        'shop_name': shop_name,\\n                                                        'all_consumer_for_base':all_consumer_for_base,\\n                                                        'transcriber_name': transcriber_name,\\n                                                        'all_user_for_base': all_user_for_base})\", \"def report_transcriber_performance_print(request):\\n    all_product = Product.objects.all()\\n    add_notification = False\\n    shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    all_user_for_base = Consumer.objects.all()\\n    transcriber_name = request.session['user']\\n    shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n\\n    return render(request, 'print/report_transcriber_performance.html',\\n                  {'all_product': all_product, 'add_notification': add_notification,\\n                   'all_consumer_for_base':all_consumer_for_base,\\n                   'transcriber_name': transcriber_name,\\n                   'shop_list_base': all_shop_for_base, 'all_user_for_base': all_user_for_base})\", \"def sr_monthly_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n\\n    return render(request, 'pages/SR/report_monthly.html', {'transcriber_name': transcriber_name,\\n                                                            'allTransaction': allTransaction})\", \"def sr_due_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    allBalance = BuyerSellerAccount.objects.filter(seller=sr_object)\\n    sell_transaction = Transaction.objects.filter(seller=sr_object)\\n    dueTransactions = dueTransaction.objects.filter(seller=sr_object)\\n\\n    return render(request, 'pages/SR/report_due.html', {'transcriber_name': transcriber_name,\\n                                                        'sell_transaction': sell_transaction,\\n                                                        'dueTransactions': dueTransactions,\\n                                                        'allBalance': allBalance})\", \"def sr_report_sales_analysis(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n\\n    post_data = request.POST\\n    print(post_data)\\n    shop_object = sr_object\\n    shop_name = shop_object.name\\n    shop_id = shop_object.id\\n    if 'month' in post_data and 'year' in post_data:\\n        month = post_data['month']\\n        year = post_data['year']\\n    else:\\n        month = datetime.date.today().month\\n        year = datetime.date.today().year\\n    return render(request, 'pages/SR/report_sales_analysis.html', {'shop_name': shop_name,\\n                                                                   # 'all_consumer_for_base' :all_consumer_for_base,\\n                                                                   'shop_id': shop_id,\\n                                                                   # 'bangla': bangla,\\n                                                                   'transcriber_name': transcriber_name,\\n                                                                   'month': month,\\n                                                                   'year': year})\", 'def sr_report_sales_analysis_json(request):\\n    get_data = request.GET\\n    shop_name = get_data[\\'shop\\']\\n    shop_object = Consumer.objects.get(id=shop_name)\\n\\n    shop_inventory = BuySellProfitInventoryIndividual.objects.filter(shop=shop_object)\\n    shop_consumer = ConsumerType.objects.get(type_name=\\'Seller\\')\\n    this_year = get_data[\\'year\\']\\n    print(this_year)\\n    this_month = get_data[\\'month\\']\\n    output = \\'{\"data\": [ \\'\\n\\n    if get_data[\\'t\\'] == \\'1\\':\\n        rank = 1\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_price = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__year=this_year,\\n                                                                   DateAdded__month=this_month):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                            if a_product.bulk_to_retail_unit == 0:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                        else:\\n                            count = count + product_in_this_transaction.quantity\\n                            product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name, str(count) + \\' \\' + a_product.retail_unit)\\n                rank += 1\\n    if get_data[\\'t\\'] == \\'2\\':\\n        rank = 1\\n        for a_product in Product.objects.all():\\n            count = 0\\n            # product_price = 0\\n            previous_product_price = 0\\n            change = 0\\n            product_name = a_product.name\\n            for this_day_transaction in Transaction.objects.filter(seller=shop_object):\\n                # start counting for this product\\n                for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n                    if product_in_this_transaction.product == a_product:\\n                        if count == 0:\\n                            previous_product_price = product_in_this_transaction.price_per_unit\\n                        product_price = product_in_this_transaction.price_per_unit\\n                        change += abs(previous_product_price - product_price)\\n                        count += 1\\n            if count > 0:\\n                output += \\'[\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name, count,\\n                                                       change/count)\\n                rank += 1\\n    if get_data[\\'t\\'] == \\'3\\':\\n\\n        print(this_month)\\n        day = 1\\n        #\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while day < 32:\\n            day_string = True\\n            rank = 1\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n\\n                for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__year=this_year,\\n                                                                       DateAdded__month=this_month, DateAdded__day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                                if a_product.bulk_to_retail_unit == 0:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                                else:\\n                                    count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n                if count > 0:\\n                    if day_string:\\n                        output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name,\\n                                                              str(count) + \\' \\' + a_product.retail_unit,\\n                                                              float(product_price / count))\\n                    rank += 1\\n            day += 1\\n            # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n    if get_data[\\'t\\'] == \\'4\\':\\n        day = 1\\n\\n        # output += \\'[\"%s/%s/%s\",\"\",\"\",\"\",\"\"] ,\\' % (day, this_month, this_year)\\n        while day < 8:\\n            day_string = True\\n            rank = 1\\n            for a_product in Product.objects.all():\\n                count = 0\\n                product_price = 0\\n                product_name = a_product.name\\n\\n                for this_day_transaction in Transaction.objects.filter(seller=shop_object, DateAdded__week_day=day):\\n                    # start counting for this product\\n\\n                    for product_in_this_transaction in ProductsInTransaction.objects.filter(TID=this_day_transaction):\\n\\n                        if product_in_this_transaction.product == a_product:\\n                            if product_in_this_transaction.unit == a_product.bulk_wholesale_unit:\\n                                if a_product.bulk_to_retail_unit == 0:\\n                                    count = count + product_in_this_transaction.quantity\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit\\n                                else:\\n                                    count = count + product_in_this_transaction.quantity * a_product.bulk_to_retail_unit\\n                                    product_price = product_price + product_in_this_transaction.price_per_unit / a_product.bulk_to_retail_unit\\n                            else:\\n                                count = count + product_in_this_transaction.quantity\\n                                product_price = product_price + product_in_this_transaction.price_per_unit\\n\\n                if count > 0:\\n                    if day_string:\\n                        if day == 1:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Sunday\\'\\n                        elif day == 2:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Monday\\'\\n                        elif day == 3:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Tuesday\\'\\n                        elif day == 4:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Wednesday\\'\\n                        elif day == 5:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Thursday\\'\\n                        elif day == 6:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Friday\\'\\n                        elif day == 7:\\n                            output += \\'[\"%s\",\"\",\"\",\"\",\"\"] ,\\' % \\'Saturday\\'\\n                        day_string = False\\n                    output += \\'[\"\",\"%s\",\"%s\",\"%s\",\"%s\"] ,\\' % (rank, product_name,\\n                                                              str(count) + \\' \\' + a_product.retail_unit,\\n                                                              float(product_price / count))\\n                    rank += 1\\n            day += 1\\n    if get_data[\\'t\\'] == \\'5\\':\\n        this_year = datetime.date.today().year\\n        day_string = True\\n        for a_product in Product.objects.all():\\n            count = 0\\n            product_profit = 0\\n            product_name = a_product.name\\n            for this_day_transaction in BuySellProfitInventoryIndividual.objects.filter(shop_id=shop_object):\\n                # start counting for this product\\n                if this_day_transaction.product == a_product:\\n                    product_profit += this_day_transaction.profit\\n                    count += 1\\n            output += \\'[\"%s\",\"%s\"] ,\\' % (product_name, product_profit)\\n    output = output[:-1]\\n    output += \\']}\\'\\n    return HttpResponse(output, content_type=\"text/plain\")', \"def add_sr_page(request):\\n    dr_name = request.session['user']\\n    dr_object = ACL.objects.get(loginID=dr_name).loginUser\\n    transcriber_name = dr_object.name\\n\\n    all_subscriber = ACL.objects.filter(distUser=dr_object)\\n    # type_of_subscriber = ConsumerType.objects.all()\\n    add_notification = False\\n    # shop_consumer = ConsumerType.objects.get(type_name='Seller')\\n    # all_shop_for_base = Consumer.objects.filter(type=shop_consumer)\\n    # all_user_for_base = Consumer.objects.all()\\n    # transcriber_name = request.session['user']\\n    # shop_consumer2 = ConsumerType.objects.get(type_name='Buyer')\\n    # all_consumer_for_base = Consumer.objects.filter(type=shop_consumer2)\\n    notification = ''\\n    if 'delete' in request.GET:\\n        get_data = request.GET\\n        add_notification = True\\n        delID = get_data['delete']\\n        if Consumer.objects.filter(id=delID).exists():\\n            item_for_delete = Consumer.objects.get(id=delID)\\n            notification = 'The Consumer : ' + item_for_delete.name + ' is deleted successfully.'\\n            item_for_delete.delete()\\n        else:\\n            notification = 'Item not found'\\n\\n    return render(request, 'pages/Distributor/add_SR.html',\\n                  {'subscribers': all_subscriber,'add_notification': add_notification,\\n                   # 'shop_list_base': all_shop_for_base,\\n                   # 'all_consumer_for_base' :all_consumer_for_base,\\n                   'transcriber_name': transcriber_name,\\n                   'notification': notification})\", \"def dr_monthly_report(request):\\n    dr_name = request.session['user']\\n    dr_object = ACL.objects.get(loginID=dr_name).loginUser\\n    transcriber_name = dr_object.name\\n    transcriber_id = dr_object.id\\n    all_subscriber = ACL.objects.filter(distUser=dr_object)\\n    post_data = request.POST\\n    print(post_data)\\n    if 'sr' in post_data:\\n        sr_object = Consumer.objects.get(id=post_data['sr'])\\n        allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n        return render(request, 'pages/Distributor/report_monthly.html', {'transcriber_name': transcriber_name,\\n                                                                         'hasReport': True,\\n                                                                         'subscribers': all_subscriber,\\n                                                                         'transcriber_id': transcriber_id,\\n                                                                         'allTransaction': allTransaction})\\n    else:\\n        # allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n        return render(request, 'pages/Distributor/report_monthly.html', {'transcriber_name': transcriber_name,\\n                                                                         'transcriber_id': transcriber_id,\\n                                                                         'subscribers': all_subscriber,\\n                                                                         'hasReport': False})\", \"def dr_due_report(request):\\n    sr_name = request.session['user']\\n    dr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = dr_object.name\\n    transcriber_id = dr_object.id\\n    all_subscriber = ACL.objects.filter(distUser=dr_object)\\n    post_data = request.POST\\n    if 'sr' in post_data:\\n        sr_object = Consumer.objects.get(id=post_data['sr'])\\n        allBalance = BuyerSellerAccount.objects.filter(seller=sr_object)\\n        sell_transaction = Transaction.objects.filter(seller=sr_object)\\n        dueTransactions = dueTransaction.objects.filter(seller=sr_object)\\n        # allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n        return render(request, 'pages/Distributor/report_due.html', {'transcriber_name': transcriber_name,\\n                                                                     'sell_transaction': sell_transaction,\\n                                                                     'dueTransactions': dueTransactions,\\n                                                                     'transcriber_id': transcriber_id,\\n                                                                     'hasReport': True,\\n                                                                     'subscribers': all_subscriber,\\n                                                                     'allBalance': allBalance})\\n\\n    else:\\n        # allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n        return render(request, 'pages/Distributor/report_due.html', {'transcriber_name': transcriber_name,\\n                                                                     'transcriber_id': transcriber_id,\\n                                                                     'subscribers': all_subscriber,\\n                                                                     'hasReport': False})\", \"def dr_report_sales_analysis(request):\\n    dr_name = request.session['user']\\n    dr_object = ACL.objects.get(loginID=dr_name).loginUser\\n    transcriber_name = dr_object.name\\n    transcriber_id = dr_object.id\\n    post_data = request.POST\\n    print(post_data)\\n    # shop_object = sr_object\\n    #\\n    all_subscriber = ACL.objects.filter(distUser=dr_object)\\n    hasReport = False\\n    if 'sr' in post_data:\\n        shop_id = post_data['sr']\\n        shop_name = Consumer.objects.get(id=shop_id).name\\n        hasReport = True\\n        if 'month' in post_data and 'year' in post_data:\\n            month = post_data['month']\\n            year = post_data['year']\\n        else:\\n            month = datetime.date.today().month\\n            year = datetime.date.today().year\\n        return render(request, 'pages/Distributor/report_sales_analysis.html', {'shop_name': shop_name,\\n                                                                                'transcriber_id': transcriber_id,\\n                                                                                'shop_id': shop_id,\\n                                                                                'subscribers': all_subscriber,\\n                                                                                'transcriber_name': transcriber_name,\\n                                                                                'month': month,\\n                                                                                'hasReport': hasReport,\\n                                                                                'year': year})\\n    else:\\n        return render(request, 'pages/Distributor/report_sales_analysis.html', {'shop_name': 'Not Selected',\\n                                                                                'transcriber_id': transcriber_id,\\n                                                                                'subscribers': all_subscriber,\\n                                                                                'transcriber_name': transcriber_name,\\n                                                                                'hasReport': hasReport})\", \"def shop_monthly_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n    allTransactionIn = BuyerSellerAccount.objects.filter(buyer=sr_object)\\n\\n    return render(request, 'pages/Shop/report_monthly.html', {'transcriber_name': transcriber_name,\\n                                                              'allTransactionIn': allTransactionIn,\\n                                                              'allTransaction': allTransaction})\", \"def shop_due_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    allBalance = BuyerSellerAccount.objects.filter(seller=sr_object)\\n    sell_transaction = Transaction.objects.filter(seller=sr_object)\\n    dueTransactions = dueTransaction.objects.filter(seller=sr_object)\\n    allBalanceIn = BuyerSellerAccount.objects.filter(buyer=sr_object)\\n    sell_transactionIn = Transaction.objects.filter(buyer=sr_object)\\n    dueTransactionsIn = dueTransaction.objects.filter(buyer=sr_object)\\n\\n    return render(request, 'pages/Shop/report_due.html', {'transcriber_name': transcriber_name,\\n                                                          'sell_transaction': sell_transaction,\\n                                                          'dueTransactions': dueTransactions,\\n                                                          'allBalance': allBalance,\\n                                                          'sell_transactionIn': sell_transactionIn,\\n                                                          'dueTransactionsIn': dueTransactionsIn,\\n                                                          'allBalanceIn': allBalanceIn})\", \"def shop_report_sales_analysis(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n\\n    post_data = request.POST\\n    print(post_data)\\n    shop_object = sr_object\\n    shop_name = shop_object.name\\n    shop_id = shop_object.id\\n    if 'month' in post_data and 'year' in post_data:\\n        month = post_data['month']\\n        year = post_data['year']\\n    else:\\n        month = datetime.date.today().month\\n        year = datetime.date.today().year\\n    return render(request, 'pages/Shop/report_sales_analysis.html', {'shop_name': shop_name,\\n                                                                     # 'all_consumer_for_base' :all_consumer_for_base,\\n                                                                     'shop_id': shop_id,\\n                                                                     # 'bangla': bangla,\\n                                                                     'transcriber_name': transcriber_name,\\n                                                                     'month': month,\\n                                                                     'year': year})\", \"def user_monthly_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    # allTransaction = BuyerSellerAccount.objects.filter(seller=sr_object)\\n    allTransactionIn = BuyerSellerAccount.objects.filter(buyer=sr_object)\\n\\n    return render(request, 'pages/Consumer/report_monthly.html', {'transcriber_name': transcriber_name,\\n                                                                  'allTransactionIn': allTransactionIn})\", \"def user_due_report(request):\\n    sr_name = request.session['user']\\n    sr_object = ACL.objects.get(loginID=sr_name).loginUser\\n    transcriber_name = sr_object.name\\n    # allBalance = BuyerSellerAccount.objects.filter(seller=sr_object)\\n    # sell_transaction = Transaction.objects.filter(seller=sr_object)\\n    # dueTransactions = dueTransaction.objects.filter(seller=sr_object)\\n    allBalanceIn = BuyerSellerAccount.objects.filter(buyer=sr_object)\\n    sell_transactionIn = Transaction.objects.filter(buyer=sr_object)\\n    dueTransactionsIn = dueTransaction.objects.filter(buyer=sr_object)\\n\\n    return render(request, 'pages/Consumer/report_due.html', {'transcriber_name': transcriber_name,\\n                                                              # 'sell_transaction': sell_transaction,\\n                                                              # 'dueTransactions': dueTransactions,\\n                                                              # 'allBalance': allBalance,\\n                                                              'sell_transactionIn': sell_transactionIn,\\n                                                              'dueTransactionsIn': dueTransactionsIn,\\n                                                              'allBalanceIn': allBalanceIn})\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def test_read_string_table(datadir):\\n    datadir.join('reader').chdir()\\n    src = 'sharedStrings.xml'\\n    with open(src) as content:\\n        assert read_string_table(content.read()) == [\\n            'This is cell A1 in Sheet 1', 'This is cell G5']\", 'def test_formatted_string_table(datadir):\\n    datadir.join(\\'reader\\').chdir()\\n    src = \\'shared-strings-rich.xml\\'\\n    with open(src) as content:\\n        assert read_string_table(content.read()) == [\\n            \\'Welcome\\', \\'to the best shop in town\\', \"     let\\'s play \"]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_toplevel_xid():\\n    if app.window.get_window():\\n        try:\\n            return app.window.get_window().get_xid()\\n        except AttributeError:  # non x11\\n            pass\\n    return 0', 'def enabled(self):\\n        if not app.player.paused:\\n            self.plugin_on_unpaused()', 'def plugin_on_unpaused(self):\\n        xid = dbus.UInt32(get_toplevel_xid())\\n        flags = dbus.UInt32(InhibitFlags.IDLE)\\n\\n        try:\\n            bus = dbus.SessionBus()\\n            obj = bus.get_object(self.DBUS_NAME, self.DBUS_PATH)\\n            iface = dbus.Interface(obj, self.DBUS_INTERFACE)\\n            self.__cookie = iface.Inhibit(\\n                self.APPLICATION_ID, xid, self.INHIBIT_REASON, flags)\\n        except dbus.DBusException:\\n            pass']}, {'features': [], 'snippets': ['def test000Exists(self):', 'def testInstantiate(self):', 'def testNoticeInvalid(self):', 'def testParseValidQuery(self):', 'def testParseValidResponse(self):', \"def suite():\\n    s = unittest.TestSuite()\\n    s.addTest( unittest.makeSuite(ns_msgTestCase, 'test') )\\n    return s\"]}, {'features': [], 'snippets': [\"def parse_args():\\n    parser = argparse.ArgumentParser(description='easy 249')\\n    parser.add_argument('stock_prices', action='store', nargs='+',\\n                        help='prices of a given stock')\\n    return parser.parse_args()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, default_factory=lambda: None):\\n            self.__factory = default_factory', 'def __init__(self, main_controller, word_list=[], comp_len=DEFAULT_COMPLETION_LENGTH):\\n        \"\"\"Constructor.\\n\\n            @type  word_list: iterable\\n            @param word_list: A list of words that should be auto-completed.\"\"\"\\n        self.main_controller = main_controller\\n        assert isinstance(word_list, list)\\n        self.comp_len = comp_len\\n        self._word_list = []\\n        self._word_freq = defaultdict(lambda: 0)\\n        self.add_words(word_list)\\n        self.widgets = set()', 'def add_words(self, words, update=True):\\n        \"\"\"Add a word or words to the list of words to auto-complete.\"\"\"\\n        for word in words:\\n            if self.isusable(word):\\n                self._word_freq[word] += 1\\n        if update:\\n            self._update_word_list()', \"def autocomplete(self, word):\\n        for w in self._word_list:\\n            if w.startswith(word):\\n                return w, w[len(word):]\\n        return None, u''\", 'def clear_words(self):\\n        \"\"\"Remove all registered words; effectively turns off auto-completion.\"\"\"\\n        self._word_freq = []\\n        self._word_list = defaultdict(lambda: 0)', 'def remove_widget(self, widget):\\n        \"\"\"Remove a widget (currently only L{TextBox}s are accepted) from\\n            the list of widgets to do auto-correction for.\\n            \"\"\"\\n        if isinstance(widget, TextBox) and widget in self.widgets:\\n            self._remove_textbox(widget)', 'def _add_text_box(self, textbox):\\n        \"\"\"Add the given L{TextBox} to the list of widgets to do auto-\\n            correction on.\"\"\"\\n        if not hasattr(self, \\'_textbox_insert_ids\\'):\\n            self._textbox_insert_ids = {}\\n        handler_id = textbox.connect(\\'text-inserted\\', self._on_insert_text)\\n        self._textbox_insert_ids[textbox] = handler_id\\n        self.widgets.add(textbox)', 'def suggest_completion():\\n                    textbox.handler_block(self._textbox_insert_ids[textbox])\\n                    #logging.debug(\"textbox.suggestion = {\\'text\\': u\\'%s\\', \\'offset\\': %d}\" % (word_postfix, insert_offset))\\n                    textbox.suggestion = {\\'text\\': word_postfix, \\'offset\\': insert_offset}\\n                    textbox.handler_unblock(self._textbox_insert_ids[textbox])\\n\\n                    sel_iter_start = buffer.get_iter_at_offset(insert_offset)\\n                    sel_iter_end   = buffer.get_iter_at_offset(insert_offset + len(word_postfix))\\n                    buffer.select_range(sel_iter_start, sel_iter_end)\\n\\n                    return False', 'def _remove_textbox(self, textbox):\\n        \"\"\"Remove the given L{TextBox} from the list of widgets to do\\n            auto-correction on.\\n            \"\"\"\\n        if not hasattr(self, \\'_textbox_insert_ids\\'):\\n            return\\n        # Disconnect the \"insert-text\" event handler\\n        textbox.disconnect(self._textbox_insert_ids[textbox])\\n\\n        self.widgets.remove(textbox)', 'def __init__(self, internal_name, main_controller):\\n        self.internal_name = internal_name\\n        self.main_controller = main_controller\\n\\n        self._init_plugin()', 'def _connect_to_textboxes(self, unitview, textboxes):\\n        for target in textboxes:\\n                self.autocomp.add_widget(target)', 'def destroy(self):\\n        \"\"\"Remove all signal-connections.\"\"\"\\n        self.autocomp.clear_words()\\n        self.autocomp.clear_widgets()\\n        self.main_controller.store_controller.disconnect(self._store_loaded_id)\\n        if getattr(self, \\'_cursor_changed_id\\', None):\\n            self.store_cursor.disconnect(self._cursor_changed_id)\\n        if self._unitview_id:\\n            self.main_controller.unit_controller.view.disconnect(self._unitview_id)', \"def _on_cursor_change(self, cursor):\\n        def add_widgets():\\n            if hasattr(self, 'lastunit'):\\n                if self.lastunit.hasplural():\\n                    for target in self.lastunit.target:\\n                        if target:\\n                            #logging.debug('Adding words: %s' % (self.autocomp.wordsep_re.split(unicode(target))))\\n                            self.autocomp.add_words(self.autocomp.wordsep_re.split(unicode(target)))\\n                else:\\n                    if self.lastunit.target:\\n                        #logging.debug('Adding words: %s' % (self.autocomp.wordsep_re.split(unicode(self.lastunit.target))))\\n                        self.autocomp.add_words(self.autocomp.wordsep_re.split(unicode(self.lastunit.target)))\\n            self.lastunit = cursor.deref()\\n        gobject.idle_add(add_widgets)\"]}, {'features': [], 'snippets': [\"def appvms(self):\\n        ''' Returns a generator containing all domains based on the current\\n            TemplateVM.\\n        '''\\n        for vm in self.app.domains:\\n            if hasattr(vm, 'template') and vm.template is self:\\n                yield vm\", 'def __init__(self, *args, **kwargs):\\n        assert \\'template\\' not in kwargs, \"A TemplateVM can not have a template\"\\n        self.volume_config = {\\n            \\'root\\': {\\n                \\'name\\': \\'root\\',\\n                \\'snap_on_start\\': False,\\n                \\'save_on_stop\\': True,\\n                \\'rw\\': True,\\n                \\'source\\': None,\\n                \\'size\\': defaults[\\'root_img_size\\'],\\n            },\\n            \\'private\\': {\\n                \\'name\\': \\'private\\',\\n                \\'snap_on_start\\': False,\\n                \\'save_on_stop\\': True,\\n                \\'rw\\': True,\\n                \\'source\\': None,\\n                \\'size\\': defaults[\\'private_img_size\\'],\\n                \\'revisions_to_keep\\': 0,\\n            },\\n            \\'volatile\\': {\\n                \\'name\\': \\'volatile\\',\\n                \\'size\\': defaults[\\'root_img_size\\'],\\n                \\'snap_on_start\\': False,\\n                \\'save_on_stop\\': False,\\n                \\'rw\\': True,\\n            },\\n            \\'kernel\\': {\\n                \\'name\\': \\'kernel\\',\\n                \\'snap_on_start\\': False,\\n                \\'save_on_stop\\': False,\\n                \\'rw\\': False\\n            }\\n        }\\n        super(TemplateVM, self).__init__(*args, **kwargs)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, config=None, **kwds) :\\n        self.logger = logging.getLogger(type(self).__name__)\\n        self.kwds = kwds\\n        self.images = {}\\n        if config is None:\\n            config = Config()\\n            config.update(dict(\\n                images= [\\n                    {\\n                        \\'path\\': \\'docker/*\\',\\n                    }\\n                ],\\n            ))\\n        self.patterns = []\\n        for image in config[\\'images\\']:\\n            # When path is provided and globbed, Dockerfile refers to its location\\n            # When path is provided but not globbed, Dockerfile refers to the current path\\n            # When Dockerfile is provided and globbed, path must not be globbed, both\\n            # refers to the current directory\\n            path = image.get(\\'path\\', None)\\n            dockerfile = image.get(\\'Dockerfile\\', \\'Dockerfile\\')\\n            name = image.get(\\'name\\', None)\\n\\n            if path is None:\\n                path = \\'.\\'\\n\\n            if \\'*\\' in path:\\n                if \\'*\\' in dockerfile:\\n                    raise ValueError(\\'Ambiguity in your configuration for %r, globbing can\\'\\n                        \\'be done either in \"Dockerfile\" or \"path\" key but not both at the\\'\\n                        \\'same time\\' % image)\\n\\n                dockerfile = os.path.join(path, dockerfile)\\n                path = re.compile(re.sub(\\'^.*/([^*]*)$\\', r\\'(?P<path>.*)/\\\\1\\', dockerfile))\\n\\n            if name is None:\\n                name = dockerfile\\n            if \\'*\\' in name:\\n                start = re.sub(\\'^([^*]*/|).*\\', r\\'^\\\\1(?P<name>.*)\\', dockerfile)\\n                end = re.sub(\\'^.*\\\\*(?:|[^/]*)(/.*)$\\', r\\'\\\\1$\\', dockerfile)\\n                name = re.compile(start + end)\\n\\n            pattern = {\\n                \\'name\\': name,\\n                \\'path\\': path,\\n                \\'Dockerfile\\': dockerfile,\\n            }\\n            self.patterns.append(pattern)\\n        self.config = config', 'def getImage(self, image_name):\\n        try:\\n            return self.images[image_name]\\n        except KeyError:\\n            self.logger.debug(\\'image builder cache miss, try to find it\\')\\n            for img_cfg in self.patterns:\\n                for path in glob.glob(img_cfg[\\'Dockerfile\\']):\\n                    found_image_name = self.get_matching_pattern(img_cfg, \\'name\\', path)\\n                    context_path = self.get_matching_pattern(img_cfg, \\'path\\', path)\\n                    if found_image_name == image_name:\\n                        image = ImageBuilder(image_name,\\n                            contextPath=context_path,\\n                            dockerfile=path,\\n                            tagResolver=self,\\n                            **self.kwds\\n                        )\\n                        self.images[image_name] = image\\n                        return image\\n        raise KeyError(\"Cannot find image %s\" % image_name)', 'def build(self, client, names=None, child_images=[]) :\\n        if isinstance(names, six.string_types):\\n            names = [names]\\n        def iter_buildable_deps(name):\\n            \"\"\"\\n            instanciates a builder for each image dependency\\n            does nothing when the image cannot be build\\n            \"\"\"\\n            for dep_name, _ in self.getImage(name).imageDeps():\\n                try:\\n                    self.getImage(dep_name)\\n                    yield dep_name\\n                except KeyError:\\n                    continue\\n        for name in names:\\n            if name in child_images:\\n                raise RuntimeError(\"dependency loop detected, %s some how depends on itself %s\" %\\n                    (name, \\' -> \\'.join(child_images + [name]))\\n                )\\n            for dep_name in iter_buildable_deps(name):\\n                self.build(client, dep_name, child_images=child_images+[name])\\n\\n        for name in names:\\n            self.getImage(name).build(client)', 'def add_options(parser):\\n    from . import addCommonOptions, commonSetUp\\n    from .dockerfile import addDockerfileOptions\\n    from .image import addImageOptions\\n    try:\\n        add = parser.add_argument\\n    except AttributeError:\\n        add = parser.add_option\\n    add(\"image\", nargs=\"*\",\\n                      help=\"images to build\")\\n    add(\"-t\", \"--tag\", dest=\"tag\", default=None, action=\\'append\\',\\n                      help=\"tag(s) to be applied to the resulting image in case of success\")\\n    add(\"--registry\", dest=\"registry\", default=[], action=\\'append\\',\\n                      help=\"Registry on which the image should tagged (<registry>/<name>:<tag>)\")\\n    addCommonOptions(parser)\\n    addDockerfileOptions(parser)\\n    addImageOptions(parser)']}, {'features': [], 'snippets': ['def __init__(self):\\n\\n        config_file = \\'/etc/func/minion.conf\\'\\n        self.config = read_config(config_file, FuncdConfig)\\n        self.__init_log()\\n        self.__base_methods = {\\n            # __\\'s so we don\\'t clobber useful names\\n            \"module_version\" : self.__module_version,\\n            \"module_api_version\" : self.__module_api_version,\\n            \"module_description\" : self.__module_description,\\n            \"list_methods\"       : self.__list_methods,\\n            \"get_method_args\"    : self.__get_method_args,\\n        }\\n        self.__init_options()', \"def __init_options(self):\\n        options_file = '/etc/func/modules/'+self.__class__.__name__+'.conf'\\n        self.options = read_config(options_file, self.Config)\\n        return\", 'def __list_handlers(self):\\n        \"\"\" Return a dict of { handler_name, method, ... }.\\n        All methods that do not being with an underscore will be exposed.\\n        We also make sure to not expose our register_rpc method.\\n        \"\"\"\\n        handlers = {}\\n        for attr in dir(self):\\n            if self.__is_public_valid_method(attr):\\n                handlers[attr] = getattr(self, attr)\\n        return handlers', 'def __module_version(self):\\n        return self.version', 'def __module_description(self):\\n        return self.description', 'def __get_method_args(self):\\n        \"\"\"\\n        Gets arguments with their formats according to ArgCompatibility\\n        class\\' rules.\\n\\n        @return : dict with args or Raise Exception if something wrong\\n        happens\\n        \"\"\"\\n        tmp_arg_dict = self.register_method_args()\\n\\n        #if it is not implemeted then return empty stuff \\n        if not tmp_arg_dict:\\n            return {}\\n\\n        #see if user tried to register an not implemented method :)\\n        for method in tmp_arg_dict.iterkeys():\\n            if not hasattr(self,method):\\n                raise NonExistingMethodRegistered(\"%s is not in %s \"%(method,self.__class__.__name__))', 'def register_method_args(self):\\n        \"\"\"\\n        That is the method where users should override in their\\n        modules according to be able to send their method arguments\\n        to the Overlord. If they dont have it nothing breaks\\n        just that one in the base class is called\\n\\n        @return : empty {}\\n        \"\"\"\\n\\n        # to know they didnt implement it\\n        return {}']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, artist, title, year=None):\\n        self.__artist = artist\\n        self.__title = title\\n        self.__year = year', 'def setArtist(self, artist):\\n        self.__artist = artist', 'def setTitle(self, title):\\n        self.__title = title', 'def setYear(self, year):\\n        self.__year = year', 'def __init__(self, artist, title, year=None):\\n        super(Painting, self).__init__(artist, title, year)', 'def __init__(self, artist, title, year=None, material=None):\\n        super(Sculpture, self).__init__(artist, title, year)\\n        self.__material = material', 'def setMaterial(self, material):\\n        self.__material = material', 'def __init__(self, width, height, depth=None):\\n        self.__width = width\\n        self.__height = height\\n        self.__depth = depth', 'def setWidth(self, width):\\n        self.__width = width', 'def setHeight(self, height):\\n        self.__height = height', 'def setDepth(self, depth):\\n        self.__depth = depth', 'def volume(self):\\n        raise NotImplemented']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _rematcher(regex):\\n    \"\"\"compile the regexp with the best available regexp engine and return a\\n    matcher function\"\"\"\\n    m = util.re.compile(regex)\\n    try:\\n        # slightly faster, provided by facebook\\'s re2 bindings\\n        return m.test_match\\n    except AttributeError:\\n        return m.match', 'def _expandsubinclude(kindpats, root):\\n    \"\"\"Returns the list of subinclude matcher args and the kindpats without the\\n    subincludes in it.\"\"\"\\n    relmatchers = []\\n    other = []\\n\\n    for kind, pat, source in kindpats:\\n        if kind == \"subinclude\":\\n            sourceroot = pathutil.dirname(util.normpath(source))\\n            pat = util.pconvert(pat)\\n            path = pathutil.join(sourceroot, pat)\\n\\n            newroot = pathutil.dirname(path)\\n            matcherargs = (newroot, \"\", [], [\"include:%s\" % path])\\n\\n            prefix = pathutil.canonpath(root, root, newroot)\\n            if prefix:\\n                prefix += \"/\"\\n            relmatchers.append((prefix, matcherargs))\\n        else:\\n            other.append((kind, pat, source))\\n\\n    return relmatchers, other', 'def match(\\n    root,\\n    cwd,\\n    patterns=None,\\n    include=None,\\n    exclude=None,\\n    default=\"glob\",\\n    exact=False,\\n    auditor=None,\\n    ctx=None,\\n    warn=None,\\n    badfn=None,\\n    icasefs=False,', 'def normalize(patterns, default, root, cwd, auditor, warn):\\n            kp = _donormalize(patterns, default, root, cwd, auditor, warn)\\n            kindpats = []\\n            for kind, pats, source in kp:\\n                if kind not in (\"re\", \"relre\"):  # regex can\\'t be normalized\\n                    p = pats\\n                    pats = dsnormalize(pats)\\n\\n                    # Preserve the original to handle a case only rename.\\n                    if p != pats and p in dirstate:\\n                        kindpats.append((kind, p, source))\\n\\n                kindpats.append((kind, pats, source))\\n            return kindpats', 'def exact(root, cwd, files, badfn=None):\\n    return exactmatcher(root, cwd, files, badfn=badfn)', 'def never(root, cwd):\\n    return nevermatcher(root, cwd)', 'def badmatch(match, badfn):\\n    \"\"\"Make a copy of the given matcher, replacing its bad method with the given\\n    one.\\n    \"\"\"\\n    m = copy.copy(match)\\n    m.bad = badfn\\n    return m', 'def _testrefastpath(repat):\\n    \"\"\"Test if a re pattern can use fast path.\\n\\n    That is, for every \"$A/$B\" path the pattern matches, \"$A\" must also be\\n    matched,\\n\\n    Return True if we\\'re sure it is. Return False otherwise.\\n    \"\"\"\\n    # XXX: It\\'s very hard to implement this. These are what need to be\\n    # supported in production and tests. Very hacky. But we plan to get rid\\n    # of re matchers eventually.\\n\\n    # Rules like \"(?!experimental/)\"\\n    if repat.startswith(\"(?!\") and repat.endswith(\")\") and repat.count(\")\") == 1:\\n        return True\\n\\n    # Rules used in doctest\\n    if repat == \"(i|j)$\":\\n        return True\\n\\n    return False', 'def __init__(self, *args, **kwargs):\\n        # If True, avoid entering subdirectories, and match everything recursively,\\n        # unconditionally.\\n        self.matchrecursive = False\\n        # If True, avoid entering subdirectories, and return \"unsure\" for\\n        # everything. This is set to True when complex re patterns (potentially\\n        # including \"/\") are used.\\n        self.unsurerecursive = False\\n        # Patterns for matching paths in this directory.\\n        self._kindpats = []\\n        # Glob patterns used to match parent directories of another glob\\n        # pattern.\\n        self._globdirpats = []\\n        super(_tree, self).__init__(*args, **kwargs)', 'def visitdir(self, path):\\n        \"\"\"Similar to matcher.visitdir\"\"\"\\n        path = normalizerootdir(path, \"visitdir\")\\n        if self.matchrecursive:\\n            return \"all\"\\n        elif self.unsurerecursive:\\n            return True\\n        elif path == \"\":\\n            return True\\n\\n        if self._kindpats and self._compiledpats(path):\\n            # XXX: This is incorrect. But re patterns are already used in\\n            # production. We should kill them!\\n            # Need to test \"if every string starting with \\'path\\' matches\".\\n            # Obviously it\\'s impossible to test *every* string with the\\n            # standard regex API, therefore pick a random strange path to test\\n            # it approximately.\\n            if self._compiledpats(\"%s/*/_/-/0/*\" % path):\\n                return \"all\"\\n            else:\\n                return True\\n\\n        if self._globdirpats and self._compileddirpats(path):\\n            return True\\n\\n        subdir, rest = self._split(path)\\n        subtree = self.get(subdir)\\n        if subtree is None:\\n            return False\\n        else:\\n            return subtree.visitdir(rest)', 'def _compiledpats(self):\\n        pat, matchfunc = _buildregexmatch(self._kindpats, \"\")\\n        return matchfunc', 'def _compileddirpats(self):\\n        pat, matchfunc = _buildregexmatch(\\n            [(\"glob\", p, \"\") for p in self._globdirpats], \"$\"\\n        )\\n        return matchfunc', 'def _remainingpats(pat, prefix):\\n    \"\"\"list of patterns with prefix stripped\\n\\n    >>> _remainingpats(\"a/b/c\", \"\")\\n    [\\'a/b/c\\']\\n    >>> _remainingpats(\"a/b/c\", \"a\")\\n    [\\'b/c\\']\\n    >>> _remainingpats(\"a/b/c\", \"a/b\")\\n    [\\'c\\']\\n    >>> _remainingpats(\"a/b/c\", \"a/b/c\")\\n    []\\n    >>> _remainingpats(\"\", \"\")\\n    []\\n    \"\"\"\\n    if prefix:\\n        if prefix == pat:\\n            return []\\n        else:\\n            assert pat[len(prefix)] == \"/\"\\n            return [pat[len(prefix) + 1 :]]\\n    else:\\n        if pat:\\n            return [pat]\\n        else:\\n            return []', 'def __init__(self, root, cwd, badfn=None, relativeuipath=True):\\n        self._root = root\\n        self._cwd = cwd\\n        if badfn is not None:\\n            self.bad = badfn\\n        self._relativeuipath = relativeuipath', 'def __call__(self, fn):\\n        return self.matchfn(fn)', 'def bad(self, f, msg):\\n        \"\"\"Callback from dirstate.walk for each explicit file that can\\'t be\\n        found/accessed, with an error message.\"\"\"', 'def abs(self, f):\\n        \"\"\"Convert a repo path back to path that is relative to the root of the\\n        matcher.\"\"\"\\n        return f', 'def uipath(self, f):\\n        \"\"\"Convert repo path to a display path.  If patterns or -I/-X were used\\n        to create this matcher, the display path will be relative to cwd.\\n        Otherwise it is relative to the root of the repo.\"\"\"\\n        return (self._relativeuipath and self.rel(f)) or self.abs(f)', 'def _files(self):\\n        return []', 'def _fileset(self):\\n        return set(self._files)', 'def matchfn(self, f):\\n        return False', 'def always(self):\\n        \"\"\"Matcher will match everything and .files() will be empty --\\n        optimization might be possible.\"\"\"\\n        return False', 'def prefix(self):\\n        \"\"\"Matcher will match the paths in .files() recursively --\\n        optimization might be possible.\"\"\"\\n        return False', 'def __init__(self, root, cwd, badfn=None, relativeuipath=False):\\n        super(alwaysmatcher, self).__init__(\\n            root, cwd, badfn, relativeuipath=relativeuipath\\n        )', 'def matchfn(self, f):\\n        return True', 'def __repr__(self):\\n        return \"<alwaysmatcher>\"', 'def __init__(self, root, cwd, badfn=None):\\n        super(nevermatcher, self).__init__(root, cwd, badfn)', 'def isexact(self):\\n        return True', 'def visitdir(self, dir):\\n        return False', 'def __init__(self, root, cwd, badfn=None, gitignorepaths=None):\\n        super(gitignorematcher, self).__init__(root, cwd, badfn)\\n        gitignorepaths = gitignorepaths or []\\n        self._matcher = pathmatcher.gitignorematcher(root, gitignorepaths)', 'def explain(self, f):\\n        return self._matcher.explain(f, True)', 'def __repr__(self):\\n        return \"<gitignorematcher>\"', 'def __init__(self, root, cwd, badfn=None, rules=[]):\\n        super(treematcher, self).__init__(root, cwd, badfn)\\n        rules = list(rules)\\n        self._matcher = pathmatcher.treematcher(rules)\\n        self._rules = rules', 'def visitdir(self, dir):\\n        matched = self._matcher.match_recursive(dir)\\n        if matched is None:\\n            return True\\n        elif matched is True:\\n            return \"all\"\\n        else:\\n            assert matched is False\\n            return False', 'def normalizerootdir(dir, funcname):\\n    if dir == \".\":\\n        util.nouideprecwarn(\\n            \"match.%s() no longer accepts \\'.\\', use \\'\\' instead.\" % funcname, \"20190805\"\\n        )\\n        return \"\"\\n    return dir', 'def _makeglobrecursive(pat):\\n    \"\"\"Make a glob pattern recursive by appending \"/**\" to it\"\"\"\\n    if pat.endswith(\"/\") or not pat:\\n        return pat + \"**\"\\n    else:\\n        return pat + \"/**\"', 'def _convertretoglobs(repat):\\n    \"\"\"Attempt to convert a regular expression pattern to glob patterns.\\n\\n    A single regular expression pattern might be converted into multiple\\n    glob patterns.\\n\\n    Return None if conversion is unsupported.\\n\\n    >>> _convertretoglobs(\"abc*\") is None\\n    True\\n    >>> _convertretoglobs(\"xx/yy/(?!zz/kk)\")\\n    [\\'xx/yy/**\\', \\'!xx/yy/zz/kk/**\\']\\n    >>> _convertretoglobs(\"x/y/(?:.*/)?BUCK\")\\n    [\\'x/y/**/BUCK\\']\\n    \"\"\"\\n    m = _repat1.match(repat)\\n    if m:\\n        prefix, excluded = m.groups()\\n        return [\"%s/**\" % prefix, \"!%s/%s/**\" % (prefix, excluded)]\\n    m = _repat2.match(repat)\\n    if m:\\n        prefix, name = m.groups()\\n        return [\"%s/**/%s\" % (prefix, name)]\\n    return None', 'def __init__(self, root, cwd, kindpats, ctx=None, badfn=None):\\n        super(patternmatcher, self).__init__(root, cwd, badfn)\\n        # kindpats are already normalized to be relative to repo-root.\\n        # Can we use tree matcher?\\n        rules = _kindpatstoglobs(kindpats, recursive=False)\\n        fallback = True\\n        if rules is not None:\\n            try:\\n                matcher = treematcher(root, cwd, badfn=badfn, rules=rules)\\n                # Replace self to \\'matcher\\'.\\n                self.__dict__ = matcher.__dict__\\n                self.__class__ = matcher.__class__\\n                fallback = False\\n            except ValueError:\\n                # for example, Regex(\"Compiled regex exceeds size limit of 10485760 bytes.\")\\n                pass\\n        if fallback:\\n            self._prefix = _prefix(kindpats)\\n            self._pats, self.matchfn = _buildmatch(ctx, kindpats, \"$\", root)\\n\\n        self._files = _explicitfiles(kindpats)', 'def _dirs(self):\\n        return set(util.dirs(self._fileset))', 'def prefix(self):\\n        return self._prefix', 'def __init__(self, root, cwd, kindpats, ctx=None, badfn=None):\\n        super(includematcher, self).__init__(root, cwd, badfn)\\n\\n        # Can we use tree matcher?\\n        rules = _kindpatstoglobs(kindpats, recursive=True)\\n        fallback = True\\n        if rules is not None:\\n            try:\\n                matcher = treematcher(root, cwd, badfn=badfn, rules=rules)\\n                # Replace self to \\'matcher\\'.\\n                self.__dict__ = matcher.__dict__\\n                self.__class__ = matcher.__class__\\n                fallback = False\\n            except ValueError:\\n                # for example, Regex(\"Compiled regex exceeds size limit of 10485760 bytes.\")\\n                pass\\n        if fallback:\\n            self._pats, self.matchfn = _buildmatch(ctx, kindpats, \"(?:/|$)\", root)\\n            # prefix is True if all patterns are recursive, so certain fast paths\\n            # can be enabled. Unfortunately, it\\'s too easy to break it (ex. by\\n            # using \"glob:*.c\", \"re:...\", etc).\\n            self._prefix = _prefix(kindpats)\\n            roots, dirs = _rootsanddirs(kindpats)\\n            # roots are directories which are recursively included.\\n            # If self._prefix is True, then _roots can have a fast path for\\n            # visitdir to return \"all\", marking things included unconditionally.\\n            # If self._prefix is False, then that optimization is unsound because\\n            # \"roots\" might contain entries that is not recursive (ex. roots will\\n            # include \"foo/bar\" for pattern \"glob:foo/bar/*.c\").\\n            self._roots = set(roots)\\n            # dirs are directories which are non-recursively included.\\n            # That is, files under that directory are included. But not\\n            # subdirectories.\\n            self._dirs = set(dirs)\\n            # Try to use a more efficient visitdir implementation\\n            visitdir = _buildvisitdir(kindpats)\\n            if visitdir:\\n                self.visitdir = visitdir', 'def __repr__(self):\\n        return \"<includematcher includes=%r>\" % self._pats', 'def __init__(self, root, cwd, files, badfn=None):\\n        super(exactmatcher, self).__init__(root, cwd, badfn)\\n\\n        if isinstance(files, list):\\n            self._files = files\\n        else:\\n            self._files = list(files)', 'def _dirs(self):\\n        return set(util.dirs(self._fileset))', 'def isexact(self):\\n        return True', 'def __init__(self, m1, m2):\\n        super(differencematcher, self).__init__(m1._root, m1._cwd)\\n        self._m1 = m1\\n        self._m2 = m2\\n        self.bad = m1.bad\\n        self.traversedir = m1.traversedir', 'def _files(self):\\n        if self.isexact():\\n            return [f for f in self._m1.files() if self(f)]\\n        # If m1 is not an exact matcher, we can\\'t easily figure out the set of\\n        # files, because its files() are not always files. For example, if\\n        # m1 is \"path:dir\" and m2 is \"rootfileins:.\", we don\\'t\\n        # want to remove \"dir\" from the set even though it would match m2,\\n        # because the \"dir\" in m1 may not be a file.\\n        return self._m1.files()', 'def isexact(self):\\n        return self._m1.isexact()', 'def intersectmatchers(m1, m2):\\n    \"\"\"Composes two matchers by matching if both of them match.\\n\\n    The second matcher\\'s non-matching-attributes (root, cwd, bad, traversedir)\\n    are ignored.\\n    \"\"\"\\n    if m1 is None or m2 is None:\\n        return m1 or m2\\n    if m1.always():\\n        m = copy.copy(m2)\\n        # TODO: Consider encapsulating these things in a class so there\\'s only\\n        # one thing to copy from m1.\\n        m.bad = m1.bad\\n        m.traversedir = m1.traversedir\\n        m.abs = m1.abs\\n        m.rel = m1.rel\\n        m._relativeuipath |= m1._relativeuipath\\n        return m\\n    if m2.always():\\n        m = copy.copy(m1)\\n        m._relativeuipath |= m2._relativeuipath\\n        return m\\n    return intersectionmatcher(m1, m2)', 'def __init__(self, m1, m2):\\n        super(intersectionmatcher, self).__init__(m1._root, m1._cwd)\\n        self._m1 = m1\\n        self._m2 = m2\\n        self.bad = m1.bad\\n        self.traversedir = m1.traversedir', 'def _files(self):\\n        if self.isexact():\\n            m1, m2 = self._m1, self._m2\\n            if not m1.isexact():\\n                m1, m2 = m2, m1\\n            return [f for f in m1.files() if m2(f)]\\n        # It neither m1 nor m2 is an exact matcher, we can\\'t easily intersect\\n        # the set of files, because their files() are not always files. For\\n        # example, if intersecting a matcher \"-I glob:foo.txt\" with matcher of\\n        # \"path:dir2\", we don\\'t want to remove \"dir2\" from the set.\\n        return self._m1.files() + self._m2.files()', 'def visitdir(self, dir):\\n        dir = normalizerootdir(dir, \"visitdir\")\\n        visit1 = self._m1.visitdir(dir)\\n        if visit1 == \"all\":\\n            return self._m2.visitdir(dir)\\n        # bool() because visit1=True + visit2=\\'all\\' should not be \\'all\\'\\n        return bool(visit1 and self._m2.visitdir(dir))', 'def isexact(self):\\n        return self._m1.isexact() or self._m2.isexact()', 'def __init__(self, path, matcher):\\n        super(subdirmatcher, self).__init__(matcher._root, matcher._cwd)\\n        self._path = path\\n        self._matcher = matcher\\n        self._always = matcher.always()\\n\\n        self._files = [\\n            f[len(path) + 1 :] for f in matcher._files if f.startswith(path + \"/\")\\n        ]\\n\\n        # If the parent repo had a path to this subrepo and the matcher is\\n        # a prefix matcher, this submatcher always matches.\\n        if matcher.prefix():\\n            self._always = any(f == path for f in matcher._files)', 'def abs(self, f):\\n        return self._matcher.abs(self._path + \"/\" + f)', 'def uipath(self, f):\\n        return self._matcher.uipath(self._path + \"/\" + f)', 'def visitdir(self, dir):\\n        dir = normalizerootdir(dir, \"visitdir\")\\n        if dir == \"\":\\n            dir = self._path\\n        else:\\n            dir = self._path + \"/\" + dir\\n        return self._matcher.visitdir(dir)', 'def prefix(self):\\n        return self._matcher.prefix() and not self._always', 'def __init__(self, matchers):\\n        m1 = matchers[0]\\n        super(unionmatcher, self).__init__(m1._root, m1._cwd)\\n        self.traversedir = m1.traversedir\\n        self._matchers = matchers', 'def visitdir(self, dir):\\n        r = False\\n        for m in self._matchers:\\n            v = m.visitdir(dir)\\n            if v == \"all\":\\n                return v\\n            r |= v\\n        return r', 'def __init__(self, m1, m2):\\n        super(xormatcher, self).__init__(m1._root, m1._cwd)\\n        self.traversedir = m1.traversedir\\n        self.m1 = m1\\n        self.m2 = m2', 'def visitdir(self, dir):\\n        m1dir = self.m1.visitdir(dir)\\n        m2dir = self.m2.visitdir(dir)\\n\\n        # if both matchers return \"all\" then we know for sure we don\\'t need\\n        # to visit this directory. Same if all matchers return False. In all\\n        # other case we have to visit a directory.\\n        if m1dir == \"all\" and m2dir == \"all\":\\n            return False\\n        if not m1dir and not m2dir:\\n            return False\\n        return True', 'def __init__(self, matcher):\\n        self._matcher = matcher', 'def visitdir(self, dir):\\n        if self(dir):\\n            return \"all\"\\n        return self._matcher.visitdir(dir)', 'def patkind(pattern, default=None):\\n    \"\"\"If pattern is \\'kind:pat\\' with a known kind, return kind.\"\"\"\\n    return _patsplit(pattern, default)[0]', 'def _globre(pat):\\n    r\"\"\"Convert an extended glob string to a regexp string.\\n\\n    >>> from . import pycompat\\n    >>> def bprint(s):\\n    ...     print(s)\\n    >>> bprint(_globre(br\\'?\\'))\\n    .\\n    >>> bprint(_globre(br\\'*\\'))\\n    [^/]*\\n    >>> bprint(_globre(br\\'**\\'))\\n    .*\\n    >>> bprint(_globre(br\\'**/a\\'))\\n    (?:.*/)?a\\n    >>> bprint(_globre(br\\'a/**/b\\'))\\n    a/(?:.*/)?b\\n    >>> bprint(_globre(br\\'[a*?!^][^b][!c]\\'))\\n    [a*?!^][\\\\^b][^c]\\n    >>> bprint(_globre(br\\'{a,b}\\'))\\n    (?:a|b)\\n    >>> bprint(_globre(br\\'.\\\\*\\\\?\\'))\\n    \\\\.\\\\*\\\\?\\n    \"\"\"\\n    i, n = 0, len(pat)\\n    res = \"\"\\n    group = 0\\n    escape = util.re.escape\\n\\n    def peek():\\n        return i < n and pat[i : i + 1]\\n\\n    while i < n:\\n        c = pat[i : i + 1]\\n        i += 1\\n        if c not in \"*?[{},\\\\\\\\\":\\n            res += escape(c)\\n        elif c == \"*\":\\n            if peek() == \"*\":\\n                i += 1\\n                if peek() == \"/\":\\n                    i += 1\\n                    res += \"(?:.*/)?\"\\n                else:\\n                    res += \".*\"\\n            else:\\n                res += \"[^/]*\"\\n        elif c == \"?\":\\n            res += \".\"\\n        elif c == \"[\":\\n            j = i\\n            if j < n and pat[j : j + 1] in \"!]\":\\n                j += 1\\n            while j < n and pat[j : j + 1] != \"]\":\\n                j += 1\\n            if j >= n:\\n                res += \"\\\\\\\\[\"\\n            else:\\n                stuff = pat[i:j].replace(\"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\")\\n                i = j + 1\\n                if stuff[0:1] == \"!\":\\n                    stuff = \"^\" + stuff[1:]\\n                elif stuff[0:1] == \"^\":\\n                    stuff = \"\\\\\\\\\" + stuff\\n                res = \"%s[%s]\" % (res, stuff)\\n        elif c == \"{\":\\n            group += 1\\n            res += \"(?:\"\\n        elif c == \"}\" and group:\\n            res += \")\"\\n            group -= 1\\n        elif c == \",\" and group:\\n            res += \"|\"\\n        elif c == \"\\\\\\\\\":\\n            p = peek()\\n            if p:\\n                i += 1\\n                res += escape(p)\\n            else:\\n                res += escape(c)\\n        else:\\n            res += escape(c)\\n    return res', 'def _buildmatch(ctx, kindpats, globsuffix, root):\\n    \"\"\"Return regexp string and a matcher function for kindpats.\\n    globsuffix is appended to the regexp of globs.\"\"\"\\n    matchfuncs = []\\n\\n    subincludes, kindpats = _expandsubinclude(kindpats, root)\\n    if subincludes:\\n        submatchers = {}\\n\\n        def matchsubinclude(f):\\n            for prefix, matcherargs in subincludes:\\n                if f.startswith(prefix):\\n                    mf = submatchers.get(prefix)\\n                    if mf is None:\\n                        mf = match(*matcherargs)\\n                        submatchers[prefix] = mf\\n\\n                    if mf(f[len(prefix) :]):\\n                        return True\\n            return False\\n\\n        matchfuncs.append(matchsubinclude)\\n\\n    fset, kindpats = _expandsets(kindpats, ctx)\\n    if fset:\\n        matchfuncs.append(fset.__contains__)\\n\\n    regex = \"\"\\n    if kindpats:\\n        regex, mf = _buildregexmatch(kindpats, globsuffix)\\n        matchfuncs.append(mf)\\n\\n    if len(matchfuncs) == 1:\\n        return regex, matchfuncs[0]\\n    else:\\n        return regex, lambda f: any(mf(f) for mf in matchfuncs)', 'def _patternrootsanddirs(kindpats):\\n    \"\"\"Returns roots and directories corresponding to each pattern.\\n\\n    This calculates the roots and directories exactly matching the patterns and\\n    returns a tuple of (roots, dirs) for each. It does not return other\\n    directories which may also need to be considered, like the parent\\n    directories.\\n    \"\"\"\\n    r = []\\n    d = []\\n    for kind, pat, source in kindpats:\\n        if kind == \"glob\":  # find the non-glob prefix\\n            root = []\\n            for p in pat.split(\"/\"):\\n                if \"[\" in p or \"{\" in p or \"*\" in p or \"?\" in p:\\n                    break\\n                root.append(p)\\n            r.append(\"/\".join(root))\\n        elif kind in (\"relpath\", \"path\"):\\n            if pat == \".\":\\n                pat = \"\"\\n            r.append(pat)\\n        elif kind in (\"rootfilesin\",):\\n            if pat == \".\":\\n                pat = \"\"\\n            d.append(pat)\\n        else:  # relglob, re, relre\\n            r.append(\"\")\\n    return r, d', 'def _rootsanddirs(kindpats):\\n    \"\"\"Returns roots and exact directories from patterns.\\n\\n    roots are directories to match recursively, whereas exact directories should\\n    be matched non-recursively. The returned (roots, dirs) tuple will also\\n    include directories that need to be implicitly considered as either, such as\\n    parent directories.\\n\\n    >>> _rootsanddirs(\\n    ...     [(b\\'glob\\', b\\'g/h/*\\', b\\'\\'), (b\\'glob\\', b\\'g/h\\', b\\'\\'),\\n    ...      (b\\'glob\\', b\\'g*\\', b\\'\\')])\\n    ([\\'g/h\\', \\'g/h\\', \\'\\'], [\\'\\', \\'g\\'])\\n    >>> _rootsanddirs(\\n    ...     [(b\\'rootfilesin\\', b\\'g/h\\', b\\'\\'), (b\\'rootfilesin\\', b\\'\\', b\\'\\')])\\n    ([], [\\'g/h\\', \\'\\', \\'\\', \\'g\\'])\\n    >>> _rootsanddirs(\\n    ...     [(b\\'relpath\\', b\\'r\\', b\\'\\'), (b\\'path\\', b\\'p/p\\', b\\'\\'),\\n    ...      (b\\'path\\', b\\'\\', b\\'\\')])\\n    ([\\'r\\', \\'p/p\\', \\'\\'], [\\'\\', \\'p\\'])\\n    >>> _rootsanddirs(\\n    ...     [(b\\'relglob\\', b\\'rg*\\', b\\'\\'), (b\\'re\\', b\\'re/\\', b\\'\\'),\\n    ...      (b\\'relre\\', b\\'rr\\', b\\'\\')])\\n    ([\\'\\', \\'\\', \\'\\'], [\\'\\'])\\n    \"\"\"\\n    r, d = _patternrootsanddirs(kindpats)\\n\\n    # Append the parents as non-recursive/exact directories, since they must be\\n    # scanned to get to either the roots or the other exact directories.\\n    d.extend(sorted(util.dirs(d)))\\n    d.extend(sorted(util.dirs(r)))\\n\\n    return r, d', 'def _prefix(kindpats):\\n    \"\"\"Whether all the patterns match a prefix (i.e. recursively)\"\"\"\\n    for kind, pat, source in kindpats:\\n        if kind not in (\"path\", \"relpath\"):\\n            return False\\n    return True', 'def readpatternfile(filepath, warn, sourceinfo=False):\\n    \"\"\"parse a pattern file, returning a list of\\n    patterns. These patterns should be given to compile()\\n    to be validated and converted into a match function.\\n\\n    trailing white space is dropped.\\n    the escape character is backslash.\\n    comments start with #.\\n    empty lines are skipped.\\n\\n    lines can be of the following formats:\\n\\n    syntax: regexp # defaults following lines to non-rooted regexps\\n    syntax: glob   # defaults following lines to non-rooted globs\\n    re:pattern     # non-rooted regular expression\\n    glob:pattern   # non-rooted glob\\n    pattern        # pattern of the current default type\\n\\n    if sourceinfo is set, returns a list of tuples:\\n    (pattern, lineno, originalline). This is useful to debug ignore patterns.\\n    \"\"\"\\n\\n    syntaxes = {\\n        \"re\": \"relre:\",\\n        \"regexp\": \"relre:\",\\n        \"glob\": \"relglob:\",\\n        \"include\": \"include\",\\n        \"subinclude\": \"subinclude\",\\n    }\\n    syntax = \"relre:\"\\n    patterns = []\\n\\n    fp = open(filepath, \"rb\")\\n    for lineno, line in enumerate(util.iterfile(fp), start=1):\\n        if \"#\" in line:\\n            global _commentre\\n            if not _commentre:\\n                _commentre = util.re.compile(br\"((?:^|[^\\\\\\\\])(?:\\\\\\\\\\\\\\\\)*)#.*\")\\n            # remove comments prefixed by an even number of escapes\\n            m = _commentre.search(line)\\n            if m:\\n                line = line[: m.end(1)]\\n            # fixup properly escaped comments that survived the above\\n            line = line.replace(\"\\\\\\\\#\", \"#\")\\n        line = line.rstrip()\\n        if not line:\\n            continue\\n\\n        if line.startswith(\"syntax:\"):\\n            s = line[7:].strip()\\n            try:\\n                syntax = syntaxes[s]\\n            except KeyError:\\n                if warn:\\n                    warn(_(\"%s: ignoring invalid syntax \\'%s\\'\\\\n\") % (filepath, s))\\n            continue\\n\\n        linesyntax = syntax\\n        for s, rels in pycompat.iteritems(syntaxes):\\n            if line.startswith(rels):\\n                linesyntax = rels\\n                line = line[len(rels) :]\\n                break\\n            elif line.startswith(s + \":\"):\\n                linesyntax = rels\\n                line = line[len(s) + 1 :]\\n                break\\n        if sourceinfo:\\n            patterns.append((linesyntax + line, lineno, line))\\n        else:\\n            patterns.append(linesyntax + line)\\n    fp.close()\\n    return patterns']}, {'features': [], 'snippets': ['def refresh():\\n\\n  win.Refresh()\\n  print type(win)', 'def create_texture():\\n  global rgbtex\\n  rgbtex = glGenTextures(1)\\n  glBindTexture(TEXTURE_TARGET, rgbtex)\\n  glTexImage2D(TEXTURE_TARGET,0,GL_RGB,640,480,0,GL_RGB,GL_UNSIGNED_BYTE,None)', 'def EVT_LEFT_DOWN(event):\\n  global _mpos\\n  _mpos = event.Position', 'def EVT_LEFT_UP(event):\\n  global _mpos\\n  _mpos = None', 'def EVT_MOTION(event):\\n  global _mpos\\n  if event.LeftIsDown():\\n    if _mpos:\\n      (x,y),(mx,my) = event.Position,_mpos\\n      rotangles[0] += y-my\\n      rotangles[1] += x-mx\\n      refresh()    \\n    _mpos = event.Position', 'def EVT_MOUSEWHEEL(event):\\n  global zoomdist\\n  dy = event.WheelRotation\\n  zoomdist *= np.power(0.95, -dy)\\n  refresh()', \"def on_draw():  \\n  if not 'rgbtex' in globals():\\n    create_texture()\\n\\n  xyz, uv = projpts\\n  if xyz is None: return\\n\\n  if not rgb is None:\\n    rgb_ = (rgb.astype(np.float32) * 4 + 70).clip(0,255).astype(np.uint8)\\n    glBindTexture(TEXTURE_TARGET, rgbtex)\\n    glTexSubImage2D(TEXTURE_TARGET, 0, 0, 0, 640, 480, GL_RGB, GL_UNSIGNED_BYTE, rgb_);\\n\\n  glClearColor(*clearcolor)\\n  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\\n  glEnable(GL_DEPTH_TEST)\\n\\n  # flush that stack in case it's broken from earlier\\n  glPushMatrix()\\n\\n  glMatrixMode(GL_PROJECTION)\\n  glLoadIdentity()\\n  gluPerspective(60, 4/3., 0.3, 200)\\n\\n  glMatrixMode(GL_MODELVIEW)\\n  glLoadIdentity()\\n\\n  def mouse_rotate(xAngle, yAngle, zAngle):\\n    glRotatef(xAngle, 1.0, 0.0, 0.0);\\n    glRotatef(yAngle, 0.0, 1.0, 0.0);\\n    glRotatef(zAngle, 0.0, 0.0, 1.0);\\n  glScale(zoomdist,zoomdist,1)\\n  glTranslate(0, 0,-3.5)\\n  mouse_rotate(rotangles[0], rotangles[1], 0);\\n  glTranslate(0,0,1.5)\\n  #glTranslate(0, 0,-1)\\n\\n  # Draw some axes\\n  if 0:\\n    glBegin(GL_LINES)\\n    glColor3f(1,0,0); glVertex3f(0,0,0); glVertex3f(1,0,0)\\n    glColor3f(0,1,0); glVertex3f(0,0,0); glVertex3f(0,1,0)\\n    glColor3f(0,0,1); glVertex3f(0,0,0); glVertex3f(0,0,1)\\n    glEnd()\\n\\n  # We can either project the points ourselves, or embed it in the opengl matrix\\n  if 0:\\n    dec = 4\\n    v,u = mgrid[:480,:640].astype(np.uint16)\\n    points = np.vstack((u[::dec,::dec].flatten(),\\n                        v[::dec,::dec].flatten(),\\n                        depth[::dec,::dec].flatten())).transpose()\\n    points = points[points[:,2]<2047,:]\", 'def playcolors():\\n  while 1:\\n    global clearcolor\\n    clearcolor = [np.random.random(),0,0,0]\\n    time.sleep(0.1)\\n    refresh()', 'def update(dt=0):\\n  global projpts, rgb, depth\\n  depth,_ = freenect.sync_get_depth()\\n  rgb,_ = freenect.sync_get_video()\\n  q = depth\\n  X,Y = np.meshgrid(range(640),range(480))\\n  # YOU CAN CHANGE THIS AND RERUN THE PROGRAM!\\n  # Point cloud downsampling\\n  d = 4\\n  projpts = calibkinect.depth2xyzuv(q[::d,::d],X[::d,::d],Y[::d,::d])\\n  refresh()', 'def update_join():\\n  update_on()\\n  try:\\n    _thread.join()\\n  except:\\n    update_off()', \"def update_on():\\n  global _updating\\n  if not '_updating' in globals(): _updating = False\\n  if _updating: return\", 'def _run():\\n    while _updating:\\n      update()', 'def update_off():\\n  global _updating\\n  _updating = False', \"def loopcv():\\n  import cv\\n  while 1:\\n    cv.ShowImage('hi',get_depth().astype(np.uint8))\\n    cv.WaitKey(10)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def is_num(usr_data):\\n    \\'\\'\\'\\n    check if usr_data is a number (float, int, long, Decimal),\\n    return boolean\\n\\n    unlike `isinstance(usr_data, Number)` does not return True for `True, False`.\\n\\n    Does not use `isinstance(usr_data, Number)` which is 6 times slower\\n    than calling this function (except in the case of Fraction, when\\n    it\\'s 6 times faster, but that\\'s rarer)\\n\\n    Runs by adding 0 to the \"number\" -- so anything that implements\\n    add to a scalar works\\n\\n    >>> is_num(3.0)\\n    True\\n    >>> is_num(3)\\n    True\\n    >>> is_num(\\'three\\')\\n    False\\n    >>> is_num([2, 3, 4])\\n    False\\n\\n    True and False are NOT numbers:\\n\\n    >>> is_num(True)\\n    False\\n    >>> is_num(False)\\n    False\\n    >>> is_num(None)\\n    False\\n\\n    :rtype: bool\\n    \\'\\'\\'\\n    try:\\n        dummy = usr_data + 0\\n        # pylint: disable=simplifiable-if-statement\\n        if usr_data is not True and usr_data is not False:\\n            return True\\n        else:\\n            return False\\n    except Exception: # pylint: disable=broad-except\\n        return False', 'def char_to_binary(char):\\n    \\'\\'\\'\\n    Convert a char into its binary representation. Useful for debugging.\\n\\n    >>> char_to_binary(\\'a\\')\\n    \\'01100001\\'\\n    \\'\\'\\'\\n    ascii_value = ord(char)\\n    binary_digits = []\\n    while ascii_value > 0:\\n        if (ascii_value & 1) == 1:\\n            binary_digits.append(\"1\")\\n        else:\\n            binary_digits.append(\"0\")\\n        ascii_value = ascii_value >> 1\\n\\n    binary_digits.reverse()\\n    binary = \\'\\'.join(binary_digits)\\n    zerofix = (8 - len(binary)) * \\'0\\'\\n    return zerofix + binary', \"def get_number(midi_str, length):\\n    '''\\n    Return the value of a string byte or bytes if length > 1\\n    from an 8-bit string or (PY3) bytes object\\n\\n    Then, return the remaining string or bytes object\\n    The `length` is the number of chars to read.\\n    This will sum a length greater than 1 if desired.\\n    Note that MIDI uses big-endian for everything.\\n    This is the inverse of Python's chr() function.\\n\\n    >>> get_number('test', 0)\\n    (0, 'test')\\n    >>> get_number('test', 2)\\n    (29797, 'st')\\n    >>> get_number('test', 4)\\n    (1952805748, '')\\n    '''\\n    summation = 0\\n    if not is_num(midi_str):\\n        for i in range(length):\\n            midi_str_or_num = midi_str[i]\\n            if is_num(midi_str_or_num):\\n                summation = (summation << 8) + midi_str_or_num\\n            else:\\n                summation = (summation << 8) + ord(midi_str_or_num)\\n        return summation, midi_str[length:]\\n    else:\\n        mid_num = midi_str\\n        summation = mid_num - ((mid_num >> (8*length)) << (8*length))\\n        big_bytes = mid_num - summation\\n        return summation, big_bytes\", \"def get_numbers_as_list(midi_str):\\n    '''\\n    Translate each char into a number, return in a list.\\n    Used for reading data messages where each byte encodes\\n    a different discrete value.\\n\\n    >>> get_numbers_as_list('\\\\\\\\x00\\\\\\\\x00\\\\\\\\x00\\\\\\\\x03')\\n    [0, 0, 0, 3]\\n    '''\\n    post = []\\n    for item in midi_str:\\n        if is_num(item):\\n            post.append(item)\\n        else:\\n            post.append(ord(item))\\n    return post\", \"def put_variable_length_number(num):\\n    '''\\n    >>> put_variable_length_number(4)\\n    b'\\\\\\\\x04'\\n    >>> put_variable_length_number(127)\\n    b'\\\\\\\\x7f'\\n    >>> put_variable_length_number(0)\\n    b'\\\\\\\\x00'\\n    >>> put_variable_length_number(1024)\\n    b'\\\\\\\\x88\\\\\\\\x00'\\n    >>> put_variable_length_number(8192)\\n    b'\\\\\\\\xc0\\\\\\\\x00'\\n    >>> put_variable_length_number(16383)\\n    b'\\\\\\\\xff\\\\\\\\x7f'\\n    >>> put_variable_length_number(-1)\\n    Traceback (most recent call last):\\n    MidiException: cannot put_variable_length_number() when number is negative: -1\\n    '''\\n    if num < 0:\\n        raise MidiException(\\n            'cannot put_variable_length_number() when number is negative: %s' % num)\\n    lst = bytearray()\\n    while True:\\n        result, num = num & 0x7F, num >> 7\\n        lst.append(result + 0x80)\\n        if num == 0:\\n            break\\n    lst.reverse()\\n    lst[-1] = lst[-1] & 0x7f\\n    return bytes(lst)\", 'def __init__(self, enum_list=None):\\n        if enum_list is None:\\n            enum_list = []\\n        lookup = {}\\n        reverse_lookup = {}\\n        num = 0\\n        unique_names = []\\n        unique_values = []\\n        for enum in enum_list:\\n            if isinstance(enum, tuple):\\n                enum, num = enum\\n            if not isinstance(enum, str):\\n                raise EnumerationException(\"enum name is not a string: \" + enum)\\n            if not isinstance(num, int):\\n                raise EnumerationException(\"enum value is not an integer: \" + num)\\n            if enum in unique_names:\\n                raise EnumerationException(\"enum name is not unique: \" + enum)\\n            if num in unique_values:\\n                raise EnumerationException(\"enum value is not unique for \" + enum)\\n            unique_names.append(enum)\\n            unique_values.append(num)\\n            lookup[enum] = num\\n            reverse_lookup[num] = enum\\n            num = num + 1\\n        self.lookup = lookup\\n        self.reverse_lookup = reverse_lookup', 'def hasattr(self, attr):\\n        if attr in self.lookup:\\n            return True\\n        return False', 'def __getattr__(self, attr):\\n        if attr not in self.lookup:\\n            raise AttributeError\\n        return self.lookup[attr]', \"def __init__(self, track, type_=None, time=None, channel=None):\\n        self.track = track\\n        self.type_ = type_\\n        self.time = time\\n        self.channel = channel\\n\\n        self._parameter1 = None # pitch or first data value\\n        self._parameter2 = None # velocity or second data value\\n\\n        # data is a property...\\n\\n        # if this is a Note on/off, need to store original\\n        # pitch space value in order to determine if this is has a microtone\\n        self.cent_shift = None\\n\\n        # store a reference to a corresponding event\\n        # if a noteOn, store the note off, and vice versa\\n        # NTODO: We should make sure that we garbage collect this -- otherwise it's a memory\\n        # leak from a circular reference.\\n        # note: that's what weak references are for\\n        # unimplemented\\n        self.corresponding_event = None\\n\\n        # store and pass on a running status if found\\n        self.last_status_byte = None\\n\\n        self.sort_order = 0\\n        self.update_sort_order()\", 'def __repr__(self):\\n        if self.track is None:\\n            track_index = None\\n        else:\\n            track_index = self.track.index\\n\\n        return_str = (\"<MidiEvent %s, t=%s, track=%s, channel=%s\" %\\n             (self.type_, repr(self.time), track_index,\\n              repr(self.channel)))\\n        if self.type_ in [\\'NOTE_ON\\', \\'NOTE_OFF\\']:\\n            attr_list = [\"pitch\", \"velocity\"]\\n        else:\\n            if self._parameter2 is None:\\n                attr_list = [\\'data\\']\\n            else:\\n                attr_list = [\\'_parameter1\\', \\'_parameter2\\']\\n\\n        for attrib in attr_list:\\n            if getattr(self, attrib) is not None:\\n                return_str = return_str + \", \" + attrib + \"=\" + repr(getattr(self, attrib))\\n        return return_str + \">\"', \"def _get_pitch(self):\\n        if self.type_ in ['NOTE_ON', 'NOTE_OFF']:\\n            return self._parameter1\\n        else:\\n            return None\", 'def _set_velocity(self, value):\\n        self._parameter2 = value', \"def _set_data(self, value):\\n        if value is not None and not isinstance(value, bytes):\\n            if isinstance(value, str):\\n                value = value.encode('utf-8')\\n        self._parameter1 = value\", \"def set_pitch_bend(self, cents, bend_range=2):\\n        '''\\n        Treat this event as a pitch bend value, and set the ._parameter1 and\\n         ._parameter2 fields appropriately given a specified bend value in cents.\\n\\n        The `bend_range` parameter gives the number of half steps in the bend range.\\n\\n        >>> mt = MidiTrack(1)\\n        >>> me1 = MidiEvent(mt)\\n        >>> me1.set_pitch_bend(50)\\n        >>> me1._parameter1, me1._parameter2\\n        (0, 80)\\n        >>> me1.set_pitch_bend(100)\\n        >>> me1._parameter1, me1._parameter2\\n        (0, 96)\\n        >>> me1.set_pitch_bend(200)\\n        >>> me1._parameter1, me1._parameter2\\n        (127, 127)\\n        >>> me1.set_pitch_bend(-50)\\n        >>> me1._parameter1, me1._parameter2\\n        (0, 48)\\n        >>> me1.set_pitch_bend(-100)\\n        >>> me1._parameter1, me1._parameter2\\n        (0, 32)\\n        '''\\n        # value range is 0, 16383\\n        # center should be 8192\\n        cent_range = bend_range * 100\\n        center = 8192\\n        top_span = 16383 - center\\n        bottom_span = center\\n\\n        if cents > 0:\\n            shift_scalar = cents / float(cent_range)\\n            shift = int(round(shift_scalar * top_span))\\n        elif cents < 0:\\n            shift_scalar = cents / float(cent_range) # will be negative\\n            shift = int(round(shift_scalar * bottom_span)) # will be negative\\n        else:\\n            shift = 0\\n        target = center + shift\\n\\n        # produce a two-char value\\n        char_value = put_variable_length_number(target)\\n        data1, _ = get_number(char_value[0], 1)\\n        # need to convert from 8 bit to 7, so using & 0x7F\\n        data1 = data1 & 0x7F\\n        if len(char_value) > 1:\\n            data2, _ = get_number(char_value[1], 1)\\n            data2 = data2 & 0x7F\\n        else:\\n            data2 = 0\\n\\n        self._parameter1 = data2\\n        self._parameter2 = data1 # data1 is msb here\", 'def read(self, time, midi_str):\\n        \\'\\'\\'\\n        Parse the string that is given and take the beginning\\n        section and convert it into data for this event and return the\\n        now truncated string.\\n        The `time` value is the number of ticks into the Track\\n        at which this event happens. This is derived from reading\\n        data the level of the track.\\n        TODO: These instructions are inadequate.\\n        >>> # all note-on messages (144-159) can be found\\n        >>> 145 & 0xF0 # testing message type_ extraction\\n        144\\n        >>> 146 & 0xF0 # testing message type_ extraction\\n        144\\n        >>> (144 & 0x0F) + 1 # getting the channel\\n        1\\n        >>> (159 & 0x0F) + 1 # getting the channel\\n        16\\n        \\'\\'\\'\\n        if len(midi_str) < 2:\\n            # often what we have here are null events:\\n            # the string is simply: 0x00\\n            print(\\n                \\'MidiEvent.read(): got bad data string\\',\\n                \\'time\\',\\n                time,\\n                \\'str\\',\\n                repr(midi_str))\\n            return \\'\\'\\n\\n        # first_byte, message_type, and second_byte define\\n        # characteristics of the first two chars\\n        # for first_byte: The left nybble (4 bits) contains the\\n        # actual command, and the right nibble\\n        # contains the midi channel number on which the command will\\n        # be executed.\\n        if is_num(midi_str[0]):\\n            first_byte = midi_str[0]\\n        else:\\n            first_byte = ord(midi_str[0])\\n\\n        # detect running status: if the status byte is less than 128, its\\n        # not a status byte, but a data byte\\n        if first_byte < 128:\\n            if self.last_status_byte is not None:\\n                rsb = self.last_status_byte\\n                if is_num(rsb):\\n                    rsb = bytes([rsb])\\n            else:\\n                rsb = bytes([0x90])\\n            # add the running status byte to the front of the string\\n            # and process as before\\n            midi_str = rsb + midi_str\\n            if is_num(midi_str[0]):\\n                first_byte = midi_str[0]\\n            else:\\n                first_byte = ord(midi_str[0])\\n        else:\\n            self.last_status_byte = midi_str[0]\\n\\n        message_type = first_byte & 0xF0\\n\\n        if is_num(midi_str[1]):\\n            second_byte = midi_str[1]\\n        else:\\n            second_byte = ord(midi_str[1])\\n\\n        if CHANNEL_VOICE_MESSAGES.has_value(message_type):\\n            return self._parse_channel_voice_message(midi_str)\\n\\n        elif message_type == 0xB0 and CHANNEL_MODE_MESSAGES.has_value(second_byte):\\n            self.channel = (first_byte & 0x0F) + 1\\n            self.type_ = CHANNEL_MODE_MESSAGES.whatis(second_byte)\\n            if self.type_ == \"LOCAL_CONTROL\":\\n                self.data = (ord(midi_str[2]) == 0x7F)\\n            elif self.type_ == \"MONO_MODE_ON\":\\n                self.data = ord(midi_str[2])\\n            else:\\n                print(\\'unhandled message:\\', midi_str[2])\\n            return midi_str[3:]\\n\\n        elif first_byte == 0xF0 or first_byte == 0xF7:\\n            self.type_ = {0xF0: \"F0_SYSEX_EVENT\",\\n                         0xF7: \"F7_SYSEX_EVENT\"}[first_byte]\\n            length, midi_str = get_variable_length_number(midi_str[1:])\\n            self.data = midi_str[:length]\\n            return midi_str[length:]\\n\\n        # SEQUENCE_TRACK_NAME and other MetaEvents are here\\n        elif first_byte == 0xFF:\\n            if not META_EVENTS.has_value(second_byte):\\n                print(\"unknown meta event: FF %02X\" % second_byte)\\n                sys.stdout.flush()\\n                raise MidiException(\"Unknown midi event type_: %r, %r\" % (first_byte, second_byte))\\n            self.type_ = META_EVENTS.whatis(second_byte)\\n            length, midi_str = get_variable_length_number(midi_str[2:])\\n            self.data = midi_str[:length]\\n            return midi_str[length:]\\n        else:\\n            # an uncaught message\\n            print(\\n                \\'got unknown midi event type_\\',\\n                repr(first_byte),\\n                \\'char_to_binary(midi_str[0])\\',\\n                char_to_binary(midi_str[0]),\\n                \\'char_to_binary(midi_str[1])\\',\\n                char_to_binary(midi_str[1]))\\n            raise MidiException(\"Unknown midi event type_\")', 'def is_note_on(self):\\n        \\'\\'\\'\\n        return a boolean if this is a NOTE_ON message and velocity is not zero_\\n\\n        >>> mt = MidiTrack(1)\\n        >>> me1 = MidiEvent(mt)\\n        >>> me1.type_ = \"NOTE_ON\"\\n        >>> me1.velocity = 120\\n        >>> me1.is_note_on()\\n        True\\n        >>> me1.is_note_off()\\n        False\\n        \\'\\'\\'\\n        return self.type_ == \"NOTE_ON\" and self.velocity != 0', 'def is_delta_time(self):\\n        \\'\\'\\'\\n        Return a boolean if this is a DeltaTime subclass.\\n\\n        >>> mt = MidiTrack(1)\\n        >>> dt = DeltaTime(mt)\\n        >>> dt.is_delta_time()\\n        True\\n        \\'\\'\\'\\n        if self.type_ == \"DeltaTime\":\\n            return True\\n        return False', 'def __init__(self, track, time=None, channel=None):\\n        MidiEvent.__init__(self, track, time=time, channel=channel)\\n        self.type_ = \"DeltaTime\"', 'def get_bytes(self):\\n        midi_str = put_variable_length_number(self.time)\\n        return midi_str', 'def __init__(self, index):\\n        self.index = index\\n        self.events = []\\n        self.length = 0 #the data length; only used on read()', 'def get_bytes(self):\\n        \\'\\'\\'\\n        returns a string of midi-data from the `.events` in the object.\\n        \\'\\'\\'\\n        # build str using MidiEvents\\n        midi_str = b\"\"\\n        for event in self.events:\\n            # this writes both delta time and message events\\n            try:\\n                event_bytes = event.get_bytes()\\n                int_array = []\\n                for byte in event_bytes:\\n                    if is_num(byte):\\n                        int_array.append(byte)\\n                    else:\\n                        int_array.append(ord(byte))\\n                event_bytes = bytes(bytearray(int_array))\\n                midi_str = midi_str + event_bytes\\n            except MidiException as err:\\n                print(\"Conversion error for %s: %s; ignored.\" % (event, err))\\n        return b\"MTrk\" + put_number(len(midi_str), 4) + midi_str', \"def update_events(self):\\n        '''\\n        We may attach events to this track before setting their `track` parameter.\\n        This method will move through all events and set their track to this track.\\n        '''\\n        for event in self.events:\\n            event.track = self\", \"def set_channel(self, value):\\n        '''Set the channel of all events in this Track.\\n        '''\\n        if value not in range(1, 17):\\n            raise MidiException('bad channel value: %s' % value)\\n        for event in self.events:\\n            event.channel = value\", \"def get_program_changes(self):\\n        '''Get all unique program changes used in this Track, sorted.\\n        '''\\n        post = []\\n        for event in self.events:\\n            if event.type_ == 'PROGRAM_CHANGE':\\n                if event.data not in post:\\n                    post.append(event.data)\\n        return post\", 'def __init__(self):\\n        self.file = None\\n        self.format = 1\\n        self.tracks = []\\n        self.ticks_per_quarter_note = 1024\\n        self.ticks_per_second = None', \"def open_file_like(self, file_like):\\n        '''Assign a file-like object, such as those provided by StringIO, as an open file object.\\n        >>> from io import StringIO\\n        >>> fileLikeOpen = StringIO()\\n        >>> mf = MidiFile()\\n        >>> mf.open_file_like(fileLikeOpen)\\n        >>> mf.close()\\n        '''\\n        self.file = file_like\", \"def close(self):\\n        '''\\n        Close the file.\\n        '''\\n        self.file.close()\", 'def readstr(self, midi_str):\\n        \\'\\'\\'\\n        Read and parse MIDI data as a string, putting the\\n        data in `.ticks_per_quarter_note` and a list of\\n        `MidiTrack` objects in the attribute `.tracks`.\\n        \\'\\'\\'\\n        if not midi_str[:4] == b\"MThd\":\\n            raise MidiException(\\'badly formated midi string, got: %s\\' % midi_str[:20])\\n\\n        # we step through the str src, chopping off characters as we go\\n        # and reassigning to str\\n        length, midi_str = get_number(midi_str[4:], 4)\\n        if length != 6:\\n            raise MidiException(\\'badly formated midi string\\')\\n\\n        midi_format_type, midi_str = get_number(midi_str, 2)\\n        self.format = midi_format_type\\n        if midi_format_type not in (0, 1):\\n            raise MidiException(\\'cannot handle midi file format: %s\\' % format)\\n\\n        num_tracks, midi_str = get_number(midi_str, 2)\\n        division, midi_str = get_number(midi_str, 2)\\n\\n        # very few midi files seem to define ticks_per_second\\n        if division & 0x8000:\\n            frames_per_second = -((division >> 8) | -128)\\n            ticks_per_frame = division & 0xFF\\n            if ticks_per_frame not in [24, 25, 29, 30]:\\n                raise MidiException(\\'cannot handle ticks per frame: %s\\' % ticks_per_frame)\\n            if ticks_per_frame == 29:\\n                ticks_per_frame = 30  # drop frame\\n            self.ticks_per_second = ticks_per_frame * frames_per_second\\n        else:\\n            self.ticks_per_quarter_note = division & 0x7FFF\\n\\n        for i in range(num_tracks):\\n            trk = MidiTrack(i) # sets the MidiTrack index parameters\\n            midi_str = trk.read(midi_str) # pass all the remaining string, reassing\\n            self.tracks.append(trk)', \"def writestr(self):\\n        '''\\n        generate the midi data header and convert the list of\\n        midi_track objects in self_tracks into midi data and return it as a string_\\n        '''\\n        midi_str = self.write_m_thd_str()\\n        for trk in self.tracks:\\n            midi_str = midi_str + trk.get_bytes()\\n        return midi_str\"]}, {'features': [], 'snippets': [\"def test_attach_volume(self):\\n        server = dict(id='server001')\\n        vol = {'id': 'volume001', 'status': 'available',\\n               'name': '', 'attachments': []}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        rattach = {'server_id': server['id'], 'device': 'device001',\\n                   'volumeId': volume['id'], 'id': 'attachmentId'}\\n        self.register_uris([\\n            dict(method='POST',\\n                 uri=self.get_mock_url(\\n                     'compute', 'public',\\n                     append=['servers', server['id'],\\n                             'os-volume_attachments']),\\n                 json={'volumeAttachment': rattach},\\n                 validate=dict(json={\\n                     'volumeAttachment': {\\n                         'volumeId': vol['id']}})\\n                 )])\\n        ret = self.cloud.attach_volume(server, volume, wait=False)\\n        self.assertEqual(rattach, ret)\\n        self.assert_calls()\", \"def test_attach_volume_wait(self):\\n        server = dict(id='server001')\\n        vol = {'id': 'volume001', 'status': 'available',\\n               'name': '', 'attachments': []}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        vol['attachments'] = [{'server_id': server['id'],\\n                               'device': 'device001'}]\\n        vol['status'] = 'attached'\\n        attached_volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        rattach = {'server_id': server['id'], 'device': 'device001',\\n                   'volumeId': volume['id'], 'id': 'attachmentId'}\\n        self.register_uris([\\n            dict(method='POST',\\n                 uri=self.get_mock_url(\\n                     'compute', 'public',\\n                     append=['servers', server['id'],\\n                             'os-volume_attachments']),\\n                 json={'volumeAttachment': rattach},\\n                 validate=dict(json={\\n                     'volumeAttachment': {\\n                         'volumeId': vol['id']}})),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [volume]}),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [attached_volume]})])\\n        # defaults to wait=True\\n        ret = self.cloud.attach_volume(server, volume)\\n        self.assertEqual(rattach, ret)\\n        self.assert_calls()\", 'def test_attach_volume_not_available(self):\\n        server = dict(id=\\'server001\\')\\n        volume = dict(id=\\'volume001\\', status=\\'error\\', attachments=[])\\n\\n        with testtools.ExpectedException(\\n            openstack.cloud.OpenStackCloudException,\\n            \"Volume %s is not available. Status is \\'%s\\'\" % (\\n                volume[\\'id\\'], volume[\\'status\\'])\\n        ):\\n            self.cloud.attach_volume(server, volume)\\n        self.assertEqual(0, len(self.adapter.request_history))', \"def test_detach_volume(self):\\n        server = dict(id='server001')\\n        volume = dict(id='volume001',\\n                      attachments=[\\n                          {'server_id': 'server001', 'device': 'device001'}\\n                      ])\\n        self.register_uris([\\n            dict(method='DELETE',\\n                 uri=self.get_mock_url(\\n                     'compute', 'public',\\n                     append=['servers', server['id'],\\n                             'os-volume_attachments', volume['id']]))])\\n        self.cloud.detach_volume(server, volume, wait=False)\\n        self.assert_calls()\", \"def test_detach_volume_wait(self):\\n        server = dict(id='server001')\\n        attachments = [{'server_id': 'server001', 'device': 'device001'}]\\n        vol = {'id': 'volume001', 'status': 'attached', 'name': '',\\n               'attachments': attachments}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        vol['status'] = 'available'\\n        vol['attachments'] = []\\n        avail_volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        self.register_uris([\\n            dict(method='DELETE',\\n                 uri=self.get_mock_url(\\n                     'compute', 'public',\\n                     append=['servers', server['id'],\\n                             'os-volume_attachments', volume.id])),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [avail_volume]})])\\n        self.cloud.detach_volume(server, volume)\\n        self.assert_calls()\", \"def test_delete_volume_deletes(self):\\n        vol = {'id': 'volume001', 'status': 'attached',\\n               'name': '', 'attachments': []}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        self.register_uris([\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [volume]}),\\n            dict(method='DELETE',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', volume.id])),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': []})])\\n        self.assertTrue(self.cloud.delete_volume(volume['id']))\\n        self.assert_calls()\", \"def test_delete_volume_force(self):\\n        vol = {'id': 'volume001', 'status': 'attached',\\n               'name': '', 'attachments': []}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        self.register_uris([\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [volume]}),\\n            dict(method='POST',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', volume.id, 'action']),\\n                 validate=dict(\\n                     json={'os-force_delete': None})),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': []})])\\n        self.assertTrue(self.cloud.delete_volume(volume['id'], force=True))\\n        self.assert_calls()\", \"def test_set_volume_bootable_false(self):\\n        vol = {'id': 'volume001', 'status': 'attached',\\n               'name': '', 'attachments': []}\\n        volume = meta.obj_to_munch(fakes.FakeVolume(**vol))\\n        self.register_uris([\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public', append=['volumes', 'detail']),\\n                 json={'volumes': [volume]}),\\n            dict(method='POST',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', volume.id, 'action']),\\n                 json={'os-set_bootable': {'bootable': False}}),\\n        ])\\n        self.cloud.set_volume_bootable(volume['id'])\\n        self.assert_calls()\", \"def test_list_volumes_with_pagination_next_link_fails_once(self):\\n        vol1 = meta.obj_to_munch(fakes.FakeVolume('01', 'available', 'vol1'))\\n        vol2 = meta.obj_to_munch(fakes.FakeVolume('02', 'available', 'vol2'))\\n        self.register_uris([\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', 'detail']),\\n                 json={\\n                     'volumes': [vol1],\\n                     'volumes_links': [\\n                         {'href': self.get_mock_url(\\n                             'volumev2', 'public',\\n                             append=['volumes', 'detail'],\\n                             qs_elements=['marker=01']),\\n                          'rel': 'next'}]}),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', 'detail'],\\n                     qs_elements=['marker=01']),\\n                 status_code=404),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', 'detail']),\\n                 json={\\n                     'volumes': [vol1],\\n                     'volumes_links': [\\n                         {'href': self.get_mock_url(\\n                             'volumev2', 'public',\\n                             append=['volumes', 'detail'],\\n                             qs_elements=['marker=01']),\\n                          'rel': 'next'}]}),\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', 'detail'],\\n                     qs_elements=['marker=01']),\\n                 json={\\n                     'volumes': [vol2],\\n                     'volumes_links': [\\n                         {'href': self.get_mock_url(\\n                             'volumev2', 'public',\\n                             append=['volumes', 'detail'],\\n                             qs_elements=['marker=02']),\\n                          'rel': 'next'}]}),\\n\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', 'detail'],\\n                     qs_elements=['marker=02']),\\n                 json={'volumes': []})])\\n        self.assertEqual(\\n            [self.cloud._normalize_volume(vol1),\\n             self.cloud._normalize_volume(vol2)],\\n            self.cloud.list_volumes())\\n        self.assert_calls()\", \"def test_get_volume_by_id(self):\\n        vol1 = meta.obj_to_munch(fakes.FakeVolume('01', 'available', 'vol1'))\\n        self.register_uris([\\n            dict(method='GET',\\n                 uri=self.get_mock_url(\\n                     'volumev2', 'public',\\n                     append=['volumes', '01']),\\n                 json={'volume': vol1}\\n                 )\\n        ])\\n        self.assertEqual(\\n            self.cloud._normalize_volume(vol1),\\n            self.cloud.get_volume_by_id('01'))\\n        self.assert_calls()\"]}, {'features': [], 'snippets': ['def list(request, nick = None):\\n  template = loader.get_template(\"list.html\")\\n\\n  from django.core.urlresolvers import reverse\\n  from forms import PresenceForm\\n  form = PresenceForm()\\n  if nick is not None:\\n    form.initial[\\'nick\\'] = nick\\n    form_target = reverse(list, kwargs = {\\'nick\\': nick})\\n  else:\\n    form_target = reverse(list)\\n\\n  if request.POST.get(\\'nick\\', \\'\\') != \\'\\':\\n    context = {\\n      \\'address\\': request.META[\\'REMOTE_ADDR\\'],\\n      \\'uri\\': request.META[\\'REQUEST_URI\\'],\\n    }\\n    if \\'enter\\' in request.POST:\\n      presence.person_entered(request.POST[\\'nick\\'], context)\\n    else: # \\'leave\\' in request.POST\\n      presence.person_left(request.POST[\\'nick\\'], context)\\n    # tell the browser to reload the page, but with GET request\\n    return django.shortcuts.redirect(request.path)\\n\\n  context = RequestContext(request, {\\n    \\'form_target\\': form_target,\\n    \\'form\\': form,\\n    \\'present\\': presence.list_people(),\\n    \\'sensors\\': presence.list_simple_sensors(),\\n    \\'complex_sensors\\': presence.list_complex_sensors(),\\n  })\\n  return HttpResponse(template.render(context))']}, {'features': [], 'snippets': ['def parse_news(item):\\n    \\'\\'\\'Parse news item\\n    return is a tuple(id, title, url)\\n    \\'\\'\\'\\n    url = \\'http://www.spa.gov.sa\\' + item[\\'href\\']\\n    url_parsed = urlparse(url)\\n    qs = parse_qs(url_parsed[4])\\n    id = qs[\\'newsid\\'][0]\\n    title = item.h2.contents[0]\\n    title = \" \".join(title.split())\\n    item_parsed = (id, title, url)\\n    return item_parsed', \"def retrieve_detail(item):\\n    '''Retrive detaill for news item\\n    return is tuple (id, title, url, text)\\n    '''\\n    url = item[2]\\n    html = urlopen(url)\\n    soup = BeautifulSoup(html, 'html.parser')\\n    detail = soup.find(class_='divNewsDetailsText')\\n    detail = detail.get_text()\\n    _list  = list(item)\\n    _list.insert(3, detail)\\n    item = tuple(_list)\\n    return item\", \"def cabinet_decision(last_id=-1):\\n    '''Retrive cabinet decisions\\n    if last_id not defiend it will return the max\\n    return list of cabinet decisions tuples up to MAX_PAGES_TO_SEARCH (page=10)\\n    [(id, title, url, text)...]\\n    '''\\n    decisions = []\\n    _news = retrieve_news(cabinet=1, last_id=last_id)\\n    for item in _news:\\n        _detail = retrieve_detail(item)\\n        decisions.append(_detail)\\n    return decisions\", \"def leave_news(person, last_id=-1):\\n    '''Retrive only leave news for person\\n    if last_id not defiend it will return the max\\n    return list of leave news tuples up to MAX_PAGES_TO_SEARCH (page = 10 news)\\n    [(id, title, url, locationFromTo)...]\\n    '''\\n    leave_news = []\\n    all_news = retrieve_news(person=person, last_id= last_id)\\n    for item in all_news:\\n        if 'يغادر' in item[1]:\\n            _list = list(item)\\n            _list.insert(3, item[1].split('يغادر')[1])\\n            item = tuple(_list)\\n            leave_news.append(item)\\n    return leave_news\"]}, {'features': [], 'snippets': [\"def __init__(self,header=''):\\n\\t\\tself._header = header\", \"def __init__(self,prompt='[$] '):\\n\\t\\tself._prompt = prompt\\n\\t\\tself._at = ''\", 'def subscribe(self,*uids):\\n\\t\\tpass', \"def __init__(self,msgs):\\n\\t\\tself._msgs = msgs\\n\\t\\tself._index = 0\\n\\t\\tself._period = 0.001\\n\\t\\tself._pid = 'SCRIPT'\", \"def poll(self,wait_s=None,uid=None):\\n\\t\\tperiod = self._period if wait_s is None else wait_s\\n\\t\\ttime.sleep(period)\\n\\t\\ttry:\\n\\t\\t\\tmsg = self._msgs[self._index]\\n\\t\\t\\tprint(self._pid+' SEND > '+msg)\\n\\t\\t\\tself._index += 1\\n\\t\\t\\treturn [msg]\\n\\t\\texcept IndexError:\\n\\t\\t\\treturn []\", 'def load(self,msg_array):\\n\\t\\tself._msgs += msg_array\\n\\t\\treturn self', \"def run(self):\\n\\t\\tt = threading.current_thread()\\n\\t\\tself._pid = str(t.ident)+' '+str(t.name)\\n\\t\\twhile len(self.poll()) > 0: pass\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def ConvertDiagnosticLineToSonqarqube(item):\\n    try:\\n        id, line, message, source_file = GetDiagnosticFieldsFromDiagnosticLine(item)\\n        WriteDiagnosticFieldsToFile(id, line, message, source_file)\\n    except:\\n        print 'Cant parse line {}'.format(item)\", 'def WriteDiagnosticFieldsToFile(id, line, message, source_file):\\n    clang_sonar_report.write(\" <error file=\\\\\"\" + str(source_file) +\\n                        \"\\\\\" line=\\\\\"\" + str(line) +\\n                        \"\\\\\" id=\\\\\"\" + str(id) +\\n                        \"\\\\\" msg=\\\\\"\" + escape(str(message)) + \"\\\\\"/>\\\\n\")', \"def ReadCompilerReportFile():\\n    file = open('clang_compiler_report_formatted', 'r')\\n    messages_xml = file.readlines()\\n    return messages_xml\", 'def WriteSonarRulesToOutputFile():\\n    item_list = clang_compiler_report\\n    for item in item_list:\\n        ConvertDiagnosticLineToSonqarqube(item)']}, {'features': [], 'snippets': ['def to_bool(val):\\n  if val is None:\\n    return false\\n  return val == 1', 'def to_int(val):\\n  return int(val)', 'def to_str(val):\\n  return val', 'def newExec(self):\\n    pass', 'def newImage(self, row=0, col=0, filename=\"\"):\\n    print(\"%d.%d > %s\" % (row, col, filename))', 'def updLine(self, row, tmpLine):\\n    #print(\"--- %d ---\" % row)\\n    pass', 'def newFinal(self, name):\\n    pass', 'def finished(self, name):\\n    print(\"==========\")', 'def checkPause(self):\\n    pass', 'def get_next_file_vfs():\\n  global previous\\n  if previous is not None:\\n    try:\\n      os.unlink(previous)\\n    except OSerror:\\n      pass', 'def get_file_details(filename):\\n  try:\\n    link = filename\\n    try:\\n      link = os.readlink(filename)\\n    except OSError:\\n      pass\\n    link = pipes.quote(link)\\n    names = link[link.index(\"/miniatures/\" if not PARAMS[\"NO_SWITCH_TO_MINI\"] else \"/images\"):].split(\"/\")[2:]\\n    theme, year, album, fname = names', 'def __init__(self, randomize):\\n    self.idx = 0\\n    self.files = os.listdir(PARAMS[\"PATH\"])', 'def get_next_file(self):\\n    to_return = self.files[self.idx]', 'def get_file_details_dir(filename):\\n  return filename[filename.rindex(\"/\")+1:]', 'def do_append(first, second, underneath=False):\\n  sign = \"-\" if underneath else \"+\"\\n  background = \"-background black\" if PARAMS[\"DO_POLAROID\"] else \"\"\\n  command = \"convert -gravity center %s %sappend %s %s %s\" % (background, sign, first, second, first)\\n  ret = subprocess.call(command, shell=True)', 'def do_polaroid (image, filename=None, background=\"black\", suffix=None):\\n  if suffix is None:\\n    suffix = PARAMS[\"IMG_FORMAT_SUFFIX\"]\\n  tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\\n  tmp.close()\\n  image.save(filename=tmp.name)', 'def do_blank_image(height, width, filename, color=\"black\"):\\n  command = \"convert -size %dx%d xc:%s %s\" % (width, height, color, filename)\\n\\n  ret = subprocess.call(command, shell=True)\\n\\n  if ret != 0:\\n    raise Exception(\"Command failed: \"+ command)', 'def photowall(name):\\n  output_final = None\\n\\n  previous_filename = None\\n  #for all the rows, \\n  for row in range(PARAMS[\"LINES\"]):    \\n    output_row = None\\n    row_width = 0\\n    #concatenate until the image width is reached\\n    img_count = 0\\n    while row_width < PARAMS[\"WIDTH\"]:\\n      # get a new file, or the end of the previous one, if it was split\\n      filename = get_next_file() if previous_filename is None else previous_filename\\n      mimetype = None\\n      previous_filename = None', 'def random_wall(real_target_filename):\\n  name = real_target_filename\\n  filename = name[name.rindex(\"/\"):]\\n  name = filename[:filename.index(\".\")]\\n  ext = filename[filename.index(\".\"):]\\n  target_filename = tempfile.gettempdir()+\"/\"+name+\".2\"+ext', 'def path_is_jnetfs(path):\\n  #check if PATH is VFS or not\\n\\n  df_output_lines = os.popen(\"df -Ph \\'%s\\'\" % path).read().splitlines()', 'def fix_args():\\n  global get_next_file', 'def do_main():\\n  fix_args()']}, {'features': [], 'snippets': ['def get_test_client(nowait=False):\\n    client = get_es_connection(\\'default\\')\\n\\n    # wait for yellow status\\n    for _ in range(1 if nowait else 5):\\n        try:\\n            client.cluster.health(wait_for_status=\"yellow\")\\n            return client\\n        except ConnectionError:\\n            time.sleep(0.1)\\n    else:\\n        # timeout\\n        raise SkipTest(\"Elasticsearch failed to start\")', 'def _get_client():\\n        return get_test_client()', 'def setUpClass(cls):\\n        if cls._overridden_settings:\\n            cls._cls_overridden_context = override_settings(**cls._overridden_settings)\\n            cls._cls_overridden_context.enable()\\n\\n        connections.configure(**settings.ELASTICSEARCH_CONNECTIONS)\\n        cls.es_client = cls._get_client()\\n\\n        IngestClient(cls.es_client).put_pipeline(id=\\'ingest_attachment\\', body={\\n            \\'description\\': \"Extract attachment information\",\\n            \\'processors\\': [\\n                {\\n                    \"attachment\": {\\n                        \"field\": \"data\",\\n                        \"indexed_chars\": \"-1\"\\n                    },\\n                    \"remove\": {\\n                        \"field\": \"data\"\\n                    }\\n                }\\n            ]\\n        })\\n\\n        super().setUpClass()', 'def tearDown(self):\\n        self.es_client.indices.delete(index=\"*\", ignore=404)\\n        self.es_client.indices.delete_template(name=\"*\", ignore=404)', \"def setUpTestData(cls):\\n        cls.url = reverse('search-list')\\n        Feature.objects.create(name='archival descriptions', enabled=True)\\n        cls.user = User.objects.create()\\n        permission = Permission.objects.get(codename='search')\\n        cls.user.user_permissions.add(permission)\\n\\n        org_group_type = GroupType.objects.create(codename='organization')\\n\\n        cls.group1 = Group.objects.create(name='group1', group_type=org_group_type)\\n        cls.group1.add_member(cls.user.essauth_member)\\n\\n        cls.group2 = Group.objects.create(name='group2', group_type=org_group_type)\\n        cls.group2.add_member(cls.user.essauth_member)\\n\\n        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)\\n        cls.archive_type = TagVersionType.objects.create(name='archive', archive_type=True)\", \"def create_agent():\\n        return Agent.objects.create(\\n            type=AgentType.objects.create(main_type=MainAgentType.objects.create()),\\n            ref_code=RefCode.objects.create(\\n                country=Country.objects.get(iso='SE'),\\n                repository_code='repo',\\n            ),\\n            level_of_detail=0,\\n            record_status=0,\\n            script=0,\\n            language=Language.objects.get(iso_639_1='sv'),\\n            create_date=timezone.now(),\\n        )\", 'def test_filter_on_component_agent(self):\\n        agent = self.create_agent()\\n\\n        component_tag = Tag.objects.create()\\n        component_tag_version = TagVersion.objects.create(\\n            tag=component_tag,\\n            type=self.component_type,\\n            elastic_index=\"component\",\\n        )\\n\\n        structure_type = StructureType.objects.create()\\n        structure_template = Structure.objects.create(type=structure_type, is_template=True)\\n\\n        archive_tag = Tag.objects.create()\\n        archive_tag_version = TagVersion.objects.create(\\n            tag=archive_tag,\\n            type=self.archive_type,\\n            elastic_index=\"archive\",\\n        )\\n        structure, archive_tag_structure = structure_template.create_template_instance(archive_tag)\\n        Archive.from_obj(archive_tag_version).save(refresh=\\'true\\')\\n\\n        TagStructure.objects.create(tag=component_tag, parent=archive_tag_structure, structure=structure)\\n\\n        AgentTagLink.objects.create(\\n            agent=agent,\\n            tag=component_tag_version,\\n            type=AgentTagLinkRelationType.objects.create(),\\n        )\\n        Component.from_obj(component_tag_version).save(refresh=\\'true\\')\\n\\n        res = self.client.get(self.url, {\\'agents\\': str(agent.pk)})\\n        self.assertEqual(res.status_code, status.HTTP_200_OK)\\n        self.assertEqual(len(res.data[\\'hits\\']), 1)\\n        self.assertEqual(res.data[\\'hits\\'][0][\\'_id\\'], str(component_tag_version.pk))', 'def test_filter_appraisal_date(self):\\n        component_tag = Tag.objects.create(appraisal_date=make_aware(datetime(year=2020, month=1, day=1)))\\n        component_tag_version = TagVersion.objects.create(\\n            tag=component_tag,\\n            type=self.component_type,\\n            elastic_index=\"component\",\\n        )\\n        doc = Component.from_obj(component_tag_version)\\n        doc.save(refresh=\\'true\\')\\n\\n        with self.subTest(\\'2020-01-01 is after or equal to 2020-01-01\\'):\\n            res = self.client.get(self.url, data={\\'appraisal_date_after\\': \\'2020-01-01\\'})\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 1)\\n\\n        with self.subTest(\\'2020-01-01 not after 2020-01-02\\'):\\n            res = self.client.get(self.url, data={\\'appraisal_date_after\\': \\'2020-01-02\\'})\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 0)\\n\\n        with self.subTest(\\'2020-01-01 not before 2019-12-31\\'):\\n            res = self.client.get(self.url, data={\\'appraisal_date_before\\': \\'2019-12-31\\'})\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 0)\\n\\n        with self.subTest(\\'2020-01-01 between 2019-01-01 and 2020-01-01\\'):\\n            res = self.client.get(self.url, data={\\n                \\'appraisal_date_after\\': \\'2019-01-01\\',\\n                \\'appraisal_date_before\\': \\'2020-01-01\\',\\n            })\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 1)\\n\\n        with self.subTest(\\'2020-01-01 between 2020-01-01 and 2020-12-31\\'):\\n            res = self.client.get(self.url, data={\\n                \\'appraisal_date_after\\': \\'2020-01-01\\',\\n                \\'appraisal_date_before\\': \\'2020-12-31\\',\\n            })\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 1)\\n\\n        with self.subTest(\\'2020-01-01 not between 2020-01-02 and 2020-12-31\\'):\\n            res = self.client.get(self.url, data={\\n                \\'appraisal_date_after\\': \\'2020-01-02\\',\\n                \\'appraisal_date_before\\': \\'2020-12-31\\',\\n            })\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 0)\\n\\n        with self.subTest(\\'2020-01-01 not between 2019-01-01 and 2019-12-31\\'):\\n            res = self.client.get(self.url, data={\\n                \\'appraisal_date_after\\': \\'2019-01-01\\',\\n                \\'appraisal_date_before\\': \\'2019-12-31\\',\\n            })\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 0)\\n\\n        with self.subTest(\\'invalid range 2020-12-31 - 2020-01-01\\'):\\n            res = self.client.get(self.url, data={\\n                \\'appraisal_date_after\\': \\'2020-12-31\\',\\n                \\'appraisal_date_before\\': \\'2020-01-01\\',\\n            })\\n            self.assertEqual(res.status_code, status.HTTP_400_BAD_REQUEST)', \"def setUpTestData(cls):\\n        cls.url = reverse('search-list')\\n        Feature.objects.create(name='archival descriptions', enabled=True)\\n\\n        org_group_type = GroupType.objects.create(codename='organization')\\n        cls.group = Group.objects.create(group_type=org_group_type)\\n        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)\\n        cls.archive_type = TagVersionType.objects.create(name='archive', archive_type=True)\", 'def test_search_document_in_ip_with_other_user_responsible_without_permission_to_see_it(self):\\n        other_user = User.objects.create(username=\\'other\\')\\n        self.group.add_member(other_user.essauth_member)\\n\\n        ip = InformationPackage.objects.create(responsible=other_user)\\n        self.group.add_object(ip)\\n\\n        document_tag = Tag.objects.create(information_package=ip)\\n        document_tag_version = TagVersion.objects.create(\\n            tag=document_tag,\\n            type=self.component_type,\\n            elastic_index=\"document\",\\n        )\\n        File.from_obj(document_tag_version).save(refresh=\\'true\\')\\n\\n        res = self.client.get(self.url)\\n        self.assertEqual(res.status_code, status.HTTP_200_OK)\\n        self.assertEqual(len(res.data[\\'hits\\']), 0)', \"def setUpTestData(cls):\\n        cls.url = reverse('search-list')\\n        Feature.objects.create(name='archival descriptions', enabled=True)\\n        cls.component_type = TagVersionType.objects.create(name='component', archive_type=False)\\n        cls.security_levels = [1, 2, 3, 4, 5]\", 'def test_user_with_no_security_level(self):\\n        component_tag = Tag.objects.create()\\n        component_tag_version = TagVersion.objects.create(\\n            tag=component_tag,\\n            type=self.component_type,\\n            elastic_index=\"component\",\\n            security_level=None,\\n        )\\n        Component.from_obj(component_tag_version).save(refresh=\\'true\\')\\n\\n        with self.subTest(\\'no security level\\'):\\n            res = self.client.get(self.url)\\n            self.assertEqual(res.status_code, status.HTTP_200_OK)\\n            self.assertEqual(len(res.data[\\'hits\\']), 1)\\n            self.assertEqual(res.data[\\'hits\\'][0][\\'_id\\'], str(component_tag_version.pk))\\n\\n        for lvl in self.security_levels[1:]:\\n            with self.subTest(f\\'security level {lvl}\\'):\\n                component_tag_version.security_level = lvl\\n                component_tag_version.save()\\n                Component.from_obj(component_tag_version).save(refresh=\\'true\\')\\n\\n                res = self.client.get(self.url)\\n                self.assertEqual(res.status_code, status.HTTP_200_OK)\\n                self.assertEqual(len(res.data[\\'hits\\']), 0)']}, {'features': [], 'snippets': ['def __init__(self, config_node, private=False):\\n        assert isinstance(config_node, EnkfConfigNode)\\n\\n        if private:\\n            c_pointer = EnkfNode.cNamespace().alloc_private(config_node)\\n        else:\\n            c_pointer = EnkfNode.cNamespace().alloc(config_node)\\n\\n        super(EnkfNode, self).__init__(c_pointer, config_node, True)', 'def asGenData(self):\\n        \"\"\" @rtype: GenData \"\"\"\\n        impl_type = EnkfNode.cNamespace().get_impl_type(self)\\n        assert impl_type == ErtImplType.GEN_DATA\\n\\n        return GenData.createCReference(self.valuePointer(), self)', 'def asCustomKW(self):\\n        \"\"\" @rtype: CustomKW \"\"\"\\n        impl_type = EnkfNode.cNamespace().get_impl_type(self)\\n        assert impl_type == ErtImplType.CUSTOM_KW\\n\\n        return CustomKW.createCReference(self.valuePointer(), self)', 'def name(self):\\n        return EnkfNode.cNamespace().get_name(self)', 'def save(self, fs, node_id):\\n        assert isinstance(fs, EnkfFs)\\n        assert isinstance(node_id, NodeId)', 'def free(self):\\n        EnkfNode.cNamespace().free(self)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_hdf(comm):\\n\\n    import h5py\\n\\n    # fake structured array\\n    dset = numpy.empty(1024, dtype=[(\\'Position\\', (\\'f8\\', 3)), (\\'Mass\\', \\'f8\\')])\\n    dset[\\'Position\\'] = numpy.random.random(size=(1024, 3))\\n    dset[\\'Mass\\'] = numpy.random.random(size=1024)\\n\\n    tmpfile = tempfile.mkstemp()[1]\\n\\n    with h5py.File(tmpfile , \\'w\\') as ff:\\n        ds = ff.create_dataset(\\'X\\', data=dset) # store structured array as dataset\\n        ds.attrs[\\'BoxSize\\'] = 1.0\\n        grp = ff.create_group(\\'Y\\')\\n        grp.create_dataset(\\'Position\\', data=dset[\\'Position\\']) # column as dataset\\n        grp.create_dataset(\\'Mass\\', data=dset[\\'Mass\\']) # column as dataset\\n\\n    cosmo = cosmology.Planck15\\n\\n    source = HDFCatalog(tmpfile, dataset=\\'X\\', attrs={\"Nmesh\":32}, comm=comm)\\n    assert_allclose(source[\\'Position\\'], dset[\\'Position\\'])\\n\\n    region = source.query_range(32, 64)\\n    assert_allclose(region[\\'Position\\'], dset[\\'Position\\'][32:64])\\n\\n    os.unlink(tmpfile)', 'def test_query_range(comm):\\n\\n    import h5py\\n\\n    # fake structured array\\n    dset = numpy.empty(1024, dtype=[(\\'Position\\', (\\'f8\\', 3)), (\\'Mass\\', \\'f8\\'), (\\'Index\\', \\'i8\\')])\\n    dset[\\'Index\\'] = numpy.arange(1024)\\n    dset[\\'Position\\'] = numpy.random.random(size=(1024, 3))\\n    dset[\\'Mass\\'] = numpy.random.random(size=1024)\\n\\n    if comm.rank == 0:\\n        tmpfile = tempfile.mkstemp()[1]\\n\\n        with h5py.File(tmpfile , \\'w\\') as ff:\\n            ds = ff.create_dataset(\\'X\\', data=dset) # store structured array as dataset\\n            ds.attrs[\\'BoxSize\\'] = 1.0\\n\\n        tmpfile = comm.bcast(tmpfile)\\n    else:\\n        tmpfile = comm.bcast(None)\\n\\n    cosmo = cosmology.Planck15\\n\\n    source = HDFCatalog(tmpfile, dataset=\\'X\\', attrs={\"Nmesh\":32}, comm=comm)\\n\\n    correct_region = source.gslice(32, 64)\\n    region = source.query_range(32, 64)\\n\\n    assert_allclose(\\n        numpy.concatenate(comm.allgather(region[\\'Index\\'].compute())), \\n        numpy.arange(32, 64)\\n    )\\n\\n    if comm.rank == 0:\\n        os.unlink(tmpfile)', \"def test_csv(comm):\\n\\n    with tempfile.NamedTemporaryFile() as ff:\\n\\n        # generate data\\n        data = numpy.random.random(size=(100,5))\\n        numpy.savetxt(ff, data, fmt='%.7e'); ff.seek(0)\\n\\n        # read nrows\\n        names =['a', 'b', 'c', 'd', 'e']\\n        f = CSVCatalog(ff.name, names, blocksize=100, comm=comm)\\n\\n        # make sure data is the same\\n        for i, name in enumerate(names):\\n            numpy.testing.assert_almost_equal(data[:,i], f[name].compute(), decimal=7)\\n\\n        # make sure all the columns are there\\n        assert all(col in f for col in names)\", \"def test_stack_glob(comm):\\n\\n    tmpfile1 = 'test-glob-1.dat'\\n    tmpfile2 = 'test-glob-2.dat'\\n\\n    # generate data\\n    data = numpy.random.random(size=(100,5))\\n    numpy.savetxt(tmpfile1, data, fmt='%.7e')\\n    numpy.savetxt(tmpfile2, data, fmt='%.7e')\\n\\n    # read using a glob\\n    names =['a', 'b', 'c', 'd', 'e']\\n    f = CSVCatalog('test-glob-*', names, blocksize=100, comm=comm)\\n\\n    # make sure print works\\n    print(f)\\n\\n    # make sure data is the same\\n    fulldata = numpy.concatenate([data, data], axis=0)\\n    for i, name in enumerate(names):\\n        numpy.testing.assert_almost_equal(fulldata[:,i], f[name].compute(), decimal=7)\\n\\n    # make sure all the columns are there\\n    assert all(col in f for col in names)\\n\\n    os.unlink(tmpfile1)\\n    os.unlink(tmpfile2)\"]}, {'features': [], 'snippets': ['def translate_unity(unity):\\n    return UNITIES.get(unity, UNITIES[\"NONE\"])', 'def upgrade():\\n    from autonomie.models.task import WorkUnit\\n    from autonomie.models.task.estimation import EstimationLine\\n    from autonomie.models.task.invoice import InvoiceLine\\n    from autonomie.models.task.invoice import CancelInvoiceLine\\n    from autonomie_base.models.base import DBSESSION\\n    # Adding some characters to the Lines\\n    for table in \"estimation_line\", \"invoice_line\", \"cancelinvoice_line\":\\n        op.alter_column(table, \"unity\", type_=sa.String(100))\\n\\n    for value in UNITS:\\n        unit = WorkUnit(label=value)\\n        DBSESSION().add(unit)\\n    for factory in (EstimationLine, InvoiceLine, CancelInvoiceLine):\\n        for line in factory.query():\\n            line.unity = translate_unity(line.unity)\\n            DBSESSION().merge(line)']}, {'features': [], 'snippets': ['def __init__(self, name, lo, iq_awg: IQAWG, sa, calibration_db_name=\"IQVG\",\\n                 default_calibration_power=-30, marker_period_divisor=None,\\n                 slave_iqvgs=None, calibration_step=10e6):\\n        \"\"\"\\n\\n        Parameters\\n        ----------\\n        lo\\n        iq_awg\\n        sa\\n        calibration_db_name\\n        default_calibration_power\\n        marker_period_divisor: int, ns\\n            by default, the marker period should be divisible by the if_period\\n            however, in some cases other divisor may be required, i.e. when\\n            m3202 is used with PXICLK10 trigger sync mode this divisor\\n            should be set to 100\\n        \"\"\"\\n        self._name = name\\n        self._lo = lo\\n        self._iqawg = iq_awg\\n        self._sa = sa\\n        self._cal_db_name = calibration_db_name\\n        self._default_calibration_power = default_calibration_power\\n        self._calibration_widget = widgets.HTML()\\n        self._recalibrate_mixer = False\\n        self._frequency = 5e9\\n        self.set_if_frequency(100e6)\\n        if marker_period_divisor is not None:\\n            self._marker_period_divisor = marker_period_divisor\\n        else:\\n            self._marker_period_divisor = self._if_period\\n        # for marker period synchronization when iqvgs are on the same AWG\\n        self._slave_iqvgs = slave_iqvgs if slave_iqvgs is not None else []\\n\\n        self._power = default_calibration_power\\n        self._dac_overridden = False\\n        self._current_cal = None\\n        self._requested_cal: lib.iq_mixer_calibration.IQCalibrationData = None\\n        self._cal_db = None\\n\\n        self._marker_period = None\\n        self._requested_marker_period = None\\n        self.set_marker_period(1000)\\n        self._calibration_initial_guess = {\"dc_offsets\": np.random.uniform(.03, 0.1, size=2),\\n                                           \"if_amplitudes\": (.1, .1),\\n                                           \"if_phase\": -np.pi * 0.54}\\n        self._calibration_step = calibration_step\\n        self._calibration_test_data = []\\n        self._load_cal_db()', 'def set_parameters(self, parameters_dict):\\n\\n        if \"power\" in parameters_dict:\\n            self.set_power(parameters_dict[\"power\"])\\n\\n        if \"freq\" in parameters_dict:\\n            self.set_frequency(parameters_dict[\"freq\"])\\n\\n        if \"dac_overridden\" in parameters_dict:\\n            self._dac_overridden = parameters_dict[\"dac_overridden\"]\\n        else:\\n            self._dac_overridden = False', 'def set_if_frequency(self, if_frequency):\\n        self._if_frequency = if_frequency\\n        self._if_period = 1 / if_frequency * 1e9  # ns', 'def set_output_state(self, state):\\n        self._lo.set_output_state(state)', 'def set_power(self, power):\\n        if power > self._default_calibration_power + 10:\\n            raise ValueError(\"Power can be % dBm max, requested %d dBm\" % (\\n                self._default_calibration_power + 10, power))\\n\\n        self._power = power\\n        self._requested_cal = self.get_calibration(self._frequency,\\n                                                   self._power)\\n        self._lo.set_power(self._requested_cal.get_lo_power())\\n        self._output_SSB()', \"def set_marker_period(self, marker_period):\\n        '''\\n        For some applications there is need to control the length of the interval between triggers\\n        output by the AWG of the IQVectorGenerator.\\n\\n        Parameters\\n        ----------\\n        marker_period: ns, float\\n            real trigger period will be recalculated to be not shorter than <marker_period> ns,\\n            but still divisible by the IF period\\n        '''\\n        self._requested_marker_period = marker_period\\n        correct_marker_period = ceil(\\n            marker_period / self._marker_period_divisor) * \\\\\\n                                self._marker_period_divisor\\n\\n        if correct_marker_period != self._marker_period:\\n            self._marker_period = correct_marker_period\\n            if self._requested_cal is not None:\\n                self._current_cal = None\\n                self._output_SSB()\\n\\n        for slave_iqvg in self._slave_iqvgs:\\n            slave_iqvg.set_marker_period(self._marker_period)\", 'def _load_cal_db(self):\\n        self._cal_db = load_IQMX_calibration_database(self._cal_db_name, 0)', 'def get_calibration(self, frequency, power):\\n        frequency = self._around_frequency(frequency)\\n        # frequency = round(frequency/self._calibration_step)*self._calibration_step\\n\\n        if self._cal_db is None:\\n            self._load_cal_db()\\n\\n        cal = \\\\\\n            self._cal_db.get(frozenset(dict(lo_power=14,\\n                                            ssb_power=self._default_calibration_power,\\n                                            lo_frequency=self._if_frequency + frequency,\\n                                            if_frequency=self._if_frequency,\\n                                            waveform_resolution=1,\\n                                            sideband_to_maintain=\\'left\\').items()))\\n        if (cal is None) or self._recalibrate_mixer:\\n            calibrator = IQCalibrator(self._iqawg, self._sa, self._lo,\\n                                      self._cal_db_name, 0,\\n                                      sidebands_to_suppress=6,\\n                                      output_widget=self._calibration_widget)\\n            ig = self._calibration_initial_guess\\n            cal = calibrator.calibrate(\\n                lo_frequency=frequency + self._if_frequency,\\n                if_frequency=self._if_frequency,\\n                lo_power=14,\\n                ssb_power=self._default_calibration_power,\\n                waveform_resolution=1,\\n                iterations=3,\\n                minimize_iterlimit=100,\\n                sa_res_bandwidth=300,\\n                initial_guess=ig)\\n            save_IQMX_calibration(cal)\\n\\n            self._load_cal_db()  # make sure to include new calibration into cache\\n            cal._ssb_power = power\\n            cal._if_amplitudes = cal._if_amplitudes / np.sqrt(\\n                10 ** ((self._default_calibration_power - power) / 10))\\n            # self._calibration_initial_guess[\"if_amplitudes\"] = cal._if_amplitudes\\n            self._calibration_initial_guess[\"if_phase\"] = cal._if_phase\\n            return cal\\n        else:\\n            cal = cal.copy()\\n            cal._if_amplitudes = cal._if_amplitudes / np.sqrt(\\n                10 ** ((self._default_calibration_power - power) / 10))\\n            return cal']}, {'features': [], 'snippets': ['def main():\\n    module = KatelloEntityAnsibleModule(\\n        argument_spec=dict(\\n            manifest_path=dict(type=\\'path\\'),\\n            state=dict(default=\\'present\\', choices=[\\'absent\\', \\'present\\', \\'refreshed\\']),\\n            repository_url=dict(aliases=[\\'redhat_repository_url\\']),\\n        ),\\n        foreman_spec=dict(\\n            organization=dict(type=\\'entity\\', required=True, thin=False),\\n        ),\\n        required_if=[\\n            [\\'state\\', \\'present\\', [\\'manifest_path\\']],\\n        ],\\n        supports_check_mode=False,\\n    )\\n\\n    module.task_timeout = 5 * 60\\n\\n    with module.api_connection():\\n        organization = module.lookup_entity(\\'organization\\')\\n        scope = module.scope_for(\\'organization\\')\\n\\n        try:\\n            existing_manifest = organization[\\'owner_details\\'][\\'upstreamConsumer\\']\\n        except KeyError:\\n            existing_manifest = None\\n\\n        if module.state == \\'present\\':\\n            if \\'repository_url\\' in module.foreman_params:\\n                payload = {\\'redhat_repository_url\\': module.foreman_params[\\'repository_url\\']}\\n                org_spec = dict(id=dict(), redhat_repository_url=dict())\\n                organization = module.ensure_entity(\\'organizations\\', payload, organization, state=\\'present\\', foreman_spec=org_spec)\\n\\n            try:\\n                with open(module.foreman_params[\\'manifest_path\\'], \\'rb\\') as manifest_file:\\n                    files = {\\'content\\': (module.foreman_params[\\'manifest_path\\'], manifest_file, \\'application/zip\\')}\\n                    params = {}\\n                    if \\'repository_url\\' in module.foreman_params:\\n                        params[\\'repository_url\\'] = module.foreman_params[\\'repository_url\\']\\n                    params.update(scope)\\n                    result = module.resource_action(\\'subscriptions\\', \\'upload\\', params, files=files, record_change=False, ignore_task_errors=True)\\n                    for error in result[\\'humanized\\'][\\'errors\\']:\\n                        if \"same as existing data\" in error:\\n                            # Nothing changed, but everything ok\\n                            break\\n                        if \"older than existing data\" in error:\\n                            module.fail_json(msg=\"Manifest is older than existing data.\")\\n                        else:\\n                            module.fail_json(msg=\"Upload of the manifest failed: %s\" % error)\\n                    else:\\n                        module.set_changed()\\n            except IOError as e:\\n                module.fail_json(msg=\"Unable to read the manifest file: %s\" % e)\\n        elif module.desired_absent and existing_manifest:\\n            module.resource_action(\\'subscriptions\\', \\'delete_manifest\\', scope)\\n        elif module.state == \\'refreshed\\':\\n            if existing_manifest:\\n                module.resource_action(\\'subscriptions\\', \\'refresh_manifest\\', scope)\\n            else:\\n                module.fail_json(msg=\"No manifest found to refresh.\")']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def read_counts(file):\\n    if not hasattr(file, \"read\"):\\n        file = open(file)', 'def add_count_attrib(counts, totals, cdb_lu_file):\\n    parser = iterparse(cdb_lu_file)']}, {'features': [], 'snippets': ['def loop():\\n    return asyncio.get_event_loop()', 'def evt():\\n    return event.build_event(\"event\")', 'def sample_plugin():\\n    class TestPlugin(BasePlugin):\\n        @event.event\\n        def on_test(self):\\n            pass\\n\\n    return TestPlugin', 'def test_type(self):\\n        assert isinstance(event.Priority.DEFAULT, int)', 'def test_lookup(self):\\n        assert event.Priority.lookup(event.Priority.CORE) is event.Priority.CORE\\n        assert event.Priority.lookup(event.Priority.CORE.value) is event.Priority.CORE\\n        assert event.Priority.lookup(-12312412) == -12312412', 'def test_build_event(self):\\n        evt = event.build_event(\"evt_name\", arg1=\"val1\", arg2=None)\\n        assert evt.name == \"evt_name\"\\n        assert evt.args == {\\'arg1\\': \"val1\", \\'arg2\\': None}', 'def test_bool(self):\\n        prio_set_list = event._PrioritizedSetList()\\n\\n        assert bool(prio_set_list) is False\\n\\n        prio_set_list.add(0, None)\\n        assert bool(prio_set_list) is True', 'def test_add_already_added(self):\\n        prio_set_list = event._PrioritizedSetList()\\n        obj = object()\\n        prio_set_list.add(0, obj)\\n\\n        with pytest.raises(ValueError) as excinfo:\\n            prio_set_list.add(0, obj)\\n        excinfo.match(r\"has already been added\")\\n\\n        with pytest.raises(ValueError) as excinfo:\\n            prio_set_list.add(1, obj)\\n        excinfo.match(r\"has already been added\")', 'def test_iter(self):\\n        prio_set_list = event._PrioritizedSetList()\\n        objs = [(i,) for i in range(5)]\\n        for i, obj in enumerate(objs):\\n            prio_set_list.add(-i, obj)\\n\\n        for i, set_ in enumerate(prio_set_list):\\n            assert set_ == (-i, {objs[i]})', 'def test_no_param_usage(self):\\n        @event.event\\n        def func_name(self):\\n            pass\\n\\n        @event.event\\n        def on_test(self):\\n            pass\\n\\n        assert hasattr(on_test, \\'_h_info\\')\\n        h_info = on_test._h_info\\n        assert h_info.event_name == \"test\"\\n        assert func_name._h_info.event_name == \"func_name\"\\n        assert h_info.handler is on_test\\n        assert h_info.priority is event.Priority.DEFAULT\\n        assert h_info.should_enable\\n        assert not h_info.is_async', 'def on_test(self):\\n            pass', \"def test_async_handler(self):\\n        @event.event(enable=False)\\n        async def on_async_test(self):\\n            pass\\n\\n        assert hasattr(on_async_test, '_h_info')\\n        h_info = on_async_test._h_info\\n        assert h_info.event_name == 'async_test'\\n        assert h_info.handler is on_async_test\\n        assert h_info.priority is event.Priority.DEFAULT\\n        assert not h_info.should_enable\\n        assert h_info.is_async\", 'def on_test(self):\\n            pass', \"def test_core_event_deco(self):\\n        @event.core_event\\n        def on_test(self):\\n            pass\\n\\n        assert hasattr(on_test, '_h_info')\\n        h_info = on_test._h_info\\n        assert h_info.priority is event.Priority.CORE\", 'def test_from_handler(self):\\n        @event.event\\n        def handler():\\n            pass\\n\\n        h_inst = event.HandlerInstance.from_handler(handler)\\n        assert h_inst.info is handler._h_info\\n        assert h_inst.enabled\\n        assert h_inst.handler is handler._h_info.handler', 'def func():\\n            pass', 'def test_hash(self):\\n        @event.event\\n        def handler():\\n            pass\\n\\n        h_inst = event.HandlerInstance.from_handler(handler)\\n        h_inst2 = event.HandlerInstance.from_handler(handler)\\n        assert h_inst is not h_inst2\\n        assert hash(h_inst) == hash(h_inst2)\\n        assert h_inst != h_inst2', 'def test_extend(self, evt, loop):\\n        async def corofunc():\\n            pass\\n\\n        coro = corofunc()\\n        coro2 = corofunc()\\n        # silence \"coroutine never awaited\" warnings\\n        loop.run_until_complete(coro)\\n        loop.run_until_complete(coro2)\\n\\n        rval = event.ReturnValue(append_events=[evt])\\n        rval2 = event.ReturnValue(eat=True, schedule={coro})\\n        rval3 = event.ReturnValue(append_events=[evt], insert_events=[evt],\\n                                  schedule={coro, coro2})\\n\\n        rset = event.ResultSet()\\n        rset2 = event.ResultSet()\\n\\n        rset.extend(rval)\\n        assert not rset.eat\\n        assert rset.append_events == [evt]\\n        rset.extend(rval2)\\n        assert rset.eat\\n        assert rset.schedule == {coro}\\n        rset2.extend(rval3)\\n        rset.extend(rset2)\\n        rset.extend(None)\\n        assert rset.eat\\n        assert rset.append_events == [evt, evt]\\n        assert rset.insert_events == [evt]\\n        assert rset.schedule == {coro, coro2}', 'def test_type(self):\\n        rset = event.ResultSet()\\n        with pytest.raises(NotImplementedError):\\n            rset.extend([])\\n        with pytest.raises(NotImplementedError):\\n            rset.extend(False)', 'def dispatcher(self):\\n        return event.EventDispatcher()', 'def test_register_plugin(self, dispatcher):\\n        name = \"some_name\"\\n\\n        class AClass:\\n            @event.event(name)\\n            def handler(self):\\n                pass\\n\\n            @event.event(name)\\n            async def hander(self):\\n                pass\\n\\n        obj = AClass()\\n        h_insts = dispatcher.register_plugin(obj)\\n        assert len(dispatcher.event_map) == 1\\n        assert len(h_insts) == 2\\n        for h_inst in h_insts:\\n            assert h_inst in dispatcher.event_map[name]', 'def test_dispatch_priority(self, dispatcher, loop, evt):\\n        called = list()\\n\\n        @event.event(evt.name, priority=0)\\n        async def corofunc():\\n            called.append(corofunc)\\n\\n        @event.event(evt.name, priority=1)\\n        def corofunc2():\\n            called.append(corofunc2)\\n\\n        h_inst = event.HandlerInstance.from_handler(corofunc)\\n        h_inst2 = event.HandlerInstance.from_handler(corofunc2)\\n        dispatcher.register(h_inst)\\n        dispatcher.register(h_inst2)\\n        loop.run_until_complete(dispatcher.dispatch(evt))\\n\\n        assert called == [corofunc2, corofunc]', 'def test_dispatch_exception(self, loop, evt):\\n        logger = mock.Mock(Logger)\\n        dispatcher = event.EventDispatcher(logger=logger)\\n        called = 0\\n\\n        @event.event(evt.name)\\n        async def corofunc():\\n            nonlocal called\\n            called += 1\\n            raise ValueError(\"yeah async\")\\n\\n        @event.event(evt.name)\\n        def handler():\\n            nonlocal called\\n            called += 1\\n            raise ValueError(\"yeah sync\")\\n\\n        dispatcher.register(event.HandlerInstance.from_handler(corofunc))\\n        dispatcher.register(event.HandlerInstance.from_handler(handler))\\n        assert not logger.exception.called\\n        loop.run_until_complete(dispatcher.dispatch(evt))\\n        assert called == 2\\n        assert logger.exception.call_count == 2', 'def test_dispatch_eat(self, loop, evt):\\n        dispatcher = event.EventDispatcher()\\n        called = [False] * 3\\n\\n        @event.event(evt.name, priority=1)\\n        def corofunc():\\n            called[0] = True\\n\\n        @event.event(evt.name, priority=0)\\n        async def corofunc2():\\n            called[1] = True\\n            return event.ReturnValue(eat=True)\\n\\n        @event.event(evt.name, priority=-1)\\n        async def corofunc3():\\n            called[2] = True\\n\\n        dispatcher.register(event.HandlerInstance.from_handler(corofunc))\\n        dispatcher.register(event.HandlerInstance.from_handler(corofunc2))\\n        dispatcher.register(event.HandlerInstance.from_handler(corofunc3))\\n        result = loop.run_until_complete(dispatcher.dispatch(evt))\\n        assert result.eat\\n        assert called == [True, True, False]', 'def corofunc1():\\n            called[0] += 1\\n            return event.ReturnValue(insert_events=[evt2], append_events=[evt])', 'def corofunc2():\\n            called[1] += 1\\n            return event.ReturnValue(insert_events=[evt3], append_events=[evt2])', 'def corofunc3():\\n            called[2] += 1\\n\\n            async def corofunc():\\n                pass\\n\\n            return event.ReturnValue(append_events=[evt3], schedule={corofunc()})']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def pytest_addoption(parser):\\n    \"\"\"Add CLI options related to Testimony token based mark collection\"\"\"\\n    parser.addoption(\\n        \\'--importance\\',\\n        help=\\'Comma separated list of importance levels to include in test collection\\',\\n    )\\n    parser.addoption(\\n        \\'--component\\',\\n        help=\\'Comma separated list of component names to include in test collection\\',\\n    )\\n    parser.addoption(\\n        \\'--assignee\\',\\n        help=\\'Comma separated list of assignees to include in test collection\\',\\n    )']}, {'features': [], 'snippets': ['def __init__(self, text):\\n        self.text = text\\n        self.clicked = Signal()', 'def __init__(self, app):\\n        self._app = app\\n        self._items = []\\n        self.model = MyMusicModel(app)', 'def create_item(cls, text):\\n        return MyMusicItem(text)']}, {'features': [], 'snippets': ['def format_epilog(self, epilog):\\n        return epilog or \"\"', 'def format_description(self, description):\\n        return description.lstrip()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_page(url):\\n    \"\"\"Retrieve the given page.\"\"\"\\n    return urllib2.urlopen(url).read()', 'def get_all_coeffs():\\n    \"\"\"Get all available calibration coefficients for the satellites.\"\"\"\\n    coeffs = {}\\n\\n    for platform in URLS:\\n        if platform not in coeffs:\\n            coeffs[platform] = {}\\n        for chan in URLS[platform].keys():\\n            url = URLS[platform][chan]\\n            print(url)\\n            page = get_page(url)\\n            coeffs[platform][chan] = get_coeffs(page)\\n\\n    return coeffs', 'def main():\\n    \"\"\"Create calibration coefficient files for AVHRR.\"\"\"\\n    out_dir = sys.argv[1]\\n    coeffs = get_all_coeffs()\\n    save_coeffs(coeffs, out_dir=out_dir)']}, {'features': [], 'snippets': [\"def relocate_repo(module, result, repo_dir, old_repo_dir, worktree_dir):\\n    if os.path.exists(repo_dir):\\n        module.fail_json(msg='Separate-git-dir path %s already exists.' % repo_dir)\\n    if worktree_dir:\\n        dot_git_file_path = os.path.join(worktree_dir, '.git')\\n        try:\\n            shutil.move(old_repo_dir, repo_dir)\\n            with open(dot_git_file_path, 'w') as dot_git_file:\\n                dot_git_file.write('gitdir: %s' % repo_dir)\\n            result['git_dir_before'] = old_repo_dir\\n            result['git_dir_now'] = repo_dir\\n        except (IOError, OSError) as err:\\n            # if we already moved the .git dir, roll it back\\n            if os.path.exists(repo_dir):\\n                shutil.move(repo_dir, old_repo_dir)\\n            module.fail_json(msg=u'Unable to move git dir. %s' % to_text(err))\", 'def unfrackgitpath(path):\\n    if path is None:\\n        return None\\n\\n    # copied from ansible.utils.path\\n    return os.path.normpath(os.path.realpath(os.path.expanduser(os.path.expandvars(path))))', 'def write_ssh_wrapper(module_tmpdir):\\n    try:\\n        # make sure we have full permission to the module_dir, which\\n        # may not be the case if we\\'re sudo\\'ing to a non-root user\\n        if os.access(module_tmpdir, os.W_OK | os.R_OK | os.X_OK):\\n            fd, wrapper_path = tempfile.mkstemp(prefix=module_tmpdir + \\'/\\')\\n        else:\\n            raise OSError\\n    except (IOError, OSError):\\n        fd, wrapper_path = tempfile.mkstemp()\\n    fh = os.fdopen(fd, \\'w+b\\')\\n    template = b(\"\"\"#!/bin/sh', 'def set_git_ssh(ssh_wrapper, key_file, ssh_opts):\\n\\n    if os.environ.get(\"GIT_SSH\"):\\n        del os.environ[\"GIT_SSH\"]\\n    os.environ[\"GIT_SSH\"] = ssh_wrapper\\n\\n    if os.environ.get(\"GIT_KEY\"):\\n        del os.environ[\"GIT_KEY\"]\\n\\n    if key_file:\\n        os.environ[\"GIT_KEY\"] = key_file\\n\\n    if os.environ.get(\"GIT_SSH_OPTS\"):\\n        del os.environ[\"GIT_SSH_OPTS\"]\\n\\n    if ssh_opts:\\n        os.environ[\"GIT_SSH_OPTS\"] = ssh_opts', \"def ssh_supports_acceptnewhostkey(module):\\n    try:\\n        ssh_path = get_bin_path('ssh')\\n    except ValueError as err:\\n        module.fail_json(\\n            msg='Remote host is missing ssh command, so you cannot '\\n            'use acceptnewhostkey option.', details=to_text(err))\\n    supports_acceptnewhostkey = True\\n    cmd = [ssh_path, '-o', 'StrictHostKeyChecking=accept-new', '-V']\\n    rc, stdout, stderr = module.run_command(cmd)\\n    if rc != 0:\\n        supports_acceptnewhostkey = False\\n    return supports_acceptnewhostkey\", 'def clone(git_path, module, repo, dest, remote, depth, version, bare,\\n          reference, refspec, git_version_used, verify_commit, separate_git_dir, result, gpg_whitelist, single_branch):\\n    \\'\\'\\' makes a new git repo if it does not already exist \\'\\'\\'\\n    dest_dirname = os.path.dirname(dest)\\n    try:\\n        os.makedirs(dest_dirname)\\n    except Exception:\\n        pass\\n    cmd = [git_path, \\'clone\\']\\n\\n    if bare:\\n        cmd.append(\\'--bare\\')\\n    else:\\n        cmd.extend([\\'--origin\\', remote])\\n\\n    is_branch_or_tag = is_remote_branch(git_path, module, dest, repo, version) or is_remote_tag(git_path, module, dest, repo, version)\\n    if depth:\\n        if version == \\'HEAD\\' or refspec:\\n            cmd.extend([\\'--depth\\', str(depth)])\\n        elif is_branch_or_tag:\\n            cmd.extend([\\'--depth\\', str(depth)])\\n            cmd.extend([\\'--branch\\', version])\\n        else:\\n            # only use depth if the remote object is branch or tag (i.e. fetchable)\\n            module.warn(\"Ignoring depth argument. \"\\n                        \"Shallow clones are only available for \"\\n                        \"HEAD, branches, tags or in combination with refspec.\")\\n    if reference:\\n        cmd.extend([\\'--reference\\', str(reference)])\\n\\n    if single_branch:\\n        if git_version_used is None:\\n            module.fail_json(msg=\\'Cannot find git executable at %s\\' % git_path)\\n\\n        if git_version_used < LooseVersion(\\'1.7.10\\'):\\n            module.warn(\"git version \\'%s\\' is too old to use \\'single-branch\\'. Ignoring.\" % git_version_used)\\n        else:\\n            cmd.append(\"--single-branch\")\\n\\n            if is_branch_or_tag:\\n                cmd.extend([\\'--branch\\', version])\\n\\n    needs_separate_git_dir_fallback = False\\n    if separate_git_dir:\\n        if git_version_used is None:\\n            module.fail_json(msg=\\'Cannot find git executable at %s\\' % git_path)\\n        if git_version_used < LooseVersion(\\'1.7.5\\'):\\n            # git before 1.7.5 doesn\\'t have separate-git-dir argument, do fallback\\n            needs_separate_git_dir_fallback = True\\n        else:\\n            cmd.append(\\'--separate-git-dir=%s\\' % separate_git_dir)\\n\\n    cmd.extend([repo, dest])\\n    module.run_command(cmd, check_rc=True, cwd=dest_dirname)\\n    if needs_separate_git_dir_fallback:\\n        relocate_repo(module, result, separate_git_dir, os.path.join(dest, \".git\"), dest)\\n\\n    if bare and remote != \\'origin\\':\\n        module.run_command([git_path, \\'remote\\', \\'add\\', remote, repo], check_rc=True, cwd=dest)\\n\\n    if refspec:\\n        cmd = [git_path, \\'fetch\\']\\n        if depth:\\n            cmd.extend([\\'--depth\\', str(depth)])\\n        cmd.extend([remote, refspec])\\n        module.run_command(cmd, check_rc=True, cwd=dest)\\n\\n    if verify_commit:\\n        verify_commit_sign(git_path, module, dest, version, gpg_whitelist)', 'def reset(git_path, module, dest):\\n    \\'\\'\\'\\n    Resets the index and working tree to HEAD.\\n    Discards any changes to tracked files in working\\n    tree since that commit.\\n    \\'\\'\\'\\n    cmd = \"%s reset --hard HEAD\" % (git_path,)\\n    return module.run_command(cmd, check_rc=True, cwd=dest)', 'def get_remote_head(git_path, module, dest, version, remote, bare):\\n    cloning = False\\n    cwd = None\\n    tag = False\\n    if remote == module.params[\\'repo\\']:\\n        cloning = True\\n    elif remote == \\'file://\\' + os.path.expanduser(module.params[\\'repo\\']):\\n        cloning = True\\n    else:\\n        cwd = dest\\n    if version == \\'HEAD\\':\\n        if cloning:\\n            # cloning the repo, just get the remote\\'s HEAD version\\n            cmd = \\'%s ls-remote %s -h HEAD\\' % (git_path, remote)\\n        else:\\n            head_branch = get_head_branch(git_path, module, dest, remote, bare)\\n            cmd = \\'%s ls-remote %s -h refs/heads/%s\\' % (git_path, remote, head_branch)\\n    elif is_remote_branch(git_path, module, dest, remote, version):\\n        cmd = \\'%s ls-remote %s -h refs/heads/%s\\' % (git_path, remote, version)\\n    elif is_remote_tag(git_path, module, dest, remote, version):\\n        tag = True\\n        cmd = \\'%s ls-remote %s -t refs/tags/%s*\\' % (git_path, remote, version)\\n    else:\\n        # appears to be a sha1.  return as-is since it appears\\n        # cannot check for a specific sha1 on remote\\n        return version\\n    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=cwd)\\n    if len(out) < 1:\\n        module.fail_json(msg=\"Could not determine remote revision for %s\" % version, stdout=out, stderr=err, rc=rc)\\n\\n    out = to_native(out)\\n\\n    if tag:\\n        # Find the dereferenced tag if this is an annotated tag.\\n        for tag in out.split(\\'\\\\n\\'):\\n            if tag.endswith(version + \\'^{}\\'):\\n                out = tag\\n                break\\n            elif tag.endswith(version):\\n                out = tag\\n\\n    rev = out.split()[0]\\n    return rev', 'def get_branches(git_path, module, dest):\\n    branches = []\\n    cmd = \\'%s branch --no-color -a\\' % (git_path,)\\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\\n    if rc != 0:\\n        module.fail_json(msg=\"Could not determine branch data - received %s\" % out, stdout=out, stderr=err)\\n    for line in out.split(\\'\\\\n\\'):\\n        if line.strip():\\n            branches.append(line.strip())\\n    return branches', \"def is_remote_branch(git_path, module, dest, remote, version):\\n    cmd = '%s ls-remote %s -h refs/heads/%s' % (git_path, remote, version)\\n    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)\\n    if to_native(version, errors='surrogate_or_strict') in out:\\n        return True\\n    else:\\n        return False\", \"def is_not_a_branch(git_path, module, dest):\\n    branches = get_branches(git_path, module, dest)\\n    for branch in branches:\\n        if branch.startswith('* ') and ('no branch' in branch or 'detached from' in branch or 'detached at' in branch):\\n            return True\\n    return False\", 'def get_head_branch(git_path, module, dest, remote, bare=False):\\n    \\'\\'\\'\\n    Determine what branch HEAD is associated with.  This is partly\\n    taken from lib/ansible/utils/__init__.py.  It finds the correct\\n    path to .git/HEAD and reads from that file the branch that HEAD is\\n    associated with.  In the case of a detached HEAD, this will look\\n    up the branch in .git/refs/remotes/<remote>/HEAD.\\n    \\'\\'\\'\\n    try:\\n        repo_path = get_repo_path(dest, bare)\\n    except (IOError, ValueError) as err:\\n        # No repo path found\\n        \"\"\"``.git`` file does not have a valid format for detached Git dir.\"\"\"\\n        module.fail_json(\\n            msg=\\'Current repo does not have a valid reference to a \\'\\n            \\'separate Git dir or it refers to the invalid path\\',\\n            details=to_text(err),\\n        )\\n    # Read .git/HEAD for the name of the branch.\\n    # If we\\'re in a detached HEAD state, look up the branch associated with\\n    # the remote HEAD in .git/refs/remotes/<remote>/HEAD\\n    headfile = os.path.join(repo_path, \"HEAD\")\\n    if is_not_a_branch(git_path, module, dest):\\n        headfile = os.path.join(repo_path, \\'refs\\', \\'remotes\\', remote, \\'HEAD\\')\\n    branch = head_splitter(headfile, remote, module=module, fail_on_error=True)\\n    return branch', 'def set_remote_url(git_path, module, repo, dest, remote):\\n    \\'\\'\\' updates repo from remote sources \\'\\'\\'\\n    # Return if remote URL isn\\'t changing.\\n    remote_url = get_remote_url(git_path, module, dest, remote)\\n    if remote_url == repo or unfrackgitpath(remote_url) == unfrackgitpath(repo):\\n        return False\\n\\n    command = [git_path, \\'remote\\', \\'set-url\\', remote, repo]\\n    (rc, out, err) = module.run_command(command, cwd=dest)\\n    if rc != 0:\\n        label = \"set a new url %s for %s\" % (repo, remote)\\n        module.fail_json(msg=\"Failed to %s: %s %s\" % (label, out, err))\\n\\n    # Return False if remote_url is None to maintain previous behavior\\n    # for Git versions prior to 1.7.5 that lack required functionality.\\n    return remote_url is not None', 'def submodules_fetch(git_path, module, remote, track_submodules, dest):\\n    changed = False\\n\\n    if not os.path.exists(os.path.join(dest, \\'.gitmodules\\')):\\n        # no submodules\\n        return changed\\n\\n    gitmodules_file = open(os.path.join(dest, \\'.gitmodules\\'), \\'r\\')\\n    for line in gitmodules_file:\\n        # Check for new submodules\\n        if not changed and line.strip().startswith(\\'path\\'):\\n            path = line.split(\\'=\\', 1)[1].strip()\\n            # Check that dest/path/.git exists\\n            if not os.path.exists(os.path.join(dest, path, \\'.git\\')):\\n                changed = True\\n\\n    # Check for updates to existing modules\\n    if not changed:\\n        # Fetch updates\\n        begin = get_submodule_versions(git_path, module, dest)\\n        cmd = [git_path, \\'submodule\\', \\'foreach\\', git_path, \\'fetch\\']\\n        (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)\\n        if rc != 0:\\n            module.fail_json(msg=\"Failed to fetch submodules: %s\" % out + err)\\n\\n        if track_submodules:\\n            # Compare against submodule HEAD\\n            # FIXME: determine this from .gitmodules\\n            version = \\'master\\'\\n            after = get_submodule_versions(git_path, module, dest, \\'%s/%s\\' % (remote, version))\\n            if begin != after:\\n                changed = True\\n        else:\\n            # Compare against the superproject\\'s expectation\\n            cmd = [git_path, \\'submodule\\', \\'status\\']\\n            (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)\\n            if rc != 0:\\n                module.fail_json(msg=\\'Failed to retrieve submodule status: %s\\' % out + err)\\n            for line in out.splitlines():\\n                if line[0] != \\' \\':\\n                    changed = True\\n                    break\\n    return changed', 'def set_remote_branch(git_path, module, dest, remote, version, depth):\\n    \"\"\"set refs for the remote branch version\\n\\n    This assumes the branch does not yet exist locally and is therefore also not checked out.\\n    Can\\'t use git remote set-branches, as it is not available in git 1.7.1 (centos6)\\n    \"\"\"\\n\\n    branchref = \"+refs/heads/%s:refs/heads/%s\" % (version, version)\\n    branchref += \\' +refs/heads/%s:refs/remotes/%s/%s\\' % (version, remote, version)\\n    cmd = \"%s fetch --depth=%s %s %s\" % (git_path, depth, remote, branchref)\\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\\n    if rc != 0:\\n        module.fail_json(msg=\"Failed to fetch branch from remote: %s\" % version, stdout=out, stderr=err, rc=rc)', 'def verify_commit_sign(git_path, module, dest, version, gpg_whitelist):\\n    if version in get_annotated_tags(git_path, module, dest):\\n        git_sub = \"verify-tag\"\\n    else:\\n        git_sub = \"verify-commit\"\\n    cmd = \"%s %s %s\" % (git_path, git_sub, version)\\n    if gpg_whitelist:\\n        cmd += \" --raw\"\\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\\n    if rc != 0:\\n        module.fail_json(msg=\\'Failed to verify GPG signature of commit/tag \"%s\"\\' % version, stdout=out, stderr=err, rc=rc)\\n    if gpg_whitelist:\\n        fingerprint = get_gpg_fingerprint(err)\\n        if fingerprint not in gpg_whitelist:\\n            module.fail_json(msg=\\'The gpg_whitelist does not include the public key \"%s\" for this commit\\' % fingerprint, stdout=out, stderr=err, rc=rc)\\n    return (rc, out, err)', 'def git_version(git_path, module):\\n    \"\"\"return the installed version of git\"\"\"\\n    cmd = \"%s --version\" % git_path\\n    (rc, out, err) = module.run_command(cmd)\\n    if rc != 0:\\n        # one could fail_json here, but the version info is not that important,\\n        # so let\\'s try to fail only on actual git commands\\n        return None\\n    rematch = re.search(\\'git version (.*)$\\', to_native(out))\\n    if not rematch:\\n        return None\\n    return LooseVersion(rematch.groups()[0])', 'def create_archive(git_path, module, dest, archive, archive_prefix, version, repo, result):\\n    \"\"\" Helper function for creating archive using git_archive \"\"\"\\n    all_archive_fmt = {\\'.zip\\': \\'zip\\', \\'.gz\\': \\'tar.gz\\', \\'.tar\\': \\'tar\\',\\n                       \\'.tgz\\': \\'tgz\\'}\\n    _, archive_ext = os.path.splitext(archive)\\n    archive_fmt = all_archive_fmt.get(archive_ext, None)\\n    if archive_fmt is None:\\n        module.fail_json(msg=\"Unable to get file extension from \"\\n                             \"archive file name : %s\" % archive,\\n                         details=\"Please specify archive as filename with \"\\n                                 \"extension. File extension can be one \"\\n                                 \"of [\\'tar\\', \\'tar.gz\\', \\'zip\\', \\'tgz\\']\")\\n\\n    repo_name = repo.split(\"/\")[-1].replace(\".git\", \"\")\\n\\n    if os.path.exists(archive):\\n        # If git archive file exists, then compare it with new git archive file.\\n        # if match, do nothing\\n        # if does not match, then replace existing with temp archive file.\\n        tempdir = tempfile.mkdtemp()\\n        new_archive_dest = os.path.join(tempdir, repo_name)\\n        new_archive = new_archive_dest + \\'.\\' + archive_fmt\\n        git_archive(git_path, module, dest, new_archive, archive_fmt, archive_prefix, version)\\n\\n        # filecmp is supposed to be efficient than md5sum checksum\\n        if filecmp.cmp(new_archive, archive):\\n            result.update(changed=False)\\n            # Cleanup before exiting\\n            try:\\n                shutil.rmtree(tempdir)\\n            except OSError:\\n                pass\\n        else:\\n            try:\\n                shutil.move(new_archive, archive)\\n                shutil.rmtree(tempdir)\\n                result.update(changed=True)\\n            except OSError as e:\\n                module.fail_json(msg=\"Failed to move %s to %s\" %\\n                                     (new_archive, archive),\\n                                 details=u\"Error occurred while moving : %s\"\\n                                         % to_text(e))\\n    else:\\n        # Perform archive from local directory\\n        git_archive(git_path, module, dest, archive, archive_fmt, archive_prefix, version)\\n        result.update(changed=True)', 'def main():\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            dest=dict(type=\\'path\\'),\\n            repo=dict(required=True, aliases=[\\'name\\']),\\n            version=dict(default=\\'HEAD\\'),\\n            remote=dict(default=\\'origin\\'),\\n            refspec=dict(default=None),\\n            reference=dict(default=None),\\n            force=dict(default=\\'no\\', type=\\'bool\\'),\\n            depth=dict(default=None, type=\\'int\\'),\\n            clone=dict(default=\\'yes\\', type=\\'bool\\'),\\n            update=dict(default=\\'yes\\', type=\\'bool\\'),\\n            verify_commit=dict(default=\\'no\\', type=\\'bool\\'),\\n            gpg_whitelist=dict(default=[], type=\\'list\\', elements=\\'str\\'),\\n            accept_hostkey=dict(default=\\'no\\', type=\\'bool\\'),\\n            accept_newhostkey=dict(default=\\'no\\', type=\\'bool\\'),\\n            key_file=dict(default=None, type=\\'path\\', required=False),\\n            ssh_opts=dict(default=None, required=False),\\n            executable=dict(default=None, type=\\'path\\'),\\n            bare=dict(default=\\'no\\', type=\\'bool\\'),\\n            recursive=dict(default=\\'yes\\', type=\\'bool\\'),\\n            single_branch=dict(default=False, type=\\'bool\\'),\\n            track_submodules=dict(default=\\'no\\', type=\\'bool\\'),\\n            umask=dict(default=None, type=\\'raw\\'),\\n            archive=dict(type=\\'path\\'),\\n            archive_prefix=dict(),\\n            separate_git_dir=dict(type=\\'path\\'),\\n        ),\\n        mutually_exclusive=[(\\'separate_git_dir\\', \\'bare\\'), (\\'accept_hostkey\\', \\'accept_newhostkey\\')],\\n        required_by={\\'archive_prefix\\': [\\'archive\\']},\\n        supports_check_mode=True\\n    )\\n\\n    dest = module.params[\\'dest\\']\\n    repo = module.params[\\'repo\\']\\n    version = module.params[\\'version\\']\\n    remote = module.params[\\'remote\\']\\n    refspec = module.params[\\'refspec\\']\\n    force = module.params[\\'force\\']\\n    depth = module.params[\\'depth\\']\\n    update = module.params[\\'update\\']\\n    allow_clone = module.params[\\'clone\\']\\n    bare = module.params[\\'bare\\']\\n    verify_commit = module.params[\\'verify_commit\\']\\n    gpg_whitelist = module.params[\\'gpg_whitelist\\']\\n    reference = module.params[\\'reference\\']\\n    single_branch = module.params[\\'single_branch\\']\\n    git_path = module.params[\\'executable\\'] or module.get_bin_path(\\'git\\', True)\\n    key_file = module.params[\\'key_file\\']\\n    ssh_opts = module.params[\\'ssh_opts\\']\\n    umask = module.params[\\'umask\\']\\n    archive = module.params[\\'archive\\']\\n    archive_prefix = module.params[\\'archive_prefix\\']\\n    separate_git_dir = module.params[\\'separate_git_dir\\']\\n\\n    result = dict(changed=False, warnings=list())\\n\\n    if module.params[\\'accept_hostkey\\']:\\n        if ssh_opts is not None:\\n            if (\"-o StrictHostKeyChecking=no\" not in ssh_opts) and (\"-o StrictHostKeyChecking=accept-new\" not in ssh_opts):\\n                ssh_opts += \" -o StrictHostKeyChecking=no\"\\n        else:\\n            ssh_opts = \"-o StrictHostKeyChecking=no\"\\n\\n    if module.params[\\'accept_newhostkey\\']:\\n        if not ssh_supports_acceptnewhostkey(module):\\n            module.warn(\"Your ssh client does not support accept_newhostkey option, therefore it cannot be used.\")\\n        else:\\n            if ssh_opts is not None:\\n                if (\"-o StrictHostKeyChecking=no\" not in ssh_opts) and (\"-o StrictHostKeyChecking=accept-new\" not in ssh_opts):\\n                    ssh_opts += \" -o StrictHostKeyChecking=accept-new\"\\n            else:\\n                ssh_opts = \"-o StrictHostKeyChecking=accept-new\"\\n\\n    # evaluate and set the umask before doing anything else\\n    if umask is not None:\\n        if not isinstance(umask, string_types):\\n            module.fail_json(msg=\"umask must be defined as a quoted octal integer\")\\n        try:\\n            umask = int(umask, 8)\\n        except Exception:\\n            module.fail_json(msg=\"umask must be an octal integer\",\\n                             details=str(sys.exc_info()[1]))\\n        os.umask(umask)\\n\\n    # Certain features such as depth require a file:/// protocol for path based urls\\n    # so force a protocol here ...\\n    if os.path.expanduser(repo).startswith(\\'/\\'):\\n        repo = \\'file://\\' + os.path.expanduser(repo)\\n\\n    # We screenscrape a huge amount of git commands so use C locale anytime we\\n    # call run_command()\\n    module.run_command_environ_update = dict(LANG=\\'C\\', LC_ALL=\\'C\\', LC_MESSAGES=\\'C\\', LC_CTYPE=\\'C\\')\\n\\n    if separate_git_dir:\\n        separate_git_dir = os.path.realpath(separate_git_dir)\\n\\n    gitconfig = None\\n    if not dest and allow_clone:\\n        module.fail_json(msg=\"the destination directory must be specified unless clone=no\")\\n    elif dest:\\n        dest = os.path.abspath(dest)\\n        try:\\n            repo_path = get_repo_path(dest, bare)\\n            if separate_git_dir and os.path.exists(repo_path) and separate_git_dir != repo_path:\\n                result[\\'changed\\'] = True\\n                if not module.check_mode:\\n                    relocate_repo(module, result, separate_git_dir, repo_path, dest)\\n                    repo_path = separate_git_dir\\n        except (IOError, ValueError) as err:\\n            # No repo path found\\n            \"\"\"``.git`` file does not have a valid format for detached Git dir.\"\"\"\\n            module.fail_json(\\n                msg=\\'Current repo does not have a valid reference to a \\'\\n                \\'separate Git dir or it refers to the invalid path\\',\\n                details=to_text(err),\\n            )\\n        gitconfig = os.path.join(repo_path, \\'config\\')\\n\\n    # create a wrapper script and export\\n    # GIT_SSH=<path> as an environment variable\\n    # for git to use the wrapper script\\n    ssh_wrapper = write_ssh_wrapper(module.tmpdir)\\n    set_git_ssh(ssh_wrapper, key_file, ssh_opts)\\n    module.add_cleanup_file(path=ssh_wrapper)\\n\\n    git_version_used = git_version(git_path, module)\\n\\n    if depth is not None and git_version_used < LooseVersion(\\'1.9.1\\'):\\n        module.warn(\"git version is too old to fully support the depth argument. Falling back to full checkouts.\")\\n        depth = None\\n\\n    recursive = module.params[\\'recursive\\']\\n    track_submodules = module.params[\\'track_submodules\\']\\n\\n    result.update(before=None)\\n\\n    local_mods = False\\n    if (dest and not os.path.exists(gitconfig)) or (not dest and not allow_clone):\\n        # if there is no git configuration, do a clone operation unless:\\n        # * the user requested no clone (they just want info)\\n        # * we\\'re doing a check mode test\\n        # In those cases we do an ls-remote\\n        if module.check_mode or not allow_clone:\\n            remote_head = get_remote_head(git_path, module, dest, version, repo, bare)\\n            result.update(changed=True, after=remote_head)\\n            if module._diff:\\n                diff = get_diff(module, git_path, dest, repo, remote, depth, bare, result[\\'before\\'], result[\\'after\\'])\\n                if diff:\\n                    result[\\'diff\\'] = diff\\n            module.exit_json(**result)\\n        # there\\'s no git config, so clone\\n        clone(git_path, module, repo, dest, remote, depth, version, bare, reference,\\n              refspec, git_version_used, verify_commit, separate_git_dir, result, gpg_whitelist, single_branch)\\n    elif not update:\\n        # Just return having found a repo already in the dest path\\n        # this does no checking that the repo is the actual repo\\n        # requested.\\n        result[\\'before\\'] = get_version(module, git_path, dest)\\n        result.update(after=result[\\'before\\'])\\n        if archive:\\n            # Git archive is not supported by all git servers, so\\n            # we will first clone and perform git archive from local directory\\n            if module.check_mode:\\n                result.update(changed=True)\\n                module.exit_json(**result)\\n\\n            create_archive(git_path, module, dest, archive, archive_prefix, version, repo, result)\\n\\n        module.exit_json(**result)\\n    else:\\n        # else do a pull\\n        local_mods = has_local_mods(module, git_path, dest, bare)\\n        result[\\'before\\'] = get_version(module, git_path, dest)\\n        if local_mods:\\n            # failure should happen regardless of check mode\\n            if not force:\\n                module.fail_json(msg=\"Local modifications exist in repository (force=no).\", **result)\\n            # if force and in non-check mode, do a reset\\n            if not module.check_mode:\\n                reset(git_path, module, dest)\\n                result.update(changed=True, msg=\\'Local modifications exist.\\')\\n\\n        # exit if already at desired sha version\\n        if module.check_mode:\\n            remote_url = get_remote_url(git_path, module, dest, remote)\\n            remote_url_changed = remote_url and remote_url != repo and unfrackgitpath(remote_url) != unfrackgitpath(repo)\\n        else:\\n            remote_url_changed = set_remote_url(git_path, module, repo, dest, remote)\\n        result.update(remote_url_changed=remote_url_changed)\\n\\n        if module.check_mode:\\n            remote_head = get_remote_head(git_path, module, dest, version, remote, bare)\\n            result.update(changed=(result[\\'before\\'] != remote_head or remote_url_changed), after=remote_head)\\n            # FIXME: This diff should fail since the new remote_head is not fetched yet?!\\n            if module._diff:\\n                diff = get_diff(module, git_path, dest, repo, remote, depth, bare, result[\\'before\\'], result[\\'after\\'])\\n                if diff:\\n                    result[\\'diff\\'] = diff\\n            module.exit_json(**result)\\n        else:\\n            fetch(git_path, module, repo, dest, version, remote, depth, bare, refspec, git_version_used, force=force)\\n\\n        result[\\'after\\'] = get_version(module, git_path, dest)\\n\\n    # switch to version specified regardless of whether\\n    # we got new revisions from the repository\\n    if not bare:\\n        switch_version(git_path, module, dest, remote, version, verify_commit, depth, gpg_whitelist)\\n\\n    # Deal with submodules\\n    submodules_updated = False\\n    if recursive and not bare:\\n        submodules_updated = submodules_fetch(git_path, module, remote, track_submodules, dest)\\n        if submodules_updated:\\n            result.update(submodules_changed=submodules_updated)\\n\\n            if module.check_mode:\\n                result.update(changed=True, after=remote_head)\\n                module.exit_json(**result)\\n\\n            # Switch to version specified\\n            submodule_update(git_path, module, dest, track_submodules, force=force)\\n\\n    # determine if we changed anything\\n    result[\\'after\\'] = get_version(module, git_path, dest)\\n\\n    if result[\\'before\\'] != result[\\'after\\'] or local_mods or submodules_updated or remote_url_changed:\\n        result.update(changed=True)\\n        if module._diff:\\n            diff = get_diff(module, git_path, dest, repo, remote, depth, bare, result[\\'before\\'], result[\\'after\\'])\\n            if diff:\\n                result[\\'diff\\'] = diff\\n\\n    if archive:\\n        # Git archive is not supported by all git servers, so\\n        # we will first clone and perform git archive from local directory\\n        if module.check_mode:\\n            result.update(changed=True)\\n            module.exit_json(**result)\\n\\n        create_archive(git_path, module, dest, archive, archive_prefix, version, repo, result)\\n\\n    module.exit_json(**result)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def notify_snatch(self, title):\\n        if lazylibrarian.TWITTER_NOTIFY_ONSNATCH:\\n            self._notifyTwitter(common.notifyStrings[common.NOTIFY_SNATCH]+': '+title)\", 'def test_notify(self):\\n        return self._notifyTwitter(\"This is a test notification from LazyLibrarian / \" + formatter.now(), force=True)', 'def _get_credentials(self, key):\\n        request_token = {}', 'def _send_tweet(self, message=None):', \"def _notifyTwitter(self, message='', force=False):\\n        prefix = lazylibrarian.TWITTER_PREFIX\"]}, {'features': [], 'snippets': ['def point_in_poly(x,y,poly):\\n\\n    n = len(poly)\\n    inside = False\\n\\n    p1x,p1y = poly[0]\\n    for i in range(n+1):\\n        p2x,p2y = poly[i % n]\\n        if y > min(p1y,p2y):\\n            if y <= max(p1y,p2y):\\n                if x <= max(p1x,p2x):\\n                    if p1y != p2y:\\n                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\\n                    if p1x == p2x or x <= xints:\\n                        inside = not inside\\n        p1x,p1y = p2x,p2y\\n\\n    return inside', 'def points_in_poly(x,y,poly):\\n    n = len(poly)\\n    inside=np.zeros(x.size,dtype=bool)\\n    xints=np.zeros(x.size)\\n\\n    p1x,p1y = poly[0]\\n    for i in range(n+1):\\n        p2x,p2y=poly[i % n]\\n        if p1y!=p2y:\\n            xints[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x)],axis=0)] = (y[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x)],axis=0)]-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\\n        if p1x==p2x:\\n            inside[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x)],axis=0)] = np.invert(inside[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x)],axis=0)])\\n        else:\\n            inside[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x),x<=xints],axis=0)] = np.invert(inside[np.all([y>min(p1y,p2y), y<=max(p1y,p2y), x<=max(p1x,p2x),x<=xints],axis=0)])\\n        p1x,p1y = p2x,p2y\\n\\n    return x[inside],y[inside], inside', 'def points_in_radius(x,y,target_x, target_y,radius):\\n    inside=np.zeros(x.size,dtype=bool)\\n    d2=(x-target_x)**2+(y-target_y)**2\\n    inside = d2<=radius**2\\n    return x[inside],y[inside], inside', 'def filter_lidar_data_by_polygon(in_pts,polygon,filter_by_first_return_location = False):\\n    pts = np.zeros((0,in_pts.shape[1]))\\n    if in_pts.shape[0]>0:\\n        if filter_by_first_return_location:\\n            # find first returns\\n            mask = in_pts[:,3]==1\\n            x_temp, y_temp, inside_temp = points_in_poly(in_pts[mask,0],in_pts[mask,1],polygon)\\n            shots = np.unique(in_pts[mask,6][inside_temp]) # index 6 refers to GPS time\\n            inside = np.in1d(in_pts[:,6],shots) # this function retrieves all points corresponding to this GPS time\\n            x = in_pts[inside,0]\\n            y = in_pts[inside,1]\\n            x_temp=None\\n            y_temp=None\\n            inside_temp=None\\n        else:\\n            x,y,inside = points_in_poly(in_pts[:,0],in_pts[:,1],polygon)\\n        pts = in_pts[inside,:]\\n    else:\\n        print(\"\\\\t\\\\t\\\\t no points in polygon\")\\n    return pts']}, {'features': [], 'snippets': [\"def test_download_indices_for_url(mock_rcw, mock_rw, mock_jw):\\n    with s.app.test_client() as c:\\n        resp = c.get('/api/v1/classify_documents/log_only?directory=test')\\n\\n        assert mock_rcw.called\\n        assert mock_rw.called\\n        assert mock_jw.called\", \"def test_classify_indices_to_db(mock_rcw, mock_rw, mock_jw):\\n    with s.app.test_client() as c:\\n        resp = c.get('/api/v1/classify_documents/to_database?directory=test')\\n\\n        assert mock_rcw.called\\n        assert mock_rw.called\\n        assert mock_jw.called\", \"def test_classify_indices_to_db_no_connection(mock_db, mock_jw):\\n    mock_db.connected_to_db.return_value = False\\n\\n    with s.app.test_client() as c:\\n        resp = c.get('/api/v1/classify_documents/to_database?directory=test')\\n        assert not mock_jw.called\", \"def test_classify_textfiles_to_db(mock_rfw, mock_rw, mock_jw):\\n    classify_documents.classify_textfiles_to_db(0, 'test')\\n\\n    assert mock_rfw.called\\n    assert mock_rw.called\\n    assert mock_jw.called\", 'def test_classify_textfiles_to_db_no_connection(mock_db, mock_jw):\\n    mock_db.connected_to_db.return_value = False\\n    classify_documents.classify_textfiles_to_db(0, None)\\n    assert not mock_jw.called', 'def test_join_file_workers():\\n    producers = [Mock()]\\n    cworker = Mock()\\n    consumers = [Mock()]\\n\\n    classify_documents._join_file_workers(cworker, producers, consumers)']}, {'features': [], 'snippets': [\"def checkinfo2(content):\\n    content[1] = content[1].decode('gbk')\\n    key = content[1].encode('utf-8')\\n    if key == '节操':\\n        return '这种东西早就没有了'\\n\\n\\n    result = animation(key)    #搜动漫\\n    return result\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def _pure_pattern(regex):\\n    pattern = regex.pattern\\n    if pattern.startswith('^'):\\n        pattern = pattern[1:]\\n    return pattern\", 'def escape(text, quote=False, smart_amp=True):\\n    \"\"\"Replace special characters \"&\", \"<\" and \">\" to HTML-safe sequences.\\n\\n    The original cgi.escape will always escape \"&\", but you can control\\n    this one for a smart escape amp.\\n\\n    :param quote: if set to True, \" and \\' will be escaped.\\n    :param smart_amp: if set to False, & will always be escaped.\\n    \"\"\"\\n    if smart_amp:\\n        text = _escape_pattern.sub(\\'&amp;\\', text)\\n    else:\\n        text = text.replace(\\'&\\', \\'&amp;\\')\\n    text = text.replace(\\'<\\', \\'&lt;\\')\\n    text = text.replace(\\'>\\', \\'&gt;\\')\\n    if quote:\\n        text = text.replace(\\'\"\\', \\'&quot;\\')\\n        text = text.replace(\"\\'\", \\'&#39;\\')\\n    return text', \"def preprocessing(text, tab=4):\\n    text = _newline_pattern.sub('\\\\n', text)\\n    text = text.expandtabs(tab)\\n    text = text.replace('\\\\u00a0', ' ')\\n    text = text.replace('\\\\u2424', '\\\\n')\\n    pattern = re.compile(r'^ +$', re.M)\\n    return pattern.sub('', text)\", 'def __init__(self, rules=None, **kwargs):\\n        self.tokens = []\\n        self.def_links = {}\\n        self.def_footnotes = {}\\n\\n        if not rules:\\n            rules = self.grammar_class()\\n\\n        self.rules = rules', \"def parse(self, text, rules=None):\\n        text = text.rstrip('\\\\n')\\n\\n        if not rules:\\n            rules = self.default_rules\\n\\n        def manipulate(text):\\n            for key in rules:\\n                rule = getattr(self.rules, key)\\n                m = rule.match(text)\\n                if not m:\\n                    continue\\n                getattr(self, 'parse_%s' % key)(m)\\n                return m\\n            return False  # pragma: no cover\\n\\n        while text:\\n            m = manipulate(text)\\n            if m is not False:\\n                text = text[len(m.group(0)):]\\n                continue\\n            if text:  # pragma: no cover\\n                raise RuntimeError('Infinite loop at: %s' % text)\\n        return self.tokens\", \"def parse_block_code(self, m):\\n        # clean leading whitespace\\n        code = _block_code_leading_pattern.sub('', m.group(0))\\n        self.tokens.append({\\n            'type': 'code',\\n            'lang': None,\\n            'text': code,\\n        })\", \"def parse_heading(self, m):\\n        self.tokens.append({\\n            'type': 'heading',\\n            'level': len(m.group(1)),\\n            'text': m.group(2),\\n        })\", \"def parse_hrule(self, m):\\n        self.tokens.append({'type': 'hrule'})\", \"def _process_list_item(self, cap, bull):\\n        cap = self.rules.list_item.findall(cap)\\n\\n        _next = False\\n        length = len(cap)\\n\\n        for i in range(length):\\n            item = cap[i][0]\\n\\n            # remove the bullet\\n            space = len(item)\\n            item = self.rules.list_bullet.sub('', item)\\n\\n            # outdent\\n            if '\\\\n ' in item:\\n                space = space - len(item)\\n                pattern = re.compile(r'^ {1,%d}' % space, flags=re.M)\\n                item = pattern.sub('', item)\\n\\n            # determine whether item is loose or not\\n            loose = _next\\n            if not loose and re.search(r'\\\\n\\\\n(?!\\\\s*$)', item):\\n                loose = True\\n\\n            rest = len(item)\\n            if i != length - 1 and rest:\\n                _next = item[rest-1] == '\\\\n'\\n                if not loose:\\n                    loose = _next\\n\\n            if loose:\\n                t = 'loose_item_start'\\n            else:\\n                t = 'list_item_start'\\n\\n            self.tokens.append({'type': t})\\n            # recurse\\n            self.parse(item, self.list_rules)\\n            self.tokens.append({'type': 'list_item_end'})\", \"def parse_def_links(self, m):\\n        key = _keyify(m.group(1))\\n        self.def_links[key] = {\\n            'link': m.group(2),\\n            'title': m.group(3),\\n        }\", \"def parse_table(self, m):\\n        item = self._process_table(m)\\n\\n        cells = re.sub(r'(?: *\\\\| *)?\\\\n$', '', m.group(3))\\n        cells = cells.split('\\\\n')\\n        for i, v in enumerate(cells):\\n            v = re.sub(r'^ *\\\\| *| *\\\\| *$', '', v)\\n            cells[i] = re.split(r' *\\\\| *', v)\\n\\n        item['cells'] = cells\\n        self.tokens.append(item)\", \"def _process_table(self, m):\\n        header = re.sub(r'^ *| *\\\\| *$', '', m.group(1))\\n        header = re.split(r' *\\\\| *', header)\\n        align = re.sub(r' *|\\\\| *$', '', m.group(2))\\n        align = re.split(r' *\\\\| *', align)\\n\\n        for i, v in enumerate(align):\\n            if re.search(r'^ *-+: *$', v):\\n                align[i] = 'right'\\n            elif re.search(r'^ *:-+: *$', v):\\n                align[i] = 'center'\\n            elif re.search(r'^ *:-+ *$', v):\\n                align[i] = 'left'\\n            else:\\n                align[i] = None\\n\\n        item = {\\n            'type': 'table',\\n            'header': header,\\n            'align': align,\\n        }\\n        return item\", \"def parse_paragraph(self, m):\\n        text = m.group(1).rstrip('\\\\n')\\n        self.tokens.append({'type': 'paragraph', 'text': text})\", 'def hard_wrap(self):\\n        \"\"\"Grammar for hard wrap linebreak. You don\\'t need to add two\\n        spaces at the end of a line.\\n        \"\"\"\\n        self.linebreak = re.compile(r\\'^ *\\\\n(?!\\\\s*$)\\')\\n        self.text = re.compile(\\n            r\\'^[\\\\s\\\\S]+?(?=[\\\\\\\\<!\\\\[_*`~]|https?://| *\\\\n|$)\\'\\n        )', \"def __init__(self, renderer, rules=None, **kwargs):\\n        self.renderer = renderer\\n        self.links = {}\\n        self.footnotes = {}\\n        self.footnote_index = 0\\n\\n        if not rules:\\n            rules = self.grammar_class()\\n\\n        kwargs.update(self.renderer.options)\\n        if kwargs.get('hard_wrap'):\\n            rules.hard_wrap()\\n\\n        self.rules = rules\\n\\n        self._in_link = False\\n        self._in_footnote = False\\n        self._parse_inline_html = kwargs.get('parse_inline_html')\", 'def setup(self, links, footnotes):\\n        self.footnote_index = 0\\n        self.links = links or {}\\n        self.footnotes = footnotes or {}', \"def manipulate(text):\\n            for key in rules:\\n                pattern = getattr(self.rules, key)\\n                m = pattern.match(text)\\n                if not m:\\n                    continue\\n                self.line_match = m\\n                out = getattr(self, 'output_%s' % key)(m)\\n                if out is not None:\\n                    return m, out\\n            return False  # pragma: no cover\", 'def output_escape(self, m):\\n        text = m.group(1)\\n        return self.renderer.escape(text)', 'def output_url(self, m):\\n        link = m.group(1)\\n        if self._in_link:\\n            return self.renderer.text(link)\\n        return self.renderer.autolink(link, False)', 'def output_footnote(self, m):\\n        key = _keyify(m.group(1))\\n        if key not in self.footnotes:\\n            return None\\n        if self.footnotes[key]:\\n            return None\\n        self.footnote_index += 1\\n        self.footnotes[key] = self.footnote_index\\n        return self.renderer.footnote_ref(key, self.footnote_index)', \"def output_reflink(self, m):\\n        key = _keyify(m.group(2) or m.group(1))\\n        if key not in self.links:\\n            return None\\n        ret = self.links[key]\\n        return self._process_link(m, ret['link'], ret['title'])\", \"def _process_link(self, m, link, title=None):\\n        line = m.group(0)\\n        text = m.group(1)\\n        if line[0] == '!':\\n            return self.renderer.image(link, title, text)\\n\\n        self._in_link = True\\n        text = self.output(text)\\n        self._in_link = False\\n        return self.renderer.link(link, title, text)\", 'def output_emphasis(self, m):\\n        text = m.group(2) or m.group(1)\\n        text = self.output(text)\\n        return self.renderer.emphasis(text)', 'def output_linebreak(self, m):\\n        return self.renderer.linebreak()', 'def output_text(self, m):\\n        text = m.group(0)\\n        return self.renderer.text(text)', 'def __init__(self, **kwargs):\\n        self.options = kwargs', 'def block_code(self, code, lang=None):\\n        \"\"\"Rendering block level code. ``pre > code``.\\n\\n        :param code: text content of the code block.\\n        :param lang: language of the given code.\\n        \"\"\"\\n        code = code.rstrip(\\'\\\\n\\')\\n        if not lang:\\n            code = escape(code, smart_amp=False)\\n            return \\'<pre><code>%s\\\\n</code></pre>\\\\n\\' % code\\n        code = escape(code, quote=True, smart_amp=False)\\n        return \\'<pre><code class=\"lang-%s\">%s\\\\n</code></pre>\\\\n\\' % (lang, code)', 'def block_html(self, html):\\n        \"\"\"Rendering block level pure html content.\\n\\n        :param html: text content of the html snippet.\\n        \"\"\"\\n        if self.options.get(\\'skip_style\\') and \\\\\\n           html.lower().startswith(\\'<style\\'):\\n            return \\'\\'\\n        if self.options.get(\\'escape\\'):\\n            return escape(html)\\n        return html', 'def hrule(self):\\n        \"\"\"Rendering method for ``<hr>`` tag.\"\"\"\\n        if self.options.get(\\'use_xhtml\\'):\\n            return \\'<hr />\\\\n\\'\\n        return \\'<hr>\\\\n\\'', 'def list_item(self, text):\\n        \"\"\"Rendering list item snippet. Like ``<li>``.\"\"\"\\n        return \\'<li>%s</li>\\\\n\\' % text', 'def table(self, header, body):\\n        \"\"\"Rendering table element. Wrap header and body in it.\\n\\n        :param header: header part of the table.\\n        :param body: body part of the table.\\n        \"\"\"\\n        return (\\n            \\'<table>\\\\n<thead>%s</thead>\\\\n\\'\\n            \\'<tbody>\\\\n%s</tbody>\\\\n</table>\\\\n\\'\\n        ) % (header, body)', 'def table_cell(self, content, **flags):\\n        \"\"\"Rendering a table cell. Like ``<th>`` ``<td>``.\\n\\n        :param content: content of current table cell.\\n        :param header: whether this is header or not.\\n        :param align: align of current table cell.\\n        \"\"\"\\n        if flags[\\'header\\']:\\n            tag = \\'th\\'\\n        else:\\n            tag = \\'td\\'\\n        align = flags[\\'align\\']\\n        if not align:\\n            return \\'<%s>%s</%s>\\\\n\\' % (tag, content, tag)\\n        return \\'<%s style=\"text-align:%s\">%s</%s>\\\\n\\' % (\\n            tag, align, content, tag\\n        )', 'def emphasis(self, text):\\n        \"\"\"Rendering *emphasis* text.\\n\\n        :param text: text content for emphasis.\\n        \"\"\"\\n        return \\'<em>%s</em>\\' % text', 'def linebreak(self):\\n        \"\"\"Rendering line break like ``<br>``.\"\"\"\\n        if self.options.get(\\'use_xhtml\\'):\\n            return \\'<br />\\\\n\\'\\n        return \\'<br>\\\\n\\'', 'def text(self, text):\\n        \"\"\"Rendering unformatted text.\\n\\n        :param text: text content.\\n        \"\"\"\\n        if self.options.get(\\'parse_block_html\\'):\\n            return text\\n        return escape(text)', 'def autolink(self, link, is_email=False):\\n        \"\"\"Rendering a given link or email address.\\n\\n        :param link: link content or email address.\\n        :param is_email: whether this is an email or not.\\n        \"\"\"\\n        text = link = escape(link)\\n        if is_email:\\n            link = \\'mailto:%s\\' % link\\n        return \\'<a href=\"%s\">%s</a>\\' % (link, text)', 'def image(self, src, title, text):\\n        \"\"\"Rendering a image with title and text.\\n\\n        :param src: source link of the image.\\n        :param title: title text of the image.\\n        :param text: alt text of the image.\\n        \"\"\"\\n        src = escape_link(src)\\n        text = escape(text, quote=True)\\n        if title:\\n            title = escape(title, quote=True)\\n            html = \\'<img src=\"%s\" alt=\"%s\" title=\"%s\"\\' % (src, text, title)\\n        else:\\n            html = \\'<img src=\"%s\" alt=\"%s\"\\' % (src, text)\\n        if self.options.get(\\'use_xhtml\\'):\\n            return \\'%s />\\' % html\\n        return \\'%s>\\' % html', 'def newline(self):\\n        \"\"\"Rendering newline element.\"\"\"\\n        return \\'\\'', 'def footnote_item(self, key, text):\\n        \"\"\"Rendering a footnote item.\\n\\n        :param key: identity key for the footnote.\\n        :param text: text content of the footnote.\\n        \"\"\"\\n        back = (\\n            \\'<a href=\"#fnref-%s\" rev=\"footnote\">&#8617;</a>\\'\\n        ) % escape(key)\\n        text = text.rstrip()\\n        if text.endswith(\\'</p>\\'):\\n            text = re.sub(r\\'<\\\\/p>$\\', r\\'%s</p>\\' % back, text)\\n        else:\\n            text = \\'%s<p>%s</p>\\' % (text, back)\\n        html = \\'<li id=\"fn-%s\">%s</li>\\\\n\\' % (escape(key), text)\\n        return html', \"def __init__(self, renderer=None, inline=None, block=None, **kwargs):\\n        if not renderer:\\n            renderer = Renderer(**kwargs)\\n        else:\\n            kwargs.update(renderer.options)\\n\\n        self.renderer = renderer\\n\\n        if inline and inspect.isclass(inline):\\n            inline = inline(renderer, **kwargs)\\n        if block and inspect.isclass(block):\\n            block = block(**kwargs)\\n\\n        if inline:\\n            self.inline = inline\\n        else:\\n            self.inline = InlineLexer(renderer, **kwargs)\\n\\n        self.block = block or BlockLexer(BlockGrammar())\\n        self.footnotes = []\\n        self.tokens = []\\n\\n        # detect if it should parse text in block html\\n        self._parse_block_html = kwargs.get('parse_block_html')\", 'def render(self, text):\\n        \"\"\"Render the Markdown text.\\n\\n        :param text: markdown formatted text content.\\n        \"\"\"\\n        return self.parse(text)', 'def pop(self):\\n        if not self.tokens:\\n            return None\\n        self.token = self.tokens.pop()\\n        return self.token', 'def output(self, text, rules=None):\\n        self.tokens = self.block(text, rules)\\n        self.tokens.reverse()\\n\\n        self.inline.setup(self.block.def_links, self.block.def_footnotes)\\n\\n        out = self.renderer.placeholder()\\n        while self.pop():\\n            out += self.tok()\\n        return out', \"def tok_text(self):\\n        text = self.token['text']\\n        while self.peek()['type'] == 'text':\\n            text += '\\\\n' + self.pop()['text']\\n        return self.inline(text)\", 'def output_hrule(self):\\n        return self.renderer.hrule()', \"def output_code(self):\\n        return self.renderer.block_code(\\n            self.token['text'], self.token['lang']\\n        )\", \"def output_block_quote(self):\\n        body = self.renderer.placeholder()\\n        while self.pop()['type'] != 'block_quote_end':\\n            body += self.tok()\\n        return self.renderer.block_quote(body)\", \"def output_list_item(self):\\n        body = self.renderer.placeholder()\\n        while self.pop()['type'] != 'list_item_end':\\n            if self.token['type'] == 'text':\\n                body += self.tok_text()\\n            else:\\n                body += self.tok()\\n\\n        return self.renderer.list_item(body)\", \"def output_footnote(self):\\n        self.inline._in_footnote = True\\n        body = self.renderer.placeholder()\\n        key = self.token['key']\\n        while self.pop()['type'] != 'footnote_end':\\n            body += self.tok()\\n        self.footnotes.append({'key': key, 'text': body})\\n        self.inline._in_footnote = False\\n        return self.renderer.placeholder()\", \"def output_open_html(self):\\n        text = self.token['text']\\n        tag = self.token['tag']\\n        if self._parse_block_html and tag not in _pre_tags:\\n            text = self.inline(text, rules=self.inline.inline_html_rules)\\n        extra = self.token.get('extra') or ''\\n        html = '<%s%s>%s</%s>' % (tag, extra, text, tag)\\n        return self.renderer.block_html(html)\", 'def output_text(self):\\n        return self.renderer.paragraph(self.tok_text())']}, {'features': [], 'snippets': ['def __init__(self):\\n        self.mainFrame = gui.mainFrame.MainFrame.getInstance()', 'def getText(self, callingWindow, itmContext, mainItem):\\n        return \"Set {} as Damage Pattern\".format(itmContext if itmContext is not None else \"Item\")', 'def getBitmap(self, callingWindow, context, mainItem):\\n        return None']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def eye(N, M=None, k=0, typecode=None, dtype=None):\\r\\n    \"\"\" eye returns a N-by-M 2-d array where the  k-th diagonal is all ones,\\r\\n        and everything else is zeros.\\r\\n    \"\"\"\\r\\n    dtype = convtypecode(typecode, dtype)\\r\\n    if M is None: M = N\\r\\n    m = np.equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)\\r\\n    if m.dtype != dtype:\\r\\n        return m.astype(dtype)', 'def tri(N, M=None, k=0, typecode=None, dtype=None):\\r\\n    \"\"\" returns a N-by-M array where all the diagonals starting from\\r\\n        lower left corner up to the k-th are all ones.\\r\\n    \"\"\"\\r\\n    dtype = convtypecode(typecode, dtype)\\r\\n    if M is None: M = N\\r\\n    m = np.greater_equal(np.subtract.outer(np.arange(N), np.arange(M)),-k)\\r\\n    if m.dtype != dtype:\\r\\n        return m.astype(dtype)', 'def trapz(y, x=None, axis=-1):\\r\\n    return _Ntrapz(y, x, axis=axis)', 'def ptp(x, axis=0):\\r\\n    return _Nptp(x, axis)', 'def cumprod(x, axis=0):\\r\\n    return _Ncumprod(x, axis)', 'def max(x, axis=0):\\r\\n    return _Nmax(x, axis)', 'def min(x, axis=0):\\r\\n    return _Nmin(x, axis)', 'def prod(x, axis=0):\\r\\n    return _Nprod(x, axis)', 'def std(x, axis=0):\\r\\n    N = asarray(x).shape[axis]\\r\\n    return _Nstd(x, axis)*sqrt(N/(N-1.))', 'def mean(x, axis=0):\\r\\n    return _Nmean(x, axis)', 'def cov(m, y=None, rowvar=0, bias=0):\\r\\n    if y is None:\\r\\n        y = m\\r\\n    else:\\r\\n        y = y\\r\\n    if rowvar:\\r\\n        m = transpose(m)\\r\\n        y = transpose(y)\\r\\n    if (m.shape[0] == 1):\\r\\n        m = transpose(m)\\r\\n    if (y.shape[0] == 1):\\r\\n        y = transpose(y)\\r\\n    N = m.shape[0]\\r\\n    if (y.shape[0] != N):\\r\\n        raise ValueError(\"x and y must have the same number of observations\")\\r\\n    m = m - _Nmean(m,axis=0)\\r\\n    y = y - _Nmean(y,axis=0)\\r\\n    if bias:\\r\\n        fact = N*1.0\\r\\n    else:\\r\\n        fact = N-1.0\\r\\n    return squeeze(dot(transpose(m), conjugate(y)) / fact)', 'def corrcoef(x, y=None):\\r\\n    c = cov(x, y)\\r\\n    d = diag(c)\\r\\n    return c/sqrt(multiply.outer(d,d))']}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Adding model 'ArticleComment'\\n        db.create_table('cms_articlecomment', (\\n            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\\n            ('article', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['cms.Article'])),\\n            ('created_at', self.gf('django.db.models.fields.DateField')(auto_now_add=True, blank=True)),\\n            ('author', self.gf('django.db.models.fields.CharField')(max_length=60)),\\n            ('comment', self.gf('django.db.models.fields.TextField')()),\\n        ))\\n        db.send_create_signal('cms', ['ArticleComment'])\"]}, {'features': [], 'snippets': ['def __init__(self, addr, dispatcher, send_delay):\\n\\t\\tsuper(Radio, self).__init__()\\n\\n\\t\\tself.addr = addr\\n\\t\\tself.neighbor = None\\n\\t\\tself.dispatcher = dispatcher\\n\\t\\tself.q = Queue.Queue()\\n\\n\\t\\tself.frequency = 0\\n\\t\\tself.bandwidth = 0\\n\\n\\t\\tself.send_delay = send_delay', 'def set_configuration(self, frequency, bandwidth, power):\\n\\t\\tself.frequency = frequency\\n\\t\\tself.bandwidth = bandwidth', 'def binrecv(self, timeout=None):\\n\\t\\tif timeout is None:\\n\\t\\t\\ttimeout = self.RECEIVE_TIMEOUT\\n\\n\\t\\ttry:\\n\\t\\t\\tbindata = self.q.get(True, timeout)\\n\\t\\texcept Queue.Empty:\\n\\t\\t\\traise RadioTimeout\\n\\t\\telse:\\n\\t\\t\\treturn bindata', 'def __init__(self, send_delay=.1, frequency_range=64, bandwidth_range=10, power_range=10, packet_size=1024):\\n\\t\\tself.send_delay = float(send_delay)\\n\\t\\tself.frequency_range = int(frequency_range)\\n\\t\\tself.bandwidth_range = int(bandwidth_range)\\n\\t\\tself.power_range = int(power_range)\\n\\n\\t\\tself.RADIO_CLASS.PACKET_SIZE = int(packet_size) + 1\\n\\n\\t\\tself.radios = []\\n\\n\\t\\t# for each channel, we keep the timestamp of the last\\n\\t\\t# transmission. we use this for simulated spectrum sensing and\\n\\t\\t# for detecting collisions.\\n\\t\\tself.channels = [0] * self.frequency_range\\n\\n\\t\\tself.i = 0', 'def _dispatcher(self, addr, bindata, frequency, bandwidth):\\n\\t\\tnow = self.time()\\n\\n\\t\\thas_collision = (now - self.channels[frequency]) > self.send_delay\\n\\t\\tself.channels[frequency] = now\\n\\n\\t\\tif has_collision:\\n\\t\\t\\t# note that when packets collide, the first one goes\\n\\t\\t\\t# through while the later ones fail. this is different\\n\\t\\t\\t# than in reality: all should fail. But this would\\n\\t\\t\\t# be complicated to implement in practice.\\n\\t\\t\\tfor radio in self.radios:\\n\\t\\t\\t\\tradio._recv(addr, bindata, frequency, bandwidth)\\n\\t\\telse:\\n\\t\\t\\tlog.debug(\"packet collision detected on channel %d\" % (frequency,))', 'def get_spectrum(self):\\n\\n\\t\\tspectrum = []\\n\\t\\tnow = self.time()\\n\\n\\t\\tfor time in self.channels:\\n\\t\\t\\tif now - time < .5:\\n\\t\\t\\t\\tp = random.randint(-40, -20)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tp = random.randint(-90, -80)\\n\\n\\t\\t\\tspectrum.append(p)\\n\\n\\t\\treturn tuple(spectrum)', 'def get_bandwidth_range(self):\\n\\t\\treturn self.bandwidth_range']}, {'features': [], 'snippets': ['def _find_acl_template(conn, acl_template):\\n    query = (\\n        sa.sql.select([acl_template_table.c.id])\\n        .where(acl_template_table.c.template == acl_template)\\n        .limit(1)\\n    )\\n    return conn.execute(query).scalar()', 'def _get_policy_uuid(conn, policy_name):\\n    policy_query = sa.sql.select([policy_table.c.uuid]).where(\\n        policy_table.c.name == policy_name\\n    )\\n\\n    for policy in conn.execute(policy_query).fetchall():\\n        return policy[0]', 'def _get_acl_template_ids(conn, policy_uuid):\\n    query = sa.sql.select([policy_template.c.template_id]).where(\\n        policy_template.c.policy_uuid == policy_uuid\\n    )\\n    return [acl_template_id for (acl_template_id,) in conn.execute(query).fetchall()]']}, {'features': [], 'snippets': ['def _find_packages(path=\\'.\\', prefix=\\'\\'):\\r\\n        yield prefix\\r\\n        prefix = prefix + \".\"\\r\\n        for _, name, ispkg in walk_packages(path, \\r\\n                                            prefix,\\r\\n                                            onerror=lambda x: x):\\r\\n            if ispkg:\\r\\n                yield name', 'def find_packages():\\r\\n        return list(_find_packages(mn.__path__, mn.__name__))']}, {'features': [], 'snippets': ['def enter_local_tempdir(prefix=\\'\\'):\\n    \"\"\"Create and set the working directory as a new temporary directory.\\n\\n    Returns the name of the temporary directory (to be passed to\\n    exit_local_tempdir).\\n    \"\"\"\\n\\n    suffix = \\'\\'.join(random.choice(string.lowercase) for i in range(6))\\n\\n    os.mkdir(prefix + suffix)\\n    os.chdir(prefix + suffix)\\n\\n    return prefix + suffix', 'def kill_local_tempdir(tempdir):\\n    \"\"\"Remove all contents of a temporary directory.\\n\\n    Call after exit_local_tempdir is called, only if killit=False.\\n    \"\"\"\\n\\n    os.system(\"rm -r \" + tempdir)', 'def send_fips_complete(make_generator):\\n    \"\"\"Call after the last county of a loop of counties, to clean up any memory.\\n    \"\"\"\\n\\n    print \"Complete the FIPS\"\\n    try:\\n        iterator = make_generator(FIPS_COMPLETE, None, None).next()\\n        print \"Success\"\\n    except StopIteration, e:\\n        pass\\n    except Exception, e:\\n        print e\\n        pass', 'def write_effect_file(path, fips, generator, collabel):\\n    \"\"\"Write the effects for a single FIPS-coded county.\\n\\n    path: relative path for file\\n    fips: the unique id of the region\\n    generator: a enumerator of tuples/lists with individual rows\\n    collabel: label for one (string) or more (list) columns after the\\n    year column\\n    \"\"\"\\n\\n    # Create the CSV file\\n    with open(os.path.join(path, fips + \\'.csv\\'), \\'wb\\') as csvfp:\\n        writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)\\n\\n        # Write the header row\\n        if not isinstance(collabel, list):\\n            writer.writerow([\"year\", collabel])\\n        else:\\n            writer.writerow([\"year\"] + collabel)\\n\\n        # Write all data rows\\n        for values in generator:\\n            writer.writerow(values)', 'def make_tar_dummy(name, acradir, make_generator, targetdir=None, collabel=\"fraction\"):\\n    \"\"\"Constructs a tar of files for each county, using NO DATA.\\n    Calls make_generator for each county, using a filename of\\n    counties.\\n\\n    name: the name of the effect bundle.\\n    acradir: path to the DMAS acra directory.\\n    make_generator(fips, times, daily): returns an iterator of (year, effect).\\n    targetdir: path to a final destination for the bundle\\n    collabel: the label for the effect column\\n    \"\"\"\\n\\n    tempdir = enter_local_tempdir()\\n    os.mkdir(name) # directory for county files\\n\\n    # Generate a effect file for each county in regionsA\\n    with open(os.path.join(acradir, \\'regions/regionsANSI.csv\\')) as countyfp:\\n        reader = csv.reader(countyfp)\\n        reader.next() # ignore header\\n\\n        # Each row is a county\\n        for row in reader:\\n            fips = canonical_fips(row[0])\\n            print fips\\n\\n            # Call generator (with no data)\\n            generator = make_generator(fips, None, None)\\n            if generator is None:\\n                continue\\n\\n            # Construct the effect file\\n            write_effect_file(name, fips, generator, collabel)\\n\\n    send_fips_complete(make_generator)\\n\\n    # Generate the bundle tar\\n    target = get_target_path(targetdir, name)\\n    os.system(\"tar -czf \" + os.path.join(\"..\", target) + \".tar.gz \" + name)\\n\\n    # Remove the working directory\\n    exit_local_tempdir(tempdir)', 'def make_tar_ncdf(name, weather_ncdf, var, make_generator, targetdir=None, collabel=\"fraction\"):\\n    \"\"\"Constructs a tar of files for each county, describing yearly results.\\n\\n    name: the name of the effect bundle.\\n    weather_ncdf: str for one, or {variable: filename} for calling\\n      generator with {variable: data}.\\n    var: str for one, or [str] for calling generator with {variable: data}\\n    make_generator(fips, times, daily): returns an iterator of (year, effect).\\n    targetdir: path to a final destination for the bundle, or a\\n      function to take the data\\n    collabel: the label for the effect column\\n    \"\"\"\\n\\n    # If this is a function, we just start iterating\\n    if hasattr(targetdir, \\'__call__\\'):\\n        call_with_generator(name, weather_ncdf, var, make_generator, targetdir)\\n        return\\n\\n    # Create the working directory\\n    tempdir = enter_local_tempdir()\\n    os.mkdir(name)\\n\\n    # Helper function for calling write_effect_file with collabel\\n    def write_csv(name, fips, generator):\\n        write_effect_file(name, fips, generator, collabel)\\n\\n    # Iterate through the data\\n    call_with_generator(name, weather_ncdf, var, make_generator, write_csv)\\n\\n    # Create the effect bundle\\n    target = get_target_path(targetdir, name)\\n    os.system(\"tar -czf \" + os.path.join(\"..\", target) + \".tar.gz \" + name)\\n\\n    # Remove the working directory\\n    exit_local_tempdir(tempdir)', 'def call_with_generator(name, weather_ncdf, var, make_generator, targetfunc):\\n    \"\"\"Helper function for calling make_generator with each variable\\n    set.  In cases with multiple weather datasets, assumes all use the\\n    same clock (sequence of times) and geography (sequence of\\n    counties).\\n\\n    name: the name of the effect bundle.\\n    weather_ncdf: str for one, or {variable: filename} for calling\\n      generator with {variable: data}.\\n    var: str for one, or [str] for calling generator with {variable: data}\\n    make_generator(fips, times, daily): returns an iterator of (year, effect).\\n    targetfunc: function(name, fips, generator) to handle results\\n    \"\"\"\\n\\n    if isinstance(weather_ncdf, dict) and isinstance(var, list):\\n        # In this case, we generate a dictionary of variables\\n        weather = {}\\n        times = None # All input assumed to have same clock\\n\\n        # Filter by the variables in var\\n        for variable in var:\\n            # Retrieve the netcdf object (rootgrp) and add to weather dict\\n            if isinstance(weather_ncdf[variable], str):\\n                # Open this up as a netCDF and read data into array\\n                rootgrp = Dataset(weather_ncdf[variable], \\'r+\\', format=\\'NETCDF4\\')\\n                weather[variable] = rootgrp.variables[variable][:,:]\\n            elif isinstance(weather_ncdf[variable], dict):\\n                # This is an {original, data, times} dictionary\\n                rootgrp = weather_ncdf[variable][\\'original\\']\\n                weather[variable] = weather_ncdf[variable][\\'data\\']\\n                if \\'times\\' in weather_ncdf[variable]:\\n                    times = weather_ncdf[variable][\\'times\\']\\n            else:\\n                # This is already a netcdf object\\n                rootgrp = weather_ncdf[variable]\\n                weather[variable] = rootgrp.variables[variable][:,:]\\n\\n            # Collect additional information from netcdf object\\n            counties = rootgrp.variables[\\'fips\\']\\n            lats = rootgrp.variables[\\'lat\\']\\n            lons = rootgrp.variables[\\'lon\\']\\n            if times is None:\\n                times = rootgrp.variables[\\'time\\']\\n    else:\\n        # We just want a single variable (not a dictionary of them)\\n        # Retrieve the netcdf object (rootgrp) and add to weather dict\\n        if isinstance(weather_ncdf, str):\\n            # Open this up as a netCDF and read into array\\n            rootgrp = Dataset(weather_ncdf, \\'r+\\', format=\\'NETCDF4\\')\\n            weather = rootgrp.variables[var][:,:]\\n        elif isinstance(weather_ncdf, dict):\\n            # This is an {original, data, times} dictionary\\n            rootgrp = weather_ncdf[\\'original\\']\\n            weather = weather_ncdf[\\'data\\']\\n        else:\\n            # This is already a netcdf object\\n            rootgrp = weather_ncdf\\n            weather = rootgrp.variables[var][:,:]\\n\\n        # Collect additional information from netcdf object\\n        counties = rootgrp.variables[\\'fips\\']\\n        lats = rootgrp.variables[\\'lat\\']\\n        lons = rootgrp.variables[\\'lon\\']\\n        times = rootgrp.variables[\\'time\\']\\n\\n    # Loop through counties, calling make_generator with each\\n    for ii in range(len(counties)):\\n        fips = canonical_fips(counties[ii])\\n        print fips\\n\\n        # Extract the weather just for this county\\n        if not isinstance(weather, dict):\\n            daily = weather[:,ii]\\n        else:\\n            daily = {}\\n            for variable in weather:\\n                daily[variable] = weather[variable][:,ii]\\n\\n        # Call make_generator for this county\\n        generator = make_generator(fips, times, daily, lat=lats[ii], lon=lons[ii])\\n        if generator is None:\\n            continue\\n\\n        # Call targetfunc with the result\\n        targetfunc(name, fips, generator)\\n\\n    # Signal the end of the counties\\n    send_fips_complete(make_generator)', 'def load_tar_make_generator(targetdir, name, column=None):\\n    \"\"\"Load existing data for additional calculations.\\n    targetdir: relative path to a directory of effect bundles.\\n    name: the effect name (so the effect bundle is at <targetdir>/<name>.tar.gz\\n    \"\"\"\\n\\n    # Extract the existing tar into a loader tempdir\\n    tempdir = enter_local_tempdir(\\'loader-\\')\\n    os.system(\"tar -xzf \" + os.path.join(\"..\", targetdir, name + \".tar.gz\"))\\n    exit_local_tempdir(tempdir, killit=False)\\n\\n    def generate(fips, yyyyddd, temps, *args, **kw):\\n        # When all of the counties are done, kill the local dir\\n        if fips == FIPS_COMPLETE:\\n            print \"Remove\", tempdir\\n            # We might be in another tempdir-- check\\n            if os.path.exists(tempdir):\\n                kill_local_tempdir(tempdir)\\n            else:\\n                kill_local_tempdir(os.path.join(\\'..\\', tempdir))\\n            return\\n\\n        # Open up the effect for this bundle\\n        fipspath = os.path.join(tempdir, name, fips + \".csv\")\\n        if not os.path.exists(fipspath):\\n            fipspath = os.path.join(\\'..\\', fipspath)\\n            if not os.path.exists(fipspath):\\n                # If we can\\'t find this, just return a single year with 0 effect\\n                print fipspath + \" doesn\\'t exist\"\\n                yield (yyyyddd[0] / 1000, 0)\\n                raise StopIteration()\\n\\n        with open(fipspath) as fp:\\n            reader = csv.reader(fp)\\n            reader.next() # ignore header\\n\\n            # yield the same values that generated this effect file\\n            for row in reader:\\n                if column is None:\\n                    yield [int(row[0])] + map(float, row[1:])\\n                else:\\n                    yield (int(row[0]), float(row[column]))\\n\\n    return generate']}, {'features': [], 'snippets': ['def __init__(self):\\n        args = self.arg_parser.parse_known_args()[0]\\n        super(ScikitBase, self).__init__()\\n        self.pipeline = self.load_pipeline(args.pipeline)\\n        if args.feature_names:\\n            self.feature_names = self.load_pipeline(args.feature_names)', 'def load_pipeline(pipeline_file):\\n        \"\"\"\\n        Loads scikit model/pipeline\\n        \"\"\"\\n        print(colored(\\'Loading pipeline: \\' + pipeline_file, \\'green\\'))\\n        return joblib.load(pipeline_file)']}, {'features': [], 'snippets': ['def combine_browsers_logs(udid):\\n    cmd = \\'rebot -N Combined --outputdir browserlogs/ \\'\\n\\n    for idx, device in enumerate(udid):\\n        #Get all the output.xml files for the devices    \\n        if platform.system() == \"Windows\":\\n            cmd += os.getcwd() + \"\\\\\\\\browserlogs\\\\\\\\\" + device + \"\\\\output.xml \"\\n        else:\\n            cmd += os.getcwd() + \"/browserlogs/\" + device + \"/output.xml \"', 'def combine_logs(udid):\\n    cmd = \\'rebot -N Combined --outputdir logs/ \\'\\n\\n    for idx, device in enumerate(udid):\\n        #Get all the output.xml files for the devices    \\n        if platform.system() == \"Windows\":\\n            cmd += os.getcwd() + \"\\\\logs\\\\\\\\\" + device + \"_\" + \"*\\\\output.xml \"\\n        else:\\n            cmd += os.getcwd() + \"/logs/\" + device + \"_\" + \"*/output.xml \"', 'def zip_logs():\\n    if platform.system() == \"Windows\":\\n        cmd = \"Compress-Archive logs logs-$(date +%Y-%m-%d-%H%M).zip\"\\n        subprocess.call([\"powershell.exe\", cmd])\\n\\n    elif platform.system() == \"Linux\" or platform.system() == \"Darwin\":\\n        cmd = \"zip -vr logs-$(date +%Y-%m-%d-%H%M).zip\" + \" logs/\"\\n        cr.run_command(cmd)', \"def delete_previous_logs():\\n    cmd = 'rm -rf logs/*'\\n    cr.run_command(cmd)\"]}, {'features': [], 'snippets': ['def main(argv=sys.argv[1:]):\\n    try:\\n        docopt.docopt(__doc__, argv=argv, version=hello.__version__)\\n    except docopt.DocoptExit as e:\\n        print(str(e), file=sys.stderr)\\n        return 2\\n    except SystemExit as e:\\n        return 0']}, {'features': [], 'snippets': ['def CanComplete():\\n  \"\"\"Returns whether it\\'s appropriate to provide any completion at the current\\n     line and column.\"\"\"\\n  try:\\n    line, column = LineAndColumnAfterLastNonWhitespace()\\n  except TypeError:\\n    return False\\n  if ( line, column ) == CurrentLineAndColumn():\\n    return True\\n  return ( ToBytes( vim.current.buffer[ line ][ column - 1 ] )\\n           in potential_hint_triggers )', 'def LineAndColumnAfterLastNonWhitespace():\\n  line, column = CurrentLineAndColumn()\\n  line_value = vim.current.line[ :column ].rstrip()\\n  while not line_value:\\n    line = line - 1\\n    if line == -1:\\n      return None\\n    line_value = vim.current.buffer[ line ].rstrip()\\n  return line, len( line_value )', 'def CurrentLineAndColumn():\\n  \"\"\"Returns the 0-based current line and 0-based current column.\"\"\"\\n  # See the comment in CurrentColumn about the calculation for the line and\\n  # column number\\n  line, column = vim.current.window.cursor\\n  line -= 1\\n  return line, column', 'def CurrentLineContents():\\n  return ToUnicode( vim.current.line )', 'def TextAfterCursor():\\n  \"\"\"Returns the text after CurrentColumn.\"\"\"\\n  return ToUnicode( vim.current.line[ CurrentColumn(): ] )', 'def GetBufferOption( buffer_object, option ):\\n  # NOTE: We used to check for the \\'options\\' property on the buffer_object which\\n  # is available in recent versions of Vim and would then use:\\n  #\\n  #   buffer_object.options[ option ]\\n  #\\n  # to read the value, BUT this caused annoying flickering when the\\n  # buffer_object was a hidden buffer (with option = \\'ft\\'). This was all due to\\n  # a Vim bug. Until this is fixed, we won\\'t use it.\\n\\n  to_eval = \\'getbufvar({0}, \"&{1}\")\\'.format( buffer_object.number, option )\\n  return GetVariableValue( to_eval )', 'def GetUnsavedAndSpecifiedBufferData( including_filepath ):\\n  \"\"\"Build part of the request containing the contents and filetypes of all\\n  dirty buffers as well as the buffer with filepath |including_filepath|.\"\"\"\\n  buffers_data = {}\\n  for buffer_object in vim.buffers:\\n    buffer_filepath = GetBufferFilepath( buffer_object )\\n    if not ( BufferModified( buffer_object ) or\\n             buffer_filepath == including_filepath ):\\n      continue\\n\\n    buffers_data[ buffer_filepath ] = {\\n      # Add a newline to match what gets saved to disk. See #1455 for details.\\n      \\'contents\\': JoinLinesAsUnicode( buffer_object ) + \\'\\\\n\\',\\n      \\'filetypes\\': FiletypesForBuffer( buffer_object )\\n    }\\n\\n  return buffers_data', 'def GetCurrentBufferFilepath():\\n  return GetBufferFilepath( vim.current.buffer )', \"def GetBufferFilepath( buffer_object ):\\n  if buffer_object.name:\\n    return buffer_object.name\\n  # Buffers that have just been created by a command like :enew don't have any\\n  # buffer name so we use the buffer number for that.\\n  return os.path.join( GetCurrentDirectory(), str( buffer_object.number ) )\", 'def GetBufferChangedTick( bufnr ):\\n  return GetIntValue( \\'getbufvar({0}, \"changedtick\")\\'.format( bufnr ) )', 'def PlaceSign( sign_id, line_num, buffer_num, is_error = True ):\\n  # libclang can give us diagnostics that point \"outside\" the file; Vim borks\\n  # on these.\\n  if line_num < 1:\\n    line_num = 1\\n\\n  sign_name = \\'YcmError\\' if is_error else \\'YcmWarning\\'\\n  vim.command( \\'sign place {0} name={1} line={2} buffer={3}\\'.format(\\n    sign_id, sign_name, line_num, buffer_num ) )', 'def AddDiagnosticSyntaxMatch( line_num,\\n                              column_num,\\n                              line_end_num = None,\\n                              column_end_num = None,\\n                              is_error = True ):\\n  \"\"\"Highlight a range in the current window starting from\\n  (|line_num|, |column_num|) included to (|line_end_num|, |column_end_num|)\\n  excluded. If |line_end_num| or |column_end_num| are not given, highlight the\\n  character at (|line_num|, |column_num|). Both line and column numbers are\\n  1-based. Return the ID of the newly added match.\"\"\"\\n  group = \\'YcmErrorSection\\' if is_error else \\'YcmWarningSection\\'\\n\\n  line_num, column_num = LineAndColumnNumbersClamped( line_num, column_num )\\n\\n  if not line_end_num or not column_end_num:\\n    return GetIntValue(\\n      \"matchadd(\\'{0}\\', \\'\\\\%{1}l\\\\%{2}c\\')\".format( group, line_num, column_num ) )\\n\\n  # -1 and then +1 to account for column end not included in the range.\\n  line_end_num, column_end_num = LineAndColumnNumbersClamped(\\n      line_end_num, column_end_num - 1 )\\n  column_end_num += 1\\n\\n  return GetIntValue(\\n    \"matchadd(\\'{0}\\', \\'\\\\%{1}l\\\\%{2}c\\\\_.\\\\\\\\{{-}}\\\\%{3}l\\\\%{4}c\\')\".format(\\n      group, line_num, column_num, line_end_num, column_end_num ) )', 'def LineAndColumnNumbersClamped( line_num, column_num ):\\n  new_line_num = line_num\\n  new_column_num = column_num\\n\\n  max_line = len( vim.current.buffer )\\n  if line_num and line_num > max_line:\\n    new_line_num = max_line\\n\\n  max_column = len( vim.current.buffer[ new_line_num - 1 ] )\\n  if column_num and column_num > max_column:\\n    new_column_num = max_column\\n\\n  return new_line_num, new_column_num', 'def OpenLocationList( focus = False, autoclose = False ):\\n  \"\"\"Open the location list to full width at the bottom of the screen with its\\n  height automatically set to fit all entries. This behavior can be overridden\\n  by using the YcmLocationOpened autocommand. When focus is set to True, the\\n  location list window becomes the active window. When autoclose is set to True,\\n  the location list window is automatically closed after an entry is\\n  selected.\"\"\"\\n  vim.command( \\'botright lopen\\' )\\n\\n  SetFittingHeightForCurrentWindow()\\n\\n  if autoclose:\\n    # This autocommand is automatically removed when the location list window is\\n    # closed.\\n    vim.command( \\'au WinLeave <buffer> q\\' )\\n\\n  if VariableExists( \\'#User#YcmLocationOpened\\' ):\\n    vim.command( \\'doautocmd User YcmLocationOpened\\' )\\n\\n  if not focus:\\n    JumpToPreviousWindow()', 'def OpenQuickFixList( focus = False, autoclose = False ):\\n  \"\"\"Open the quickfix list to full width at the bottom of the screen with its\\n  height automatically set to fit all entries. This behavior can be overridden\\n  by using the YcmQuickFixOpened autocommand.\\n  See the OpenLocationList function for the focus and autoclose options.\"\"\"\\n  vim.command( \\'botright copen\\' )\\n\\n  SetFittingHeightForCurrentWindow()\\n\\n  if autoclose:\\n    # This autocommand is automatically removed when the quickfix window is\\n    # closed.\\n    vim.command( \\'au WinLeave <buffer> q\\' )\\n\\n  if VariableExists( \\'#User#YcmQuickFixOpened\\' ):\\n    vim.command( \\'doautocmd User YcmQuickFixOpened\\' )\\n\\n  if not focus:\\n    JumpToPreviousWindow()', 'def ConvertDiagnosticsToQfList( diagnostics ):\\n  def ConvertDiagnosticToQfFormat( diagnostic ):\\n    # See :h getqflist for a description of the dictionary fields.\\n    # Note that, as usual, Vim is completely inconsistent about whether\\n    # line/column numbers are 1 or 0 based in its various APIs. Here, it wants\\n    # them to be 1-based. The documentation states quite clearly that it\\n    # expects a byte offset, by which it means \"1-based column number\" as\\n    # described in :h getqflist (\"the first column is 1\").\\n    location = diagnostic[ \\'location\\' ]\\n    line_num = location[ \\'line_num\\' ]\\n\\n    # libclang can give us diagnostics that point \"outside\" the file; Vim borks\\n    # on these.\\n    if line_num < 1:\\n      line_num = 1\\n\\n    text = diagnostic[ \\'text\\' ]\\n    if diagnostic.get( \\'fixit_available\\', False ):\\n      text += \\' (FixIt available)\\'\\n\\n    return {\\n      \\'bufnr\\' : GetBufferNumberForFilename( location[ \\'filepath\\' ] ),\\n      \\'lnum\\'  : line_num,\\n      \\'col\\'   : location[ \\'column_num\\' ],\\n      \\'text\\'  : text,\\n      \\'type\\'  : diagnostic[ \\'kind\\' ][ 0 ],\\n      \\'valid\\' : 1\\n    }\\n\\n  return [ ConvertDiagnosticToQfFormat( x ) for x in diagnostics ]', 'def VimExpressionToPythonType( vim_expression ):\\n  \"\"\"Returns a Python type from the return value of the supplied Vim expression.\\n  If the expression returns a list, dict or other non-string type, then it is\\n  returned unmodified. If the string return can be converted to an\\n  integer, returns an integer, otherwise returns the result converted to a\\n  Unicode string.\"\"\"\\n\\n  result = vim.eval( vim_expression )\\n  if not ( isinstance( result, str ) or isinstance( result, bytes ) ):\\n    return result\\n\\n  try:\\n    return int( result )\\n  except ValueError:\\n    return ToUnicode( result )', 'def BufferIsUsable( buffer_object ):\\n  return not BufferModified( buffer_object ) or HiddenEnabled( buffer_object )', \"def TryJumpLocationInOpenedTab( filename, line, column ):\\n  filepath = os.path.realpath( filename )\\n\\n  for tab in vim.tabpages:\\n    for win in tab.windows:\\n      if win.buffer.name == filepath:\\n        vim.current.tabpage = tab\\n        vim.current.window = win\\n        vim.current.window.cursor = ( line, column - 1 )\\n\\n        # Center the screen on the jumped-to location\\n        vim.command( 'normal! zz' )\\n        return True\\n  # 'filename' is not opened in any tab pages\\n  return False\", \"def GetVimCommand( user_command, default = 'edit' ):\\n  vim_command = BUFFER_COMMAND_MAP.get( user_command, default )\\n  if vim_command == 'edit' and not BufferIsUsable( vim.current.buffer ):\\n    vim_command = 'split'\\n  return vim_command\", 'def JumpToLocation( filename, line, column ):\\n  # Add an entry to the jumplist\\n  vim.command( \"normal! m\\'\" )\\n\\n  if filename != GetCurrentBufferFilepath():\\n    # We prefix the command with \\'keepjumps\\' so that opening the file is not\\n    # recorded in the jumplist. So when we open the file and move the cursor to\\n    # a location in it, the user can use CTRL-O to jump back to the original\\n    # location, not to the start of the newly opened file.\\n    # Sadly this fails on random occasions and the undesired jump remains in the\\n    # jumplist.\\n    user_command = user_options_store.Value( \\'goto_buffer_command\\' )\\n\\n    if user_command == \\'new-or-existing-tab\\':\\n      if TryJumpLocationInOpenedTab( filename, line, column ):\\n        return\\n      user_command = \\'new-tab\\'\\n\\n    vim_command = GetVimCommand( user_command )\\n    try:\\n      vim.command( \\'keepjumps {0} {1}\\'.format( vim_command,\\n                                               EscapedFilepath( filename ) ) )\\n    # When the file we are trying to jump to has a swap file\\n    # Vim opens swap-exists-choices dialog and throws vim.error with E325 error,\\n    # or KeyboardInterrupt after user selects one of the options.\\n    except vim.error as e:\\n      if \\'E325\\' not in str( e ):\\n        raise\\n      # Do nothing if the target file is still not opened (user chose (Q)uit)\\n      if filename != GetCurrentBufferFilepath():\\n        return\\n    # Thrown when user chooses (A)bort in .swp message box\\n    except KeyboardInterrupt:\\n      return\\n  vim.current.window.cursor = ( line, column - 1 )\\n\\n  # Center the screen on the jumped-to location\\n  vim.command( \\'normal! zz\\' )', 'def PostVimMessage( message, warning = True, truncate = False ):\\n  \"\"\"Display a message on the Vim status line. By default, the message is\\n  highlighted and logged to Vim command-line history (see :h history).\\n  Unset the |warning| parameter to disable this behavior. Set the |truncate|\\n  parameter to avoid hit-enter prompts (see :h hit-enter) when the message is\\n  longer than the window width.\"\"\"\\n  echo_command = \\'echom\\' if warning else \\'echo\\'\\n\\n  # Displaying a new message while previous ones are still on the status line\\n  # might lead to a hit-enter prompt or the message appearing without a\\n  # newline so we do a redraw first.\\n  vim.command( \\'redraw\\' )\\n\\n  if warning:\\n    vim.command( \\'echohl WarningMsg\\' )\\n\\n  message = ToUnicode( message )\\n\\n  if truncate:\\n    vim_width = GetIntValue( \\'&columns\\' )\\n\\n    message = message.replace( \\'\\\\n\\', \\' \\' )\\n    if len( message ) > vim_width:\\n      message = message[ : vim_width - 4 ] + \\'...\\'\\n\\n    old_ruler = GetIntValue( \\'&ruler\\' )\\n    old_showcmd = GetIntValue( \\'&showcmd\\' )\\n    vim.command( \\'set noruler noshowcmd\\' )\\n\\n    vim.command( \"{0} \\'{1}\\'\".format( echo_command,\\n                                     EscapeForVim( message ) ) )\\n\\n    SetVariableValue( \\'&ruler\\', old_ruler )\\n    SetVariableValue( \\'&showcmd\\', old_showcmd )\\n  else:\\n    for line in message.split( \\'\\\\n\\' ):\\n      vim.command( \"{0} \\'{1}\\'\".format( echo_command,\\n                                       EscapeForVim( line ) ) )\\n\\n  if warning:\\n    vim.command( \\'echohl None\\' )', 'def Confirm( message ):\\n  \"\"\"Display |message| with Ok/Cancel operations. Returns True if the user\\n  selects Ok\"\"\"\\n  return bool( PresentDialog( message, [ \"Ok\", \"Cancel\" ] ) == 0 )', 'def EscapeForVim( text ):\\n  return ToUnicode( text.replace( \"\\'\", \"\\'\\'\" ) )', 'def GetBufferFiletypes( bufnr ):\\n  command = \\'getbufvar({0}, \"&ft\")\\'.format( bufnr )\\n  return VimExpressionToPythonType( command ).split( \\'.\\' )', 'def VariableExists( variable ):\\n  return GetBoolValue( \"exists( \\'{0}\\' )\".format( EscapeForVim( variable ) ) )', 'def GetVariableValue( variable ):\\n  return vim.eval( variable )', 'def GetIntValue( variable ):\\n  return int( vim.eval( variable ) )', 'def _GetNumNonVisibleFiles( file_list ):\\n  \"\"\"Returns the number of file in the iterable list of files |file_list| which\\n  are not curerntly open in visible windows\"\"\"\\n  return len(\\n      [ f for f in file_list\\n        if not BufferIsVisible( GetBufferNumberForFilename( f, False ) ) ] )', 'def ReplaceChunks( chunks ):\\n  \"\"\"Apply the source file deltas supplied in |chunks| to arbitrary files.\\n  |chunks| is a list of changes defined by ycmd.responses.FixItChunk,\\n  which may apply arbitrary modifications to arbitrary files.\\n\\n  If a file specified in a particular chunk is not currently open in a visible\\n  buffer (i.e., one in a window visible in the current tab), we:\\n    - issue a warning to the user that we\\'re going to open new files (and offer\\n      her the option to abort cleanly)\\n    - open the file in a new split, make the changes, then hide the buffer.\\n\\n  If for some reason a file could not be opened or changed, raises RuntimeError.\\n  Otherwise, returns no meaningful value.\"\"\"\\n\\n  # We apply the edits file-wise for efficiency, and because we must track the\\n  # file-wise offset deltas (caused by the modifications to the text).\\n  chunks_by_file = _SortChunksByFile( chunks )\\n\\n  # We sort the file list simply to enable repeatable testing\\n  sorted_file_list = sorted( iterkeys( chunks_by_file ) )\\n\\n  # Make sure the user is prepared to have her screen mutilated by the new\\n  # buffers\\n  num_files_to_open = _GetNumNonVisibleFiles( sorted_file_list )\\n\\n  if num_files_to_open > 0:\\n    if not Confirm(\\n            FIXIT_OPENING_BUFFERS_MESSAGE_FORMAT.format( num_files_to_open ) ):\\n      return\\n\\n  # Store the list of locations where we applied changes. We use this to display\\n  # the quickfix window showing the user where we applied changes.\\n  locations = []\\n\\n  for filepath in sorted_file_list:\\n    ( buffer_num, close_window ) = _OpenFileInSplitIfNeeded( filepath )\\n\\n    ReplaceChunksInBuffer( chunks_by_file[ filepath ],\\n                           vim.buffers[ buffer_num ],\\n                           locations )\\n\\n    # When opening tons of files, we don\\'t want to have a split for each new\\n    # file, as this simply does not scale, so we open the window, make the\\n    # edits, then hide the window.\\n    if close_window:\\n      # Some plugins (I\\'m looking at you, syntastic) might open a location list\\n      # for the window we just opened. We don\\'t want that location list hanging\\n      # around, so we close it. lclose is a no-op if there is no location list.\\n      vim.command( \\'lclose\\' )\\n\\n      # Note that this doesn\\'t lose our changes. It simply \"hides\" the buffer,\\n      # which can later be re-accessed via the quickfix list or `:ls`\\n      vim.command( \\'hide\\' )\\n\\n  # Open the quickfix list, populated with entries for each location we changed.\\n  if locations:\\n    SetQuickFixList( locations )\\n    OpenQuickFixList()\\n\\n  PostVimMessage( \\'Applied {0} changes\\'.format( len( chunks ) ),\\n                  warning = False )', \"def ReplaceChunk( start, end, replacement_text, line_delta, char_delta,\\n                  vim_buffer, locations = None ):\\n  # ycmd's results are all 1-based, but vim's/python's are all 0-based\\n  # (so we do -1 on all of the values)\\n  start_line = start[ 'line_num' ] - 1 + line_delta\\n  end_line = end[ 'line_num' ] - 1 + line_delta\\n\\n  source_lines_count = end_line - start_line + 1\\n  start_column = start[ 'column_num' ] - 1 + char_delta\\n  end_column = end[ 'column_num' ] - 1\\n  if source_lines_count == 1:\\n    end_column += char_delta\\n\\n  # NOTE: replacement_text is unicode, but all our offsets are byte offsets,\\n  # so we convert to bytes\\n  replacement_lines = ToBytes( replacement_text ).splitlines( False )\\n  if not replacement_lines:\\n    replacement_lines = [ bytes( b'' ) ]\\n  replacement_lines_count = len( replacement_lines )\\n\\n  # NOTE: Vim buffers are a list of byte objects on Python 2 but unicode\\n  # objects on Python 3.\\n  end_existing_text = ToBytes( vim_buffer[ end_line ] )[ end_column : ]\\n  start_existing_text = ToBytes( vim_buffer[ start_line ] )[ : start_column ]\\n\\n  new_char_delta = ( len( replacement_lines[ -1 ] )\\n                     - ( end_column - start_column ) )\\n  if replacement_lines_count > 1:\\n    new_char_delta -= start_column\\n\\n  replacement_lines[ 0 ] = start_existing_text + replacement_lines[ 0 ]\\n  replacement_lines[ -1 ] = replacement_lines[ -1 ] + end_existing_text\\n\\n  vim_buffer[ start_line : end_line + 1 ] = replacement_lines[:]\\n\\n  if locations is not None:\\n    locations.append( {\\n      'bufnr': vim_buffer.number,\\n      'filename': vim_buffer.name,\\n      # line and column numbers are 1-based in qflist\\n      'lnum': start_line + 1,\\n      'col': start_column + 1,\\n      'text': replacement_text,\\n      'type': 'F',\\n    } )\\n\\n  new_line_delta = replacement_lines_count - source_lines_count\\n  return ( new_line_delta, new_char_delta )\", 'def SearchInCurrentBuffer( pattern ):\\n  \"\"\" Returns the 1-indexed line on which the pattern matches\\n  (going UP from the current position) or 0 if not found \"\"\"\\n  return GetIntValue( \"search(\\'{0}\\', \\'Wcnb\\')\".format( EscapeForVim( pattern )))', 'def ClosePreviewWindow():\\n  \"\"\" Close the preview window if it is present, otherwise do nothing \"\"\"\\n  vim.command( \\'silent! pclose!\\' )', 'def JumpToPreviousWindow():\\n  \"\"\" Jump the vim cursor to its previous window position \"\"\"\\n  vim.command( \\'silent! wincmd p\\' )', 'def OpenFileInPreviewWindow( filename ):\\n  \"\"\" Open the supplied filename in the preview window \"\"\"\\n  vim.command( \\'silent! pedit! \\' + filename )', 'def BufferIsVisibleForFilename( filename ):\\n  \"\"\"Check if a buffer exists for a specific file.\"\"\"\\n  buffer_number = GetBufferNumberForFilename( filename, False )\\n  return BufferIsVisible( buffer_number )', 'def OpenFilename( filename, options = {} ):\\n  \"\"\"Open a file in Vim. Following options are available:\\n  - command: specify which Vim command is used to open the file. Choices\\n  are same-buffer, horizontal-split, vertical-split, and new-tab (default:\\n  horizontal-split);\\n  - size: set the height of the window for a horizontal split or the width for\\n  a vertical one (default: \\'\\');\\n  - fix: set the winfixheight option for a horizontal split or winfixwidth for\\n  a vertical one (default: False). See :h winfix for details;\\n  - focus: focus the opened file (default: False);\\n  - watch: automatically watch for changes (default: False). This is useful\\n  for logs;\\n  - position: set the position where the file is opened (default: start).\\n  Choices are start and end.\"\"\"\\n\\n  # Set the options.\\n  command = GetVimCommand( options.get( \\'command\\', \\'horizontal-split\\' ),\\n                           \\'horizontal-split\\' )\\n  size = ( options.get( \\'size\\', \\'\\' ) if command in [ \\'split\\', \\'vsplit\\' ] else\\n           \\'\\' )\\n  focus = options.get( \\'focus\\', False )\\n\\n  # There is no command in Vim to return to the previous tab so we need to\\n  # remember the current tab if needed.\\n  if not focus and command == \\'tabedit\\':\\n    previous_tab = GetIntValue( \\'tabpagenr()\\' )\\n  else:\\n    previous_tab = None\\n\\n  # Open the file.\\n  try:\\n    vim.command( \\'{0}{1} {2}\\'.format( size, command, filename ) )\\n  # When the file we are trying to jump to has a swap file,\\n  # Vim opens swap-exists-choices dialog and throws vim.error with E325 error,\\n  # or KeyboardInterrupt after user selects one of the options which actually\\n  # opens the file (Open read-only/Edit anyway).\\n  except vim.error as e:\\n    if \\'E325\\' not in str( e ):\\n      raise\\n\\n    # Otherwise, the user might have chosen Quit. This is detectable by the\\n    # current file not being the target file\\n    if filename != GetCurrentBufferFilepath():\\n      return\\n  except KeyboardInterrupt:\\n    # Raised when the user selects \"Abort\" after swap-exists-choices\\n    return\\n\\n  _SetUpLoadedBuffer( command,\\n                      filename,\\n                      options.get( \\'fix\\', False ),\\n                      options.get( \\'position\\', \\'start\\' ),\\n                      options.get( \\'watch\\', False ) )\\n\\n  # Vim automatically set the focus to the opened file so we need to get the\\n  # focus back (if the focus option is disabled) when opening a new tab or\\n  # window.\\n  if not focus:\\n    if command == \\'tabedit\\':\\n      JumpToTab( previous_tab )\\n    if command in [ \\'split\\', \\'vsplit\\' ]:\\n      JumpToPreviousWindow()']}, {'features': [], 'snippets': ['def __init__(self, args):\\n        self.args = args\\n        self.setattributes(self.args)', 'def summarise(self, base_list, trusted_placements, reverse_pipe, times,\\n                  hit_read_count_list, max_samples_for_krona):\\n        \\'\\'\\'\\n        summarise - write summary information to file, including otu table, biom\\n                    file, krona plot, and timing information\\n\\n        Parameters\\n        ----------\\n        base_list : array\\n            list of each of the files processed by graftm, with the path and\\n            and suffixed removed\\n        trusted_placements : dict\\n            dictionary of placements with entry as the key, a taxonomy string\\n            as the value\\n        reverse_pipe : bool\\n            True = run reverse pipe, False = run normal pipeline\\n        times : array\\n            list of the recorded times for each step in the pipeline in the\\n            format: [search_step_time, alignment_step_time, placement_step_time]\\n        hit_read_count_list : array\\n            list containing sublists, one for each file run through the GraftM\\n            pipeline, each two entries, the first being the number of putative\\n            eukaryotic reads (when searching 16S), the second being the number\\n            of hits aligned and placed in the tree.\\n        max_samples_for_krona: int\\n            If the number of files processed is greater than this number, then\\n            do not generate a krona diagram.\\n        Returns\\n        -------\\n        \\'\\'\\'\\n\\n        # Summary steps.\\n        placements_list = []\\n        for base in base_list:\\n            # First assign the hash that contains all of the trusted placements\\n            # to a variable to it can be passed to otu_builder, to be written\\n            # to a file. :)\\n            placements = trusted_placements[base]\\n            self.s.readTax(placements, GraftMFiles(base, self.args.output_directory, False).read_tax_output_path(base))\\n            placements_list.append(placements)\\n\\n        #Generate coverage table\\n        #logging.info(\\'Building coverage table for %s\\' % base)\\n        #self.s.coverage_of_hmm(self.args.aln_hmm_file,\\n        #                         self.gmf.summary_table_output_path(base),\\n        #                         self.gmf.coverage_table_path(base),\\n        #                         summary_dict[base][\\'read_length\\'])\\n\\n        logging.info(\\'Writing summary table\\')\\n        with open(self.gmf.combined_summary_table_output_path(), \\'w\\') as f:\\n            self.s.write_tabular_otu_table(base_list, placements_list, f)\\n\\n        logging.info(\\'Writing biom file\\')\\n        with biom_open(self.gmf.combined_biom_output_path(), \\'w\\') as f:\\n            biom_successful = self.s.write_biom(base_list, placements_list, f)\\n        if not biom_successful:\\n            os.remove(self.gmf.combined_biom_output_path())\\n\\n        logging.info(\\'Building summary krona plot\\')\\n        if len(base_list) > max_samples_for_krona:\\n            logging.warn(\"Skipping creation of Krona diagram since there are too many input files. The maximum can be overridden using --max_samples_for_krona\")\\n        else:\\n            self.s.write_krona_plot(base_list, placements_list, self.gmf.krona_output_path())\\n\\n        # Basic statistics\\n        placed_reads=[len(trusted_placements[base]) for base in base_list]\\n        self.s.build_basic_statistics(times, hit_read_count_list, placed_reads, \\\\\\n                                      base_list, self.gmf.basic_stats_path())\\n\\n        # Delete unnecessary files\\n        logging.info(\\'Cleaning up\\')\\n        for base in base_list:\\n            directions = [\\'forward\\', \\'reverse\\']\\n            if reverse_pipe:\\n                for i in range(0,2):\\n                    self.gmf = GraftMFiles(base, self.args.output_directory, directions[i])\\n                    self.hk.delete([self.gmf.for_aln_path(base),\\n                                    self.gmf.rev_aln_path(base),\\n                                    self.gmf.conv_output_rev_path(base),\\n                                    self.gmf.conv_output_for_path(base),\\n                                    self.gmf.euk_free_path(base),\\n                                    self.gmf.euk_contam_path(base),\\n                                    self.gmf.readnames_output_path(base),\\n                                    self.gmf.sto_output_path(base),\\n                                    self.gmf.orf_titles_output_path(base),\\n                                    self.gmf.orf_output_path(base),\\n                                    self.gmf.output_for_path(base),\\n                                    self.gmf.output_rev_path(base)])\\n            else:\\n                self.gmf = GraftMFiles(base, self.args.output_directory, False)\\n                self.hk.delete([self.gmf.for_aln_path(base),\\n                                self.gmf.rev_aln_path(base),\\n                                self.gmf.conv_output_rev_path(base),\\n                                self.gmf.conv_output_for_path(base),\\n                                self.gmf.euk_free_path(base),\\n                                self.gmf.euk_contam_path(base),\\n                                self.gmf.readnames_output_path(base),\\n                                self.gmf.sto_output_path(base),\\n                                self.gmf.orf_titles_output_path(base),\\n                                self.gmf.orf_output_path(base),\\n                                self.gmf.output_for_path(base),\\n                                self.gmf.output_rev_path(base)])\\n\\n        logging.info(\\'Done, thanks for using graftM!\\\\n\\')', 'def _assign_taxonomy_with_diamond(self, base_list, db_search_results,\\n                                      graftm_package, graftm_files):\\n        \\'\\'\\'Run diamond to assign taxonomy\\n\\n        Parameters\\n        ----------\\n        base_list: list of str\\n            list of sequence block names\\n        db_search_results: list of DBSearchResult\\n            the result of running hmmsearches\\n        graftm_package: GraftMPackage object\\n            Diamond is run against this database\\n        graftm_files: GraftMFiles object\\n            Result files are written here\\n\\n        Returns\\n        -------\\n        list of\\n        1. time taken for assignment\\n        2. assignments i.e. dict of base_list entry to dict of read names to\\n            to taxonomies, or None if there was no hit detected.\\n        \\'\\'\\'\\n        runner = Diamond(graftm_package.diamond_database_path(),\\n                         self.args.threads,\\n                         self.args.evalue)\\n        taxonomy_definition = Getaxnseq().read_taxtastic_taxonomy_and_seqinfo\\\\\\n                (open(graftm_package.taxtastic_taxonomy_path()),\\n                 open(graftm_package.taxtastic_seqinfo_path()))\\n        results = {}\\n\\n        # For each of the search results,\\n        for i, search_result in enumerate(db_search_results):\\n            if search_result.hit_fasta() is None:\\n                sequence_id_to_taxonomy = {}\\n            else:\\n                sequence_id_to_hit = {}\\n                # Run diamond\\n                logging.debug(\"Running diamond on %s\" % search_result.hit_fasta())\\n                diamond_result = runner.run(search_result.hit_fasta(),\\n                                            UnpackRawReads.PROTEIN_SEQUENCE_TYPE,\\n                                            daa_file_basename=graftm_files.diamond_assignment_output_basename(base_list[i]))\\n                for res in diamond_result.each([SequenceSearchResult.QUERY_ID_FIELD,\\n                                                SequenceSearchResult.HIT_ID_FIELD]):\\n                    if res[0] in sequence_id_to_hit:\\n                        # do not accept duplicates\\n                        if sequence_id_to_hit[res[0]] != res[1]:\\n                            raise Exception(\"Diamond unexpectedly gave two hits for a single query sequence for %s\" % res[0])\\n                    else:\\n                        sequence_id_to_hit[res[0]] = res[1]\\n\\n                # Extract taxonomy of the best hit, and add in the no hits\\n                sequence_id_to_taxonomy = {}\\n                for seqio in SequenceIO().read_fasta_file(search_result.hit_fasta()):\\n                    name = seqio.name\\n                    if name in sequence_id_to_hit:\\n                        # Add Root; to be in line with pplacer assignment method\\n                        sequence_id_to_taxonomy[name] = [\\'Root\\']+taxonomy_definition[sequence_id_to_hit[name]]\\n                    else:\\n                        # picked up in the initial search (by hmmsearch, say), but diamond misses it\\n                        sequence_id_to_taxonomy[name] = [\\'Root\\']\\n\\n            results[base_list[i]] = sequence_id_to_taxonomy\\n        return results']}, {'features': [], 'snippets': ['def trappedWater(a, size) :']}, {'features': [], 'snippets': [\"def test ():\\n\\n    global options, args\\n    # TODO: Do something more interesting here...\\n    print 'Hello from the test() function!'\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def render(self, name, value, attrs=None, renderer=None):\\r\\n        output = []\\r\\n        output.append(super(AdminFileWidget, self).render(name, value, attrs)) # really for AdminFileWidget\\r\\n        instance = getattr(value, \\'instance\\', None)\\r\\n        if instance is not None and value:\\r\\n            output = [\\'<a target=\"_blank\" href=\"%s\"><img src=\"%s\" alt=\"%s\"/></a>\\' % \\\\\\r\\n                (instance.image.url, instance.thumb.url, instance.image)] + output\\r\\n        return mark_safe(u\\'\\'.join(output))', 'def value_from_datadict(self, data, files, name):', 'def value_from_datadict(self, data, files, name):\\r\\n        for key, file in files.items():\\r\\n            filename = file._get_name()\\r\\n            ext = u\"\"\\r\\n            if \\'.\\' in filename:\\r\\n                ext = u\".\" + filename.rpartition(\\'.\\')[2]\\r\\n            filename = filename.rpartition(\\'.\\')[0]\\r\\n            filename = re.sub(r\\'[_.,:;@#$%^&?*|()\\\\[\\\\]]\\', \\'-\\', filename)\\r\\n            filename = slugify(unidecode(smart_text(filename))) + ext\\r\\n            files[key]._set_name(filename)']}, {'features': [], 'snippets': ['def __init__(self, categories_by_concept, terms,\\n                 categories, tfidf, max_depth=5, min_df=20', 'def build(self):\\n        for i, c in enumerate(self.concept_by_id.values()):\\n            self(c)\\n            if not i % 100:\\n                t = float(len(self.concept_by_id.keys()))\\n                print i, int(t), round(i / t, 2)', \"def dump(self):\\n\\n        # Simulate a file with StringIO\\n        out = open('vector.dump.txt', 'wb')\\n\\n        for i, (_id, projections) in enumerate(self.projected.items()):\", 'def __call__(self, category):\\n        self.pulling = set([])\\n        return self.__pull(None, 0, category, dict([]))', 'def __pull(self, vector, depth, category, tree):\\n        _id = self.id_by_concept[category]\\n        if not self.pulling:', 'def __project(self, vector, tree):\\n        if not tree.keys():\\n            return\\n        else:\\n            for key, subtree in tree.items():\\n                _id = self.id_by_concept[key]\\n                self.projected[_id] += 1\\n                self.__add2vec(vector, _id)\\n                self.__project(vector, subtree)', 'def load(self):\\n        self.__load_terms()\\n        self.__load_categories()\\n        self.__load_assignments()', \"def __load_terms(self):\\n        for term, _id in from_csv(self.path_terms):\\n            _id = int(_id)\\n            self.term_by_id[_id] = term\\n            self.id_by_term[term] = _id\\n            if not term.startswith('Category:'):\\n                continue\\n            self.term_is_category[term] = True\\n            self.term_is_category[_id] = True\"]}, {'features': [], 'snippets': ['def drawSquare(turtle, x, y, length):\\n    turtle.up()\\n    turtle.move(x, y)\\n    turtle.setDirection(270)\\n    turtle.down()\\n    for count in xrange(4):\\n        turtle.move(length)\\n        turtle.turn(90)']}, {'features': [], 'snippets': [\"def is_old(node_info):\\n    '''\\n    Check if node or node.bl_idname is among\\n    the old nodes\\n    '''\\n    if isinstance(node_info, str):\\n        # assumes bl_idname\\n        return node_info in old_bl_idnames\\n    elif isinstance(node_info, bpy.types.Node):\\n        return node_info.bl_idname in old_bl_idnames\\n    else:\\n        return False\", 'def mark_old(node):\\n    if node.parent and node.parent.label == \"Deprecated node!\":\\n        return\\n    ng = node.id_data\\n    frame = ng.nodes.new(\"NodeFrame\")\\n    if node.parent:\\n        frame.parent = node.parent\\n    node.parent = frame\\n    frame.label = \"Deprecated node!\"\\n    frame.use_custom_color = True\\n    frame.color = (.8, 0, 0)\\n    frame.shrink = True', 'def load_old(ng):', 'def register_old(bl_id):\\n    if bl_id in old_bl_idnames:\\n        mod = importlib.import_module(\".{}\".format(old_bl_idnames[bl_id]), __name__)\\n        res = inspect.getmembers(mod)\\n        for name, cls in res:\\n            if inspect.isclass(cls):\\n                if issubclass(cls, bpy.types.Node) and cls.bl_idname == bl_id:\\n                    if bl_id not in imported_mods:\\n                        try:\\n                            mod.register()\\n                        except:\\n                            traceback.print_exc()\\n                        imported_mods[bl_id] = mod\\n                        return mod', 'def unregister_old(bl_id):\\n    global imported_mods\\n    mod = imported_mods.get(bl_id)\\n    if mod:\\n        #print(\"Unloaded old node type {}\".format(bl_id)) \\n        mod.unregister()\\n        del imported_mods[bl_id]']}, {'features': [], 'snippets': ['def requires(self):\\n        yield DownloadRITACatalogs()\\n        yield DownloadRITAData()', 'def requires(self):\\n        baseurl = \"https://www.transtats.bts.gov\"\\n        url = \"https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236\"\\n        page = requests.get(url)\\n\\n        soup = BeautifulSoup(page.content, \"lxml\")\\n        for link in soup.find_all(\\'a\\', href=re.compile(\\'Download_Lookup\\')):\\n            catalog_name = link.get(\\'href\\').split(\\'=L_\\')[-1]\\n            catalog_url = \\'{}/{}\\'.format(baseurl, link.get(\\'href\\'))\\n            yield DownloadCatalog(catalog_name=catalog_name, catalog_url=catalog_url)', 'def run(self):\\n        logger.debug(\"Guardando en {} el catálogo {}\".format(self.output().path, self.catalog_name))\\n\\n        with closing(requests.get(self.catalog_url, stream= True)) as response, \\\\\\n             self.output().open(\\'w\\') as output_file:\\n            for chunk in response.iter_lines(chunk_size=1024*8):\\n                if chunk:\\n                    output_file.write(chunk.decode(\\'utf-8\\') + \\'\\\\n\\')', 'def requires(self):\\n        today = datetime.date.today() + datetime.timedelta(days=-90)\\n\\n        max_year = today.year\\n        max_month = today.month\\n\\n        years = range(self.start_year, max_year)\\n\\n        logger.info(\"Descargando datos de los años {}\".format(years))\\n\\n        for año in years:\\n            if año != max_year:\\n                months = range(1,13)\\n            else:\\n                month = range(1, max_month+1)\\n            for mes in months:\\n                yield DownloadRITAMonthlyData(year=año, month=mes)', \"def cmd(self):\\n        return '''\\n               docker run --rm --env AWS_ACCESS_KEY_ID={} --env AWS_SECRET_ACCESS_KEY={} rita/download-rita --year {} --month {} --data_path {}/{} \\n        '''.format(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, self.year, self.month, self.root_path, self.raw_path)\", 'def requires(self):\\n        return DownloadRITA(year=self.year, month=self.month)', \"def output(self):\\n        return luigi.s3.S3Target('{}/{}/{}/YEAR={}/{}.psv'.format(self.root_path,\\n                                                                  self.etl_path,\\n                                                                  self.task_name,\\n                                                                  self.year,\\n                                                                  str(self.month).zfill(2)))\", 'def requires(self):\\n        return RawData()', 'def output(self):\\n        return luigi.LocalTarget(os.path.join(os.getcwd(), \"data\", \"hola_mundo_desde_R.psv\"))', 'def requires(self):\\n        return RTask()']}, {'features': [], 'snippets': ['def qInitResources():\\n    QtCore.qRegisterResourceData(rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n\\n        if Env.get(\\'desktop\\'):\\n            self.updater = DesktopUpdater()\\n        elif os.path.isdir(os.path.join(Env.get(\\'app_dir\\'), \\'.git\\')):\\n            self.updater = GitUpdater(self.conf(\\'git_command\\', default = \\'git\\'))\\n        else:\\n            self.updater = SourceUpdater()\\n\\n        fireEvent(\\'schedule.interval\\', \\'updater.check\\', self.autoUpdate, hours = 6)\\n        addEvent(\\'app.load\\', self.autoUpdate)\\n        addEvent(\\'updater.info\\', self.info)\\n\\n        addApiView(\\'updater.info\\', self.getInfo, docs = {\\n            \\'desc\\': \\'Get updater information\\',\\n            \\'return\\': {\\n                \\'type\\': \\'object\\',\\n                \\'example\\': \"\"\"{\\n        \\'last_check\\': \"last checked for update\",\\n        \\'update_version\\': \"available update version or empty\",\\n        \\'version\\': current_cp_version', 'def autoUpdate(self):\\n        if self.check() and self.conf(\\'automatic\\') and not self.updater.update_failed:\\n            if self.updater.doUpdate():\\n\\n                # Notify before restarting\\n                try:\\n                    if self.conf(\\'notification\\'):\\n                        info = self.updater.info()\\n                        version_date = datetime.fromtimestamp(info[\\'update_version\\'][\\'date\\'])\\n                        fireEvent(\\'updater.updated\\', \\'Updated to a new version with hash \"%s\", this version is from %s\\' % (info[\\'update_version\\'][\\'hash\\'], version_date), data = info)\\n                except:\\n                    log.error(\\'Failed notifying for update: %s\\', traceback.format_exc())\\n\\n                fireEventAsync(\\'app.restart\\')\\n\\n                return True\\n\\n        return False', 'def info(self):\\n        return self.updater.info()', \"def checkView(self):\\n        return jsonified({\\n            'update_available': self.check(),\\n            'info': self.updater.info()\\n        })\", 'def doUpdate(self):\\n        pass', \"def info(self):\\n        return {\\n            'last_check': self.last_check,\\n            'update_version': self.update_version,\\n            'version': self.getVersion(),\\n            'repo_name': '%s/%s' % (self.repo_user, self.repo_name),\\n            'branch': self.branch,\\n        }\", \"def deletePyc(self, only_excess = True):\\n\\n        for root, dirs, files in os.walk(ss(Env.get('app_dir'))):\\n\\n            pyc_files = filter(lambda filename: filename.endswith('.pyc'), files)\\n            py_files = set(filter(lambda filename: filename.endswith('.py'), files))\\n            excess_pyc_files = filter(lambda pyc_filename: pyc_filename[:-1] not in py_files, pyc_files) if only_excess else pyc_files\\n\\n            for excess_pyc_file in excess_pyc_files:\\n                full_path = os.path.join(root, excess_pyc_file)\\n                log.debug('Removing old PYC file: %s', full_path)\\n                try:\\n                    os.remove(full_path)\\n                except:\\n                    log.error('Couldn\\\\'t remove %s: %s', (full_path, traceback.format_exc()))\\n\\n            for dir_name in dirs:\\n                full_path = os.path.join(root, dir_name)\\n                if len(os.listdir(full_path)) == 0:\\n                    try:\\n                        os.rmdir(full_path)\\n                    except:\\n                        log.error('Couldn\\\\'t remove empty directory %s: %s', (full_path, traceback.format_exc()))\", \"def __init__(self, git_command):\\n        self.repo = LocalRepository(Env.get('app_dir'), command = git_command)\", \"def getVersion(self):\\n\\n        if not self.version:\\n            try:\\n                output = self.repo.getHead() # Yes, please\\n                log.debug('Git version output: %s', output.hash)\\n                self.version = {\\n                    'hash': output.hash[:8],\\n                    'date': output.getDate(),\\n                    'type': 'git',\\n                }\\n            except Exception, e:\\n                log.error('Failed using GIT updater, running from source, you need to have GIT installed. %s', e)\\n                return 'No GIT'\\n\\n        return self.version\", \"def __init__(self):\\n\\n        # Create version file in cache\\n        self.version_file = os.path.join(Env.get('cache_dir'), 'version')\\n        if not os.path.isfile(self.version_file):\\n            self.createFile(self.version_file, json.dumps(self.latestCommit()))\", 'def replaceWith(self, path):\\n        app_dir = ss(Env.get(\\'app_dir\\'))\\n\\n        # Get list of files we want to overwrite\\n        self.deletePyc()\\n        existing_files = []\\n        for root, subfiles, filenames in os.walk(app_dir):\\n            for filename in filenames:\\n                existing_files.append(os.path.join(root, filename))\\n\\n        for root, subfiles, filenames in os.walk(path):\\n            for filename in filenames:\\n                fromfile = os.path.join(root, filename)\\n                tofile = os.path.join(app_dir, fromfile.replace(path + os.path.sep, \\'\\'))\\n\\n                if not Env.get(\\'dev\\'):\\n                    try:\\n                        if os.path.isfile(tofile):\\n                            os.remove(tofile)\\n\\n                        dirname = os.path.dirname(tofile)\\n                        if not os.path.isdir(dirname):\\n                            self.makeDir(dirname)\\n\\n                        shutil.move(fromfile, tofile)\\n                        try:\\n                            existing_files.remove(tofile)\\n                        except ValueError:\\n                            pass\\n                    except:\\n                        log.error(\\'Failed overwriting file \"%s\": %s\\', (tofile, traceback.format_exc()))\\n                        return False\\n\\n        if Env.get(\\'app_dir\\') not in Env.get(\\'data_dir\\'):\\n            for still_exists in existing_files:\\n                try:\\n                    os.remove(still_exists)\\n                except:\\n                    log.error(\\'Failed removing non-used file: %s\\', traceback.format_exc())\\n\\n        return True', \"def getVersion(self):\\n\\n        if not self.version:\\n            try:\\n                f = open(self.version_file, 'r')\\n                output = json.loads(f.read())\\n                f.close()\\n\\n                log.debug('Source version output: %s', output)\\n                self.version = output\\n                self.version['type'] = 'source'\\n            except Exception, e:\\n                log.error('Failed using source updater. %s', e)\\n                return {}\\n\\n        return self.version\", \"def latestCommit(self):\\n        try:\\n            url = 'https://api.github.com/repos/%s/%s/commits?per_page=1&sha=%s' % (self.repo_user, self.repo_name, self.branch)\\n            data = self.getCache('github.commit', url = url)\\n            commit = json.loads(data)[0]\\n\\n            return {\\n                'hash': commit['sha'],\\n                'date':  int(time.mktime(parse(commit['commit']['committer']['date']).timetuple())),\\n            }\\n        except:\\n            log.error('Failed getting latest request from github: %s', traceback.format_exc())\\n\\n        return {}\", \"def __init__(self):\\n        self.desktop = Env.get('desktop')\", \"def do_restart(e):\\n                if e['status'] == 'done':\\n                    fireEventAsync('app.restart')\\n                elif e['status'] == 'error':\\n                    log.error('Failed updating desktop: %s', e['exception'])\\n                    self.update_failed = True\", \"def info(self):\\n        return {\\n            'last_check': self.last_check,\\n            'update_version': self.update_version,\\n            'version': self.getVersion(),\\n            'branch': self.branch,\\n        }\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def module_org():\\n    return entities.Organization().create()', 'def module_custom_product(module_org):\\n    return entities.Product(organization=module_org).create()', 'def module_org_with_manifest():\\n    org = entities.Organization().create()\\n    manifests.upload_manifest_locked(org.id)\\n    return org', 'def test_positive_sync_custom_repo(session, module_custom_product):\\n    \"\"\"Create Content Custom Sync with minimal input parameters\\n\\n    :id: 00fb0b04-0293-42c2-92fa-930c75acee89\\n\\n    :expectedresults: Sync procedure is successful\\n\\n    :CaseImportance: Critical\\n    \"\"\"\\n    repo = entities.Repository(\\n        url=FAKE_1_YUM_REPO, product=module_custom_product).create()\\n    with session:\\n        results = session.sync_status.synchronize([\\n            (module_custom_product.name, repo.name)])\\n        assert len(results) == 1\\n        assert results[0] == \\'Syncing Complete.\\'', 'def test_positive_sync_rh_repos(session, module_org_with_manifest):\\n    \"\"\"Create Content RedHat Sync with two repos.\\n\\n    :id: e30f6509-0b65-4bcc-a522-b4f3089d3911\\n\\n    :expectedresults: Sync procedure for RedHat Repos is successful\\n\\n    :CaseLevel: Integration\\n    \"\"\"\\n    repos = (\\n        SatelliteCapsuleRepository(cdn=True),\\n        RHELCloudFormsTools(cdn=True)\\n    )\\n    distros = [DISTRO_RHEL7, DISTRO_RHEL6]\\n    repo_collections = [\\n        RepositoryCollection(distro=distro, repositories=[repo])\\n        for distro, repo in zip(distros, repos)\\n    ]\\n    for repo_collection in repo_collections:\\n        repo_collection.setup(module_org_with_manifest.id, synchronize=False)\\n    repo_paths = [\\n        (\\n            repo.repo_data[\\'product\\'],\\n            repo.repo_data.get(\\'releasever\\'),\\n            repo.repo_data.get(\\'arch\\'),\\n            repo.repo_data[\\'name\\'],\\n        )\\n        for repo in repos\\n    ]\\n    with session:\\n        session.organization.select(org_name=module_org_with_manifest.name)\\n        results = session.sync_status.synchronize(repo_paths)\\n        assert len(results) == len(repo_paths)\\n        assert all([result == \\'Syncing Complete.\\' for result in results])', 'def test_positive_sync_custom_ostree_repo(session, module_custom_product):\\n    \"\"\"Create custom ostree repository and sync it.\\n\\n    :id: e4119b9b-0356-4661-a3ec-e5807224f7d2\\n\\n    :expectedresults: ostree repo should be synced successfully\\n\\n    :CaseLevel: Integration\\n    \"\"\"\\n    repo = entities.Repository(\\n        content_type=\\'ostree\\',\\n        url=FEDORA27_OSTREE_REPO,\\n        product=module_custom_product,\\n        unprotected=False,\\n    ).create()\\n    with session:\\n        results = session.sync_status.synchronize([\\n            (module_custom_product.name, repo.name)])\\n        assert len(results) == 1\\n        assert results[0] == \\'Syncing Complete.\\'', 'def test_positive_sync_rh_ostree_repo(session, module_org_with_manifest):\\n    \"\"\"Sync CDN based ostree repository.\\n\\n    :id: 4d28fff0-5fda-4eee-aa0c-c5af02c31de5\\n\\n    :Steps:\\n        1. Import a valid manifest\\n        2. Enable the OStree repo and sync it\\n\\n    :expectedresults: ostree repo should be synced successfully from CDN\\n\\n    :CaseLevel: Integration\\n    \"\"\"\\n    enable_rhrepo_and_fetchid(\\n        basearch=None,\\n        org_id=module_org_with_manifest.id,\\n        product=PRDS[\\'rhah\\'],\\n        repo=REPOS[\\'rhaht\\'][\\'name\\'],\\n        reposet=REPOSET[\\'rhaht\\'],\\n        releasever=None,\\n    )\\n    with session:\\n        session.organization.select(org_name=module_org_with_manifest.name)\\n        results = session.sync_status.synchronize([\\n            (PRDS[\\'rhah\\'], REPOS[\\'rhaht\\'][\\'name\\'])])\\n        assert len(results) == 1\\n        assert results[0] == \\'Syncing Complete.\\'']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        self.xe = IOSXE(node=node, username=username, password=password, disable_warnings=True)', \"def test_invalid_user_pass_returns_auth_error(self):\\n\\n        self.assertRaises(AuthError, IOSXE, node=node, username='stuff', password='things',\\n                          disable_warnings=True)\", \"def test_token_uri(self):\\n        self.assertEqual(self.xe.token_uri, '/auth/token-services')\"]}, {'features': [], 'snippets': ['def __init__(self, routes, **kwargs):\\n        super(ProxyArchivalRouter, self).__init__(routes, **kwargs)\\n        self.proxy = ProxyRouter(routes, **kwargs)', 'def __init__(self, routes, **kwargs):\\n        self.error_view = kwargs.get(\\'error_view\\')\\n\\n        proxy_options = kwargs.get(\\'config\\', {})\\n        if proxy_options:\\n            proxy_options = proxy_options.get(\\'proxy_options\\', {})\\n\\n        self.magic_name = proxy_options.get(\\'magic_name\\')\\n        if not self.magic_name:\\n            self.magic_name = self.DEF_MAGIC_NAME\\n            proxy_options[\\'magic_name\\'] = self.magic_name\\n\\n        self.extra_headers = proxy_options.get(\\'extra_headers\\')\\n        if not self.extra_headers:\\n            self.extra_headers = self.EXTRA_HEADERS\\n            proxy_options[\\'extra_headers\\'] = self.extra_headers\\n\\n        res_type = proxy_options.get(\\'cookie_resolver\\', True)\\n        if res_type == \\'auth\\' or not res_type:\\n            self.resolver = ProxyAuthResolver(routes, proxy_options)\\n        elif res_type == \\'ip\\':\\n            self.resolver = IPCacheResolver(routes, proxy_options)\\n        #elif res_type == True or res_type == \\'cookie\\':\\n        #    self.resolver = CookieResolver(routes, proxy_options)\\n        else:\\n            self.resolver = CookieResolver(routes, proxy_options)\\n\\n        self.use_banner = proxy_options.get(\\'use_banner\\', True)\\n        self.use_wombat = proxy_options.get(\\'use_client_rewrite\\', True)\\n\\n        self.proxy_cert_dl_view = proxy_options.get(\\'proxy_cert_download_view\\')\\n\\n        if not proxy_options.get(\\'enable_https_proxy\\'):\\n            self.ca = None\\n            return\\n\\n        try:\\n            from certauth.certauth import CertificateAuthority\\n        except ImportError:  #pragma: no cover\\n            print(\\'HTTPS proxy is not available as the \"certauth\" module \\' +\\n                  \\'is not installed\\')\\n            print(\\'Please install via \"pip install certauth\" \\' +\\n                  \\'to enable HTTPS support\\')\\n            self.ca = None\\n            return\\n\\n        # HTTPS Only Options\\n        ca_file = proxy_options.get(\\'root_ca_file\\', self.CA_ROOT_FILE)\\n\\n        # attempt to create the root_ca_file if doesn\\'t exist\\n        # (generally recommended to create this seperately)\\n        ca_name = proxy_options.get(\\'root_ca_name\\', self.CA_ROOT_NAME)\\n\\n        certs_dir = proxy_options.get(\\'certs_dir\\', self.CA_CERTS_DIR)\\n        self.ca = CertificateAuthority(ca_file=ca_file,\\n                                       certs_dir=certs_dir,\\n                                       ca_name=ca_name)\\n\\n        self.use_wildcard = proxy_options.get(\\'use_wildcard_certs\\', True)', \"def _chunk_encode(orig_iter):\\n        for chunk in orig_iter:\\n            if not len(chunk):\\n                continue\\n            chunk_len = b'%X\\\\r\\\\n' % len(chunk)\\n            yield chunk_len\\n            yield chunk\\n            yield b'\\\\r\\\\n'\\n\\n        yield b'0\\\\r\\\\n\\\\r\\\\n'\", \"def _buffer_response(status_headers, iterator):\\n        out = SpooledTemporaryFile(ProxyRouter.BUFF_RESPONSE_MEM_SIZE)\\n        size = 0\\n\\n        for buff in iterator:\\n            size += len(buff)\\n            out.write(buff)\\n\\n        content_length_str = str(size)\\n        # remove existing content length\\n        status_headers.replace_header('Content-Length',\\n                                      content_length_str)\\n\\n        out.seek(0)\\n        return RewriteContent.stream_to_gen(out)\", 'def handle_connect(self, env):\\n        sock = self.get_request_socket(env)\\n        if not sock:\\n            return WbResponse.text_response(\\'HTTPS Proxy Not Supported\\',\\n                                            \\'405 HTTPS Proxy Not Supported\\')\\n\\n        sock.send(b\\'HTTP/1.0 200 Connection Established\\\\r\\\\n\\')\\n        sock.send(b\\'Proxy-Connection: close\\\\r\\\\n\\')\\n        sock.send(b\\'Server: pywb proxy\\\\r\\\\n\\')\\n        sock.send(b\\'\\\\r\\\\n\\')\\n\\n        hostname, port = env[\\'REL_REQUEST_URI\\'].split(\\':\\')\\n\\n        if not self.use_wildcard:\\n            certfile = self.ca.cert_for_host(hostname)\\n        else:\\n            certfile = self.ca.get_wildcard_cert(hostname)\\n\\n        try:\\n            ssl_sock = ssl.wrap_socket(sock,\\n                                       server_side=True,\\n                                       certfile=certfile,\\n                                       #ciphers=\"ALL\",\\n                                       suppress_ragged_eofs=False,\\n                                       ssl_version=ssl.PROTOCOL_SSLv23\\n                                       )\\n            env[\\'pywb.proxy_ssl_sock\\'] = ssl_sock\\n\\n            buffreader = BufferedReader(ssl_sock, block_size=self.BLOCK_SIZE)\\n\\n            statusline = to_native_str(buffreader.readline().rstrip())\\n\\n        except Exception as se:\\n            raise BadRequestException(se.message)\\n\\n        statusparts = statusline.split(\\' \\')\\n\\n        if len(statusparts) < 3:\\n            raise BadRequestException(\\'Invalid Proxy Request: \\' + statusline)\\n\\n        env[\\'REQUEST_METHOD\\'] = statusparts[0]\\n        env[\\'REL_REQUEST_URI\\'] = (\\'https://\\' +\\n                                  env[\\'REL_REQUEST_URI\\'].replace(\\':443\\', \\'\\') +\\n                                  statusparts[1])\\n\\n        env[\\'SERVER_PROTOCOL\\'] = statusparts[2].strip()\\n\\n        env[\\'pywb.proxy_scheme\\'] = \\'https\\'\\n\\n        env[\\'pywb.proxy_host\\'] = hostname\\n        env[\\'pywb.proxy_port\\'] = port\\n        env[\\'pywb.proxy_req_uri\\'] = statusparts[1]\\n\\n        queryparts = env[\\'REL_REQUEST_URI\\'].split(\\'?\\', 1)\\n        env[\\'PATH_INFO\\'] = queryparts[0]\\n        env[\\'QUERY_STRING\\'] = queryparts[1] if len(queryparts) > 1 else \\'\\'\\n        env[\\'pywb.proxy_query\\'] = env[\\'QUERY_STRING\\']\\n\\n        while True:\\n            line = to_native_str(buffreader.readline())\\n            if line:\\n                line = line.rstrip()\\n\\n            if not line:\\n                break\\n\\n            parts = line.split(\\':\\', 1)\\n            if len(parts) < 2:\\n                continue\\n\\n            name = parts[0].strip()\\n            value = parts[1].strip()\\n\\n            name = name.replace(\\'-\\', \\'_\\').upper()\\n\\n            if name not in (\\'CONTENT_LENGTH\\', \\'CONTENT_TYPE\\'):\\n                name = \\'HTTP_\\' + name\\n\\n            env[name] = value\\n\\n        env[\\'wsgi.input\\'] = buffreader\\n        #remain = buffreader.rem_length()\\n        #if remain > 0:\\n            #remainder = buffreader.read()\\n            #env[\\'wsgi.input\\'] = BufferedReader(BytesIO(remainder))\\n            #remainder = buffreader.read(self.BLOCK_SIZE)\\n            #env[\\'wsgi.input\\'] = BufferedReader(ssl_sock,\\n            #                                   block_size=self.BLOCK_SIZE,\\n            #                                   starting_data=remainder)']}, {'features': [], 'snippets': ['def listeEuler(f, x0, y0, pas, n):\\n    x, y, L = x0, y0, []\\n    for k in range(n):\\n        L += [(x, y)]\\n        x += pas\\n        y += pas * f(x, y)\\n    return L']}, {'features': [], 'snippets': ['def get_rest_of_frame_molecule(frame_molecule, selected_molecule):\\n    # calc the rest\\n    selector = bridge.Select_AtomGroup(selected_molecule)\\n    selected = frame_molecule.select(selector)\\n    rest_molecule = frame_molecule ^ selected\\n\\n    return rest_molecule', 'def main():\\n    parser = argparse.ArgumentParser(\\n        description=\\'restructure brd file by reference file\\')\\n    parser.add_argument(\\'target_brd_path\\',\\n                        nargs=1,\\n                        help=\\'target brd file\\')\\n    parser.add_argument(\\'ref_brd_path\\',\\n                        nargs=1,\\n                        help=\\'reference brd file\\')\\n    parser.add_argument(\\'-o\\', \\'--output_path\\',\\n                        nargs=1,\\n                        default=[\"output.brd\"])\\n    parser.add_argument(\\'-r\\', \\'--range\\',\\n                        nargs=1,\\n                        default=[1.0E-5])\\n    parser.add_argument(\\'-v\\', \\'--verbose\\',\\n                        action=\\'store_true\\',\\n                        default=False)\\n    args = parser.parse_args()\\n    # print(args)\\n\\n    target_brd_path = args.target_brd_path[0]\\n    ref_brd_path = args.ref_brd_path[0]\\n    output_path = args.output_path[0]\\n    range = float(args.range[0])\\n    verbose = args.verbose\\n\\n    if verbose:\\n        print(\"target: {}\".format(target_brd_path))\\n        print(\"reference: {}\".format(ref_brd_path))\\n\\n    # load\\n    target_ag = bridge.load_atomgroup(target_brd_path)\\n    ref_ag = bridge.load_atomgroup(ref_brd_path)\\n\\n    # matching\\n    #target_selector = bridge.Select_AtomGroup(target_ag)\\n    #restructured = ref_ag.select(target_selector)\\n\\n    # calc the rest\\n    #rest_of_target = get_rest_of_frame_molecule(target_ag, restructured)\\n    #assign_rest_molecule(rest_of_target, restructured)\\n\\n    restructured = target_ag.restructure(ref_ag, range)\\n\\n    if output_path:\\n        if verbose:\\n            print(\"output brd file: {}\".format(output_path))\\n        bridge.save_atomgroup(restructured, output_path)']}, {'features': [], 'snippets': [\"def setup():\\n\\tGPIO.setmode(GPIO.BOARD)       # Numbers GPIOs by physical location\\n\\tGPIO.setup(LedPin, GPIO.OUT)   # Set LedPin's mode is output\\n\\tGPIO.setup(KnockPin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\\n\\tGPIO.output(LedPin, GPIO.HIGH) # Set LedPin high(+3.3V) to off led\", \"def loop():\\n\\tGPIO.add_event_detect(KnockPin, GPIO.FALLING, callback=swLed, bouncetime=200) # wait for falling\\n\\twhile True:\\n\\t\\tpass   # Don't do anything\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, base_uri=None):\\n        super(BiophysicalPerisomaticApi, self).__init__(base_uri)\\n        self.cache_stimulus = True\\n        self.ids = {}\\n        self.sweeps = []\\n        self.manifest = {}', \"def build_rma(self, neuronal_model_id, fmt='json'):\\n        '''Construct a query to find all files related to a neuronal model.\", \"def read_json(self, json_parsed_data):\\n        '''Get the list of well_known_file ids from a response body\\n        containing nested sample,microarray_slides,well_known_files.\", \"def is_well_known_file_type(self, wkf, name):\\n        '''Check if a structure has the expected name.\", \"def get_well_known_file_ids(self, neuronal_model_id):\\n        '''Query the current RMA endpoint with a neuronal_model id\\n        to get the corresponding well known file ids.\", \"def create_manifest(self,\\n                        fit_path='',\\n                        stimulus_filename='',\\n                        swc_morphology_path='',\\n                        sweeps=[]):\\n        '''Generate a json configuration file with parameters for a \\n        a biophysical experiment.\", \"def cache_data(self,\\n                   neuronal_model_id,\\n                   working_directory=None):\\n        '''Take a an experiment id, query the Api RMA to get well-known-files\\n        download the files, and store them in the working directory.\"]}, {'features': [], 'snippets': ['def testScaBasicBehavior(self):\\n        #######################################################################\\n        # Launch the component with the default execparams\\n        execparams = self.getPropertySet(kinds=(\"execparam\",), modes=(\"readwrite\", \"writeonly\"), includeNil=False)\\n        execparams = dict([(x.id, any.from_any(x.value)) for x in execparams])\\n        self.launch(execparams)']}, {'features': [], 'snippets': ['def ImportGames():\\r\\n\\tgames = list()\\r\\n\\tuser_games = dict()', \"def BuildMatrix(games, user_games):\\r\\n\\twith open('C:\\\\\\\\Users\\\\\\\\Diogo\\\\\\\\Documents\\\\\\\\Monografia FIA\\\\\\\\UserGamesMatrix.tab', 'a', encoding = 'utf-8') as lines:\\r\\n\\t\\tlines.write('user\\\\t' + '\\\\t'.join(games) + '\\\\n')\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def parse_date(date_str):\\n    \"\"\"Parse elastic datetime string.\"\"\"\\n    try:\\n        date = arrow.get(date_str)\\n    except TypeError:\\n        date = arrow.get(date_str[0])\\n    return date.datetime', 'def format_doc(hit, schema, dates):\\n    \"\"\"Format given doc to match given schema.\"\"\"\\n    doc = hit.get(\\'_source\\', {})\\n    doc.setdefault(config.ID_FIELD, hit.get(\\'_id\\'))\\n    doc.setdefault(\\'_type\\', hit.get(\\'_type\\'))\\n\\n    for key in dates:\\n        if key in doc:\\n            doc[key] = parse_date(doc[key])\\n\\n    return doc', 'def is_elastic(datasource):\\n    \"\"\"Detect if given resource uses elastic.\"\"\"\\n    return datasource.get(\\'backend\\') == \\'elastic\\' or datasource.get(\\'search_backend\\') == \\'elastic\\'', 'def default(self, value):\\n        \"\"\"Convert mongo.ObjectId.\"\"\"\\n        if isinstance(value, ObjectId):\\n            return str(value)\\n        return super(ElasticJSONSerializer, self).default(value)', 'def __init__(self, hits=None, docs=None):\\n        \"\"\"Parse hits into docs.\"\"\"\\n        self.hits = hits if hits else self.no_hits\\n        self.docs = docs if docs else []', 'def first(self):\\n        \"\"\"Get first doc.\"\"\"\\n        return self.docs[0] if self.docs else None', 'def extra(self, response):\\n        \"\"\"Add extra info to response.\"\"\"\\n        if \\'facets\\' in self.hits:\\n            response[\\'_facets\\'] = self.hits[\\'facets\\']\\n        if \\'aggregations\\' in self.hits:\\n            response[\\'_aggregations\\'] = self.hits[\\'aggregations\\']', \"def set_sort(query, sort):\\n    query['sort'] = []\\n    for (key, sortdir) in sort:\\n        sort_dict = dict([(key, 'asc' if sortdir > 0 else 'desc')])\\n        query['sort'].append(sort_dict)\", 'def get_indices(es):\\n    return elasticsearch.client.IndicesClient(es)', \"def init_app(self, app):\\n        app.config.setdefault('ELASTICSEARCH_URL', 'http://localhost:9200/')\\n        app.config.setdefault('ELASTICSEARCH_INDEX', 'eve')\\n\\n        self.index = app.config['ELASTICSEARCH_INDEX']\\n        self.es = get_es(app.config['ELASTICSEARCH_URL'])\\n\\n        self.create_index(self.index)\\n        self.put_mapping(app)\", 'def create_index(self, index=None):\\n        if index is None:\\n            index = self.index\\n        try:\\n            get_indices(self.es).create(self.index)\\n        except elasticsearch.TransportError:\\n            pass', \"def find(self, resource, req, sub_resource_lookup):\\n        args = getattr(req, 'args', request.args if request else {})\\n        source_config = config.SOURCES[resource]\\n\\n        if args.get('source'):\\n            query = json.loads(args.get('source'))\\n            if 'filtered' not in query.get('query', {}):\\n                _query = query.get('query')\\n                query['query'] = {'filtered': {}}\\n                if _query:\\n                    query['query']['filtered']['query'] = _query\\n        else:\\n            query = {'query': {'filtered': {}}}\\n\\n        if args.get('q', None):\\n            query['query']['filtered']['query'] = _build_query_string(args.get('q'),\\n                                                                      default_field=args.get('df', '_all'))\\n\\n        if 'sort' not in query:\\n            if req.sort:\\n                sort = ast.literal_eval(req.sort)\\n                set_sort(query, sort)\\n            elif self._default_sort(resource) and 'query' not in query['query']['filtered']:\\n                set_sort(query, self._default_sort(resource))\\n\\n        if req.max_results:\\n            query.setdefault('size', req.max_results)\\n\\n        if req.page > 1:\\n            query.setdefault('from', (req.page - 1) * req.max_results)\\n\\n        filters = []\\n        filters.append(source_config.get('elastic_filter'))\\n        filters.append(source_config.get('elastic_filter_callback', noop)())\\n        filters.append({'term': sub_resource_lookup} if sub_resource_lookup else None)\\n        filters.append(json.loads(args.get('filter')) if 'filter' in args else None)\\n        set_filters(query, filters)\\n\\n        if 'facets' in source_config:\\n            query['facets'] = source_config['facets']\\n\\n        if 'aggregations' in source_config:\\n            query['aggs'] = source_config['aggregations']\\n\\n        args = self._es_args(resource)\\n        hits = self.es.search(body=query, **args)\\n        return self._parse_hits(hits, resource)\", \"def is_found(hit):\\n            if 'exists' in hit:\\n                hit['found'] = hit['exists']\\n            return hit.get('found', False)\", \"def find_one_raw(self, resource, _id):\\n        args = self._es_args(resource)\\n        hit = self.es.get(id=_id, **args)\\n        return self._parse_hits({'hits': {'hits': [hit]}}, resource).first()\", \"def insert(self, resource, doc_or_docs, **kwargs):\\n        ids = []\\n        kwargs.update(self._es_args(resource))\\n        for doc in doc_or_docs:\\n            doc.update(self.es.index(body=doc, id=doc.get('_id'), **kwargs))\\n            ids.append(doc['_id'])\\n        get_indices(self.es).refresh(self.index)\\n        return ids\", 'def replace(self, resource, id_, document):\\n        args = self._es_args(resource, refresh=True)\\n        return self.es.index(body=document, id=id_, **args)', \"def is_empty(self, resource):\\n        args = self._es_args(resource)\\n        res = self.es.count(body={'query': {'match_all': {}}}, **args)\\n        return res.get('count', 0) == 0\", 'def _parse_hits(self, hits, resource):\\n        \"\"\"Parse hits response into documents.\"\"\"\\n        datasource = self._datasource(resource)\\n        schema = config.DOMAIN[datasource[0]][\\'schema\\']\\n        dates = get_dates(schema)\\n        docs = []\\n        for hit in hits.get(\\'hits\\', {}).get(\\'hits\\', []):\\n            docs.append(format_doc(hit, schema, dates))\\n        return ElasticCursor(hits, docs)', 'def _fields(self, resource):\\n        \"\"\"Get projection fields for given resource.\"\"\"\\n        datasource = self._datasource(resource)\\n        keys = datasource[2].keys()\\n        return \\',\\'.join(keys) + \\',\\'.join([config.LAST_UPDATED, config.DATE_CREATED])', 'def build_elastic_query(doc):\\n    \"\"\"\\n    Builds a query which follows ElasticSearch syntax from doc.\\n    1. Converts {\"q\":\"cricket\"} to the below elastic query\\n    {\\n        \"query\": {\\n            \"filtered\": {\\n                \"query\": {\\n                    \"query_string\": {\\n                        \"query\": \"cricket\",\\n                        \"lenient\": false,\\n                        \"default_operator\": \"AND\"\\n                    }\\n                }\\n            }\\n        }\\n    }\\n\\n    2. Converts a faceted query\\n    {\"q\":\"cricket\", \"type\":[\\'text\\'], \"source\": \"AAP\"}\\n    to the below elastic query\\n    {\\n        \"query\": {\\n            \"filtered\": {\\n                \"filter\": {\\n                    \"and\": [\\n                        {\"terms\": {\"type\": [\"text\"]}},\\n                        {\"term\": {\"source\": \"AAP\"}}\\n                    ]\\n                },\\n                \"query\": {\\n                    \"query_string\": {\\n                        \"query\": \"cricket\",\\n                        \"lenient\": false,\\n                        \"default_operator\": \"AND\"\\n                    }\\n                }\\n            }\\n        }\\n    }\\n\\n    :param doc: A document object which is inline with the syntax specified in the examples.\\n                It\\'s the developer responsibility to pass right object.\\n    :returns ElasticSearch query\\n    \"\"\"\\n\\n    elastic_query, filters = {\"query\": {\"filtered\": {}}}, []\\n\\n    for key in doc.keys():\\n        if key == \\'q\\':\\n            elastic_query[\\'query\\'][\\'filtered\\'][\\'query\\'] = _build_query_string(doc[\\'q\\'])\\n        else:\\n            _value = doc[key]\\n            filters.append({\"terms\": {key: _value}} if isinstance(_value, list) else {\"term\": {key: _value}})\\n\\n    set_filters(elastic_query, filters)\\n    return elastic_query']}, {'features': [], 'snippets': ['def _get_available_ports():\\n    \"\"\" Tries to find the available usb2serial port on your system. \"\"\"\\n    if platform.system() == \\'Darwin\\':\\n        return glob.glob(\\'/dev/tty.usb*\\')\\n\\n    elif platform.system() == \\'Linux\\':\\n        return glob.glob(\\'/dev/ttyACM*\\') + glob.glob(\\'/dev/ttyUSB*\\')\\n\\n    elif platform.system() == \\'Windows\\':\\n        import _winreg\\n        import itertools\\n\\n        ports = []\\n        path = \\'HARDWARE\\\\\\\\DEVICEMAP\\\\\\\\SERIALCOMM\\'\\n        key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE, path)\\n\\n        for i in itertools.count():\\n            try:\\n                ports.append(str(_winreg.EnumValue(key, i)[1]))\\n            except WindowsError:\\n                return ports\\n\\n    return []', 'def find_port(ids, strict=True):\\n    \"\"\" Find the port with the specified attached motor ids.\\n\\n        :param list ids: list of motor ids to find\\n        :param bool strict: specify if all ids should be find (when set to False, only half motor must be found)\\n\\n        .. warning:: If two (or more) ports are attached to the same list of motor ids the first match will be returned.\\n\\n    \"\"\"\\n    for port in get_available_ports():\\n        for DxlIOCls in (DxlIO, Dxl320IO):\\n            try:\\n                with DxlIOCls(port) as dxl:\\n                    founds = len(dxl.scan(ids))\\n\\n                    if strict and founds == len(ids):\\n                        return port\\n\\n                    if not strict and founds >= len(ids) / 2:\\n                        return port\\n            except DxlError:\\n                continue\\n\\n    raise IndexError(\\'No suitable port found for ids {}!\\'.format(ids))']}, {'features': [], 'snippets': ['def __init__(self, df = 2, mu = 0):\\n        d1 = NormalDistr(mu, 1)\\n        d2 = distr_sqrt(ChiSquareDistr(df) / df)\\n        super(NoncentralTDistr, self).__init__(d1, d2)\\n        self.df = df\\n        self.mu = mu', 'def getName(self):\\n        return \"NoncT({0},{1})\".format(self.df, self.mu)', 'def __new__(cls, df, lmbda = 0):\\n        assert df >= 1\\n        d1 = NormalDistr(sqrt(lmbda))**2\\n        if df == 1:\\n            return d1\\n        d2 = ChiSquareDistr(df - 1)\\n        ncc2 = super(NoncentralChiSquareDistr, cls).__new__(cls, d1, d2)\\n        super(NoncentralChiSquareDistr, ncc2).__init__(d1, d2)\\n        ncc2.df = df\\n        ncc2.lmbda = lmbda\\n        return ncc2', 'def __str__(self):\\n        return \"NoncentralChiSquare(df={0},lambda={1})#{2}\".format(self.df, self.lmbda, self.id())', 'def __init__(self, alpha = 1, beta = 1, lmbda = 0):\\n        d = 1 + ChiSquareDistr(2.0 * beta) / NoncentralChiSquareDistr(2 * alpha, lmbda)\\n        super(NoncentralBetaDistr, self).__init__(d)\\n        self.alpha = alpha\\n        self.beta = beta\\n        self.lmbda = lmbda', 'def getName(self):\\n        return \"NoncBeta({0},{1},{2})\".format(self.alpha, self.beta, self.lmbda)', 'def __init__(self, df1 = 1, df2 = 1, lmbda = 0):\\n        d1 = NoncentralChiSquareDistr(df1, lmbda) / df1\\n        d2 = ChiSquareDistr(df2) / df2\\n        super(NoncentralFDistr, self).__init__(d1, d2)\\n        self.df1 = df1\\n        self.df2 = df2\\n        self.lmbda = lmbda']}, {'features': [], 'snippets': ['def __init__(self):\\n        c_ptr = SummaryKeyMatcher.cNamespace().alloc()\\n\\n        super(SummaryKeyMatcher, self).__init__(c_ptr)', 'def __len__(self):\\n        return SummaryKeyMatcher.cNamespace().size(self)', 'def isRequired(self, key):\\n        \"\"\" @rtype: bool \"\"\"\\n        return SummaryKeyMatcher.cNamespace().is_required(self, key)', 'def free(self):\\n        SummaryKeyMatcher.cNamespace().free(self)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def requirements():\\n    \"\"\"Build the requirements list for this project\"\"\"\\n    requirements_list = []\\n\\n    with open(\\'requirements.txt\\') as requirements:\\n        for install in requirements:\\n            requirements_list.append(install.strip())\\n\\n    return requirements_list']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def setUpTestData(cls):\\n        batch_num = 0\\n        section_num = 0\\n        voter_num = 0\\n        party_num = 0\\n        position_num = 0\\n        candidate_num = 0\\n        num_elections = 2\\n        voters = list()\\n        positions = dict()\\n        for i in range(num_elections):\\n            election = Election.objects.create(name='Election {}'.format(i))\\n            positions[str(election.name)] = list()\\n\\n            num_batches = 2\\n            for j in range(num_batches):\\n                batch = Batch.objects.create(year=batch_num, election=election)\\n                batch_num += 1\\n\\n                num_sections = 2 if j == 0 else 1\\n                for k in range(num_sections):\\n                    section = Section.objects.create(\\n                        section_name=str(section_num)\\n                    )\\n                    section_num += 1\\n\\n                    num_students = 2\\n                    for l in range(num_students):\\n                        voter = User.objects.create(\\n                            username='user{}'.format(voter_num),\\n                            first_name=str(voter_num),\\n                            last_name=str(voter_num),\\n                            type=UserType.VOTER\\n                        )\\n                        voter.set_password('voter')\\n                        voter.save()\\n                        voter_num += 1\\n\\n                        VoterProfile.objects.create(\\n                            user=voter,\\n                            batch=batch,\\n                            section=section\\n                        )\\n\\n                        voters.append(voter)\\n\\n            num_positions = 3\\n            for i in range(num_positions):\\n                position = CandidatePosition.objects.create(\\n                    position_name='Position {}'.format(position_num),\\n                    election=election\\n                )\\n\\n                positions[str(election.name)].append(position)\\n\\n                position_num += 1\\n\\n            num_parties = 3\\n            for j in range(num_parties):\\n                party = CandidateParty.objects.create(\\n                    party_name='Party {}'.format(party_num),\\n                    election=election\\n                )\\n                party_num += 1\\n\\n                if j != 2:  # Let every third party have no candidates.\\n                    num_positions = 3\\n                    for k in range(num_positions):\\n                        position = positions[str(election.name)][k]\\n\\n                        candidate = Candidate.objects.create(\\n                            user=voters[candidate_num],\\n                            party=party,\\n                            position=position,\\n                            election=election\\n                        )\\n\\n                        Vote.objects.create(\\n                            user=voters[candidate_num],\\n                            candidate=candidate,\\n                            election=election\\n                        )\\n\\n                        candidate_num += 1\\n\\n        # Let's give one candidate an additional vote to really make sure that\\n        # we all got the correct number of votes.\\n        Vote.objects.create(\\n            user=voters[0],\\n            # NOTE: The voter in voter[1] is a Position 1 candidate of\\n            #       Party 1, where the voter in voter[0] is a member.\\n            candidate=Candidate.objects.get(user=voters[1]),\\n            election=Election.objects.get(name='Election 0')\\n        )\\n\\n        _admin = User.objects.create(username='admin', type=UserType.ADMIN)\\n        _admin.set_password('root')\\n        _admin.save()\", \"def test_anonymous_get_requests_redirected_to_index(self):\\n        self.client.logout()\\n\\n        response = self.client.get(reverse('results-export'), follow=True)\\n        self.assertRedirects(response, '/?next=%2Fadmin%2Fresults')\", 'def test_get_all_elections_xlsx(self):\\n        response = self.client.get(reverse(\\'results-export\\'))\\n\\n        self.assertEqual(response.status_code, 200)\\n\\n        self.assertEqual(\\n            response[\\'Content-Disposition\\'],\\n            \\'attachment; filename=\"Election Results.xlsx\"\\'\\n        )\\n\\n        wb = openpyxl.load_workbook(io.BytesIO(response.content))\\n\\n        self.assertEqual(len(wb.worksheets), 2)\\n\\n        # Check first worksheet.\\n        ws = wb.worksheets[0]\\n\\n        self.assertEqual(wb.sheetnames[0], \\'Election 0\\')\\n\\n        row_count = ws.max_row\\n        col_count = ws.max_column\\n        self.assertEqual(row_count, 25)\\n        self.assertEqual(col_count, 5)\\n\\n        self.assertEqual(str(ws.cell(1, 1).value), \\'Election 0 Results\\')\\n\\n        self.assertEqual(str(ws.cell(2, 1).value), \\'Candidates\\')\\n\\n        cellContents = [\\n            \\'Position 0\\',\\n            \\'Party 0\\',\\n            \\'0, 0\\',\\n            \\'Party 1\\',\\n            \\'3, 3\\',\\n            \\'Party 2\\',\\n            \\'None\\',\\n            \\'Position 1\\',\\n            \\'Party 0\\',\\n            \\'1, 1\\',\\n            \\'Party 1\\',\\n            \\'4, 4\\',\\n            \\'Party 2\\',\\n            \\'None\\',\\n            \\'Position 2\\',\\n            \\'Party 0\\',\\n            \\'2, 2\\',\\n            \\'Party 1\\',\\n            \\'5, 5\\',\\n            \\'Party 2\\',\\n            \\'None\\'\\n        ]\\n        for cellIndex, content in enumerate(cellContents, 5):\\n            self.assertEqual(str(ws.cell(cellIndex, 1).value), content) \\n\\n        self.assertEqual(str(ws.cell(2, 2).value), \\'Number of Votes\\')\\n\\n        self.assertEqual(str(ws.cell(3, 2).value), \\'0\\')\\n\\n        self.assertEqual(str(ws.cell(4, 2).value), \\'0\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 2).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(9, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 2).value), \\'2\\')\\n        self.assertEqual(str(ws.cell(16, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(23, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(4, 3).value), \\'1\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(9, 3).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(16, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 3).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(23, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(3, 4).value), \\'1\\')\\n\\n        self.assertEqual(str(ws.cell(4, 4).value), \\'2\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(9, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(16, 4).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(23, 4).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(3, 5).value), \\'Total Votes\\')\\n        self.assertEqual(str(ws.cell(7, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(9, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 5).value), \\'2\\')\\n        self.assertEqual(str(ws.cell(16, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(23, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        # Check second worksheet.\\n        ws = wb.worksheets[1]\\n\\n        self.assertEqual(wb.sheetnames[1], \\'Election 1\\')\\n\\n        row_count = ws.max_row\\n        col_count = ws.max_column\\n        self.assertEqual(row_count, 25)\\n        self.assertEqual(col_count, 5)\\n\\n        self.assertEqual(str(ws.cell(1, 1).value), \\'Election 1 Results\\')\\n\\n        self.assertEqual(str(ws.cell(2, 1).value), \\'Candidates\\')\\n\\n        self.assertEqual(str(ws.cell(2, 1).value), \\'Candidates\\')\\n\\n        cellContents = [\\n            \\'Position 3\\',\\n            \\'Party 3\\',\\n            \\'6, 6\\',\\n            \\'Party 4\\',\\n            \\'9, 9\\',\\n            \\'Party 5\\',\\n            \\'None\\',\\n            \\'Position 4\\',\\n            \\'Party 3\\',\\n            \\'7, 7\\',\\n            \\'Party 4\\',\\n            \\'10, 10\\',\\n            \\'Party 5\\',\\n            \\'None\\',\\n            \\'Position 5\\',\\n            \\'Party 3\\',\\n            \\'8, 8\\',\\n            \\'Party 4\\',\\n            \\'11, 11\\',\\n            \\'Party 5\\',\\n            \\'None\\'\\n        ]\\n        for cellIndex, content in enumerate(cellContents, 5):\\n            self.assertEqual(str(ws.cell(cellIndex, 1).value), content) \\n\\n        self.assertEqual(str(ws.cell(2, 2).value), \\'Number of Votes\\')\\n\\n        self.assertEqual(str(ws.cell(3, 2).value), \\'2\\')\\n\\n        self.assertEqual(str(ws.cell(4, 2).value), \\'3\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 2).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(9, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 2).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(16, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(23, 2).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(4, 3).value), \\'4\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(9, 3).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(16, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 3).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(23, 3).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(3, 4).value), \\'3\\')\\n\\n        self.assertEqual(str(ws.cell(4, 4).value), \\'5\\') # Section\\n\\n        self.assertEqual(str(ws.cell(7, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(9, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(16, 4).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 4).value), \\'0\\')\\n        self.assertEqual(str(ws.cell(23, 4).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')\\n\\n        self.assertEqual(str(ws.cell(3, 5).value), \\'Total Votes\\')\\n        self.assertEqual(str(ws.cell(7, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(9, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(11, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(14, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(16, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(18, 2).value), \\'N/A\\')\\n        self.assertEqual(str(ws.cell(21, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(23, 5).value), \\'1\\')\\n        self.assertEqual(str(ws.cell(25, 2).value), \\'N/A\\')', \"def test_get_with_invalid_election_id_non_existent_election_id(self):\\n        response = self.client.get(\\n            reverse('results-export'),\\n            { 'election': '69' },\\n            HTTP_REFERER=reverse('results'),\\n            follow=True\\n        )\\n\\n        messages = list(response.context['messages'])\\n        self.assertEqual(\\n            messages[0].message,\\n            'You specified an ID for a non-existent election.'\\n        )\\n        self.assertRedirects(response, reverse('results'))\", \"def test_ref_get_with_invalid_election_id_non_existent_election_id(self):\\n        response = self.client.get(\\n            reverse('results-export'),\\n            { 'election': '69' },\\n            HTTP_REFERER=reverse('results'),\\n            follow=True\\n        )\\n\\n        messages = list(response.context['messages'])\\n        self.assertEqual(\\n            messages[0].message,\\n            'You specified an ID for a non-existent election.'\\n        )\\n        self.assertRedirects(response, reverse('results'))\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def centres(request):\\n\\t#Python练习项目管理中心Center\\n\\treturn render(request, 'centres/centres.html')\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def setUp(self):\\n        self.escala = Escala('fixtures/escala.xml')\\n        self.dir = dirs.TestDir()\\n        self.maxDiff = None\", \"def test_attributos_voo_1(self):\\n        p_voo = self.escala.escalas[0]\\n\\n        self.assertEqual(p_voo.activity_date, datetime(2013, 3, 1, 11, 36))\\n        self.assertEqual(p_voo.present_location, 'VCP')\\n        self.assertEqual(p_voo.flight_no, '4148')\\n        self.assertEqual(p_voo.origin, 'VCP')\\n        self.assertEqual(p_voo.destination, 'GYN')\\n        self.assertEqual(p_voo.actype, 'E95')\\n        self.assertTrue(p_voo.checkin)\\n        self.assertEqual(p_voo.checkin_time, datetime(2013, 3, 1, 10, 36))\\n        self.assertEqual(p_voo.std, datetime(2013, 3, 1, 13, 13))\\n        self.assertEqual(p_voo.sta, datetime(2013, 3, 1, 11, 36))\\n        self.assertEqual(p_voo.activity_info, 'AD4148')\\n        self.assertFalse(p_voo.duty_design)\", \"def test_attributos_voo_18(self):\\n        p_voo = self.escala.escalas[18]\\n\\n        self.assertEqual(p_voo.activity_date, datetime(2013, 10, 29, 4, 58))\\n        self.assertEqual(p_voo.present_location, 'VCP')\\n        self.assertEqual(p_voo.flight_no, '4050')\\n        self.assertEqual(p_voo.origin, 'VCP')\\n        self.assertEqual(p_voo.destination, 'FLN')\\n        self.assertEqual(p_voo.activity_info, 'AD4050')\\n        self.assertEqual(p_voo.actype, 'E95')\\n        self.assertEqual(p_voo.sta, datetime(2013, 10, 29, 4, 58))\\n        self.assertEqual(p_voo.std, datetime(2013, 10, 29, 6, 15))\\n        self.assertTrue(p_voo.checkin)\\n        self.assertEqual(p_voo.checkin_time, datetime(2013, 10, 29, 5, 8))\\n        self.assertFalse(p_voo.duty_design)\\n        self.assertEqual(p_voo.horas_de_voo, '1:17')\", \"def test_calculo_horas_voadas(self):\\n        s_horas = {\\n            'h_diurno': '6:40',\\n            'h_noturno': '6:47',\\n            'h_total_voo': '13:27',\\n            'h_faixa2': '0:00',\\n            'h_sobreaviso': '40:00',\\n            'h_reserva': '29:13'\\n        }\\n        self.assertEqual(self.escala.soma_horas(), s_horas)\", 'def test_csv(self):\\n        \"\"\"\\n        Check CSV output\\n        \"\"\"\\n        f_result = open(self.dir.get_data_dir() + \\'fixtures/escala.csv\\')\\n\\n        self.assertEqual(self.escala.csv(), f_result.read())\\n        f_result.close()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def is_good(n):\\n\\n    return 1 + ((int(n) - 1) % 9) == 9', 'def generate_subsequences(n):\\n    subsequences = []\\n    combinations_list = []\\n    index = 4']}, {'features': [], 'snippets': [\"def run():\\n    test_dir = './testcase'\\n    suite = unittest.defaultTestLoader.discover(start_dir=test_dir,pattern='test*.py')\\n\\n    now = time.strftime('%Y-%m-%d_%H_%M_%S')\\n    reportname = globalparam.report_path + '\\\\\\\\' + 'TestResult' + now + '.html'\\n    with open(reportname,'wb') as f:\\n        runner = HTMLTestRunner.HTMLTestRunner(\\n            stream=f,\\n            title='测试报告',\\n            description='Test the import testcase'\\n        )\\n        runner.run(suite)\\n    time.sleep(3)\\n    # 发送邮件\\n    mail = sendmail.SendMail()\\n    mail.send()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        self.transport = None\\n        self.connected = threading.Event()\\n        self.disconnected = threading.Event()\\n        self.port = None', 'def data_received(self, data):\\n        \"\"\"Called with snippets received from the serial port\"\"\"\\n        raise NotImplementedError', \"def __init__(self, protocol_class, comport, **kwargs):\\n        super(KeepAliveReader, self).__init__()\\n        self.daemon = True\\n        self.protocol_class = protocol_class\\n        self.comport = comport\\n        self.kwargs = kwargs\\n        self.protocol = None\\n        self.default_timeout_s = kwargs.pop('default_timeout_s', None)\\n\\n        # Event to indicate serial connection has been established.\\n        self.connected = threading.Event()\\n        # Event to request a break from the run loop.\\n        self.close_request = threading.Event()\\n        # Event to indicate thread has been closed.\\n        self.closed = threading.Event()\\n        # Event to indicate an exception has occurred.\\n        self.error = threading.Event()\\n        # Event to indicate that the thread has connected to the specified port\\n        # **at least once**.\\n        self.has_connected = threading.Event()\", 'def alive(self):\\n        return not self.closed.is_set()', \"def write(self, data, timeout_s=None):\\n        '''\\n        Write to serial port.\\n\\n        Waits for serial connection to be established before writing.\\n\\n        Parameters\\n        ----------\\n        data : str or bytes\\n            Data to write to serial port.\\n        timeout_s : float, optional\\n            Maximum number of seconds to wait for serial connection to be\\n            established.\\n\\n            By default, block until serial connection is ready.\\n        '''\\n        self.connected.wait(timeout_s)\\n        self.protocol.transport.write(data)\", 'def close(self):\\n        self.close_request.set()', 'def __enter__(self):\\n        \"\"\"\\\\\\n        Enter context handler. May raise RuntimeError in case the connection\\n        could not be created.\\n        \"\"\"\\n        self.start()\\n        # Wait for protocol to connect.\\n        event = OrEvent(self.connected, self.closed)\\n        event.wait(self.default_timeout_s)\\n        return self']}, {'features': [], 'snippets': ['def mesh(self):\\n        n = 8\\n        return UnitCubeMesh(n, n, n)', 'def dirichlet_values(self):\\n        clamp = Expression((\"0.0\", \"0.0\", \"0.0\"))\\n        twist = Expression((\"0.0\",\\n                            \"y0 + (x[1] - y0) * cos(theta) - (x[2] - z0) * sin(theta) - x[1]\",\\n                            \"z0 + (x[1] - y0) * sin(theta) + (x[2] - z0) * cos(theta) - x[2]\"),\\n                           y0=0.5, z0=0.5, theta=pi/6)\\n        return [clamp, twist]', 'def material_model(self):\\n        # Material parameters can either be numbers or spatially\\n        # varying fields. For example,\\n        mu       = 3.8461\\n        lmbda    = Expression(\"x[0]*5.8 + (1 - x[0])*5.7\")\\n        C10 = 0.171; C01 = 4.89e-3; C20 = -2.4e-4; C30 = 5.e-4\\n        delka = 1.0/sqrt(2.0)\\n        M = Constant((0.0,1.0,0.0))\\n        k1 = 1e2; k2 = 1e1\\n\\n\\n        materials = []\\n        materials.append(MooneyRivlin({\\'C1\\':mu/2, \\'C2\\':mu/2, \\'bulk\\':lmbda}))\\n        materials.append(StVenantKirchhoff({\\'mu\\':mu, \\'bulk\\':lmbda}))\\n        materials.append(neoHookean({\\'half_nkT\\':mu, \\'bulk\\':lmbda}))\\n        materials.append(Isihara({\\'C10\\':C10,\\'C01\\':C01,\\'C20\\':C20,\\'bulk\\':lmbda}))\\n        materials.append(Biderman({\\'C10\\':C10,\\'C01\\':C01,\\'C20\\':C20,\\'C30\\':C30,\\'bulk\\':lmbda}))\\n        materials.append(AnisoTest({\\'mu1\\':mu,\\'mu2\\':2*mu,\\'M\\':M,\\'bulk\\':lmbda}))\\n        materials.append(GasserHolzapfelOgden({\\'mu\\':mu,\\'k1\\':k1,\\'k2\\':k2,\\'M\\':M,\\'bulk\\':lmbda}))\\n        materials.append(Ogden({\\'alpha1\\':1.3,\\'alpha2\\':5.0,\\'alpha3\\':-2.0,\\\\\\n                                \\'mu1\\':6.3e5,\\'mu2\\':0.012e5,\\'mu3\\':-0.1e5}))', 'def name_method(self, method):\\n        self.method = method']}, {'features': [], 'snippets': ['def _init(self, maxsize):\\n        Queue._init(self, maxsize) \\n        self.all_items = set()', \"def signal_handler(signal, frame):\\n\\tprint('You pressed Ctrl+C!')\\n\\tsys.exit(0)\", \"def getAtomFeed(url, login, pwd):\\n\\t# var\\n\\tMAX_TRY = 10\\n\\tessai = 0\\n\\n\\t# get atom document\\n\\twhile essai < MAX_TRY:\\n\\t\\ttry:\\n\\t\\t\\tr = requests.get('http://' + url, auth=(login,pwd), timeout=10)\\n\\t\\texcept:\\n\\t\\t\\tessai += 1\\n\\t\\t\\tcontinue\\n\\t\\tbreak\\n\\telse:\\n\\t\\traise ('Erreur lors de la requête')\\n\\n\\t# parse atom document\\n\\ttry:\\n\\t\\tdom = xml.dom.minidom.parseString(r.text)\\n\\texcept:\\n\\t\\traise ('Erreur lors du parsing du document Atom')\\n\\n\\treturn dom\", 'def buildUrlSearchList(server, login, pwd, q):\\n\\t# var\\n\\talphabet = [\\'a\\',\\'b\\',\\'c\\',\\'d\\',\\'e\\',\\'f\\',\\'g\\',\\'h\\',\\'i\\',\\'j\\',\\'k\\',\\'l\\',\\'m\\',\\'n\\',\\'o\\',\\'p\\',\\'q\\',\\'r\\',\\'s\\',\\'t\\',\\'u\\',\\'v\\',\\'w\\',\\'x\\',\\'y\\',\\'z\\']\\n\\t#alphabet = [\\'a\\']\\n\\tfor i in alphabet:\\n\\t\\turl = server + \\'/profiles/atom/search.do?search=\\' + i + \\'*&ps=250\\'\\n\\t\\tdom = getAtomFeed(url, login, pwd)\\n\\t\\ttotalResult = dom.getElementsByTagName(\\'opensearch:totalResults\\')[0]\\n\\t\\ttotalResult = int(totalResult.firstChild.data)\\n\\t\\tif totalResult > 250:\\n\\t\\t\\tnbPage = int(float(totalResult) / 250) + 1\\n\\t\\t\\tfor n in range(1,nbPage,1):\\n\\t\\t\\t\\titem = url + \"&page=\" + str(n) \\n\\t\\t\\t\\tq.put(item)\\n\\t\\telse:\\n\\t\\t\\tnbPage = 1\\n\\t\\t\\tq.put(url)', 'def getRelationsWorker(server, login, pwd, qin, qout, getManager, qmgmt):\\n\\twhile True:\\n\\t\\tuserid = qin.get()\\n\\t\\tif userid == None:\\n\\t\\t\\tbreak\\n\\t\\tqin.task_done()\\n\\t\\turl = server + \\'/profiles/atom/connections.do?userid=\\' + userid + \\'&connectionType=colleague&ps=250\\'\\n\\t\\ttry:\\n\\t\\t\\tdom = getAtomFeed(url, login, pwd)\\n\\t\\texcept:\\n\\t\\t\\tcontinue\\n\\t\\tfeed = dom.firstChild\\n\\t\\tentries = feed.getElementsByTagName(\\'entry\\')\\n\\t\\tfor entry in entries:\\n\\t\\t\\t# get date\\n\\t\\t\\tdateRelation = entry.getElementsByTagName(\\'updated\\')[0]\\n\\t\\t\\tdateRelation = dateRelation.firstChild.data\\n\\t\\t\\tdateRelation = dateRelation[:10]\\n\\t\\t\\t# get author user id\\n\\t\\t\\tauthor = entry.getElementsByTagName(\\'author\\')[0]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tauthorName = author.getElementsByTagName(\\'name\\')[0]\\n\\t\\t\\t\\tauthorName = authorName.firstChild.data\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tauthorName = \"\"\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tauthorEMail = author.getElementsByTagName(\\'email\\')[0]\\n\\t\\t\\t\\tauthorEMail = authorEMail.firstChild.data\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tauthorEMail = \"\"\\n\\t\\t\\tauthorUserId = author.getElementsByTagName(\\'snx:userid\\')[0]\\n\\t\\t\\tauthorUserId = authorUserId.firstChild.data\\n\\n\\t\\t\\t# get contributor user id\\n\\t\\t\\tcontributor = entry.getElementsByTagName(\\'contributor\\')[0]\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcontribName = contributor.getElementsByTagName(\\'name\\')[0]\\n\\t\\t\\t\\tcontribName = contribName.firstChild.data\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tcontribName = \"\"\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tcontribEMail = contributor.getElementsByTagName(\\'email\\')[0]\\n\\t\\t\\t\\tcontribEMail = contribEMail.firstChild.data\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tcontribEMail = \"\"\\n\\t\\t\\tcontribUserId = contributor.getElementsByTagName(\\'snx:userid\\')[0]\\n\\t\\t\\tcontribUserId = contribUserId.firstChild.data\\n\\n\\t\\t\\t# build dict\\n\\t\\t\\tauthorInfo = { \"userid\" : authorUserId, \"name\" : authorName, \"email\" : authorEMail }\\n\\t\\t\\tcontribInfo = { \"userid\" : contribUserId, \"name\" : contribName, \"email\" : contribEMail }\\n\\t\\t\\trelation = \"\\\\\"\" + authorUserId + \"\\\\\",\\\\\"\" + contribUserId + \"\\\\\",\\\\\"<(\" + str(dateRelation) + \",Infinity)>\\\\\"\"\\n\\t\\t\\tqout.put(authorInfo)\\n\\t\\t\\tqout.put(contribInfo)\\n\\t\\t\\tqout.put(relation)\\n\\n\\t\\t# get manager\\n\\t\\tif getManager == True:\\n\\t\\t\\turl = server + \"/profiles/atom/reportingChain.do?userid=\" + userid\\n\\t\\t\\trc = getAtomFeed(url, login, pwd)\\n\\t\\t\\tmanagerId = getManagerInfo(rc)\\n\\t\\t\\tif managerId is not None:\\n\\t\\t\\t\\treportingChain = str(userid) + \",\" + str(managerId)\\n\\t\\t\\t\\tqmgmt.put(reportingChain)', 'def printStatusThread(q0, q1, q2, q3):\\n\\tstrtime = time.time()\\n\\twhile True:\\n\\t\\tsys.stdout.write(\\'\\\\r\\\\x1b[K\\')\\n\\t\\tsys.stdout.write(\"urls:\" + str(q0.qsize()) + \" | \")\\n\\t\\tsys.stdout.write(\"userids:\" + str(q1.qsize()) + \" | \")\\n\\t\\tsys.stdout.write(\"user infos:\" + str(q2.qsize()) + \" | \")\\n\\t\\tsys.stdout.write(\"manager infos:\" + str(q3.qsize()))\\n\\t\\tsys.stdout.flush()\\n\\t\\ttime.sleep(1)', 'def writeManagerFileThread(managerFilename, qin):\\n\\tm = open(managerFilename + \".csv\", \"w\")\\n\\tm.write(\"Source,Target\\\\n\")\\n\\twhile True:\\n\\t\\tdata = qin.get()\\n\\t\\tif data == None:\\n\\t\\t\\tbreak\\n\\t\\tm.write(str(data) + \"\\\\n\")\\n\\t\\tqin.task_done()', 'def main(argv):\\n\\t# global\\n\\tserverUrl = \"\"\\n\\tlogin = \"\"\\n\\tpwd = \"\"\\n\\tgetManager = False\\n\\turlQueue = SetQueue(maxsize=5000)\\n\\tuserIdsQueue = SetQueue(maxsize=5000)\\n\\tuserInfosQueue = Queue(maxsize=5000)\\n\\tuserManagerQueue = Queue(maxsize=5000)\\n\\n\\t# signal handler\\n\\tsignal.signal(signal.SIGINT, signal_handler)\\n\\n\\t# retrive arguments\\n\\ttry:\\n\\t\\topts, args = getopt.getopt(argv, \"hs:u:p:m\", [\"help\", \"server=\", \"user=\", \"password=\", \"manager\"])\\n\\t\\tfor opt, arg in opts:\\n\\t\\t\\tif opt in (\"-h\", \"--help\"):\\n\\t\\t\\t\\tusage()\\n\\t\\t\\t\\tsys.exit()\\n\\t\\t\\telif opt in (\"-s\", \"--server\"):\\n\\t\\t\\t\\tserverUrl = arg\\n\\t\\t\\telif opt in (\"-u\", \"--user\"):\\n\\t\\t\\t\\tlogin = arg\\n\\t\\t\\telif opt in (\"-p\", \"--password\"):\\n\\t\\t\\t\\tpwd = arg\\n\\t\\t\\telif opt in (\"-m\", \"--manager\"):\\n\\t\\t\\t\\tgetManager = True\\n\\texcept:\\n\\t\\tusage()\\n\\t\\tsys.exit()\\n\\n\\t# threading get userinfo worker\\n\\tuserIdWorker = []\\n\\tfor i in range(10):\\n\\t\\tw1 = Thread(target=getUserIdsWorker, args=(login, pwd, urlQueue, userIdsQueue,))\\n\\t\\tw1.setDaemon(True)\\n\\t\\tw1.start()\\n\\t\\tuserIdWorker.append(w1)\\n\\n\\t# threading get relations worker\\n\\tuserInfoWorker = []\\n\\tfor i in range(20):\\n\\t\\tw2 = Thread(target=getRelationsWorker, args=(serverUrl, login, pwd, userIdsQueue, userInfosQueue, getManager, userManagerQueue,))\\n\\t\\tw2.setDaemon(True)\\n\\t\\tw2.start()\\n\\t\\tuserInfoWorker.append(w2)\\n\\n\\t# thread to print size of queue\\n\\tw3 = Thread(target=printStatusThread, args=(urlQueue, userIdsQueue, userInfosQueue, userManagerQueue,))\\n\\tw3.setDaemon(True)\\n\\tw3.start()\\n\\n\\t# thread to write files\\n\\tw4 = Thread(target=writeFileThread, args=(\"users\", \"relations\", userInfosQueue,))\\n\\tw4.setDaemon(True)\\n\\tw4.start()\\n\\n\\tif getManager == True:\\n\\t\\tw5 = Thread(target=writeManagerFileThread, args=(\"manager\", userManagerQueue,))\\n\\t\\tw5.setDaemon(True)\\n\\t\\tw5.start()\\n\\n\\t# build Queue url list\\n\\tMAX_TRY = 10\\n\\tessai = 0\\n\\twhile essai < MAX_TRY:\\n\\t\\ttry:\\n\\t\\t\\tbuildUrlSearchList(serverUrl, login, pwd, urlQueue)\\n\\t\\texcept KeyboardInterrupt:\\n\\t\\t\\tbreak\\n\\t\\texcept:\\n\\t\\t\\tessai += 1\\n\\t\\t\\tcontinue\\n\\t\\tbreak\\n\\n\\twhile not (urlQueue.empty() and userIdsQueue.empty() and userInfosQueue.empty()):\\n\\t\\tpass\\n\\n\\tprint (\"end threads\")\\n\\turlQueue.put(None)\\n\\tuserIdsQueue.put(None)\\n\\tuserInfosQueue.put(None)\\n\\n\\t# end of workers\\n\\tfor i in userIdWorker:\\n\\t\\ti.join()\\n\\tfor i in userInfoWorker:\\n\\t\\ti.join()\\n\\n\\ttime.sleep(5)\\n\\n\\tsys.exit(0)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def serial_ports():\\n    \"\"\" Lists serial port names\\n\\n        :raises EnvironmentError:\\n            On unsupported or unknown platforms\\n        :returns:\\n            A list of the serial ports available on the system\\n    \"\"\"\\n    if sys.platform.startswith(\\'win\\'):\\n        ports = [\\'COM%s\\' % (i + 1) for i in range(256)]\\n    elif sys.platform.startswith(\\'linux\\') or sys.platform.startswith(\\'cygwin\\'):\\n        # this excludes your current terminal \"/dev/tty\"\\n        ports = glob.glob(\\'/dev/cu[A-Za-z]*\\')\\n    elif sys.platform.startswith(\\'darwin\\'):\\n        ports = glob.glob(\\'/dev/cu.*\\')\\n    else:\\n        raise EnvironmentError(\\'Unsupported platform\\')\\n\\n    result = []\\n    for port in ports:\\n        try:\\n            s = serial.Serial(port)\\n            s.close()\\n            result.append(port)\\n        except (OSError, serial.SerialException):\\n            pass\\n    return result']}, {'features': [], 'snippets': ['def _setup_pkgresources():\\n    import pkg_resources\\n    import os\\n    import plistlib\\n\\n    pl = plistlib.readPlist(os.path.join(\\n        os.path.dirname(os.getenv(\\'RESOURCEPATH\\')), \"Info.plist\"))\\n    appname = pl.get(\\'CFBundleIdentifier\\')\\n    if appname is None:\\n        appname = pl[\\'CFBundleDisplayName\\']\\n    path = os.path.expanduser(\\'~/Library/Caches/%s/python-eggs\\' % (appname,))\\n    pkg_resources.set_extraction_path(path)']}, {'features': [], 'snippets': [\"def main():\\n    jira = JIRA()\\n    JIRA(options={'server': 'http://localhost:8100'})\\n    projects = jira.projects()\\n    print projects\\n    for project in projects:\\n        print project.key\"]}, {'features': [], 'snippets': [\"def __init__( self, **kwargs ):\\n\\n    Client.__init__( self, **kwargs )\\n    opsH = Operations()\\n    self.maxResetCounter = opsH.getValue( 'Productions/ProductionFilesMaxResetCounter', 10 )\\n\\n    self.setServer( 'Transformation/TransformationManager' )\", \"def getCounters( self, table, attrList, condDict, older = None, newer = None, timeStamp = None,\\n                   rpc = '', url = '' ):\\n    rpcClient = self._getRPC( rpc = rpc, url = url )\\n    return rpcClient. getCounters( table, attrList, condDict, older, newer, timeStamp )\", 'def getTransformations( self, condDict = {}, older = None, newer = None, timeStamp = \\'CreationDate\\',\\n                          orderAttribute = None, limit = 100, extraParams = False, rpc = \\'\\', url = \\'\\', timeout = None ):\\n    \"\"\" gets all the transformations in the system, incrementally. \"limit\" here is just used to determine the offset.\\n    \"\"\"\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n\\n    transformations = []\\n    # getting transformations - incrementally\\n    offsetToApply = 0\\n    while True:\\n      res = rpcClient.getTransformations( condDict, older, newer, timeStamp, orderAttribute, limit,\\n                                          extraParams, offsetToApply )\\n      if not res[\\'OK\\']:\\n        return res\\n      else:\\n        gLogger.verbose( \"Result for limit %d, offset %d: %d\" % ( limit, offsetToApply, len( res[\\'Value\\'] ) ) )\\n        if res[\\'Value\\']:\\n          transformations = transformations + res[\\'Value\\']\\n          offsetToApply += limit\\n        if len( res[\\'Value\\'] ) < limit:\\n          break\\n    return S_OK( transformations )', 'def getTransformationFiles( self, condDict = {}, older = None, newer = None, timeStamp = \\'LastUpdate\\',\\n                              orderAttribute = None, limit = 10000, rpc = \\'\\', url = \\'\\', timeout = 1800 ):\\n    \"\"\" gets all the transformation files for a transformation, incrementally.\\n        \"limit\" here is just used to determine the offset.\\n    \"\"\"\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    transformationFiles = []\\n    # getting transformationFiles - incrementally\\n    offsetToApply = 0\\n    while True:\\n      res = rpcClient.getTransformationFiles( condDict, older, newer, timeStamp, orderAttribute, limit, offsetToApply )\\n      if not res[\\'OK\\']:\\n        return res\\n      else:\\n        gLogger.verbose( \"Result for limit %d, offset %d: %d\" % ( limit, offsetToApply, len( res[\\'Value\\'] ) ) )\\n        if res[\\'Value\\']:\\n          transformationFiles = transformationFiles + res[\\'Value\\']\\n          offsetToApply += limit\\n        if len( res[\\'Value\\'] ) < limit:\\n          break\\n    return S_OK( transformationFiles )', 'def cleanTransformation( self, transID, rpc = \\'\\', url = \\'\\', timeout = None ):\\n    \"\"\" Clean the transformation, and set the status parameter (doing it here, for easier extensibility)\\n    \"\"\"\\n    # Cleaning\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    res = rpcClient.cleanTransformation( transID )\\n    if not res[\\'OK\\']:\\n      return res\\n    # Setting the status\\n    return self.setTransformationParameter( transID, \\'Status\\', \\'TransformationCleaned\\' )', 'def setFileStatusForTransformation( self, transName, newLFNsStatus = {}, lfns = [], force = False,\\n                                          rpc = \\'\\', url = \\'\\', timeout = 120 ):\\n    \"\"\" sets the file status for LFNs of a transformation\\n\\n        For backward compatibility purposes, the status and LFNs can be passed in 2 ways:\\n        - newLFNsStatus is a dictionary with the form:\\n          {\\'/this/is/an/lfn1.txt\\': \\'StatusA\\', \\'/this/is/an/lfn2.txt\\': \\'StatusB\\',  ... }\\n          and at this point lfns is not considered\\n        - newLFNStatus is a string, that applies to all the LFNs in lfns\\n    \"\"\"\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n\\n    # create dictionary in case newLFNsStatus is a string\\n    if type( newLFNsStatus ) == type( \\'\\' ):\\n      newLFNsStatus = dict( [( lfn, newLFNsStatus ) for lfn in lfns ] )\\n\\n    # gets status as of today\\n    tsFiles = self.getTransformationFiles( {\\'TransformationID\\':transName, \\'LFN\\': newLFNsStatus.keys()} )\\n    if not tsFiles[\\'OK\\']:\\n      return tsFiles\\n    tsFiles = tsFiles[\\'Value\\']\\n    if tsFiles:\\n      # for convenience, makes a small dictionary out of the tsFiles, with the lfn as key\\n      tsFilesAsDict = {}\\n      for tsFile in tsFiles:\\n        tsFilesAsDict[tsFile[\\'LFN\\']] = [tsFile[\\'Status\\'], tsFile[\\'ErrorCount\\'], tsFile[\\'FileID\\']]\\n\\n      # applying the state machine to the proposed status\\n      newStatuses = self._applyTransformationFilesStateMachine( tsFilesAsDict, newLFNsStatus, force )\\n\\n      if newStatuses:  # if there\\'s something to update\\n        # must do it for the file IDs...\\n        newStatusForFileIDs = dict( [( tsFilesAsDict[lfn][2], newStatuses[lfn] ) for lfn in newStatuses.keys()] )\\n        res = rpcClient.setFileStatusForTransformation( transName, newStatusForFileIDs )\\n        if not res[\\'OK\\']:\\n          return res\\n\\n    return S_OK( newStatuses )', 'def setTransformationParameter( self, transID, paramName, paramValue, force = False,\\n                                      rpc = \\'\\', url = \\'\\', timeout = 120 ):\\n    \"\"\" Sets a transformation parameter. There\\'s a special case when coming to setting the status of a transformation.\\n    \"\"\"\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n\\n    if paramName.lower() == \\'status\\':\\n      # get transformation Type\\n      transformation = self.getTransformation( transID )\\n      if not transformation[\\'OK\\']:\\n        return transformation\\n      transformationType = transformation[\\'Value\\'][\\'Type\\']\\n\\n      # get status as of today\\n      originalStatus = self.getTransformationParameters( transID, \\'Status\\' )\\n      if not originalStatus[\\'OK\\']:\\n        return originalStatus\\n      originalStatus = originalStatus[\\'Value\\']\\n\\n      transIDAsDict = {transID: [originalStatus, transformationType]}\\n      dictOfProposedstatus = {transID: paramValue}\\n      # applying the state machine to the proposed status\\n      value = self._applyTransformationStatusStateMachine( transIDAsDict, dictOfProposedstatus, force )\\n    else:\\n      value = paramValue\\n\\n    return rpcClient.setTransformationParameter( transID, paramName, value )', 'def isOK( self ):\\n    return self.valid', \"def addDirectory( self, path, force = False, rpc = '', url = '', timeout = None ):\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    return rpcClient.addDirectory( path, force )\", \"def addFile( self, lfn, force = False, rpc = '', url = '', timeout = None ):\\n    res = self.__checkArgumentFormat( lfn )\\n    if not res['OK']:\\n      return res\\n    lfndicts = res['Value']\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    return rpcClient.addFile( lfndicts, force )\", \"def removeFile( self, lfn, rpc = '', url = '', timeout = None ):\\n    res = self.__checkArgumentFormat( lfn )\\n    if not res['OK']:\\n      return res\\n    lfns = res['Value'].keys()\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    successful = {}\\n    failed = {}\\n    listOfLists = breakListIntoChunks( lfns, 100 )\\n    for fList in listOfLists:\\n      res = rpcClient.removeFile( fList )\\n      if not res['OK']:\\n        return res\\n      successful.update( res['Value']['Successful'] )\\n      failed.update( res['Value']['Failed'] )\\n    resDict = {'Successful': successful, 'Failed':failed}\\n    return S_OK( resDict )\", \"def getReplicaStatus( self, lfn, rpc = '', url = '', timeout = None ):\\n    res = self.__checkArgumentFormat( lfn )\\n    if not res['OK']:\\n      return res\\n    lfndict = res['Value']\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    return rpcClient.getReplicaStatus( lfndict )\", \"def setReplicaHost( self, lfn, rpc = '', url = '', timeout = None ):\\n    res = self.__checkArgumentFormat( lfn )\\n    if not res['OK']:\\n      return res\\n    lfndict = res['Value']\\n    rpcClient = self._getRPC( rpc = rpc, url = url, timeout = timeout )\\n    return rpcClient.setReplicaHost( lfndict )\", \"def createDirectory( self, lfn, rpc = '', url = '', timeout = None ):\\n    return self.__returnOK( lfn )\", \"def removeLink( self, lfn, rpc = '', url = '', timeout = None ):\\n    return self.__returnOK( lfn )\"]}, {'features': [], 'snippets': ['def __unicode__(self):\\n\\t\\treturn self.contenido', 'def __unicode__(self):\\n\\t\\treturn nom_estado', 'def __unicode__(self):\\n\\t\\treturn self.titulo']}, {'features': [], 'snippets': ['def make_otp(self):\\n        \"\"\"this constructs a single OTP molecule\"\"\"\\n        otp = RigidFragment()\\n        otp.add_atom(\"O\", np.array([0.0, -2./3 * np.sin( 7.*pi/24.), 0.0]), 1.)\\n        otp.add_atom(\"O\", np.array([cos( 7.*pi/24.),  1./3. * sin( 7.* pi/24.), 0.0]), 1.)\\n        otp.add_atom(\"O\", np.array([-cos( 7.* pi/24.),  1./3. * sin( 7.*pi/24), 0.0]), 1.)\\n        otp.finalize_setup()\\n        return otp', 'def setUp(self):\\n        nrigid = 3\\n        self.topology = RBTopology()\\n        self.topology.add_sites([self.make_otp() for i in xrange(nrigid)])\\n        self.topology.finalize_setup()', 'def test_energy(self):\\n        e = self.pot.getEnergy(self.x0)\\n        self.assertAlmostEqual(e, self.e0, delta=1e-4)', 'def test_to_atomistic(self):\\n        xatom = self.topology.to_atomistic(self.x0).flatten()\\n        for i in xrange(xatom.size):\\n            self.assertAlmostEqual(xatom[i], self.x0atomistic[i], 2)', 'def test_site_to_atomistic(self):\\n        rf = self.make_otp()\\n        p = np.array([1., 2, 3])\\n        p /= np.linalg.norm(p)\\n        com = np.array([4., 5, 6])\\n        print \"otp to atomistic\"\\n        print rf.to_atomistic(com, p)', 'def test_to_atomistic2(self):\\n        x0 = np.array(range(self.nrigid * 6), dtype=float)\\n        x2 = x0.reshape([-1,3])\\n        for p in x2[self.nrigid:,:]:\\n            p /= np.linalg.norm(p)\\n        atomistic = self.topology.to_atomistic(x0).flatten()', 'def test_pot_wrapper(self):\\n        from pele.angleaxis import _cpp_aa\\n        from pele.potentials import LJ\\n        rbpot_cpp = _cpp_aa.RBPotentialWrapper(self.topology, LJ())\\n        rbpot = RBPotentialWrapper(self.topology, LJ())', 'def setUp(self):\\n        np.random.seed(0)\\n        self.nmol = 4\\n        self.system = OTPCluster(self.nmol)\\n        pot = self.system.get_potential()\\n        self.db = self.system.create_database()\\n        self.m1 = self.db.addMinimum(pot.getEnergy(_x1), _x1)\\n        self.m2 = self.db.addMinimum(pot.getEnergy(_x2), _x2)', 'def test1(self):\\n        pot = self.system.get_potential()\\n        self.assertLess(np.linalg.norm(pot.getGradient(self.m1.coords)), .1)\\n        self.assertLess(np.linalg.norm(pot.getGradient(self.m2.coords)), .1)', 'def test_basinhopping(self):\\n        db = self.system.create_database()\\n        bh = self.system.get_basinhopping(db)\\n        bh.setPrinting(ostream=None)\\n        bh.run(5)\\n        self.assertGreaterEqual(db.number_of_minima(), 1)', 'def test_thermodynamics(self):\\n        get_thermodynamic_information(self.system, self.db, nproc=None, recalculate=True)\\n        self.assertIsNotNone(self.m1.fvib)', 'def setUp(self):\\n        np.random.seed(0)\\n        self.nmol = 3\\n        self.system = OTPCluster(self.nmol)', 'def test_transform_rotate(self):\\n        print \"\\\\ntest rotate\"\\n        x = self.x0.copy()\\n        p = np.array(range(1,4), dtype=float)\\n        p /= np.linalg.norm(p)\\n        self.transform.rotate(x, rotations.aa2mx(p))', 'def test_align_path(self):\\n        print \"\\\\ntest align_path\"\\n        x1 = self.x0.copy()\\n        x2 = self.x0 + 5', 'def test_cpp_zero_ev(self):\\n        print \"\\\\ntest zeroEV cpp\"\\n        x = self.x0.copy()\\n        zev = self.topology._zeroEV_python(x)\\n        czev = self.topology.cpp_topology.get_zero_modes(x)\\n        self.assertEqual(len(czev), 6)\\n        for ev, cev in izip(zev, czev):\\n            for v1, v2 in izip(ev, cev):\\n                self.assertAlmostEqual(v1, v2, 5)', 'def test_site_distance_squared(self):\\n        print \"\\\\ntest site distance squared\"\\n        c0 = np.zeros(3)\\n        c1 = np.ones(3)\\n        p0 = self.p0.copy()\\n        p1 = p0 + 1\\n        site = self.system.make_otp()\\n        d2 = site.distance_squared(c0, p0, c1, p1)\\n        d2p = _sitedist(c1-c0, p0, p1, site.S, site.W, site.cog)\\n        self.assertAlmostEqual(d2, 10.9548367929, 5)', 'def test_distance_squared_grad(self):\\n        print \"\\\\ntest distance squared grad\"\\n        x1 = self.x0.copy()\\n        x2 = self.x0 + 1.1\\n        grad = self.topology.distance_squared_grad(x1, x2)\\n        g2 = self.topology._distance_squared_grad_python(x1, x2)', 'def test_measure_align(self):\\n        print \"\\\\ntest measure align\"\\n        x1 = self.x0.copy()\\n        x2 = self.x0 + 5.1\\n        x2[-1] = x1[-1] + .1\\n        x20 = x2.copy()\\n        measure = MeasureRigidBodyCluster(self.topology)\\n        measure.align(x1, x2)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_root_input_type_from_json(data):\\n    \"\"\"Return the root input type from JSON formatted string.\"\"\"\\n    return parse_format(json.loads(data))', 'def is_scalar(input_type):\\n    \"\"\"Returns True if input_type is scalar.\"\"\"\\n    return input_type[\\'base_type\\'] in SCALAR', 'def is_param(value):\\n    \"\"\"Determine whether given value is a parameter string.\"\"\"\\n    if not isinstance(value, str):\\n        return False\\n    return RE_PARAM.match(value)', 'def _substitute_implementations():\\n        \"\"\"Replaces implementation ids with input_types.\"\"\"\\n        impls = {}\\n        for id_ in input_type[\\'implementations\\']:\\n            type_ = input_types[id_]\\n            impls[type_[\\'name\\']] = type_\\n        input_type[\\'implementations\\'] = impls', 'def _substitute_key_type():\\n        \"\"\"Replaces key type with input_type.\"\"\"\\n        # pylint: disable=unused-variable, invalid-name\\n        for __, value in input_type[\\'keys\\'].items():\\n            value[\\'type\\'] = input_types[value[\\'type\\']]', 'def _get_input_type(data):\\n    \"\"\"Returns the input_type data structure that defines an input type\\n    and its constraints for validation.\"\"\"\\n    if \\'id\\' not in data or \\'input_type\\' not in data:\\n        return None\\n    input_type = dict(\\n        id=data[\\'id\\'],\\n        base_type=data[\\'input_type\\']\\n    )\\n    input_type[\\'name\\'] = data.get(\\'name\\', \\'\\')\\n    input_type[\\'full_name\\'] = data.get(\\'full_name\\', \\'\\')\\n    input_type[\\'description\\'] = data.get(\\'description\\', \\'\\')\\n    input_type[\\'attributes\\'] = data.get(\\'attributes\\', {})\\n    if input_type[\\'base_type\\'] in [\\'Double\\', \\'Integer\\']:\\n        input_type.update(_parse_range(data))\\n    elif input_type[\\'base_type\\'] == \\'Array\\':\\n        input_type.update(_parse_range(data))\\n        if input_type[\\'min\\'] < 0:\\n            input_type[\\'min\\'] = 0\\n        input_type[\\'subtype\\'] = data[\\'subtype\\']\\n    elif input_type[\\'base_type\\'] == \\'FileName\\':\\n        input_type[\\'file_mode\\'] = data[\\'file_mode\\']\\n    elif input_type[\\'base_type\\'] == \\'Selection\\':\\n        input_type[\\'values\\'] = _list_to_dict(data[\\'values\\'], \\'name\\')\\n    elif input_type[\\'base_type\\'] == \\'Record\\':\\n        input_type[\\'keys\\'] = _list_to_dict(data[\\'keys\\'])\\n        input_type[\\'implements\\'] = data.get(\\'implements\\', [])\\n        input_type[\\'reducible_to_key\\'] = data.get(\\'reducible_to_key\\', None)\\n    elif input_type[\\'base_type\\'] == \\'Abstract\\':\\n        input_type[\\'implementations\\'] = data[\\'implementations\\']\\n        input_type[\\'default_descendant\\'] = data.get(\\'default_descendant\\', None)\\n    return input_type']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, bot):\\n        self.bot = bot\\n        \"\"\"\\n        imLink = http://services.runescape.com/m=hiscore_ironman/index_lite.ws?player=\\n        nmLink = http://services.runescape.com/m=hiscore/index_lite.ws?player=\\n        \"\"\"']}, {'features': [], 'snippets': ['def __init__(self):\\n        self.group = ast.Group()\\n        self.is_alt = False', 'def alt(self) -> ast.Alternative:\\n        assert self.is_alt\\n        return self.group.seq[0]', 'def __init__(self, parent: None, initial: bool=False):\\n        self.is_initial = initial\\n        self.parent = parent  # OpeningOfGroup or ContentOfGroup\\n        self.g = WrappedGroup()\\n        self.content_of_initial = None\\n\\n        # forward of function\\n        self.add = self.g.add\\n\\n        # if this group is the initial, their is no parent but we must refer\\n        # to itself as the returning state\\n        # but if it is a nested group, it must be added into its global group\\n        if self.is_initial:\\n            self.content_of_initial = ContentOfGroup(self, initial)\\n        else:\\n            self.parent.add(self.g.group)', 'def __init__(self, parent: OpeningOfGroup):\\n        self.parent = parent', 'def __init__(self, parent: OpeningOfGroup):\\n        self.parent = parent', 'def __init__(self, parent: OpeningOfGroup, initial: bool=False):\\n        self.parent = parent\\n        self.is_initial = initial\\n        self.limited_prev = parent if initial else self\\n        self.quantified = ContentOfGroup.NotQuantified\\n\\n        # forward of function\\n        self.add = self.parent.add', 'def _last_or_fail(self, psm: PSM):\\n        if self.parent.g.group.seq:\\n            return self.parent.g.group.seq[-1]\\n        else:\\n            psm.error = \"nothing to repeat\"', 'def __init__(self, parent: ContentOfGroup):\\n        self.parent = parent\\n        self.between = ast.Between()\\n        self.min = []', 'def _interpret(self):\\n        if not self.min:\\n            return\\n\\n        try:\\n            count = int(\"\".join(self.min))\\n        except ValueError:\\n            assert False, \"internal error: cannot convert to number minimum of repetition\"\\n        self.between.min = count', 'def __init__(self, repeat: MinimumOfRepetition):\\n        self.repeat = repeat\\n        self.max = []', 'def _interpret(self):\\n        if not self.max:\\n            return\\n\\n        try:\\n            count = int(\"\".join(self.max))\\n        except ValueError:\\n            assert False, \"internal error: cannot convert to number maximum of repetition\"\\n        self.repeat.between.max = count', 'def __init__(self, prev, as_single_chars=(), as_pattern_chars=()):\\n        self.prev = prev  # ContentOfGroup or CharClass\\n        self.single_chars = as_single_chars\\n        self.pattern_chars = as_pattern_chars', 'def __init__(self, prev):\\n        self.prev = prev  # ContentOfGroup or CharClass\\n        self.pattern = ast.PatternChar()\\n        self.pattern.type = ast.PatternChar.Ascii\\n\\n        self.prev.add(self.pattern)', 'def __init__(self, prev):\\n        self.prev = prev  # ContentOfGroup or CharClass\\n        self.pattern = ast.PatternChar()\\n        self.pattern.type = ast.PatternChar.Unicode\\n\\n        self.prev.add(self.pattern)', 'def __init__(self):\\n        # ast is CharClass or may be changed to PatternClass in one case\\n        self.ast = ast.CharClass()', 'def pop(self):\\n        assert isinstance(self.ast, ast.CharClass)\\n        last = self.ast.elems[-1]\\n        self.ast.elems = self.ast.elems[:-1]\\n        return last', 'def __init__(self, prev):\\n        self.prev = prev  # ContentOfGroup or CharClass\\n        self.q = WrappedCharClass()\\n\\n        # forward function\\n        self.add = self.q.add\\n\\n        self.next_is_range = False\\n        self.empty = True\\n        self.can_mutate = True', 'def mutate_if_posix_like(self):\\n        \"\"\"\\n        Change from character class to pattern char if the content is matching\\n        POSIX-like classe.\\n        \"\"\"\\n        assert isinstance(self.q.ast, ast.CharClass)\\n\\n        # put in this variable everything that had happen but not saved into\\n        # the single char object\\n        # because mutation is only possible if the exact string of the content\\n        # match a pre-definied list, so if an unlogged char is consumed, it\\n        # must prevent mutation\\n        if not self.can_mutate:\\n            return\\n\\n        if len(self.q.ast.elems) < SpecialPattern.min_len_posix_class + 2:\\n            return\\n\\n        opening = self.q.ast.elems[0]\\n        if not isinstance(opening, ast.SingleChar) or opening.char != \":\":\\n            return\\n\\n        closing = self.q.ast.elems[-1]\\n        if not isinstance(closing, ast.SingleChar) or closing.char != \":\":\\n            return\\n\\n        is_only_ascii = lambda x: (isinstance(x, ast.SingleChar)\\n                                   and len(x.char) == 1\\n                                   and x.char.isalpha())\\n        class_may_be_a_word = not any(\\n            not is_only_ascii(x) for x in self.q.ast.elems[1:-1])\\n        if not class_may_be_a_word:\\n            return\\n\\n        word = \"\".join(s.char for s in self.q.ast.elems[1:-1])\\n        if word not in SpecialPattern.posix_classes:\\n            return\\n\\n        t = ast.PatternChar()\\n        t.pattern = word\\n        t.type = ast.PatternChar.Posix\\n        self.q.ast = t']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, protocol: ProtocolAnalyzerContainer, label_index: int, msg_index: int, proto_view: int,\\n                 parent=None):\\n        super().__init__(parent)\\n        self.ui = Ui_FuzzingDialog()\\n        self.ui.setupUi(self)\\n        self.setAttribute(Qt.WA_DeleteOnClose)\\n        self.setWindowFlags(Qt.Window)\\n\\n        self.protocol = protocol\\n        msg_index = msg_index if msg_index != -1 else 0\\n        self.ui.spinBoxFuzzMessage.setValue(msg_index + 1)\\n        self.ui.spinBoxFuzzMessage.setMinimum(1)\\n        self.ui.spinBoxFuzzMessage.setMaximum(self.protocol.num_messages)\\n\\n        self.ui.comboBoxFuzzingLabel.addItems([l.name for l in self.message.message_type])\\n        self.ui.comboBoxFuzzingLabel.setCurrentIndex(label_index)\\n\\n        self.proto_view = proto_view\\n        self.fuzz_table_model = FuzzingTableModel(self.current_label, proto_view)\\n        self.fuzz_table_model.remove_duplicates = self.ui.chkBRemoveDuplicates.isChecked()\\n        self.ui.tblFuzzingValues.setModel(self.fuzz_table_model)\\n        self.fuzz_table_model.update()\\n\\n        self.ui.spinBoxFuzzingStart.setValue(self.current_label_start + 1)\\n        self.ui.spinBoxFuzzingEnd.setValue(self.current_label_end)\\n        self.ui.spinBoxFuzzingStart.setMaximum(len(self.message_data))\\n        self.ui.spinBoxFuzzingEnd.setMaximum(len(self.message_data))\\n\\n        self.update_message_data_string()\\n        self.ui.tblFuzzingValues.resize_me()\\n\\n        self.create_connects()\\n        self.restoreGeometry(settings.read(\"{}/geometry\".format(self.__class__.__name__), type=bytes))', 'def message(self):\\n        return self.protocol.messages[int(self.ui.spinBoxFuzzMessage.value() - 1)]', 'def current_label_index(self):\\n        return self.ui.comboBoxFuzzingLabel.currentIndex()', 'def current_label(self) -> ProtocolLabel:\\n        if len(self.message.message_type) == 0:\\n            return None\\n\\n        cur_label = self.message.message_type[self.current_label_index].get_copy()\\n        self.message.message_type[self.current_label_index] = cur_label\\n        cur_label.fuzz_values = [fv for fv in cur_label.fuzz_values if fv]  # Remove empty strings\\n\\n        if len(cur_label.fuzz_values) == 0:\\n            cur_label.fuzz_values.append(self.message.plain_bits_str[cur_label.start:cur_label.end])\\n        return cur_label', 'def current_label_start(self):\\n        if self.current_label and self.message:\\n            return self.message.get_label_range(self.current_label, self.proto_view, False)[0]\\n        else:\\n            return -1', 'def current_label_end(self):\\n        if self.current_label and self.message:\\n            return self.message.get_label_range(self.current_label, self.proto_view, False)[1]\\n        else:\\n            return -1', 'def message_data(self):\\n        if self.proto_view == 0:\\n            return self.message.plain_bits_str\\n        elif self.proto_view == 1:\\n            return self.message.plain_hex_str\\n        elif self.proto_view == 2:\\n            return self.message.plain_ascii_str\\n        else:\\n            return None', 'def update_message_data_string(self):\\n        fuz_start = self.current_label_start\\n        fuz_end = self.current_label_end\\n        num_proto_bits = 10\\n        num_fuz_bits = 16\\n\\n        proto_start = fuz_start - num_proto_bits\\n        preambel = \"... \"\\n        if proto_start <= 0:\\n            proto_start = 0\\n            preambel = \"\"\\n\\n        proto_end = fuz_end + num_proto_bits\\n        postambel = \" ...\"\\n        if proto_end >= len(self.message_data) - 1:\\n            proto_end = len(self.message_data) - 1\\n            postambel = \"\"\\n\\n        fuzamble = \"\"\\n        if fuz_end - fuz_start > num_fuz_bits:\\n            fuz_end = fuz_start + num_fuz_bits\\n            fuzamble = \"...\"\\n\\n        self.ui.lPreBits.setText(preambel + self.message_data[proto_start:self.current_label_start])\\n        self.ui.lFuzzedBits.setText(self.message_data[fuz_start:fuz_end] + fuzamble)\\n        self.ui.lPostBits.setText(self.message_data[self.current_label_end:proto_end] + postambel)\\n        self.set_add_spinboxes_maximum_on_label_change()', 'def on_fuzzing_start_changed(self, value: int):\\n        self.ui.spinBoxFuzzingEnd.setMinimum(self.ui.spinBoxFuzzingStart.value())\\n        new_start = self.message.convert_index(value - 1, self.proto_view, 0, False)[0]\\n        self.current_label.start = new_start\\n        self.current_label.fuzz_values[:] = []\\n        self.update_message_data_string()\\n        self.fuzz_table_model.update()\\n        self.ui.tblFuzzingValues.resize_me()', 'def on_fuzzing_end_changed(self, value: int):\\n        self.ui.spinBoxFuzzingStart.setMaximum(self.ui.spinBoxFuzzingEnd.value())\\n        new_end = self.message.convert_index(value - 1, self.proto_view, 0, False)[1] + 1\\n        self.current_label.end = new_end\\n        self.current_label.fuzz_values[:] = []\\n        self.update_message_data_string()\\n        self.fuzz_table_model.update()\\n        self.ui.tblFuzzingValues.resize_me()', 'def on_combo_box_fuzzing_label_current_index_changed(self, index: int):\\n        self.fuzz_table_model.fuzzing_label = self.current_label\\n        self.fuzz_table_model.update()\\n        self.update_message_data_string()\\n        self.ui.tblFuzzingValues.resize_me()\\n\\n        self.ui.spinBoxFuzzingStart.blockSignals(True)\\n        self.ui.spinBoxFuzzingStart.setValue(self.current_label_start + 1)\\n        self.ui.spinBoxFuzzingStart.blockSignals(False)\\n\\n        self.ui.spinBoxFuzzingEnd.blockSignals(True)\\n        self.ui.spinBoxFuzzingEnd.setValue(self.current_label_end)\\n        self.ui.spinBoxFuzzingEnd.blockSignals(False)', 'def on_btn_add_row_clicked(self):\\n        self.current_label.add_fuzz_value()\\n        self.fuzz_table_model.update()', 'def on_btn_del_row_clicked(self):\\n        min_row, max_row, _, _ = self.ui.tblFuzzingValues.selection_range()\\n        self.delete_lines(min_row, max_row)', 'def delete_lines(self, min_row, max_row):\\n        if min_row == -1:\\n            self.current_label.fuzz_values = self.current_label.fuzz_values[:-1]\\n        else:\\n            self.current_label.fuzz_values = self.current_label.fuzz_values[:min_row] + self.current_label.fuzz_values[\\n                                                                                        max_row + 1:]\\n\\n        _ = self.current_label  # if user deleted all, this will restore a fuzz value\\n\\n        self.fuzz_table_model.update()', 'def on_remove_duplicates_state_changed(self):\\n        self.fuzz_table_model.remove_duplicates = self.ui.chkBRemoveDuplicates.isChecked()\\n        self.fuzz_table_model.update()\\n        self.remove_duplicates()', 'def set_add_spinboxes_maximum_on_label_change(self):\\n        nbits = self.current_label.end - self.current_label.start  # Use Bit Start/End for maximum calc.\\n        if nbits >= 32:\\n            nbits = 31\\n        max_val = 2 ** nbits - 1\\n        self.ui.sBAddRangeStart.setMaximum(max_val - 1)\\n        self.ui.sBAddRangeEnd.setMaximum(max_val)\\n        self.ui.sBAddRangeEnd.setValue(max_val)\\n        self.ui.sBAddRangeStep.setMaximum(max_val)\\n        self.ui.spinBoxLowerBound.setMaximum(max_val - 1)\\n        self.ui.spinBoxUpperBound.setMaximum(max_val)\\n        self.ui.spinBoxUpperBound.setValue(max_val)\\n        self.ui.spinBoxBoundaryNumber.setMaximum(int(max_val / 2) + 1)\\n        self.ui.spinBoxRandomMinimum.setMaximum(max_val - 1)\\n        self.ui.spinBoxRandomMaximum.setMaximum(max_val)\\n        self.ui.spinBoxRandomMaximum.setValue(max_val)', 'def on_fuzzing_range_start_changed(self, value: int):\\n        self.ui.sBAddRangeEnd.setMinimum(value)\\n        self.ui.sBAddRangeStep.setMaximum(self.ui.sBAddRangeEnd.value() - value)', 'def on_fuzzing_range_end_changed(self, value: int):\\n        self.ui.sBAddRangeStart.setMaximum(value - 1)\\n        self.ui.sBAddRangeStep.setMaximum(value - self.ui.sBAddRangeStart.value())', 'def on_lower_bound_checked_changed(self):\\n        if self.ui.checkBoxLowerBound.isChecked():\\n            self.ui.spinBoxLowerBound.setEnabled(True)\\n            self.ui.spinBoxBoundaryNumber.setEnabled(True)\\n        elif not self.ui.checkBoxUpperBound.isChecked():\\n            self.ui.spinBoxLowerBound.setEnabled(False)\\n            self.ui.spinBoxBoundaryNumber.setEnabled(False)\\n        else:\\n            self.ui.spinBoxLowerBound.setEnabled(False)', 'def on_upper_bound_checked_changed(self):\\n        if self.ui.checkBoxUpperBound.isChecked():\\n            self.ui.spinBoxUpperBound.setEnabled(True)\\n            self.ui.spinBoxBoundaryNumber.setEnabled(True)\\n        elif not self.ui.checkBoxLowerBound.isChecked():\\n            self.ui.spinBoxUpperBound.setEnabled(False)\\n            self.ui.spinBoxBoundaryNumber.setEnabled(False)\\n        else:\\n            self.ui.spinBoxUpperBound.setEnabled(False)', 'def on_lower_bound_changed(self):\\n        self.ui.spinBoxUpperBound.setMinimum(self.ui.spinBoxLowerBound.value())\\n        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()\\n                                                            - self.ui.spinBoxLowerBound.value()) / 2))', 'def on_upper_bound_changed(self):\\n        self.ui.spinBoxLowerBound.setMaximum(self.ui.spinBoxUpperBound.value() - 1)\\n        self.ui.spinBoxBoundaryNumber.setMaximum(math.ceil((self.ui.spinBoxUpperBound.value()\\n                                                            - self.ui.spinBoxLowerBound.value()) / 2))', 'def on_random_range_min_changed(self):\\n        self.ui.spinBoxRandomMaximum.setMinimum(self.ui.spinBoxRandomMinimum.value())', 'def on_random_range_max_changed(self):\\n        self.ui.spinBoxRandomMinimum.setMaximum(self.ui.spinBoxRandomMaximum.value() - 1)', 'def on_btn_add_fuzzing_values_clicked(self):\\n        if self.ui.comboBoxStrategy.currentIndex() == 0:\\n            self.__add_fuzzing_range()\\n        elif self.ui.comboBoxStrategy.currentIndex() == 1:\\n            self.__add_fuzzing_boundaries()\\n        elif self.ui.comboBoxStrategy.currentIndex() == 2:\\n            self.__add_random_fuzzing_values()', 'def __add_fuzzing_boundaries(self):\\n        lower_bound = -1\\n        if self.ui.spinBoxLowerBound.isEnabled():\\n            lower_bound = self.ui.spinBoxLowerBound.value()\\n\\n        upper_bound = -1\\n        if self.ui.spinBoxUpperBound.isEnabled():\\n            upper_bound = self.ui.spinBoxUpperBound.value()\\n\\n        num_vals = self.ui.spinBoxBoundaryNumber.value()\\n        self.fuzz_table_model.add_boundaries(lower_bound, upper_bound, num_vals)', 'def remove_duplicates(self):\\n        if self.ui.chkBRemoveDuplicates.isChecked():\\n            for lbl in self.message.message_type:\\n                seq = lbl.fuzz_values[:]\\n                seen = set()\\n                add_seen = seen.add\\n                lbl.fuzz_values = [l for l in seq if not (l in seen or add_seen(l))]', 'def set_current_label_name(self):\\n        self.current_label.name = self.ui.comboBoxFuzzingLabel.currentText()\\n        self.ui.comboBoxFuzzingLabel.setItemText(self.ui.comboBoxFuzzingLabel.currentIndex(), self.current_label.name)', 'def on_fuzz_msg_changed(self, index: int):\\n        self.ui.comboBoxFuzzingLabel.setDisabled(False)\\n\\n        sel_label_ind = self.ui.comboBoxFuzzingLabel.currentIndex()\\n        self.ui.comboBoxFuzzingLabel.blockSignals(True)\\n        self.ui.comboBoxFuzzingLabel.clear()\\n\\n        if len(self.message.message_type) == 0:\\n            self.ui.comboBoxFuzzingLabel.setDisabled(True)\\n            return\\n\\n        self.ui.comboBoxFuzzingLabel.addItems([lbl.name for lbl in self.message.message_type])\\n        self.ui.comboBoxFuzzingLabel.blockSignals(False)\\n\\n        if sel_label_ind < self.ui.comboBoxFuzzingLabel.count():\\n            self.ui.comboBoxFuzzingLabel.setCurrentIndex(sel_label_ind)\\n        else:\\n            self.ui.comboBoxFuzzingLabel.setCurrentIndex(0)\\n\\n        self.fuzz_table_model.fuzzing_label = self.current_label\\n        self.fuzz_table_model.update()\\n        self.update_message_data_string()']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.oFile = vhdlFile.vhdlFile(lFile)\\n        self.assertIsNone(eError)\\n        self.oFile.set_indent_map(dIndentMap)']}, {'features': [], 'snippets': ['def test_configure_logging_sets_converter(self):\\n        out = StringIO()\\n        c_log, f_log, formatter = logging_support.configure_logging(out)\\n        self.assertEqual(c_log, logging.root.handlers[0])\\n        self.assertEqual(f_log, logging.root.handlers[1])\\n        self.assertEqual(None, c_log.formatter)\\n        self.assertEqual(formatter, f_log.formatter)\\n        self.assertEqual(time.gmtime, formatter.converter)\\n        self.assertEqual(\"%Y-%m-%d %H:%M:%SZ\", formatter.datefmt)\\n        self.assertEqual(logging.StreamHandler, c_log.__class__)\\n        self.assertEqual(out, c_log.stream)\\n        self.assertEqual(logging.FileHandler, f_log.__class__)\\n        self.assertEqual(os.path.expanduser(\"~/.cache/lmirror/log\"), f_log.baseFilename)']}, {'features': [], 'snippets': ['def __init__(self,name,arguments,as_written=\"\",position=0):\\n        self.arguments = arguments\\n        self.number_of_arguments = len(arguments)\\n        self.name = name\\n        self.as_written = as_written\\n        self.arguments_list = arguments\\n        self.position = position', 'def change_argument(self,num,func):\\n        r\"\"\"\\n        Apply the function <func> to the <n>th argument of self. Then return a new object.\\n        \"\"\"\\n        n=num-1     # Internally, the arguments are numbered from 0.\\n        arguments=self.arguments_list\\n        configuration=self.configuration()\\n        arguments[n]=func(arguments[n])\\n        new_text=self.name\\n        if len(arguments) != len(configuration):\\n            print(\"Error : length of the configuration list has to be the same as the number of arguments\")\\n            raise ValueError\\n        for i in range(len(arguments)):\\n            new_text=new_text+configuration[i]+\"{\"+arguments[i]+\"}\"\\n        return Occurrence(self.name,arguments,new_text,self.position)', 'def __getitem__(self,a):\\n        return self.arguments[a]', 'def __init__(self,occurrence):\\n        self.occurrence = occurrence\\n        self.arguments = self.occurrence.arguments\\n        if len(self.arguments) == 0 :\\n            self.name = \"Non interesting; probably the definition\"\\n            self.listoche = [None,None,None,None,None]\\n            self.value,self.page,self.section_name,self.fourth,self.fifth=(None,None,None,None,None)\\n        else :\\n            self.name = self.arguments[0][0]\\n            self.listoche = [a[0] for a in SearchArguments(self.arguments[1][0],5)[0]]\\n            self.value = self.listoche[0]\\n            self.page = self.listoche[1]\\n            self.section_name = self.listoche[2].replace(r\"\\\\relax\",\"\")\\n            self.fourth = self.listoche[3]      # I don\\'t know the role of the fourth argument of \\\\newlabel\\n            self.fifth = self.listoche[4]       # I don\\'t know the role of the fifth argument of \\\\newlabel', 'def __init__(self,Occurrence):\\n        self.directory=Occurrence[0]', 'def __init__(self,occurrence):\\n        self.label = occurrence[0]', 'def __init__(self,occurrence):\\n        self.occurrence = occurrence\\n        self.number_of_arguments = 0\\n        if self.occurrence[1][1] == \"[]\":\\n            self.number_of_arguments = self.occurrence[1][0]\\n        self.name = self.occurrence[0][0]#[0]\\n        self.definition = self.occurrence[-1][0]', 'def __init__(self,occurrence):\\n        self.occurrence=occurrence\\n        self.label=self.occurrence.arguments[0]', 'def __init__(self,occurrence):\\n        self.occurrence=occurrence\\n        self.label=self.occurrence.arguments[0]', 'def __init__(self,occurrence):\\n        self.occurrence=occurrence\\n        self.label=self.occurrence.arguments[0]', 'def __init__(self,occurrence):\\n        Occurrence.__init__(self,occurrence.name,occurrence.arguments,as_written=occurrence.as_written,position=occurrence.position)\\n        self.occurrence = occurrence\\n        self.filename = self.occurrence[0]\\n        self.input_paths=InputPaths()\\n        self._file_content=None        # Make file_content \"lazy\"']}, {'features': [], 'snippets': ['def __init__(self, cs_bar_pin, clk_pin=1000000, mosi_pin=0, miso_pin=0, chip=\\'MCP3208\\',\\n                 channel_max=None, bit_length=None, single_ended=True):\\n        \"\"\"Initialize the code and set the GPIO pins.\\n        The last argument, ch_max, is 2 for the MCP3202, 4 for the\\n        MCP3204 or 8 for the MCS3208.\"\"\"\\n\\n        self._CLK = clk_pin\\n        self._MOSI = mosi_pin\\n        self._MISO = miso_pin\\n        self._CS_bar = cs_bar_pin\\n\\n        chip_dictionary = {\\n                \"MCP3202\": (2, 12),\\n                \"MCP3204\": (4, 12),\\n                \"MCP3208\": (8, 12),\\n                \"MCP3002\": (2, 10),\\n                \"MCP3004\": (4, 10),\\n                \"MCP3008\": (8, 10)\\n        }\\n\\n        if chip in chip_dictionary:\\n            self._ChannelMax = chip_dictionary[chip][0]\\n            self._BitLength = chip_dictionary[chip][1]\\n        elif chip is None and (channel_max is not None) and (bit_length is not None):\\n            self._ChannelMax = channel_max\\n            self._BitLength = bit_length\\n        else:\\n            print(\"Unknown chip: {} - Please re-initialize.\")\\n            self._ChannelMax = 0\\n            self._BitLength = 0\\n            return\\n\\n        self._SingleEnded = single_ended\\n        self._Vref = 3.3\\n        self._values = MyValues(self.read_adc, self._ChannelMax)\\n        self._volts = MyValues(self.read_volts, self._ChannelMax)\\n\\n        # This is used to speed up the SPIDEV communication. Send out MSB first.\\n        # control[0] - bit7-3: upper 5 bits 0, because we can only send 8 bit sequences.\\n        #            - bit2   : Start bit - starts conversion in ADCs\\n        #            - bit1   : Select single_ended=1 or differential=0\\n        #            - bit0   : D2 high bit of channel select.\\n        # control[1] - bit7   : D1 middle bit of channel select.\\n        #            - bit6   : D0 low bit of channel select.\\n        #            - bit5-0 : Don\\'t care.\\n        if self._SingleEnded:\\n            self._control0 = [0b00000110, 0b00100000, 0]  # Pre-compute part of the control word.\\n        else:\\n            self._control0 = [0b00000100, 0b00100000, 0]  # Pre-compute part of the control word.\\n\\n        if self._MOSI > 0:  # Bing Bang mode\\n            assert self._MISO != 0 and self._CLK < 32\\n            if GPIO.getmode() != 11:\\n                GPIO.setmode(GPIO.BCM)        # Use the BCM numbering scheme\\n\\n            GPIO.setup(self._CLK, GPIO.OUT)     # Setup the ports for in and output\\n            GPIO.setup(self._MOSI, GPIO.OUT)\\n            GPIO.setup(self._MISO, GPIO.IN)\\n            GPIO.setup(self._CS_bar, GPIO.OUT)\\n\\n            GPIO.output(self._CLK, 0)           # Set the clock low.\\n            GPIO.output(self._MOSI, 0)          # Set the Master Out low\\n            GPIO.output(self._CS_bar, 1)        # Set the CS_bar high\\n\\n        else:\\n            self._dev = spidev.SpiDev(0, self._CS_bar)  # Start a SpiDev device\\n            self._dev.mode = 0                          # Set SPI mode (phase)\\n            self._dev.max_speed_hz = self._CLK          # Set the data rate\\n            self._dev.bits_per_word = 8                 # Number of bit per word. ALWAYS 8', 'def get_channel_max(self):\\n        \"\"\"Return the maximum number of channels\"\"\"\\n        return self._ChannelMax', 'def get_value_max(self):\\n        \"\"\"Return the maximum value possible for an ADC read\"\"\"\\n        return 2 ** self._BitLength - 1', 'def read_bit(self):\\n        \"\"\" Read a single bit from the ADC and pulse clock.\"\"\"\\n        if self._MOSI == 0:\\n            return 0\\n        #\\n        # The output is going out on the falling edge of the clock,\\n        # and is to be read on the rising edge of the clock.\\n\\n        # Clock should be already low, and data should already be set.\\n        GPIO.output(self._CLK, 1)     # Set the clock high. Ready to read.\\n        bit = GPIO.input(self._MISO)  # Read the bit.\\n        GPIO.output(self._CLK, 0)     # Return clock low, next bit will be set.\\n\\n        return bit', 'def read_volts(self, channel):\\n        \"\"\"Read the ADC value from channel and convert to volts, assuming that Vref is set correctly. \"\"\"\\n        return self._Vref * self.read_adc(channel) / self.get_value_max()', 'def values(self):\\n        \"\"\"ADC values presented as a list.\"\"\"\\n        return self._values', 'def volts(self):\\n        \"\"\"ADC voltages presented as a list\"\"\"\\n        return self._volts', 'def accuracy(self):\\n        \"\"\"The fractional voltage of the least significant bit. \"\"\"\\n        return self._Vref / float(self.get_value_max())', 'def vref(self):\\n        \"\"\"Reference voltage used by the chip. You need to set this. It defaults to 3.3V\"\"\"\\n        return self._Vref', 'def vref(self, vr):\\n        self._Vref = vr']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def root():\\n    \"\"\" Web interface landing page. \"\"\"\\n    return render_template(\\'index.html\\')', 'def error():\\n    \"\"\" Display errors. \"\"\"\\n    return render_template(\\'error.html\\')', \"def map_svg():\\n    return make_map(request, format='svg')\", \"def map_png():\\n    return make_map(request, format='png')\", \"def map_jpg():\\n    return make_map(request, format='jpg')\", 'def process():\\n    \"\"\" Process submitted form data. \"\"\"\\n    format = request.form[\\'format\\']\\n\\n    try:\\n        node = {\\n            \\'png\\': \\'map_png\\',\\n            \\'svg\\': \\'map_svg\\',\\n            \\'jpg\\': \\'map_jpg\\',\\n        }[format]\\n    except KeyError:\\n        flash(\"The output format you selected is not supported.\")\\n        return redirect(url_for(\\'error\\'))\\n    else:\\n        return redirect(url_for(node, _method=\\'POST\\'), code=307)']}, {'features': [], 'snippets': ['def __init__(self,fromIp,toIp):\\r\\n        startTime = time.time()\\r\\n        self.fromIp = fromIp # from 192.168.1.x\\r\\n        self.toIp = toIp # to 192.168.x.x\\r\\n        self.__checkIfIpIsValid(fromIp)\\r\\n        self.__checkIfIpIsValid(toIp)\\r\\n        self.__getRange(fromIp,toIp)\\r\\n        self.__shellToQueue()\\r\\n        #self.__checkIfUp() # run by the shellToQueue queue organizer\\r\\n        self.__computerInfoInQueue()\\r\\n        endTime = time.time()\\r\\n        self.executionTime = round(endTime - startTime,3)', 'def __checkIfIpIsValid(self,ip):\\r\\n        def validateRange(val):\\r\\n            # valid range => 1 <-> 255\\r\\n            try:\\r\\n                val = int(val)\\r\\n                if val < 0 or val > 255:\\r\\n                    print \"Invalid IP Range (\"+str(val)+\")\"\\r\\n                    sys.exit(0)\\r\\n            except:\\r\\n                print \"Invalid IP\"\\r\\n                sys.exit(0)\\r\\n        ip = ip.split(\".\")\\r\\n        firstVal = validateRange(ip[0])\\r\\n        secondVal = validateRange(ip[1])\\r\\n        thirdVal = validateRange(ip[2])\\r\\n        fourthVal = validateRange(ip[3])\\r\\n        return True', 'def __getRange(self,fromIp,toIp):\\r\\n        fromIp = fromIp.split(\".\")\\r\\n        toIp = toIp.split(\".\")', 'def ip3chars(ipBlock):\\r\\n            # input 1; output 001\\r\\n            ipBlock = str(ipBlock)\\r\\n            while len(ipBlock) != 3:\\r\\n                ipBlock = \"0\"+ipBlock\\r\\n            return ipBlock', \"def __shellToQueue(self):\\r\\n        # write them in the shell queue\\r\\n        maxPingsAtOnce = 200\\r\\n        currentQueuedPings = 0\\r\\n        for pingIp in self.__ipsToCheck:\\r\\n            proc = subprocess.Popen(['ping','-n','1',pingIp],stdout=subprocess.PIPE,shell=True)\\r\\n            self.__shellPings.append(proc)\\r\\n            currentQueuedPings += 1\\r\\n            if currentQueuedPings >= maxPingsAtOnce:\\r\\n                #execute shells\\r\\n                self.__checkIfUp()\\r\\n                currentQueuedPings = 0\\r\\n                self.__shellPings = []\\r\\n        self.__checkIfUp() # execute last queue\", 'def __checkIfUp(self):\\r\\n        # execute the shells & determine whether the host is up or not\\r\\n        for shellInQueue in self.__shellPings:\\r\\n            pingResult = \"\"\\r\\n            shellInQueue.wait()\\r\\n            while True:\\r\\n                line = shellInQueue.stdout.readline()\\r\\n                if line != \"\":\\r\\n                    pingResult += line\\r\\n                else:\\r\\n                    break;\\r\\n            self.checkedIps += 1\\r\\n            if \\'unreachable\\' in pingResult:\\r\\n                self.unreachable += 1\\r\\n            elif \\'timed out\\' in pingResult:\\r\\n                self.timedOut += 1\\r\\n            else:\\r\\n                self.onlineIps += 1\\r\\n                currentIp = self.__ipsToCheck[self.checkedIps-1]\\r\\n                self.upIpsAddress.append(currentIp)', \"def __computerInfoInQueue(self):\\r\\n        # shell queue for online hosts\\r\\n        maxShellsAtOnce = 255\\r\\n        currentQueuedNbst = 0\\r\\n        for onlineIp in self.upIpsAddress:\\r\\n            proc = subprocess.Popen(['\\\\\\\\Windows\\\\\\\\sysnative\\\\\\\\nbtstat.exe','-a',onlineIp],stdout=subprocess.PIPE,shell=True)\\r\\n            self.__shell2Nbst.append(proc)\\r\\n            currentQueuedNbst += 1\\r\\n            if currentQueuedNbst >= maxShellsAtOnce:\\r\\n                # execute shells\\r\\n                self.__gatherComputerInfo()\\r\\n                currentQueuedNbst = 0\\r\\n                self.__shell2Nbst = []\\r\\n        self.__gatherComputerInfo() # execute last queue\", 'def __gatherComputerInfo(self):\\r\\n        # execute the shells and find host Name and MAC\\r\\n        for shellInQueue in self.__shell2Nbst:\\r\\n            nbstResult = \"\"\\r\\n            shellInQueue.wait()', 'def readValue(self):\\r\\n        # debugging use only\\r\\n        ips = []\\r\\n        for ip in self.completeMacAddress:\\r\\n            ips.append(ip)\\r\\n        return ips']}, {'features': [], 'snippets': ['def __init__(self, app, parent, model, **kwargs):\\n        flags = Qt.CustomizeWindowHint | Qt.WindowTitleHint | Qt.WindowSystemMenuHint\\n        super().__init__(parent, flags, **kwargs)\\n        self.app = app\\n        self.specific_actions = frozenset()\\n        self._setupUI()\\n        self.model = model  # ExcludeListDialogCore\\n        self.model.view = self\\n        self.table = ExcludeListTable(app, view=self.tableView)  # Qt ExcludeListTable\\n        self._row_matched = False  # test if at least one row matched our test string\\n        self._input_styled = False\\n\\n        self.buttonAdd.clicked.connect(self.addStringFromLineEdit)\\n        self.buttonRemove.clicked.connect(self.removeSelected)\\n        self.buttonRestore.clicked.connect(self.restoreDefaults)\\n        self.buttonClose.clicked.connect(self.accept)\\n        self.buttonHelp.clicked.connect(self.display_help_message)\\n        self.buttonTestString.clicked.connect(self.onTestStringButtonClicked)\\n        self.inputLine.textEdited.connect(self.reset_input_style)\\n        self.testLine.textEdited.connect(self.reset_input_style)\\n        self.testLine.textEdited.connect(self.reset_table_style)', 'def show(self):\\n        super().show()\\n        self.inputLine.setFocus()', 'def addStringFromLineEdit(self):\\n        text = self.inputLine.text()\\n        if not text:\\n            return\\n        try:\\n            self.model.add(text)\\n        except AlreadyThereException:\\n            self.app.show_message(\"Expression already in the list.\")\\n            return\\n        except Exception as e:\\n            self.app.show_message(f\"Expression is invalid: {e}\")\\n            return\\n        self.inputLine.clear()', 'def restoreDefaults(self):\\n        self.model.restore_defaults()', 'def reset_input_style(self):\\n        \"\"\"Reset regex input line background\"\"\"\\n        if self._input_styled:\\n            self.inputLine.setStyleSheet(self.styleSheet())\\n            self._input_styled = False', 'def display_help_message(self):\\n        self.app.show_message(\\n            tr(\\n                \"\"\"\\\\']}, {'features': [], 'snippets': ['def myroot():\\n    return_data = get_index()\\n    return return_data', 'def bmarks():\\n    return_data = get_bmarklet()\\n    return return_data', 'def bmarks():\\n    return_data = add_tags()\\n    return return_data', 'def bmarks():\\n    return_data = get_bmarklet()\\n    return return_data', 'def bmarks():\\n    return_data = get_bmarks()\\n    return return_data', 'def bmarks():\\n    return_data = do_edit()\\n    return return_data', 'def bmarks():\\n    return_data = get_edit_tags()\\n    return return_data', 'def bmarks():\\n    return_data = get_import_bm()\\n    return return_data', 'def bmarks():\\n    return_data = do_login()\\n    return return_data', 'def bmarks():\\n    return_data = do_register()\\n    return return_data', 'def bmarks():\\n    return_data = get_tags()\\n    return return_data', \"def send_css(filename):\\n    return static_file(filename, root='css')\", \"def send_js(filename):\\n    return static_file(filename, root='js')\", \"def send_img(filename):\\n    return static_file(filename, root='images')\", \"def send_font(filename):\\n    return static_file(filename, root='fonts')\", \"def handle404(error):\\n    return '<H1>Ooops, its not here<BR>'\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def onload(self):\\n\\t\\tsuper(Item, self).onload()\\n\\n\\t\\tself.set_onload('stock_exists', self.stock_ledger_created())\\n\\t\\tself.set_asset_naming_series()\", 'def autoname(self):\\n\\t\\tif frappe.db.get_default(\"item_naming_by\") == \"Naming Series\":\\n\\t\\t\\tif self.variant_of:\\n\\t\\t\\t\\tif not self.item_code:\\n\\t\\t\\t\\t\\ttemplate_item_name = frappe.db.get_value(\"Item\", self.variant_of, \"item_name\")\\n\\t\\t\\t\\t\\tself.item_code = make_variant_item_code(self.variant_of, template_item_name, self)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tfrom frappe.model.naming import set_name_by_naming_series\\n\\t\\t\\t\\tset_name_by_naming_series(self)\\n\\t\\t\\t\\tself.item_code = self.name\\n\\n\\t\\tself.item_code = strip(self.item_code)\\n\\t\\tself.name = self.item_code', \"def after_insert(self):\\n\\t\\t'''set opening stock and item price'''\\n\\t\\tif self.standard_rate:\\n\\t\\t\\tfor default in self.item_defaults:\\n\\t\\t\\t\\tself.add_price(default.default_price_list)\\n\\n\\t\\tif self.opening_stock:\\n\\t\\t\\tself.set_opening_stock()\", 'def on_update(self):\\n\\t\\tinvalidate_cache_for_item(self)\\n\\t\\tself.validate_name_with_item_group()\\n\\t\\tself.update_variants()\\n\\t\\tself.update_item_price()\\n\\t\\tself.update_template_item()', 'def add_price(self, price_list=None):\\n\\t\\t\\'\\'\\'Add a new price\\'\\'\\'\\n\\t\\tif not price_list:\\n\\t\\t\\tprice_list = (frappe.db.get_single_value(\\'Selling Settings\\', \\'selling_price_list\\')\\n\\t\\t\\t\\t\\t\\tor frappe.db.get_value(\\'Price List\\', _(\\'Standard Selling\\')))\\n\\t\\tif price_list:\\n\\t\\t\\titem_price = frappe.get_doc({\\n\\t\\t\\t\\t\"doctype\": \"Item Price\",\\n\\t\\t\\t\\t\"price_list\": price_list,\\n\\t\\t\\t\\t\"item_code\": self.name,\\n\\t\\t\\t\\t\"currency\": erpnext.get_default_currency(),\\n\\t\\t\\t\\t\"price_list_rate\": self.standard_rate\\n\\t\\t\\t})\\n\\t\\t\\titem_price.insert()', \"def make_route(self):\\n\\t\\tif not self.route:\\n\\t\\t\\treturn cstr(frappe.db.get_value('Item Group', self.item_group,\\n\\t\\t\\t\\t\\t'route')) + '/' + self.scrub((self.item_name if self.item_name else self.item_code) + '-' + random_string(5))\", 'def make_thumbnail(self):\\n\\t\\tif frappe.flags.in_import:\\n\\t\\t\\treturn\\n\\n\\t\\t\"\"\"Make a thumbnail of `website_image`\"\"\"\\n\\t\\timport requests.exceptions\\n\\n\\t\\tif not self.is_new() and self.website_image != frappe.db.get_value(self.doctype, self.name, \"website_image\"):\\n\\t\\t\\tself.thumbnail = None\\n\\n\\t\\tif self.website_image and not self.thumbnail:\\n\\t\\t\\tfile_doc = None\\n\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tfile_doc = frappe.get_doc(\"File\", {\\n\\t\\t\\t\\t\\t\"file_url\": self.website_image,\\n\\t\\t\\t\\t\\t\"attached_to_doctype\": \"Item\",\\n\\t\\t\\t\\t\\t\"attached_to_name\": self.name\\n\\t\\t\\t\\t})\\n\\t\\t\\texcept frappe.DoesNotExistError:\\n\\t\\t\\t\\tpass\\n\\t\\t\\t\\t# cleanup\\n\\t\\t\\t\\tfrappe.local.message_log.pop()\\n\\n\\t\\t\\texcept requests.exceptions.HTTPError:\\n\\t\\t\\t\\tfrappe.msgprint(_(\"Warning: Invalid attachment {0}\").format(self.website_image))\\n\\t\\t\\t\\tself.website_image = None\\n\\n\\t\\t\\texcept requests.exceptions.SSLError:\\n\\t\\t\\t\\tfrappe.msgprint(\\n\\t\\t\\t\\t\\t_(\"Warning: Invalid SSL certificate on attachment {0}\").format(self.website_image))\\n\\t\\t\\t\\tself.website_image = None\\n\\n\\t\\t\\t# for CSV import\\n\\t\\t\\tif self.website_image and not file_doc:\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tfile_doc = frappe.get_doc({\\n\\t\\t\\t\\t\\t\\t\"doctype\": \"File\",\\n\\t\\t\\t\\t\\t\\t\"file_url\": self.website_image,\\n\\t\\t\\t\\t\\t\\t\"attached_to_doctype\": \"Item\",\\n\\t\\t\\t\\t\\t\\t\"attached_to_name\": self.name\\n\\t\\t\\t\\t\\t}).insert()\\n\\n\\t\\t\\t\\texcept IOError:\\n\\t\\t\\t\\t\\tself.website_image = None\\n\\n\\t\\t\\tif file_doc:\\n\\t\\t\\t\\tif not file_doc.thumbnail_url:\\n\\t\\t\\t\\t\\tfile_doc.make_thumbnail()\\n\\n\\t\\t\\t\\tself.thumbnail = file_doc.thumbnail_url', 'def validate_retain_sample(self):\\n\\t\\tif self.retain_sample and not frappe.db.get_single_value(\\'Stock Settings\\', \\'sample_retention_warehouse\\'):\\n\\t\\t\\tfrappe.throw(_(\"Please select Sample Retention Warehouse in Stock Settings first\"))\\n\\t\\tif self.retain_sample and not self.has_batch_no:\\n\\t\\t\\tfrappe.throw(_(\" {0} Retain Sample is based on batch, please check Has Batch No to retain sample of item\").format(\\n\\t\\t\\t\\tself.item_code))', 'def set_variant_context(self, context):\\n\\t\\tif self.has_variants:\\n\\t\\t\\tcontext.no_cache = True\\n\\n\\t\\t\\t# load variants\\n\\t\\t\\t# also used in set_attribute_context\\n\\t\\t\\tcontext.variants = frappe.get_all(\"Item\",\\n\\t\\t\\t\\t filters={\"variant_of\": self.name, \"show_variant_in_website\": 1},\\n\\t\\t\\t\\t order_by=\"name asc\")\\n\\n\\t\\t\\tvariant = frappe.form_dict.variant\\n\\t\\t\\tif not variant and context.variants:\\n\\t\\t\\t\\t# the case when the item is opened for the first time from its list\\n\\t\\t\\t\\tvariant = context.variants[0]\\n\\n\\t\\t\\tif variant:\\n\\t\\t\\t\\tcontext.variant = frappe.get_doc(\"Item\", variant)\\n\\n\\t\\t\\t\\tfor fieldname in (\"website_image\", \"web_long_description\", \"description\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\"website_specifications\"):\\n\\t\\t\\t\\t\\tif context.variant.get(fieldname):\\n\\t\\t\\t\\t\\t\\tvalue = context.variant.get(fieldname)\\n\\t\\t\\t\\t\\t\\tif isinstance(value, list):\\n\\t\\t\\t\\t\\t\\t\\tvalue = [d.as_dict() for d in value]\\n\\n\\t\\t\\t\\t\\t\\tcontext[fieldname] = value\\n\\n\\t\\tif self.slideshow:\\n\\t\\t\\tif context.variant and context.variant.slideshow:\\n\\t\\t\\t\\tcontext.update(get_slideshow(context.variant))\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcontext.update(get_slideshow(self))', 'def set_disabled_attributes(self, context):\\n\\t\\t\"\"\"Disable selection options of attribute combinations that do not result in a variant\"\"\"\\n\\t\\tif not self.attributes or not self.has_variants:\\n\\t\\t\\treturn\\n\\n\\t\\tcontext.disabled_attributes = {}\\n\\t\\tattributes = [attr.attribute for attr in self.attributes]\\n\\n\\t\\tdef find_variant(combination):\\n\\t\\t\\tfor variant in context.variants:\\n\\t\\t\\t\\tif len(variant.attributes) < len(attributes):\\n\\t\\t\\t\\t\\tcontinue\\n\\n\\t\\t\\t\\tif \"combination\" not in variant:\\n\\t\\t\\t\\t\\tref_combination = []\\n\\n\\t\\t\\t\\t\\tfor attr in variant.attributes:\\n\\t\\t\\t\\t\\t\\tidx = attributes.index(attr.attribute)\\n\\t\\t\\t\\t\\t\\tref_combination.insert(idx, attr.attribute_value)\\n\\n\\t\\t\\t\\t\\tvariant[\"combination\"] = ref_combination\\n\\n\\t\\t\\t\\tif not (set(combination) - set(variant[\"combination\"])):\\n\\t\\t\\t\\t\\t# check if the combination is a subset of a variant combination\\n\\t\\t\\t\\t\\t# eg. [Blue, 0.5] is a possible combination if exists [Blue, Large, 0.5]\\n\\t\\t\\t\\t\\treturn True\\n\\n\\t\\tfor i, attr in enumerate(self.attributes):\\n\\t\\t\\tif i == 0:\\n\\t\\t\\t\\tcontinue\\n\\n\\t\\t\\tcombination_source = []\\n\\n\\t\\t\\t# loop through previous attributes\\n\\t\\t\\tfor prev_attr in self.attributes[:i]:\\n\\t\\t\\t\\tcombination_source.append([context.selected_attributes.get(prev_attr.attribute)])\\n\\n\\t\\t\\tcombination_source.append(context.attribute_values[attr.attribute])\\n\\n\\t\\t\\tfor combination in itertools.product(*combination_source):\\n\\t\\t\\t\\tif not find_variant(combination):\\n\\t\\t\\t\\t\\tcontext.disabled_attributes.setdefault(attr.attribute, []).append(combination[-1])', 'def update_template_tables(self):\\n\\t\\ttemplate = frappe.get_doc(\"Item\", self.variant_of)\\n\\n\\t\\t# add item taxes from template\\n\\t\\tfor d in template.get(\"taxes\"):\\n\\t\\t\\tself.append(\"taxes\", {\"tax_type\": d.tax_type, \"tax_rate\": d.tax_rate})\\n\\n\\t\\t# copy re-order table if empty\\n\\t\\tif not self.get(\"reorder_levels\"):\\n\\t\\t\\tfor d in template.get(\"reorder_levels\"):\\n\\t\\t\\t\\tn = {}\\n\\t\\t\\t\\tfor k in (\"warehouse\", \"warehouse_reorder_level\",\\n\\t\\t\\t\\t\\t\"warehouse_reorder_qty\", \"material_request_type\"):\\n\\t\\t\\t\\t\\tn[k] = d.get(k)\\n\\t\\t\\t\\tself.append(\"reorder_levels\", n)', 'def validate_item_type(self):\\n\\t\\tif self.has_serial_no == 1 and self.is_stock_item == 0 and not self.is_fixed_asset:\\n\\t\\t\\tmsgprint(_(\"\\'Has Serial No\\' can not be \\'Yes\\' for non-stock item\"), raise_exception=1)\\n\\n\\t\\tif self.has_serial_no == 0 and self.serial_no_series:\\n\\t\\t\\tself.serial_no_series = None', 'def fill_customer_code(self):\\n\\t\\t\"\"\" Append all the customer codes and insert into \"customer_code\" field of item table \"\"\"\\n\\t\\tcust_code = []\\n\\t\\tfor d in self.get(\\'customer_items\\'):\\n\\t\\t\\tcust_code.append(d.ref_code)\\n\\t\\tself.customer_code = \\',\\'.join(cust_code)', 'def validate_barcode(self):\\n\\t\\tfrom stdnum import ean\\n\\t\\tif len(self.barcodes) > 0:\\n\\t\\t\\tfor item_barcode in self.barcodes:\\n\\t\\t\\t\\toptions = frappe.get_meta(\"Item Barcode\").get_options(\"barcode_type\").split(\\'\\\\n\\')\\n\\t\\t\\t\\tif item_barcode.barcode:\\n\\t\\t\\t\\t\\tduplicate = frappe.db.sql(\\n\\t\\t\\t\\t\\t\\t\"\"\"select parent from `tabItem Barcode` where barcode = %s and parent != %s\"\"\", (item_barcode.barcode, self.name))\\n\\t\\t\\t\\t\\tif duplicate:\\n\\t\\t\\t\\t\\t\\tfrappe.throw(_(\"Barcode {0} already used in Item {1}\").format(\\n\\t\\t\\t\\t\\t\\t\\titem_barcode.barcode, duplicate[0][0]), frappe.DuplicateEntryError)\\n\\n\\t\\t\\t\\t\\titem_barcode.barcode_type = \"\" if item_barcode.barcode_type not in options else item_barcode.barcode_type\\n\\t\\t\\t\\t\\tif item_barcode.barcode_type and item_barcode.barcode_type.upper() in (\\'EAN\\', \\'UPC-A\\', \\'EAN-13\\', \\'EAN-8\\'):\\n\\t\\t\\t\\t\\t\\tif not ean.is_valid(item_barcode.barcode):\\n\\t\\t\\t\\t\\t\\t\\tfrappe.throw(_(\"Barcode {0} is not a valid {1} code\").format(\\n\\t\\t\\t\\t\\t\\t\\t\\titem_barcode.barcode, item_barcode.barcode_type), InvalidBarcode)', 'def stock_ledger_created(self):\\n\\t\\tif not hasattr(self, \\'_stock_ledger_created\\'):\\n\\t\\t\\tself._stock_ledger_created = len(frappe.db.sql(\"\"\"select name from `tabStock Ledger Entry`\\n\\t\\t\\t\\twhere item_code = %s limit 1\"\"\", self.name))\\n\\t\\treturn self._stock_ledger_created', 'def update_item_price(self):\\n\\t\\tfrappe.db.sql(\"\"\"update `tabItem Price` set item_name=%s,\\n\\t\\t\\titem_description=%s, brand=%s where item_code=%s\"\"\",\\n\\t\\t\\t\\t\\t(self.item_name, self.description, self.brand, self.name))', 'def before_rename(self, old_name, new_name, merge=False):\\n\\t\\tif self.item_name == old_name:\\n\\t\\t\\tfrappe.db.set_value(\"Item\", old_name, \"item_name\", new_name)\\n\\n\\t\\tif merge:\\n\\t\\t\\t# Validate properties before merging\\n\\t\\t\\tif not frappe.db.exists(\"Item\", new_name):\\n\\t\\t\\t\\tfrappe.throw(_(\"Item {0} does not exist\").format(new_name))\\n\\n\\t\\t\\tfield_list = [\"stock_uom\", \"is_stock_item\", \"has_serial_no\", \"has_batch_no\"]\\n\\t\\t\\tnew_properties = [cstr(d) for d in frappe.db.get_value(\"Item\", new_name, field_list)]\\n\\t\\t\\tif new_properties != [cstr(self.get(fld)) for fld in field_list]:\\n\\t\\t\\t\\tfrappe.throw(_(\"To merge, following properties must be same for both items\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t+ \": \\\\n\" + \", \".join([self.meta.get_label(fld) for fld in field_list]))', 'def set_last_purchase_rate(self, new_name):\\n\\t\\tlast_purchase_rate = get_last_purchase_details(new_name).get(\"base_rate\", 0)\\n\\t\\tfrappe.db.set_value(\"Item\", new_name, \"last_purchase_rate\", last_purchase_rate)', 'def copy_specification_from_item_group(self):\\n\\t\\tself.set(\"website_specifications\", [])\\n\\t\\tif self.item_group:\\n\\t\\t\\tfor label, desc in frappe.db.get_values(\"Item Website Specification\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   {\"parent\": self.item_group}, [\"label\", \"description\"]):\\n\\t\\t\\t\\trow = self.append(\"website_specifications\")\\n\\t\\t\\t\\trow.label = label\\n\\t\\t\\t\\trow.description = desc', 'def update_template_item(self):\\n\\t\\t\"\"\"Set Show in Website for Template Item if True for its Variant\"\"\"\\n\\t\\tif self.variant_of:\\n\\t\\t\\tif self.show_in_website:\\n\\t\\t\\t\\tself.show_variant_in_website = 1\\n\\t\\t\\t\\tself.show_in_website = 0\\n\\n\\t\\t\\tif self.show_variant_in_website:\\n\\t\\t\\t\\t# show template\\n\\t\\t\\t\\ttemplate_item = frappe.get_doc(\"Item\", self.variant_of)\\n\\n\\t\\t\\t\\tif not template_item.show_in_website:\\n\\t\\t\\t\\t\\ttemplate_item.show_in_website = 1\\n\\t\\t\\t\\t\\ttemplate_item.flags.dont_update_variants = True\\n\\t\\t\\t\\t\\ttemplate_item.flags.ignore_permissions = True\\n\\t\\t\\t\\t\\ttemplate_item.save()', 'def update_defaults_from_item_group(self):\\n\\t\\t\"\"\"Get defaults from Item Group\"\"\"\\n\\t\\tif self.item_group and not self.item_defaults:\\n\\t\\t\\titem_defaults = frappe.db.get_values(\"Item Default\", {\"parent\": self.item_group},\\n\\t\\t\\t\\t[\\'company\\', \\'default_warehouse\\',\\'default_price_list\\',\\'buying_cost_center\\',\\'default_supplier\\',\\n\\t\\t\\t\\t\\'expense_account\\',\\'selling_cost_center\\',\\'income_account\\'], as_dict = 1)\\n\\t\\t\\tif item_defaults:\\n\\t\\t\\t\\tfor item in item_defaults:\\n\\t\\t\\t\\t\\tself.append(\\'item_defaults\\', {\\n\\t\\t\\t\\t\\t\\t\\'company\\': item.company,\\n\\t\\t\\t\\t\\t\\t\\'default_warehouse\\': item.default_warehouse,\\n\\t\\t\\t\\t\\t\\t\\'default_price_list\\': item.default_price_list,\\n\\t\\t\\t\\t\\t\\t\\'buying_cost_center\\': item.buying_cost_center,\\n\\t\\t\\t\\t\\t\\t\\'default_supplier\\': item.default_supplier,\\n\\t\\t\\t\\t\\t\\t\\'expense_account\\': item.expense_account,\\n\\t\\t\\t\\t\\t\\t\\'selling_cost_center\\': item.selling_cost_center,\\n\\t\\t\\t\\t\\t\\t\\'income_account\\': item.income_account\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\telse:\\n\\t\\t\\t\\twarehouse = \\'\\'\\n\\t\\t\\t\\tdefaults = frappe.defaults.get_defaults() or {}\\n\\n\\t\\t\\t\\t# To check default warehouse is belong to the default company\\n\\t\\t\\t\\tif defaults.get(\"default_warehouse\") and frappe.db.exists(\"Warehouse\",\\n\\t\\t\\t\\t\\t{\\'name\\': defaults.default_warehouse, \\'company\\': defaults.company}):\\n\\t\\t\\t\\t\\twarehouse = defaults.default_warehouse\\n\\n\\t\\t\\t\\tself.append(\"item_defaults\", {\\n\\t\\t\\t\\t\\t\"company\": defaults.get(\"company\"),\\n\\t\\t\\t\\t\\t\"default_warehouse\": warehouse\\n\\t\\t\\t\\t})', 'def validate_has_variants(self):\\n\\t\\tif not self.has_variants and frappe.db.get_value(\"Item\", self.name, \"has_variants\"):\\n\\t\\t\\tif frappe.db.exists(\"Item\", {\"variant_of\": self.name}):\\n\\t\\t\\t\\tfrappe.throw(_(\"Item has variants.\"))', 'def validate_variant_based_on_change(self):\\n\\t\\tif not self.is_new() and (self.variant_of or (self.has_variants and frappe.get_all(\"Item\", {\"variant_of\": self.name}))):\\n\\t\\t\\tif self.variant_based_on != frappe.db.get_value(\"Item\", self.name, \"variant_based_on\"):\\n\\t\\t\\t\\tfrappe.throw(_(\"Variant Based On cannot be changed\"))', 'def validate_uom_conversion_factor(self):\\n\\t\\tif self.uoms:\\n\\t\\t\\tfor d in self.uoms:\\n\\t\\t\\t\\tvalue = get_uom_conv_factor(d.uom, self.stock_uom)\\n\\t\\t\\t\\tif value:\\n\\t\\t\\t\\t\\td.conversion_factor = value', 'def validate_variant_attributes(self):\\n\\t\\tif self.is_new() and self.variant_of and self.variant_based_on == \\'Item Attribute\\':\\n\\t\\t\\targs = {}\\n\\t\\t\\tfor d in self.attributes:\\n\\t\\t\\t\\tif cstr(d.attribute_value).strip() == \\'\\':\\n\\t\\t\\t\\t\\tfrappe.throw(_(\"Please specify Attribute Value for attribute {0}\").format(d.attribute))\\n\\t\\t\\t\\targs[d.attribute] = d.attribute_value\\n\\n\\t\\t\\tvariant = get_variant(self.variant_of, args, self.name)\\n\\t\\t\\tif variant:\\n\\t\\t\\t\\tfrappe.throw(_(\"Item variant {0} exists with same attributes\")\\n\\t\\t\\t\\t\\t.format(variant), ItemVariantExistsError)\\n\\n\\t\\t\\tvalidate_item_variant_attributes(self, args)', \"def get_timeline_data(doctype, name):\\n\\t'''returns timeline data based on stock ledger entry'''\\n\\tout = {}\\n\\titems = dict(frappe.db.sql('''select posting_date, count(*)\\n\\t\\tfrom `tabStock Ledger Entry` where item_code=%s\\n\\t\\t\\tand posting_date > date_sub(curdate(), interval 1 year)\\n\\t\\t\\tgroup by posting_date''', name))\\n\\n\\tfor date, count in iteritems(items):\\n\\t\\ttimestamp = get_timestamp(date)\\n\\t\\tout.update({timestamp: count})\\n\\n\\treturn out\", 'def validate_is_stock_item(item_code, is_stock_item=None, verbose=1):\\n\\tif not is_stock_item:\\n\\t\\tis_stock_item = frappe.db.get_value(\"Item\", item_code, \"is_stock_item\")\\n\\n\\tif is_stock_item != 1:\\n\\t\\tmsg = _(\"Item {0} is not a stock Item\").format(item_code)\\n\\n\\t\\t_msgprint(msg, verbose)', 'def _msgprint(msg, verbose):\\n\\tif verbose:\\n\\t\\tmsgprint(msg, raise_exception=True)\\n\\telse:\\n\\t\\traise frappe.ValidationError(msg)', 'def invalidate_cache_for_item(doc):\\n\\tinvalidate_cache_for(doc, doc.item_group)\\n\\n\\twebsite_item_groups = list(set((doc.get(\"old_website_item_groups\") or [])\\n\\t\\t\\t\\t\\t\\t\\t\\t+ [d.item_group for d in doc.get({\"doctype\": \"Website Item Group\"}) if d.item_group]))\\n\\n\\tfor item_group in website_item_groups:\\n\\t\\tinvalidate_cache_for(doc, item_group)\\n\\n\\tif doc.get(\"old_item_group\") and doc.get(\"old_item_group\") != doc.item_group:\\n\\t\\tinvalidate_cache_for(doc, doc.old_item_group)', 'def get_item_defaults(item_code, company):\\n\\titem = frappe.get_cached_doc(\\'Item\\', item_code)\\n\\n\\tout = item.as_dict()\\n\\n\\tfor d in item.item_defaults:\\n\\t\\tif d.company == company:\\n\\t\\t\\trow = copy.deepcopy(d.as_dict())\\n\\t\\t\\trow.pop(\"name\")\\n\\t\\t\\tout.update(row)\\n\\treturn out', 'def get_uom_conv_factor(uom, stock_uom):\\n\\tuoms = [uom, stock_uom]\\n\\tvalue = \"\"\\n\\tuom_details = frappe.db.sql(\"\"\"select to_uom, from_uom, value from `tabUOM Conversion Factor`\\\\\\n\\t\\twhere to_uom in ({0})\\n\\t\\t\"\"\".format(\\', \\'.join([\\'\"\\' + frappe.db.escape(i, percent=False) + \\'\"\\' for i in uoms])), as_dict=True)\\n\\n\\tfor d in uom_details:\\n\\t\\tif d.from_uom == stock_uom and d.to_uom == uom:\\n\\t\\t\\tvalue = 1/flt(d.value)\\n\\t\\telif d.from_uom == uom and d.to_uom == stock_uom:\\n\\t\\t\\tvalue = d.value\\n\\n\\tif not value:\\n\\t\\tuom_stock = frappe.db.get_value(\"UOM Conversion Factor\", {\"to_uom\": stock_uom}, [\"from_uom\", \"value\"], as_dict=1)\\n\\t\\tuom_row = frappe.db.get_value(\"UOM Conversion Factor\", {\"to_uom\": uom}, [\"from_uom\", \"value\"], as_dict=1)\\n\\n\\t\\tif uom_stock and uom_row:\\n\\t\\t\\tif uom_stock.from_uom == uom_row.from_uom:\\n\\t\\t\\t\\tvalue = flt(uom_stock.value) * 1/flt(uom_row.value)\\n\\n\\treturn value', 'def get_item_attribute(parent, attribute_value=\\'\\'):\\n\\tif not frappe.has_permission(\"Item\"):\\n\\t\\tfrappe.msgprint(_(\"No Permission\"), raise_exception=1)\\n\\n\\treturn frappe.get_all(\"Item Attribute Value\", fields = [\"attribute_value\"],\\n\\t\\tfilters = {\\'parent\\': parent, \\'attribute_value\\': (\"like\", \"%%%s%%\" % attribute_value)})']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.m = main.MainWindow()', 'def test_mainWindow(self):\\n        assert(self.m)', \"def test_dataframe(self):\\n        import numpy\\n        #Random 25x4 Numpy Matrix\\n        self.m.render_dataframe(numpy.random.rand(25,4) ,name='devel',rownames=xrange(0,25))\\n        assert(self.m.active_robject)\\n        assert(self.m.active_robject.columns)\\n        assert(self.m.active_robject.column_data)\", \"def test_imports(self):\\n        datasets = ['iris','Nile','morley','freeny','sleep','mtcars']\\n        for a in datasets:\\n            main.rsession.r('%s=%s' % (a,a))\\n            self.m.sync_with_r()\\n            assert(a in self.m.robjects)\"]}, {'features': [], 'snippets': ['def _generate(cls, create, attrs):\\n        # These parameters are only used inside _generate() and won\\'t be saved in the database,\\n        # which is why we use attrs.pop() (they are removed from attrs).\\n        light = attrs.pop(\"light\", True)\\n        author_list = attrs.pop(\"author_list\", None)\\n        add_license = attrs.pop(\"add_license\", True)\\n        add_category = attrs.pop(\"add_category\", True)\\n\\n        # This parameter will be saved in the database,\\n        # which is why we use attrs.get() (it stays in attrs).\\n        licence = attrs.get(\"licence\", None)\\n\\n        auths = author_list or []\\n        if add_license:\\n            given_licence = licence or Licence.objects.first()\\n            if isinstance(given_licence, str) and given_licence:\\n                given_licence = Licence.objects.filter(title=given_licence).first() or Licence.objects.first()\\n            licence = given_licence or LicenceFactory()\\n\\n        text = text_content\\n        if not light:\\n            text = tricky_text_content\\n\\n        publishable_content = super()._generate(create, attrs)\\n        publishable_content.gallery = GalleryFactory()\\n        publishable_content.licence = licence\\n        for auth in auths:\\n            publishable_content.authors.add(auth)\\n\\n        if add_category:\\n            publishable_content.subcategory.add(SubCategoryFactory())\\n\\n        publishable_content.save()\\n\\n        for author in publishable_content.authors.all():\\n            UserGalleryFactory(user=author, gallery=publishable_content.gallery, mode=\"W\")\\n\\n        init_new_repo(publishable_content, text, text)\\n\\n        return publishable_content', 'def _generate(cls, create, attrs):\\n        # These parameters are only used inside _generate() and won\\'t be saved in the database,\\n        # which is why we use attrs.pop() (they are removed from attrs).\\n        db_object = attrs.pop(\"db_object\", None)\\n        light = attrs.pop(\"light\", True)\\n\\n        # This parameter will be saved in the database,\\n        # which is why we use attrs.get() (it stays in attrs).\\n        parent = attrs.get(\"parent\", None)\\n\\n        # Needed because we use container.title later\\n        container = super()._generate(create, attrs)\\n\\n        text = text_content\\n        if not light:\\n            text = tricky_text_content\\n\\n        sha = parent.repo_add_container(container.title, text, text)\\n        container = parent.children[-1]\\n\\n        if db_object:\\n            db_object.sha_draft = sha\\n            db_object.save()\\n\\n        return container', 'def _generate(cls, create, attrs):\\n        # These parameters are only used inside _generate() and won\\'t be saved in the database,\\n        # which is why we use attrs.pop() (they are removed from attrs).\\n        light = attrs.pop(\"light\", True)\\n        db_object = attrs.pop(\"db_object\", None)\\n\\n        # This parameter will be saved in the database,\\n        # which is why we use attrs.get() (it stays in attrs).\\n        container = attrs.get(\"container\", None)\\n\\n        # Needed because we use extract.title later\\n        extract = super()._generate(create, attrs)\\n\\n        parent = container\\n        text = text_content\\n        if not light:\\n            text = tricky_text_content\\n\\n        sha = parent.repo_add_extract(extract.title, text)\\n        extract = parent.children[-1]\\n\\n        if db_object:\\n            db_object.sha_draft = sha\\n            db_object.save()\\n\\n        return extract', 'def _generate(cls, create, attrs):\\n        note = super()._generate(create, attrs)\\n        note.pubdate = datetime.now()\\n        note.save()\\n        note.related_content.last_note = note\\n        note.related_content.save()\\n        return note', 'def _generate(cls, create, attrs):\\n        # This parameter is only used inside _generate() and won\\'t be saved in the database,\\n        # which is why we use attrs.pop() (it is removed from attrs).\\n        beta_forum = attrs.pop(\"forum\", None)\\n\\n        # Creates the PublishableContent (see PublishableContentFactory._generate() for more info)\\n        publishable_content = super()._generate(create, attrs)\\n\\n        if publishable_content.authors.count() > 0 and beta_forum is not None:\\n            beta_topic = TopicFactory(\\n                title=\"[beta]\" + publishable_content.title, author=publishable_content.authors.first(), forum=beta_forum\\n            )\\n            publishable_content.sha_beta = publishable_content.sha_draft\\n            publishable_content.beta_topic = beta_topic\\n            publishable_content.save()\\n            PostFactory(topic=beta_topic, position=1, author=publishable_content.authors.first())\\n            beta_topic.save()\\n        return publishable_content', 'def _generate(cls, create, attrs):\\n        # This parameter is only used inside _generate() and won\\'t be saved in the database,\\n        # which is why we use attrs.pop() (it is removed from attrs).\\n        is_major_update = attrs.pop(\"is_major_update\", True)\\n\\n        # Creates the PublishableContent (see PublishableContentFactory._generate() for more info)\\n        content = super()._generate(create, attrs)\\n\\n        published = publish_content(content, content.load_version(), is_major_update)\\n        content.sha_public = content.sha_draft\\n        content.public_version = published\\n\\n        content.save()\\n\\n        return content']}, {'features': [], 'snippets': ['def __init__(self, group, data):\\n        self.group = group\\n        self.data = data\\n        self.initWindow()', 'def dimensionsFrame(self, label):       \\n        frame = gtk.Frame(label)\\n        table = gtk.Table(rows=4, columns=2)\\n        ignore = gtk.RadioButton(group=None, label=\"do not change\")\\n        ignore.show()\\n        smallest = gtk.RadioButton(group=ignore, label=\"shrink to smallest\")\\n        smallest.show()\\n        largest = gtk.RadioButton(group=ignore, label=\"enlarge to largest\")\\n        largest.show()\\n        specify = gtk.RadioButton(group=ignore, label=\"resize to:\")        \\n        specify.show()\\n        value = gtk.Entry()\\n        value.show()\\n        specify.connect(\"toggled\", self.enableValueEntry, value)\\n        self.enableValueEntry(specify, value)\\n        table.attach (ignore, 0, 1, 0, 1)\\n        table.attach (smallest, 0, 1, 1, 2)\\n        table.attach (largest, 0, 1, 2, 3)\\n        table.attach (specify, 0, 1, 3, 4)\\n        table.attach (value, 1, 2, 3, 4)\\n        frame.add(table)\\n        table.show()\\n        frame.show()\\n\\n        options = {\\n            \\'ignore\\': ignore,\\n            \\'smallest\\': smallest,\\n            \\'largest\\': largest,\\n            \\'specify\\': specify,\\n            \\'value\\': value\\n        }        \\n        return frame, options', \"def contentsFrameWidth(self):\\n        frame, self.widthOptions = self.dimensionsFrame('Width')\\n        return frame\", 'def dialogContents(self):        \\n        contents = gtk.VBox(spacing=5)\\n        contents.pack_start(self.contentsFrameWidth(), fill=True, expand=True)\\n        contents.pack_start(self.contentsFrameHeight(), fill=True, expand=True)\\n        contents.show()\\n        return contents', \"def getValue(self, opt, value, elProperty):\\n        if opt == 'specify':\\n            return self.toFloat(value)\\n        else:\\n            values = [ x.properties[elProperty].value for x in self.group if x.properties.has_key(elProperty) ]\\n            if opt == 'smallest':\\n                return min(values)\\n            else:\\n                return max(values)\", 'def adjustHeight(self, value):\\n        for obj in self.group:\\n            pos = obj.properties[\\'obj_pos\\'].value\\n            if obj.properties.has_key(\"elem_height\"):\\n                difference = value - obj.properties[\\'elem_height\\'].value\\n                handleTop = obj.handles[1]\\n                handleBottom = obj.handles[6]\\n                amount = difference/2\\n                obj.move_handle(handleTop, (handleTop.pos.x, handleTop.pos.y - amount), 0, 0)\\n                obj.move_handle(handleBottom, (handleBottom.pos.x, handleBottom.pos.y + amount), 0, 0)\\n                obj.move(pos.x, pos.y)', 'def toFloat(self, valor):\\n        return locale.atof(valor)', \"def clickAplicar(self, *args):\\n        optWidth = self.getSelectedGroupOption(self.widthOptions)\\n        optHeight = self.getSelectedGroupOption(self.heightOptions)\\n\\n        try:\\n            if optWidth[0] != 'ignore':\\n                width = self.getValue(optWidth[0], optWidth[1], 'elem_width')\\n                self.adjustWidth(width)\\n            if optHeight[0] != 'ignore':\\n                height = self.getValue(optHeight[0], optHeight[1], 'elem_height')\\n                self.adjustHeight(height)\\n\\n            if dia.active_display():\\n                diagram = dia.active_display().diagram\\n                for obj in self.group:\\n                    diagram.update_connections(obj)\", 'def show(self):\\n        self.dlg.show()', 'def hide(self, *args):\\n        self.dlg.hide()', 'def dia_group_resize_db (data,flags):\\n    diagram = dia.active_display().diagram\\n    group = diagram.get_sorted_selected()\\n    if len(group) > 0:\\n        win = ResizeWindow(group, data)\\n        win.show()\\n    else:\\n        dia.message(gtk.MESSAGE_INFO, \"Please select a group of objects\")']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        super().__init__()']}, {'features': [], 'snippets': ['def bloom(radius):\\n    turtle.colormode(255)\\n\\n    for rad in range(40, 10, -5):\\n        for looper in range(360//rad):\\n            turtle.up()\\n            turtle.circle(radius+rad, rad)\\n            turtle.begin_fill()\\n            turtle.fillcolor((200+random.randint(0, rad),\\n                              200+random.randint(0, rad),\\n                              200+random.randint(0, rad)))\\n            turtle.down()\\n            turtle.circle(-rad)\\n            turtle.end_fill()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def bind_sockets(port, address=None, family=socket.AF_UNSPEC, backlog=128, flags=None):\\n    \"\"\"Creates listening sockets bound to the given port and address.\\n\\n    Returns a list of socket objects (multiple sockets are returned if\\n    the given address maps to multiple IP addresses, which is most common\\n    for mixed IPv4 and IPv6 use).\\n\\n    Address may be either an IP address or hostname.  If it\\'s a hostname,\\n    the server will listen on all IP addresses associated with the\\n    name.  Address may be an empty string or None to listen on all\\n    available interfaces.  Family may be set to either `socket.AF_INET`\\n    or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\\n    both will be used if available.\\n\\n    The ``backlog`` argument has the same meaning as for\\n    `socket.listen() <socket.socket.listen>`.\\n\\n    ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like\\n    ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``.\\n    \"\"\"\\n    sockets = []\\n    if address == \"\":\\n        address = None\\n    if not socket.has_ipv6 and family == socket.AF_UNSPEC:\\n        # Python can be compiled with --disable-ipv6, which causes\\n        # operations on AF_INET6 sockets to fail, but does not\\n        # automatically exclude those results from getaddrinfo\\n        # results.\\n        # http://bugs.python.org/issue16208\\n        family = socket.AF_INET\\n    if flags is None:\\n        flags = socket.AI_PASSIVE\\n    for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM,\\n                                      0, flags)):\\n        af, socktype, proto, canonname, sockaddr = res\\n        sock = socket.socket(af, socktype, proto)\\n        set_close_exec(sock.fileno())\\n        if os.name != \\'nt\\':\\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\\n        if af == socket.AF_INET6:\\n            # On linux, ipv6 sockets accept ipv4 too by default,\\n            # but this makes it impossible to bind to both\\n            # 0.0.0.0 in ipv4 and :: in ipv6.  On other systems,\\n            # separate sockets *must* be used to listen for both ipv4\\n            # and ipv6.  For consistency, always disable ipv4 on our\\n            # ipv6 sockets and use a separate ipv4 socket when needed.\\n            #\\n            # Python 2.x on windows doesn\\'t have IPPROTO_IPV6.\\n            if hasattr(socket, \"IPPROTO_IPV6\"):\\n                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\\n        sock.setblocking(0)\\n        sock.bind(sockaddr)\\n        sock.listen(backlog)\\n        sockets.append(sock)\\n    return sockets', 'def bind_unix_socket(file, mode=0o600, backlog=128):\\n        \"\"\"Creates a listening unix socket.\\n\\n        If a socket with the given name already exists, it will be deleted.\\n        If any other file with that name exists, an exception will be\\n        raised.\\n\\n        Returns a socket object (not a list of socket objects like\\n        `bind_sockets`)\\n        \"\"\"\\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\\n        set_close_exec(sock.fileno())\\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\\n        sock.setblocking(0)\\n        try:\\n            st = os.stat(file)\\n        except OSError as err:\\n            if err.errno != errno.ENOENT:\\n                raise\\n        else:\\n            if stat.S_ISSOCK(st.st_mode):\\n                os.remove(file)\\n            else:\\n                raise ValueError(\"File %s exists and is not a socket\", file)\\n        sock.bind(file)\\n        os.chmod(file, mode)\\n        sock.listen(backlog)\\n        return sock', 'def accept_handler(fd, events):\\n        while True:\\n            try:\\n                connection, address = sock.accept()\\n            except socket.error as e:\\n                if e.args[0] in (errno.EWOULDBLOCK, errno.EAGAIN):\\n                    return\\n                raise\\n            callback(connection, address)', 'def is_valid_ip(ip):\\n    \"\"\"Returns true if the given string is a well-formed IP address.\\n\\n    Supports IPv4 and IPv6.\\n    \"\"\"\\n    try:\\n        res = socket.getaddrinfo(ip, 0, socket.AF_UNSPEC,\\n                                 socket.SOCK_STREAM,\\n                                 0, socket.AI_NUMERICHOST)\\n        return bool(res)\\n    except socket.gaierror as e:\\n        if e.args[0] == socket.EAI_NONAME:\\n            return False\\n        raise\\n    return True', 'def configurable_base(cls):\\n        return Resolver', 'def configurable_default(cls):\\n        return BlockingResolver', 'def initialize(self, io_loop=None, executor=None):\\n        self.io_loop = io_loop or IOLoop.current()\\n        self.executor = executor or dummy_executor', 'def resolve(self, host, port, family=socket.AF_UNSPEC):\\n        addrinfo = socket.getaddrinfo(host, port, family)\\n        results = []\\n        for family, socktype, proto, canonname, address in addrinfo:\\n            results.append((family, address))\\n        return results', 'def initialize(self, io_loop=None):\\n        super(BlockingResolver, self).initialize(io_loop=io_loop)', 'def initialize(self, io_loop=None, num_threads=10):\\n        from concurrent.futures import ThreadPoolExecutor\\n        super(ThreadedResolver, self).initialize(\\n            io_loop=io_loop, executor=ThreadPoolExecutor(num_threads))', 'def initialize(self, resolver, mapping):\\n        self.resolver = resolver\\n        self.mapping = mapping', 'def ssl_options_to_context(ssl_options):\\n    \"\"\"Try to convert an ``ssl_options`` dictionary to an\\n    `~ssl.SSLContext` object.\\n\\n    The ``ssl_options`` dictionary contains keywords to be passed to\\n    `ssl.wrap_socket`.  In Python 3.2+, `ssl.SSLContext` objects can\\n    be used instead.  This function converts the dict form to its\\n    `~ssl.SSLContext` equivalent, and may be used when a component which\\n    accepts both forms needs to upgrade to the `~ssl.SSLContext` version\\n    to use features like SNI or NPN.\\n    \"\"\"\\n    if isinstance(ssl_options, dict):\\n        assert all(k in _SSL_CONTEXT_KEYWORDS for k in ssl_options), ssl_options\\n    if (not hasattr(ssl, \\'SSLContext\\') or\\n            isinstance(ssl_options, ssl.SSLContext)):\\n        return ssl_options\\n    context = ssl.SSLContext(\\n        ssl_options.get(\\'ssl_version\\', ssl.PROTOCOL_SSLv23))\\n    if \\'certfile\\' in ssl_options:\\n        context.load_cert_chain(ssl_options[\\'certfile\\'], ssl_options.get(\\'keyfile\\', None))\\n    if \\'cert_reqs\\' in ssl_options:\\n        context.verify_mode = ssl_options[\\'cert_reqs\\']\\n    if \\'ca_certs\\' in ssl_options:\\n        context.load_verify_locations(ssl_options[\\'ca_certs\\'])\\n    if \\'ciphers\\' in ssl_options:\\n        context.set_ciphers(ssl_options[\\'ciphers\\'])\\n    return context', \"def _dnsname_to_pat(dn):\\n        pats = []\\n        for frag in dn.split(r'.'):\\n            if frag == '*':\\n                # When '*' is a fragment by itself, it matches a non-empty dotless\\n                # fragment.\\n                pats.append('[^.]+')\\n            else:\\n                # Otherwise, '*' matches any dotless fragment.\\n                frag = re.escape(frag)\\n                pats.append(frag.replace(r'\\\\*', '[^.]*'))\\n        return re.compile(r'\\\\A' + r'\\\\.'.join(pats) + r'\\\\Z', re.IGNORECASE)\"]}, {'features': [], 'snippets': ['def __init__(self):\\n\\n        super().__init__(\"Pretome\")\\n\\n        self.username = None\\n        self.password = None\\n        self.pin = None\\n        self.minseed = 0\\n        self.minleech = 0\\n\\n        self.urls = {\\n            \"base_url\": \"https://pretome.info\",\\n            \"login\": \"https://pretome.info/takelogin.php\",\\n            \"detail\": \"https://pretome.info/details.php?id=%s\",\\n            \"search\": \"https://pretome.info/browse.php?search=%s%s\",\\n            \"download\": \"https://pretome.info/download.php/%s/%s.torrent\",\\n        }\\n\\n        self.url = self.urls[\"base_url\"]\\n\\n        self.categories = \"&st=1&cat%5B%5D=7\"\\n\\n        self.proper_strings = [\"PROPER\", \"REPACK\"]\\n\\n        self.cache = tvcache.TVCache(self)', 'def login(self):\\n        if any(dict_from_cookiejar(self.session.cookies).values()):\\n            return True\\n\\n        login_params = {\"username\": self.username, \"password\": self.password, \"login_pin\": self.pin}\\n\\n        response = self.get_url(self.urls[\"login\"], post_data=login_params, returns=\"text\")\\n        if not response:\\n            logger.warning(\"Unable to connect to provider\")\\n            return False\\n\\n        if re.search(\"Username or password incorrect\", response):\\n            logger.warning(\"Invalid username or password. Check your settings\")\\n            return False\\n\\n        return True']}, {'features': [], 'snippets': ['def parse (sText, sCountry=\"${country_default}\", bDebug=False, dOptions=None):\\n    \"analyses the paragraph sText and returns list of errors\"\\n    aErrors = None\\n    sAlt = sText\\n    dDA = {}\\n    dOpt = _dOptions  if not dOptions  else dOptions\\n\\n    # parse paragraph\\n    try:\\n        sNew, aErrors = _proofread(sText, sAlt, 0, True, dDA, sCountry, dOpt, bDebug)\\n        if sNew:\\n            sText = sNew\\n    except:\\n        raise\\n\\n    # parse sentences\\n    for iStart, iEnd in _getSentenceBoundaries(sText):\\n        if 4 < (iEnd - iStart) < 2000:\\n            dDA.clear()\\n            try:\\n                _, errs = _proofread(sText[iStart:iEnd], sAlt[iStart:iEnd], iStart, False, dDA, sCountry, dOpt, bDebug)\\n                aErrors.extend(errs)\\n            except:\\n                raise\\n    return aErrors', 'def _proofread (s, sx, nOffset, bParagraph, dDA, sCountry, dOptions, bDebug):\\n    aErrs = []\\n    bChange = False', 'def _createWriterError (s, sRepl, nOffset, m, iGroup, sId, bUppercase, sMsg, sURL, bIdRule, sOption):\\n    \"error for Writer (LO/OO)\"\\n    xErr = SingleProofreadingError()\\n    #xErr = uno.createUnoStruct( \"com.sun.star.linguistic2.SingleProofreadingError\" )\\n    xErr.nErrorStart        = nOffset + m.start(iGroup)\\n    xErr.nErrorLength       = m.end(iGroup) - m.start(iGroup)\\n    xErr.nErrorType         = PROOFREADING\\n    xErr.aRuleIdentifier    = sId\\n    # suggestions\\n    if sRepl[0:1] == \"=\":\\n        sugg = _GLOBALS[sRepl[1:]](s, m)\\n        if sugg:\\n            if bUppercase and m.group(iGroup)[0:1].isupper():\\n                xErr.aSuggestions = tuple(map(str.capitalize, sugg.split(\"|\")))\\n            else:\\n                xErr.aSuggestions = tuple(sugg.split(\"|\"))\\n        else:\\n            xErr.aSuggestions = ()\\n    elif sRepl == \"_\":\\n        xErr.aSuggestions = ()\\n    else:\\n        if bUppercase and m.group(iGroup)[0:1].isupper():\\n            xErr.aSuggestions = tuple(map(str.capitalize, m.expand(sRepl).split(\"|\")))\\n        else:\\n            xErr.aSuggestions = tuple(m.expand(sRepl).split(\"|\"))\\n    # Message\\n    if sMsg[0:1] == \"=\":\\n        sMessage = _GLOBALS[sMsg[1:]](s, m)\\n    else:\\n        sMessage = m.expand(sMsg)\\n    xErr.aShortComment      = sMessage   # sMessage.split(\"|\")[0]     # in context menu\\n    xErr.aFullComment       = sMessage   # sMessage.split(\"|\")[-1]    # in dialog\\n    if bIdRule:\\n        xErr.aShortComment += \"  # \" + sId\\n    # URL\\n    if sURL:\\n        p = PropertyValue()\\n        p.Name = \"FullCommentURL\"\\n        p.Value = sURL\\n        xErr.aProperties    = (p,)\\n    else:\\n        xErr.aProperties    = ()\\n    return xErr', 'def _rewrite (s, sRepl, iGroup, m, bUppercase):\\n    \"text processor: write sRepl in s at iGroup position\"\\n    ln = m.end(iGroup) - m.start(iGroup)\\n    if sRepl == \"*\":\\n        sNew = \" \" * ln\\n    elif sRepl == \">\" or sRepl == \"_\" or sRepl == u\"~\":\\n        sNew = sRepl + \" \" * (ln-1)\\n    elif sRepl == \"@\":\\n        sNew = \"@\" * ln\\n    elif sRepl[0:1] == \"=\":\\n        if sRepl[1:2] != \"@\":\\n            sNew = _GLOBALS[sRepl[1:]](s, m)\\n            sNew = sNew + \" \" * (ln-len(sNew))\\n        else:\\n            sNew = _GLOBALS[sRepl[2:]](s, m)\\n            sNew = sNew + \"@\" * (ln-len(sNew))\\n        if bUppercase and m.group(iGroup)[0:1].isupper():\\n            sNew = sNew.capitalize()\\n    else:\\n        sNew = m.expand(sRepl)\\n        sNew = sNew + \" \" * (ln-len(sNew))\\n    return s[0:m.start(iGroup)] + sNew + s[m.end(iGroup):]', 'def resetIgnoreRules ():\\n    _aIgnoredRules.clear()', 'def load ():\\n    global _oDict\\n    try:\\n        _oDict = IBDAWG(\"${binary_dic}\")\\n    except:\\n        traceback.print_exc()', 'def getOptions ():\\n    return _dOptions', 'def resetOptions ():\\n    global _dOptions\\n    _dOptions = dict(gc_options.dOpt)', 'def _getRules (bParagraph):\\n    try:\\n        if not bParagraph:\\n            return _rules.lSentenceRules\\n        return _rules.lParagraphRules\\n    except:\\n        _loadRules()\\n    if not bParagraph:\\n        return _rules.lSentenceRules\\n    return _rules.lParagraphRules', 'def _loadRules ():\\n    from itertools import chain\\n    from . import gc_rules\\n    global _rules\\n    _rules = gc_rules\\n    # compile rules regex\\n    for rulegroup in chain(_rules.lParagraphRules, _rules.lSentenceRules):\\n        for rule in rulegroup[1]:\\n            try:\\n                rule[0] = re.compile(rule[0])\\n            except:\\n                echo(\"Bad regular expression in # \" + str(rule[2]))\\n                rule[0] = \"(?i)<Grammalecte>\"', 'def option (sOpt):\\n    \"return True if option sOpt is active\"\\n    return _dOptions.get(sOpt, False)', 'def _storeMorphFromFSA (sWord):\\n    \"retrieves morphologies list from _oDict -> _dAnalyses\"\\n    global _dAnalyses\\n    _dAnalyses[sWord] = _oDict.getMorph(sWord)\\n    return True  if _dAnalyses[sWord]  else False', 'def morphex (dDA, tWord, sPattern, sNegPattern, bNoWord=False):\\n    \"analyse a tuple (position, word), returns True if not sNegPattern in word morphologies and sPattern in word morphologies (disambiguation on)\"\\n    if not tWord:\\n        return bNoWord\\n    if tWord[1] not in _dAnalyses and not _storeMorphFromFSA(tWord[1]):\\n        return False\\n    lMorph = dDA[tWord[0]]  if tWord[0] in dDA  else _dAnalyses[tWord[1]]\\n    # check negative condition\\n    np = re.compile(sNegPattern)\\n    if any(np.search(s)  for s in lMorph):\\n        return False\\n    # search sPattern\\n    p = re.compile(sPattern)\\n    return any(p.search(s)  for s in lMorph)', 'def analysex (sWord, sPattern, sNegPattern):\\n    \"analyse a word, returns True if not sNegPattern in word morphologies and sPattern in word morphologies (disambiguation off)\"\\n    if sWord not in _dAnalyses and not _storeMorphFromFSA(sWord):\\n        return False\\n    # check negative condition\\n    np = re.compile(sNegPattern)\\n    if any(np.search(s)  for s in _dAnalyses[sWord]):\\n        return False\\n    # search sPattern\\n    p = re.compile(sPattern)\\n    return any(p.search(s)  for s in _dAnalyses[sWord])', 'def nextword (s, iStart, n):\\n    \"get the nth word of the input string or empty string\"\\n    m = re.match(u\"( +[\\\\\\\\w%-]+){\" + str(n-1) + u\"} +([\\\\\\\\w%-]+)\", s[iStart:])\\n    if not m:\\n        return None\\n    return (iStart+m.start(2), m.group(2))', 'def nextword1 (s, iStart):\\n    \"get next word (optimization)\"\\n    m = _zNextWord.match(s[iStart:])\\n    if not m:\\n        return None\\n    return (iStart+m.start(1), m.group(1))', 'def look (s, sPattern, sNegPattern=None):\\n    \"seek sPattern in s (before/after/fulltext), if sNegPattern not in s\"\\n    if sNegPattern and re.search(sNegPattern, s):\\n        return False\\n    if re.search(sPattern, s):\\n        return True\\n    return False', 'def select (dDA, nPos, sWord, sPattern, lDefault=None):\\n    if not sWord:\\n        return True\\n    if nPos in dDA:\\n        return True\\n    if sWord not in _dAnalyses and not _storeMorphFromFSA(sWord):\\n        return True\\n    if len(_dAnalyses[sWord]) == 1:\\n        return True\\n    lSelect = [ sMorph  for sMorph in _dAnalyses[sWord]  if re.search(sPattern, sMorph) ]\\n    if lSelect:\\n        if len(lSelect) != len(_dAnalyses[sWord]):\\n            dDA[nPos] = lSelect\\n            #echo(\"= \"+sWord+\" \"+str(dDA.get(nPos, \"null\")))\\n    elif lDefault:\\n        dDA[nPos] = lDefault\\n        #echo(\"= \"+sWord+\" \"+str(dDA.get(nPos, \"null\")))\\n    return True', 'def define (dDA, nPos, lMorph):\\n    dDA[nPos] = lMorph\\n    #echo(\"= \"+str(nPos)+\" \"+str(dDA[nPos]))\\n    return True']}, {'features': [], 'snippets': ['def last_two_digits(year):\\n    return year - ((year // 100) * 100)', \"def swap_format_elements(format, first, second):\\n    # format is a DateFormat\\n    swapped = format.copy()\\n    elems = swapped.elements\\n    TYPE2CHAR = {DAY: 'd', MONTH: 'M', YEAR: 'y'}\\n    first_char = TYPE2CHAR[first]\\n    second_char = TYPE2CHAR[second]\\n    first_index = [i for i, x in enumerate(elems) if x.startswith(first_char)][0]\\n    second_index = [i for i, x in enumerate(elems) if x.startswith(second_char)][0]\\n    elems[first_index], elems[second_index] = elems[second_index], elems[first_index]\\n    return swapped\", 'def __init__(self, iwin, account, target_account, parsing_date_format):\\n        self.iwin = iwin\\n        self.account = account\\n        self._selected_target = target_account\\n        self.name = account.name\\n        entries = iwin.loader.accounts.entries_for_account(account)\\n        self.count = len(entries)\\n        self.matches = [] # [[ref, imported]]\\n        self.parsing_date_format = parsing_date_format\\n        self.max_day = 31\\n        self.max_month = 12\\n        self.max_year = 99 # 2 digits\\n        self._match_entries()\\n        self._swap_possibilities = set()\\n        self._compute_swap_possibilities()', 'def _match_entries(self):\\n        to_import = list(self.iwin.loader.accounts.entries_for_account(self.account))\\n        reference2entry = {}\\n        for entry in (e for e in to_import if e.reference):\\n            reference2entry[entry.reference] = entry\\n        self.matches = []\\n        if self.selected_target is not None:\\n            entries = self.iwin.document.accounts.entries_for_account(self.selected_target)\\n            for entry in entries:\\n                if entry.reference in reference2entry:\\n                    other = reference2entry[entry.reference]\\n                    if entry.reconciled:\\n                        self.iwin.import_table.dont_import.add(other)\\n                    to_import.remove(other)\\n                    del reference2entry[entry.reference]\\n                else:\\n                    other = None\\n                if other is not None or not entry.reconciled:\\n                    self.matches.append([entry, other])\\n        self.matches += [[None, entry] for entry in to_import]\\n        self._sort_matches()', 'def bind(self, existing, imported):\\n        [match1] = [m for m in self.matches if m[0] is existing]\\n        [match2] = [m for m in self.matches if m[1] is imported]\\n        assert match1[1] is None\\n        assert match2[0] is None\\n        match1[1] = match2[1]\\n        self.matches.remove(match2)', 'def match_entries_by_date_and_amount(self, threshold):\\n        delta = datetime.timedelta(days=threshold)\\n        unmatched = (\\n            to_import for ref, to_import in self.matches if ref is None)\\n        unmatched_refs = (\\n            ref for ref, to_import in self.matches if to_import is None)\\n        amount2refs = defaultdict(list)\\n        for entry in unmatched_refs:\\n            amount2refs[entry.amount].append(entry)\\n        for entry in unmatched:\\n            if entry.amount not in amount2refs:\\n                continue\\n            potentials = amount2refs[entry.amount]\\n            for ref in potentials:\\n                if abs(ref.date - entry.date) <= delta:\\n                    self.bind(ref, entry)\\n                    potentials.remove(ref)\\n        self._sort_matches()', 'def selected_target(self):\\n        return self._selected_target', 'def selected_target(self, value):\\n        self._selected_target = value\\n        self._match_entries()', 'def __init__(self, mainwindow, target_account=None):\\n        super().__init__()\\n        if not hasattr(mainwindow, \\'loader\\'):\\n            raise ValueError(\"Nothing to import!\")\\n        self.mainwindow = mainwindow\\n        self.document = mainwindow.document\\n        self.app = self.document.app\\n        self._selected_pane_index = 0\\n        self._selected_target_index = 0\\n\\n        def setfunc(index):\\n            self.view.set_swap_button_enabled(self.can_perform_swap())\\n        self.swap_type_list = LinkedSelectableList(items=[\\n            \"<placeholder> Day <--> Month\",\\n            \"<placeholder> Month <--> Year\",\\n            \"<placeholder> Day <--> Year\",\\n            tr(\"Description <--> Payee\"),\\n            tr(\"Invert Amounts\"),\\n        ], setfunc=setfunc)\\n        self.swap_type_list.selected_index = SwapType.DayMonth\\n        self.panes = []\\n        self.import_table = ImportTable(self)\\n\\n        self.loader = self.mainwindow.loader\\n        self.target_accounts = [\\n            a for a in self.document.accounts if a.is_balance_sheet_account()]\\n        self.target_accounts.sort(key=lambda a: a.name.lower())\\n        accounts = []\\n        for account in self.loader.accounts:\\n            if account.is_balance_sheet_account():\\n                entries = self.loader.accounts.entries_for_account(account)\\n                if len(entries):\\n                    new_name = self.document.accounts.new_name(account.name)\\n                    if new_name != account.name:\\n                        self.loader.accounts.rename_account(account, new_name)\\n                    accounts.append(account)\\n        parsing_date_format = DateFormat.from_sysformat(self.loader.parsing_date_format)\\n        for account in accounts:\\n            target = target_account\\n            if target is None and account.reference:\\n                target = getfirst(\\n                    t for t in self.target_accounts if t.reference == account.reference\\n                )\\n            self.panes.append(\\n                AccountPane(self, account, target, parsing_date_format))', \"def _can_swap_date_fields(self, first, second): # 'day', 'month', 'year'\\n        pane = self.selected_pane\\n        if pane is None:\\n            return False\\n        return pane.can_swap_date_fields(first, second)\", 'def _refresh_target_selection(self):\\n        if not self.panes:\\n            return\\n        target = self.selected_pane.selected_target\\n        self._selected_target_index = 0\\n        if target is not None:\\n            try:\\n                self._selected_target_index = self.target_accounts.index(target) + 1\\n            except ValueError:\\n                pass', \"def _swap_date_fields(self, first, second, apply_to_all): # 'day', 'month', 'year'\\n        assert self._can_swap_date_fields(first, second)\\n        if apply_to_all:\\n            panes = [p for p in self.panes if p.can_swap_date_fields(first, second)]\\n        else:\\n            panes = [self.selected_pane]\\n\\n        def switch_func(txn):\\n            txn.date = swapped_date(txn.date, first, second)\\n\\n        self._swap_fields(panes, switch_func)\\n        # Now, lets' change the date format on these panes\\n        for pane in panes:\\n            basefmt = self.selected_pane.parsing_date_format\\n            swapped = swap_format_elements(basefmt, first, second)\\n            pane.parsing_date_format = swapped\\n            pane._sort_matches()\\n        self.import_table.refresh()\\n        self._refresh_swap_list_items()\", 'def switch_func(txn):\\n            txn.description, txn.payee = txn.payee, txn.description', \"def _swap_fields(self, panes, switch_func):\\n        seen = set()\\n        for pane in panes:\\n            entries = self.loader.accounts.entries_for_account(pane.account)\\n            txns = dedupe(e.transaction for e in entries)\\n            for txn in txns:\\n                if txn.affected_accounts() & seen:\\n                    # We've already swapped this txn in a previous pane.\\n                    continue\\n                switch_func(txn)\\n            seen.add(pane.account)\\n        self.import_table.refresh()\", 'def _view_updated(self):\\n        if self.document.can_restore_from_prefs():\\n            self.restore_view()\\n        # XXX Logically, we should call _update_selected_pane() but doing so\\n        # make tests fail. to investigate.\\n        self._refresh_target_selection()\\n        self.view.update_selected_pane()\\n        self._refresh_swap_list_items()\\n        self.import_table.refresh()', 'def can_perform_swap(self):\\n        index = self.swap_type_list.selected_index\\n        if index == SwapType.DayMonth:\\n            return self._can_swap_date_fields(DAY, MONTH)\\n        elif index == SwapType.MonthYear:\\n            return self._can_swap_date_fields(MONTH, YEAR)\\n        elif index == SwapType.DayYear:\\n            return self._can_swap_date_fields(DAY, YEAR)\\n        else:\\n            return True', 'def import_selected_pane(self):\\n        pane = self.selected_pane\\n        matches = pane.matches\\n        matches = [\\n            (e, ref) for ref, e in matches\\n            if e is not None and e not in self.import_table.dont_import]\\n        if pane.selected_target is not None:\\n            # We import in an existing account, adjust all the transactions accordingly\\n            target_account = pane.selected_target\\n        else:\\n            target_account = None\\n        self.document.import_entries(target_account, pane.account, matches)\\n        self.mainwindow.revalidate()\\n        self.close_pane(self.selected_pane_index)\\n        self.view.close_selected_tab()', 'def perform_swap(self, apply_to_all=False):\\n        index = self.swap_type_list.selected_index\\n        if index == SwapType.DayMonth:\\n            self._swap_date_fields(DAY, MONTH, apply_to_all=apply_to_all)\\n        elif index == SwapType.MonthYear:\\n            self._swap_date_fields(MONTH, YEAR, apply_to_all=apply_to_all)\\n        elif index == SwapType.DayYear:\\n            self._swap_date_fields(DAY, YEAR, apply_to_all=apply_to_all)\\n        elif index == SwapType.DescriptionPayee:\\n            self._swap_description_payee(apply_to_all=apply_to_all)\\n        elif index == SwapType.InvertAmount:\\n            self._invert_amounts(apply_to_all=apply_to_all)', 'def selected_pane(self):\\n        return self.panes[self.selected_pane_index] if self.panes else None', 'def selected_pane_index(self):\\n        return self._selected_pane_index', 'def selected_pane_index(self, value):\\n        if value >= len(self.panes):\\n            return\\n        self._selected_pane_index = value\\n        self._refresh_target_selection()\\n        self._update_selected_pane()', 'def selected_target_account(self):\\n        return self.selected_pane.selected_target', 'def selected_target_account_index(self):\\n        return self._selected_target_index', 'def selected_target_account_index(self, value):\\n        target = self.target_accounts[value - 1] if value > 0 else None\\n        self.selected_pane.selected_target = target\\n        self._selected_target_index = value\\n        self.import_table.refresh()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def signal_found(arr):\\n\\tlst = arr[-15:]\\n\\tfirst = 0\\n\\tsecond = 0\\n\\tthird = 0\\n\\tfor i in range(0,5):\\n\\t\\tif lst[i]==\"mid\":\\n\\t\\t\\tfirst += 1\\n\\tfor i in range(5,10):\\n\\t\\tif lst[i]==\"mid\":\\n\\t\\t\\tsecond += 1\\n\\tfor i in range(10,15):\\n\\t\\tif lst[i]==\"mid\":\\n\\t\\t\\tthird += 1\\n\\n\\tif first >= 5 and second >= 5 and third >= 5:\\n\\t\\treturn True\\n\\telse:\\n\\t\\treturn False', 'def get_freq(arr):\\n\\tlo_count = 0\\n\\thi_count = 0\\n\\tmid_count = 0\\n\\tfor i in arr:\\n\\t\\tif i==\"lo\":\\n\\t\\t\\tlo_count+=1\\n\\t\\tif i==\"hi\":\\n\\t\\t\\thi_count+=1\\n\\t\\tif i==\"mid\":\\n\\t\\t\\tmid_count+=1\\n\\n\\tif mid_count > hi_count and mid_count > lo_count:\\n\\t\\treturn 2']}, {'features': [], 'snippets': ['def __init__(self, main_model):\\n        self.main_view = None\\n        self.main_model = main_model\\n\\n        self.main_model.begin_job_fetch.connect(self.on_begin_job_fetch)\\n        self.main_model.update_job_fetch_progress.connect(self.on_job_fetch_update)\\n        self.main_model.fetched_job.connect(self.on_fetched_job)', 'def init_hotkeys(self):\\n        self.main_model.hotkey_model.add_hotkey([\"Lcontrol\", \"Lmenu\", \"J\"], self.main_view.focus_job_num_edit)\\n        self.main_model.hotkey_model.add_hotkey([\"Lcontrol\", \"Lmenu\", \"O\"], self.main_view.open_current_job_folder)\\n        self.main_model.hotkey_model.add_hotkey([\"Lcontrol\", \"Lmenu\", \"B\"], self.main_view.open_current_job_basecamp)\\n        self.main_model.hotkey_model.start_detection()', 'def cancel_job_fetch(self):\\n        self.main_model.cancel_job_fetch()', 'def on_job_fetch_update(self, progress):\\n        self.main_view.update_job_fetch_progress_dialog(progress)']}, {'features': [], 'snippets': ['def process(r1, r2, c1, c2):\\n    for i in range(r1, r2):\\n        for j in range(c1, c2):\\n            if 0 <= i < m and 0 <= j < n:\\n                if g[i][j] == None:\\n                    s[i][j] = 0\\n                elif i == 0 or j == 0:\\n                    s[i][j] = 1\\n                elif g[i-1][j] != g[i][j] and g[i][j-1] != g[i][j] and \\\\\\n                        g[i-1][j-1] == g[i][j]:\\n                    s[i][j] = 1 + min(s[i-1][j], s[i][j-1], s[i-1][j-1])\\n                else:\\n                    s[i][j] = 1\\n                heappush(q, (-s[i][j], i, j))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_trivial(self):\\n        # A couple trivial tests\\n\\n        self.assertRaises(ValueError, urllib.request.urlopen, \\'bogus url\\')\\n\\n        # XXX Name hacking to get this to work on Windows.\\n        fname = os.path.abspath(urllib.request.__file__).replace(\\'\\\\\\\\\\', \\'/\\')\\n\\n        # And more hacking to get it to work on MacOS. This assumes\\n        # urllib.pathname2url works, unfortunately...\\n        if os.name == \\'mac\\':\\n            fname = \\'/\\' + fname.replace(\\':\\', \\'/\\')\\n\\n        if os.name == \\'nt\\':\\n            file_url = \"file:///%s\" % fname\\n        else:\\n            file_url = \"file://%s\" % fname\\n\\n        f = urllib.request.urlopen(file_url)\\n\\n        buf = f.read()\\n        f.close()', 'def test_request_headers_dict():\\n    \"\"\"\\n    The Request.headers dictionary is not a documented interface.  It should\\n    stay that way, because the complete set of headers are only accessible\\n    through the .get_header(), .has_header(), .header_items() interface.\\n    However, .headers pre-dates those methods, and so real code will be using\\n    the dictionary.\\n\\n    The introduction in 2.4 of those methods was a mistake for the same reason:\\n    code that previously saw all (urllib2 user)-provided headers in .headers\\n    now sees only a subset (and the function interface is ugly and incomplete).\\n    A better change would have been to replace .headers dict with a dict\\n    subclass (or UserDict.DictMixin instance?)  that preserved the .headers\\n    interface and also provided access to the \"unredirected\" headers.  It\\'s\\n    probably too late to fix that, though.\\n\\n\\n    Check .capitalize() case normalization:\\n\\n    >>> url = \"http://example.com\"\\n    >>> Request(url, headers={\"Spam-eggs\": \"blah\"}).headers[\"Spam-eggs\"]\\n    \\'blah\\'\\n    >>> Request(url, headers={\"spam-EggS\": \"blah\"}).headers[\"Spam-eggs\"]\\n    \\'blah\\'\\n\\n    Currently, Request(url, \"Spam-eggs\").headers[\"Spam-Eggs\"] raises KeyError,\\n    but that could be changed in future.\\n\\n    \"\"\"', 'def test_password_manager(self):\\n    \"\"\"\\n    >>> mgr = urllib.request.HTTPPasswordMgr()\\n    >>> add = mgr.add_password\\n    >>> add(\"Some Realm\", \"http://example.com/\", \"joe\", \"password\")\\n    >>> add(\"Some Realm\", \"http://example.com/ni\", \"ni\", \"ni\")\\n    >>> add(\"c\", \"http://example.com/foo\", \"foo\", \"ni\")\\n    >>> add(\"c\", \"http://example.com/bar\", \"bar\", \"nini\")\\n    >>> add(\"b\", \"http://example.com/\", \"first\", \"blah\")\\n    >>> add(\"b\", \"http://example.com/\", \"second\", \"spam\")\\n    >>> add(\"a\", \"http://example.com\", \"1\", \"a\")\\n    >>> add(\"Some Realm\", \"http://c.example.com:3128\", \"3\", \"c\")\\n    >>> add(\"Some Realm\", \"d.example.com\", \"4\", \"d\")\\n    >>> add(\"Some Realm\", \"e.example.com:3128\", \"5\", \"e\")\\n\\n    >>> mgr.find_user_password(\"Some Realm\", \"example.com\")\\n    (\\'joe\\', \\'password\\')\\n    >>> mgr.find_user_password(\"Some Realm\", \"http://example.com\")\\n    (\\'joe\\', \\'password\\')\\n    >>> mgr.find_user_password(\"Some Realm\", \"http://example.com/\")\\n    (\\'joe\\', \\'password\\')\\n    >>> mgr.find_user_password(\"Some Realm\", \"http://example.com/spam\")\\n    (\\'joe\\', \\'password\\')\\n    >>> mgr.find_user_password(\"Some Realm\", \"http://example.com/spam/spam\")\\n    (\\'joe\\', \\'password\\')\\n    >>> mgr.find_user_password(\"c\", \"http://example.com/foo\")\\n    (\\'foo\\', \\'ni\\')\\n    >>> mgr.find_user_password(\"c\", \"http://example.com/bar\")\\n    (\\'bar\\', \\'nini\\')\\n\\n    Actually, this is really undefined ATM', 'def test_password_manager_default_port(self):\\n    \"\"\"\\n    >>> mgr = urllib.request.HTTPPasswordMgr()\\n    >>> add = mgr.add_password\\n\\n    The point to note here is that we can\\'t guess the default port if there\\'s\\n    no scheme.  This applies to both add_password and find_user_password.\\n\\n    >>> add(\"f\", \"http://g.example.com:80\", \"10\", \"j\")\\n    >>> add(\"g\", \"http://h.example.com\", \"11\", \"k\")\\n    >>> add(\"h\", \"i.example.com:80\", \"12\", \"l\")\\n    >>> add(\"i\", \"j.example.com\", \"13\", \"m\")\\n    >>> mgr.find_user_password(\"f\", \"g.example.com:100\")\\n    (None, None)\\n    >>> mgr.find_user_password(\"f\", \"g.example.com:80\")\\n    (\\'10\\', \\'j\\')\\n    >>> mgr.find_user_password(\"f\", \"g.example.com\")\\n    (None, None)\\n    >>> mgr.find_user_password(\"f\", \"http://g.example.com:100\")\\n    (None, None)\\n    >>> mgr.find_user_password(\"f\", \"http://g.example.com:80\")\\n    (\\'10\\', \\'j\\')\\n    >>> mgr.find_user_password(\"f\", \"http://g.example.com\")\\n    (\\'10\\', \\'j\\')\\n    >>> mgr.find_user_password(\"g\", \"h.example.com\")\\n    (\\'11\\', \\'k\\')\\n    >>> mgr.find_user_password(\"g\", \"h.example.com:80\")\\n    (\\'11\\', \\'k\\')\\n    >>> mgr.find_user_password(\"g\", \"http://h.example.com:80\")\\n    (\\'11\\', \\'k\\')\\n    >>> mgr.find_user_password(\"h\", \"i.example.com\")\\n    (None, None)\\n    >>> mgr.find_user_password(\"h\", \"i.example.com:80\")\\n    (\\'12\\', \\'l\\')\\n    >>> mgr.find_user_password(\"h\", \"http://i.example.com:80\")\\n    (\\'12\\', \\'l\\')\\n    >>> mgr.find_user_password(\"i\", \"j.example.com\")\\n    (\\'13\\', \\'m\\')\\n    >>> mgr.find_user_password(\"i\", \"j.example.com:80\")\\n    (None, None)\\n    >>> mgr.find_user_password(\"i\", \"http://j.example.com\")\\n    (\\'13\\', \\'m\\')\\n    >>> mgr.find_user_password(\"i\", \"http://j.example.com:80\")\\n    (None, None)\\n\\n    \"\"\"', 'def open(self, req, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\\n        self.req, self.data, self.timeout = req, data, timeout', 'def read(self, count=None): pass', 'def close(self): pass', 'def getheaders(self, name):\\n        return list(self.values())', 'def __init__(self, code, msg, headers, data, url=None):\\n        io.StringIO.__init__(self, data)\\n        self.code, self.msg, self.headers, self.url = code, msg, headers, url', 'def geturl(self):\\n        return self.url', 'def add_cookie_header(self, request):\\n        self.ach_req = request', 'def __init__(self, meth_name, action, handle):\\n        self.meth_name = meth_name\\n        self.handle = handle\\n        self.action = action', 'def __init__(self, fp, msg, status, reason):\\n        self.fp = fp\\n        self.msg = msg\\n        self.status = status\\n        self.reason = reason\\n        self.code = 200', 'def info(self):\\n        return {}', 'def __init__(self):\\n        self.level = 0\\n        self.req_headers = []\\n        self.data = None\\n        self.raise_on_endheaders = False\\n        self._tunnel_headers = {}', 'def set_debuglevel(self, level):\\n        self.level = level', 'def request(self, method, url, body=None, headers=None):\\n        self.method = method\\n        self.selector = url\\n        if headers is not None:\\n            self.req_headers += headers.items()\\n        self.req_headers.sort()\\n        if body:\\n            self.data = body\\n        if self.raise_on_endheaders:\\n            import socket\\n            raise socket.error()', 'def __init__(self, methods):\\n        self._define_methods(methods)', 'def handle(self, fn_name, action, *args, **kwds):\\n        self.parent.calls.append((self, fn_name, args, kwds))\\n        if action is None:\\n            return None\\n        elif action == \"return self\":\\n            return self\\n        elif action == \"return response\":\\n            res = MockResponse(200, \"OK\", {}, \"\")\\n            return res\\n        elif action == \"return request\":\\n            return Request(\"http://blah/\")\\n        elif action.startswith(\"error\"):\\n            code = action[action.rfind(\" \")+1:]\\n            try:\\n                code = int(code)\\n            except ValueError:\\n                pass\\n            res = MockResponse(200, \"OK\", {}, \"\")\\n            return self.parent.error(\"http\", args[0], res, code, \"\", {})\\n        elif action == \"raise\":\\n            raise urllib.error.URLError(\"blah\")\\n        assert False', 'def add_parent(self, parent):\\n        self.parent = parent\\n        self.parent.calls = []', 'def add_ordered_mock_handlers(opener, meth_spec):\\n    \"\"\"Create MockHandlers and add them to an OpenerDirector.\\n\\n    meth_spec: list of lists of tuples and strings defining methods to define\\n    on handlers.  eg:\\n\\n    [[\"http_error\", \"ftp_open\"], [\"http_open\"]]\\n\\n    defines methods .http_error() and .ftp_open() on one handler, and\\n    .http_open() on another.  These methods just record their arguments and\\n    return None.  Using a tuple instead of a string causes the method to\\n    perform some action (see MockHandler.handle()), eg:\\n\\n    [[\"http_error\"], [(\"http_open\", \"return request\")]]\\n\\n    defines .http_error() on one handler (which simply returns None), and\\n    .http_open() on another handler, which returns a Request object.\\n\\n    \"\"\"\\n    handlers = []\\n    count = 0\\n    for meths in meth_spec:\\n        class MockHandlerSubclass(MockHandler): pass\\n        h = MockHandlerSubclass(meths)\\n        h.handler_order += count\\n        h.add_parent(opener)\\n        count = count + 1\\n        handlers.append(h)\\n        opener.add_handler(h)\\n    return handlers', 'def __init__(self, code, headers):\\n        self.code = code\\n        self.headers = headers\\n        self.reset()', 'def http_open(self, req):\\n        import email, http.client, copy\\n        from io import StringIO\\n        self.requests.append(copy.deepcopy(req))\\n        if self._count == 0:\\n            self._count = self._count + 1\\n            name = http.client.responses[self.code]\\n            msg = email.message_from_string(self.headers)\\n            return self.parent.error(\\n                \"http\", req, MockFile(), self.code, name, msg)\\n        else:\\n            self.req = req\\n            msg = email.message_from_string(\"\\\\r\\\\n\\\\r\\\\n\")\\n            return MockResponse(200, \"OK\", msg, \"\", req.get_full_url())', 'def __init__(self):\\n        urllib.request.AbstractHTTPHandler.__init__(self)\\n        self.httpconn = MockHTTPClass()', 'def add_password(self, realm, uri, user, password):\\n        self.realm = realm\\n        self.url = uri\\n        self.user = user\\n        self.password = password', 'def test_add_non_handler(self):\\n        class NonHandler(object):\\n            pass\\n        self.assertRaises(TypeError,\\n                          OpenerDirector().add_handler, NonHandler())', 'def test_handled(self):\\n        # handler returning non-None means no more handlers will be called\\n        o = OpenerDirector()\\n        meth_spec = [\\n            [\"http_open\", \"ftp_open\", \"http_error_302\"],\\n            [\"ftp_open\"],\\n            [(\"http_open\", \"return self\")],\\n            [(\"http_open\", \"return self\")],\\n            ]\\n        handlers = add_ordered_mock_handlers(o, meth_spec)\\n\\n        req = Request(\"http://example.com/\")\\n        r = o.open(req)\\n        # Second .http_open() gets called, third doesn\\'t, since second returned\\n        # non-None.  Handlers without .http_open() never get any methods called\\n        # on them.\\n        # In fact, second mock handler defining .http_open() returns self\\n        # (instead of response), which becomes the OpenerDirector\\'s return\\n        # value.\\n        self.assertEqual(r, handlers[2])\\n        calls = [(handlers[0], \"http_open\"), (handlers[2], \"http_open\")]\\n        for expected, got in zip(calls, o.calls):\\n            handler, name, args, kwds = got\\n            self.assertEqual((handler, name), expected)\\n            self.assertEqual(args, (req,))', 'def test_raise(self):\\n        # raising URLError stops processing of request\\n        o = OpenerDirector()\\n        meth_spec = [\\n            [(\"http_open\", \"raise\")],\\n            [(\"http_open\", \"return self\")],\\n            ]\\n        handlers = add_ordered_mock_handlers(o, meth_spec)\\n\\n        req = Request(\"http://example.com/\")\\n        self.assertRaises(urllib.error.URLError, o.open, req)\\n        self.assertEqual(o.calls, [(handlers[0], \"http_open\", (req,), {})])', 'def test_http_error(self):\\n        # XXX http_error_default\\n        # http errors are a special case\\n        o = OpenerDirector()\\n        meth_spec = [\\n            [(\"http_open\", \"error 302\")],\\n            [(\"http_error_400\", \"raise\"), \"http_open\"],\\n            [(\"http_error_302\", \"return response\"), \"http_error_303\",\\n             \"http_error\"],\\n            [(\"http_error_302\")],\\n            ]\\n        handlers = add_ordered_mock_handlers(o, meth_spec)\\n\\n        class Unknown:\\n            def __eq__(self, other): return True\\n\\n        req = Request(\"http://example.com/\")\\n        r = o.open(req)\\n        assert len(o.calls) == 2\\n        calls = [(handlers[0], \"http_open\", (req,)),\\n                 (handlers[2], \"http_error_302\",\\n                  (req, Unknown(), 302, \"\", {}))]\\n        for expected, got in zip(calls, o.calls):\\n            handler, method_name, args = expected\\n            self.assertEqual((handler, method_name), got[:2])\\n            self.assertEqual(args, got[2])', 'def sanepathname2url(path):\\n    urlpath = urllib.request.pathname2url(path)\\n    if os.name == \"nt\" and urlpath.startswith(\"///\"):\\n        urlpath = urlpath[2:]\\n    # XXX don\\'t ask me about the mac...\\n    return urlpath', 'def test_ftp(self):\\n        class MockFTPWrapper:\\n            def __init__(self, data): self.data = data\\n            def retrfile(self, filename, filetype):\\n                self.filename, self.filetype = filename, filetype\\n                return io.StringIO(self.data), len(self.data)\\n\\n        class NullFTPHandler(urllib.request.FTPHandler):\\n            def __init__(self, data): self.data = data\\n            def connect_ftp(self, user, passwd, host, port, dirs,\\n                            timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\\n                self.user, self.passwd = user, passwd\\n                self.host, self.port = host, port\\n                self.dirs = dirs\\n                self.ftpwrapper = MockFTPWrapper(self.data)\\n                return self.ftpwrapper\\n\\n        import ftplib\\n        data = \"rheum rhaponicum\"\\n        h = NullFTPHandler(data)\\n        o = h.parent = MockOpener()\\n\\n        for url, host, port, user, passwd, type_, dirs, filename, mimetype in [\\n            (\"ftp://localhost/foo/bar/baz.html\",\\n             \"localhost\", ftplib.FTP_PORT, \"\", \"\", \"I\",\\n             [\"foo\", \"bar\"], \"baz.html\", \"text/html\"),\\n            (\"ftp://parrot@localhost/foo/bar/baz.html\",\\n             \"localhost\", ftplib.FTP_PORT, \"parrot\", \"\", \"I\",\\n             [\"foo\", \"bar\"], \"baz.html\", \"text/html\"),\\n            (\"ftp://%25parrot@localhost/foo/bar/baz.html\",\\n             \"localhost\", ftplib.FTP_PORT, \"%parrot\", \"\", \"I\",\\n             [\"foo\", \"bar\"], \"baz.html\", \"text/html\"),\\n            (\"ftp://%2542parrot@localhost/foo/bar/baz.html\",\\n             \"localhost\", ftplib.FTP_PORT, \"%42parrot\", \"\", \"I\",\\n             [\"foo\", \"bar\"], \"baz.html\", \"text/html\"),\\n            (\"ftp://localhost:80/foo/bar/\",\\n             \"localhost\", 80, \"\", \"\", \"D\",\\n             [\"foo\", \"bar\"], \"\", None),\\n            (\"ftp://localhost/baz.gif;type=a\",\\n             \"localhost\", ftplib.FTP_PORT, \"\", \"\", \"A\",\\n             [], \"baz.gif\", None),  # XXX really this should guess image/gif\\n            ]:\\n            req = Request(url)\\n            req.timeout = None\\n            r = h.ftp_open(req)\\n            # ftp authentication not yet implemented by FTPHandler\\n            self.assertEqual(h.user, user)\\n            self.assertEqual(h.passwd, passwd)\\n            self.assertEqual(h.host, socket.gethostbyname(host))\\n            self.assertEqual(h.port, port)\\n            self.assertEqual(h.dirs, dirs)\\n            self.assertEqual(h.ftpwrapper.filename, filename)\\n            self.assertEqual(h.ftpwrapper.filetype, type_)\\n            headers = r.info()\\n            self.assertEqual(headers.get(\"Content-type\"), mimetype)\\n            self.assertEqual(int(headers[\"Content-length\"]), len(data))', 'def test_http(self):\\n\\n        h = urllib.request.AbstractHTTPHandler()\\n        o = h.parent = MockOpener()\\n\\n        url = \"http://example.com/\"\\n        for method, data in [(\"GET\", None), (\"POST\", \"blah\")]:\\n            req = Request(url, data, {\"Foo\": \"bar\"})\\n            req.timeout = None\\n            req.add_unredirected_header(\"Spam\", \"eggs\")\\n            http = MockHTTPClass()\\n            r = h.do_open(http, req)\\n\\n            # result attributes\\n            r.read; r.readline  # wrapped MockFile methods\\n            r.info; r.geturl  # addinfourl methods\\n            r.code, r.msg == 200, \"OK\"  # added from MockHTTPClass.getreply()\\n            hdrs = r.info()\\n            hdrs.get; hdrs.__contains__  # r.info() gives dict from .getreply()\\n            self.assertEqual(r.geturl(), url)\\n\\n            self.assertEqual(http.host, \"example.com\")\\n            self.assertEqual(http.level, 0)\\n            self.assertEqual(http.method, method)\\n            self.assertEqual(http.selector, \"/\")\\n            self.assertEqual(http.req_headers,\\n                             [(\"Connection\", \"close\"),\\n                              (\"Foo\", \"bar\"), (\"Spam\", \"eggs\")])\\n            self.assertEqual(http.data, data)\\n\\n        # check socket.error converted to URLError\\n        http.raise_on_endheaders = True\\n        self.assertRaises(urllib.error.URLError, h.do_open, http, req)\\n\\n        # check adding of standard headers\\n        o.addheaders = [(\"Spam\", \"eggs\")]\\n        for data in \"\", None:  # POST, GET\\n            req = Request(\"http://example.com/\", data)\\n            r = MockResponse(200, \"OK\", {}, \"\")\\n            newreq = h.do_request_(req)\\n            if data is None:  # GET\\n                self.assertTrue(\"Content-length\" not in req.unredirected_hdrs)\\n                self.assertTrue(\"Content-type\" not in req.unredirected_hdrs)\\n            else:  # POST\\n                self.assertEqual(req.unredirected_hdrs[\"Content-length\"], \"0\")\\n                self.assertEqual(req.unredirected_hdrs[\"Content-type\"],\\n                             \"application/x-www-form-urlencoded\")\\n            # XXX the details of Host could be better tested\\n            self.assertEqual(req.unredirected_hdrs[\"Host\"], \"example.com\")\\n            self.assertEqual(req.unredirected_hdrs[\"Spam\"], \"eggs\")\\n\\n            # don\\'t clobber existing headers\\n            req.add_unredirected_header(\"Content-length\", \"foo\")\\n            req.add_unredirected_header(\"Content-type\", \"bar\")\\n            req.add_unredirected_header(\"Host\", \"baz\")\\n            req.add_unredirected_header(\"Spam\", \"foo\")\\n            newreq = h.do_request_(req)\\n            self.assertEqual(req.unredirected_hdrs[\"Content-length\"], \"foo\")\\n            self.assertEqual(req.unredirected_hdrs[\"Content-type\"], \"bar\")\\n            self.assertEqual(req.unredirected_hdrs[\"Host\"], \"baz\")\\n            self.assertEqual(req.unredirected_hdrs[\"Spam\"], \"foo\")', \"def test_fixpath_in_weirdurls(self):\\n        # Issue4493: urllib2 to supply '/' when to urls where path does not\\n        # start with'/'\\n\\n        h = urllib.request.AbstractHTTPHandler()\\n        o = h.parent = MockOpener()\\n\\n        weird_url = 'http://www.python.org?getspam'\\n        req = Request(weird_url)\\n        newreq = h.do_request_(req)\\n        self.assertEqual(newreq.host,'www.python.org')\\n        self.assertEqual(newreq.selector,'/?getspam')\\n\\n        url_without_path = 'http://www.python.org'\\n        req = Request(url_without_path)\\n        newreq = h.do_request_(req)\\n        self.assertEqual(newreq.host,'www.python.org')\\n        self.assertEqual(newreq.selector,'')\", 'def test_cookies(self):\\n        cj = MockCookieJar()\\n        h = urllib.request.HTTPCookieProcessor(cj)\\n        o = h.parent = MockOpener()\\n\\n        req = Request(\"http://example.com/\")\\n        r = MockResponse(200, \"OK\", {}, \"\")\\n        newreq = h.http_request(req)\\n        self.assertIs(cj.ach_req, req)\\n        self.assertIs(cj.ach_req, newreq)\\n        self.assertEqual(req.get_origin_req_host(), \"example.com\")\\n        self.assertFalse(req.is_unverifiable())\\n        newr = h.http_response(req, r)\\n        self.assertIs(cj.ec_req, req)\\n        self.assertIs(cj.ec_r, r)\\n        self.assertIs(r, newr)', 'def redirect(h, req, url=to_url):\\n            h.http_error_302(req, MockFile(), 302, \"Blah\",\\n                             MockHeaders({\"location\": url}))', 'def test_cookie_redirect(self):\\n        # cookies shouldn\\'t leak into redirected requests\\n        from http.cookiejar import CookieJar\\n        from test.test_http_cookiejar import interact_netscape\\n\\n        cj = CookieJar()\\n        interact_netscape(cj, \"http://www.example.com/\", \"spam=eggs\")\\n        hh = MockHTTPHandler(302, \"Location: http://www.cracker.com/\\\\r\\\\n\\\\r\\\\n\")\\n        hdeh = urllib.request.HTTPDefaultErrorHandler()\\n        hrh = urllib.request.HTTPRedirectHandler()\\n        cp = urllib.request.HTTPCookieProcessor(cj)\\n        o = build_test_opener(hh, hdeh, hrh, cp)\\n        o.open(\"http://www.example.com/\")\\n        self.assertFalse(hh.req.has_header(\"Cookie\"))', 'def test_proxy_no_proxy(self):\\n        os.environ[\\'no_proxy\\'] = \\'python.org\\'\\n        o = OpenerDirector()\\n        ph = urllib.request.ProxyHandler(dict(http=\"proxy.example.com\"))\\n        o.add_handler(ph)\\n        req = Request(\"http://www.perl.org/\")\\n        self.assertEqual(req.get_host(), \"www.perl.org\")\\n        r = o.open(req)\\n        self.assertEqual(req.get_host(), \"proxy.example.com\")\\n        req = Request(\"http://www.python.org\")\\n        self.assertEqual(req.get_host(), \"www.python.org\")\\n        r = o.open(req)\\n        self.assertEqual(req.get_host(), \"www.python.org\")\\n        del os.environ[\\'no_proxy\\']', 'def test_proxy_https_proxy_authorization(self):\\n        o = OpenerDirector()\\n        ph = urllib.request.ProxyHandler(dict(https=\\'proxy.example.com:3128\\'))\\n        o.add_handler(ph)\\n        https_handler = MockHTTPSHandler()\\n        o.add_handler(https_handler)\\n        req = Request(\"https://www.example.com/\")\\n        req.add_header(\"Proxy-Authorization\",\"FooBar\")\\n        req.add_header(\"User-Agent\",\"Grail\")\\n        self.assertEqual(req.get_host(), \"www.example.com\")\\n        self.assertIsNone(req._tunnel_host)\\n        r = o.open(req)\\n        # Verify Proxy-Authorization gets tunneled to request.\\n        # httpsconn req_headers do not have the Proxy-Authorization header but\\n        # the req will have.\\n        self.assertFalse((\"Proxy-Authorization\",\"FooBar\") in\\n                         https_handler.httpconn.req_headers)\\n        self.assertTrue((\"User-Agent\",\"Grail\") in\\n                        https_handler.httpconn.req_headers)\\n        self.assertIsNotNone(req._tunnel_host)\\n        self.assertEqual(req.get_host(), \"proxy.example.com:3128\")\\n        self.assertEqual(req.get_header(\"Proxy-authorization\"),\"FooBar\")', 'def test_basic_auth_with_single_quoted_realm(self):\\n        self.test_basic_auth(quote_char=\"\\'\")', 'def test_basic_and_digest_auth_handlers(self):\\n        # HTTPDigestAuthHandler threw an exception if it couldn\\'t handle a 40*\\n        # response (http://python.org/sf/1479302), where it should instead\\n        # return None to allow another handler (especially\\n        # HTTPBasicAuthHandler) to handle the response.\\n\\n        # Also (http://python.org/sf/14797027, RFC 2617 section 1.2), we must\\n        # try digest first (since it\\'s the strongest auth scheme), so we record\\n        # order of calls here to check digest comes first:\\n        class RecordingOpenerDirector(OpenerDirector):\\n            def __init__(self):\\n                OpenerDirector.__init__(self)\\n                self.recorded = []\\n            def record(self, info):\\n                self.recorded.append(info)\\n        class TestDigestAuthHandler(urllib.request.HTTPDigestAuthHandler):\\n            def http_error_401(self, *args, **kwds):\\n                self.parent.record(\"digest\")\\n                urllib.request.HTTPDigestAuthHandler.http_error_401(self,\\n                                                             *args, **kwds)\\n        class TestBasicAuthHandler(urllib.request.HTTPBasicAuthHandler):\\n            def http_error_401(self, *args, **kwds):\\n                self.parent.record(\"basic\")\\n                urllib.request.HTTPBasicAuthHandler.http_error_401(self,\\n                                                            *args, **kwds)\\n\\n        opener = RecordingOpenerDirector()\\n        password_manager = MockPasswordManager()\\n        digest_handler = TestDigestAuthHandler(password_manager)\\n        basic_handler = TestBasicAuthHandler(password_manager)\\n        realm = \"ACME Networks\"\\n        http_handler = MockHTTPHandler(\\n            401, \\'WWW-Authenticate: Basic realm=\"%s\"\\\\r\\\\n\\\\r\\\\n\\' % realm)\\n        opener.add_handler(basic_handler)\\n        opener.add_handler(digest_handler)\\n        opener.add_handler(http_handler)\\n\\n        # check basic auth isn\\'t blocked by digest handler failing\\n        self._test_basic_auth(opener, basic_handler, \"Authorization\",\\n                              realm, http_handler, password_manager,\\n                              \"http://acme.example.com/protected\",\\n                              \"http://acme.example.com/protected\",\\n                              )\\n        # check digest was tried before basic (twice, because\\n        # _test_basic_auth called .open() twice)\\n        self.assertEqual(opener.recorded, [\"digest\", \"basic\"]*2)', 'def test_build_opener(self):\\n        class MyHTTPHandler(urllib.request.HTTPHandler): pass\\n        class FooHandler(urllib.request.BaseHandler):\\n            def foo_open(self): pass\\n        class BarHandler(urllib.request.BaseHandler):\\n            def bar_open(self): pass\\n\\n        build_opener = urllib.request.build_opener\\n\\n        o = build_opener(FooHandler, BarHandler)\\n        self.opener_has_handler(o, FooHandler)\\n        self.opener_has_handler(o, BarHandler)\\n\\n        # can take a mix of classes and instances\\n        o = build_opener(FooHandler, BarHandler())\\n        self.opener_has_handler(o, FooHandler)\\n        self.opener_has_handler(o, BarHandler)\\n\\n        # subclasses of default handlers override default handlers\\n        o = build_opener(MyHTTPHandler)\\n        self.opener_has_handler(o, MyHTTPHandler)\\n\\n        # a particular case of overriding: default handlers can be passed\\n        # in explicitly\\n        o = build_opener()\\n        self.opener_has_handler(o, urllib.request.HTTPHandler)\\n        o = build_opener(urllib.request.HTTPHandler)\\n        self.opener_has_handler(o, urllib.request.HTTPHandler)\\n        o = build_opener(urllib.request.HTTPHandler())\\n        self.opener_has_handler(o, urllib.request.HTTPHandler)\\n\\n        # Issue2670: multiple handlers sharing the same base class\\n        class MyOtherHTTPHandler(urllib.request.HTTPHandler): pass\\n        o = build_opener(MyHTTPHandler, MyOtherHTTPHandler)\\n        self.opener_has_handler(o, MyHTTPHandler)\\n        self.opener_has_handler(o, MyOtherHTTPHandler)', 'def setUp(self):\\n        self.get = Request(\"http://www.python.org/~jeremy/\")\\n        self.post = Request(\"http://www.python.org/~jeremy/\",\\n                            \"data\",\\n                            headers={\"X-Test\": \"test\"})', 'def test_add_data(self):\\n        self.assertFalse(self.get.has_data())\\n        self.assertEqual(\"GET\", self.get.get_method())\\n        self.get.add_data(\"spam\")\\n        self.assertTrue(self.get.has_data())\\n        self.assertEqual(\"POST\", self.get.get_method())', 'def test_selector(self):\\n        self.assertEqual(\"/~jeremy/\", self.get.get_selector())\\n        req = Request(\"http://www.python.org/\")\\n        self.assertEqual(\"/\", req.get_selector())', 'def test_get_host(self):\\n        self.assertEqual(\"www.python.org\", self.get.get_host())', 'def test_proxy(self):\\n        self.assertFalse(self.get.has_proxy())\\n        self.get.set_proxy(\"www.perl.org\", \"http\")\\n        self.assertTrue(self.get.has_proxy())\\n        self.assertEqual(\"www.python.org\", self.get.get_origin_req_host())\\n        self.assertEqual(\"www.perl.org\", self.get.get_host())', 'def test_urlwith_fragment(self):\\n        req = Request(\"http://www.python.org/?qs=query#fragment=true\")\\n        self.assertEqual(\"/?qs=query\", req.get_selector())\\n        req = Request(\"http://www.python.org/#fun=true\")\\n        self.assertEqual(\"/\", req.get_selector())']}, {'features': [], 'snippets': [\"def make(self):\\n        comps = {}\\n        for name, component in self.components.items():\\n            comps[name] = eval(component['class'])(config=eval(component['config']))\\n        return Compound(linkages=self.linkages, **comps)\"]}, {'features': [], 'snippets': ['def __init__(self, language=\"en\"):\\n        \"\"\"\\n        Constructor for the wordnet manager.\\n        It takes a main language.\\n        \"\"\"\\n        self.__language = language', 'def __nameToWordnetCode(self, name):\\n        \"\"\"\\n        It returns the wordnet code for a given language name\\n        \"\"\"\\n        if not self.__isLanguageAvailable(language_name=name):\\n            raise Exception(\"Wordnet code not found for the language name %s \" % name)\\n        name = name.lower()\\n        languageShortCode = AVAILABLE_LANGUAGES_NAMES[name]\\n\\n        wordnetCode = self.__shortCodeToWordnetCode(code=languageShortCode)\\n        return wordnetCode', 'def __getSynsets(self, word, wordNetCode):\\n        \"\"\"\\n        It returns the synsets given both word and language code\\n        \"\"\"\\n        from nltk.corpus import wordnet as wn\\n\\n        synsets = wn.synsets(word, lang=wordNetCode)\\n        return synsets', 'def getSynonyms(self, words=[], language_code=\"en\"):\\n        \"\"\"\\n        Get the synonyms from a list of words.\\n        :words: A list of words\\n        :language_code: the language for the synonyms.\\n        \"\"\"\\n        if words is None or not isinstance(words, list) or list(words) <= 0:\\n            return []\\n\\n        if not self.__isLanguageAvailable(code=language_code):\\n            return []\\n\\n        wnCode = self.__shortCodeToWordnetCode(language_code)\\n        result = {}\\n        for word in words:\\n            result[word] = dict([(\\'lemmas\\', self.getLemmas(word,languageCode=language_code))])\\n        return result']}, {'features': [], 'snippets': ['def __init__( self ):\\n    \"\"\"c\\'tor\\n\\n    :param self: self reference\\n    \"\"\"\\n    self.log = gLogger.getSubLogger( self.__class__.__name__, True )\\n\\n    # # final states tuple\\n    self.finalStates = ( \\'Canceled\\', \\'Failed\\', \\'Hold\\',\\n                         \\'Finished\\', \\'FinishedDirty\\' )\\n    # # failed states tuple\\n    self.failedStates = ( \\'Canceled\\', \\'Failed\\',\\n                          \\'Hold\\', \\'FinishedDirty\\' )\\n    # # successful states tuple\\n    self.successfulStates = ( \\'Finished\\', \\'Done\\' )\\n    # # all file states tuple\\n    self.fileStates = ( \\'Done\\', \\'Active\\', \\'Pending\\', \\'Ready\\', \\'Canceled\\', \\'Failed\\',\\n                        \\'Finishing\\', \\'Finished\\', \\'Submitted\\', \\'Hold\\', \\'Waiting\\' )\\n\\n    self.statusSummary = {}\\n\\n    # # request status\\n    self.requestStatus = \\'Unknown\\'\\n\\n    # # dict for FTS job files\\n    self.fileDict = {}\\n    # # dict for replicas information\\n    self.catalogReplicas = {}\\n    # # dict for metadata information\\n    self.catalogMetadata = {}\\n    # # dict for files that failed to register\\n    self.failedRegistrations = {}\\n\\n    # # placehoder for FileCatalog reference\\n    self.oCatalog = None\\n\\n    # # submit timestamp\\n    self.submitTime = \\'\\'\\n\\n    # # placeholder FTS job GUID\\n    self.ftsGUID = \\'\\'\\n    # # placeholder for FTS server URL\\n    self.ftsServer = \\'\\'\\n\\n    # # flag marking FTS job completness\\n    self.isTerminal = False\\n    # # completness percentage\\n    self.percentageComplete = 0.0\\n\\n    # # source SE name\\n    self.sourceSE = \\'\\'\\n    # # flag marking source SE validity\\n    self.sourceValid = False\\n    # # source space token\\n    self.sourceToken = \\'\\'\\n\\n    # # target SE name\\n    self.targetSE = \\'\\'\\n    # # flag marking target SE validity\\n    self.targetValid = False\\n    # # target space token\\n    self.targetToken = \\'\\'\\n\\n    # # placeholder for target StorageElement\\n    self.oTargetSE = None\\n    # # placeholder for source StorageElement\\n    self.oSourceSE = None\\n\\n    # # checksum type, set it to default\\n    self.__cksmType = self.__defaultCksmType\\n    # # disable checksum test by default\\n    self.__cksmTest = False\\n\\n    # # statuses that prevent submitting to FTS\\n    self.noSubmitStatus = ( \\'Failed\\', \\'Done\\', \\'Staging\\' )\\n\\n    # # were sources resolved?\\n    self.sourceResolved = False\\n\\n    # # Number of file transfers actually submitted\\n    self.submittedFiles = 0\\n    self.transferTime = 0\\n\\n    self.submitCommand = Operations().getValue( \\'DataManagement/FTSPlacement/FTS2/SubmitCommand\\', \\'glite-transfer-submit\\' )\\n    self.monitorCommand = Operations().getValue( \\'DataManagement/FTSPlacement/FTS2/MonitorCommand\\', \\'glite-transfer-status\\' )\\n    self.ftsJob = None\\n    self.ftsFiles = []', 'def setSourceSE( self, se ):\\n    \"\"\" set SE for source\\n\\n    :param self: self reference\\n    :param str se: source SE name\\n    \"\"\"\\n    if se == self.targetSE:\\n      return S_ERROR( \"SourceSE is TargetSE\" )\\n    self.sourceSE = se\\n    self.oSourceSE = StorageElement( self.sourceSE )\\n    return self.__checkSourceSE()', 'def setTargetSE( self, se ):\\n    \"\"\" set target SE\\n\\n    :param self: self reference\\n    :param str se: target SE name\\n    \"\"\"\\n    if se == self.sourceSE:\\n      return S_ERROR( \"TargetSE is SourceSE\" )\\n    self.targetSE = se\\n    self.oTargetSE = StorageElement( self.targetSE )\\n    return self.__checkTargetSE()', 'def __checkTargetSE( self ):\\n    \"\"\" check target SE availability\\n\\n    :param self: self reference\\n    \"\"\"\\n    if not self.targetSE:\\n      return S_ERROR( \"TargetSE not set\" )\\n    res = self.oTargetSE.isValid( \\'Write\\' )\\n    if not res[\\'OK\\']:\\n      return S_ERROR( \"TargetSE not available for writing\" )\\n    res = self.__getSESpaceToken( self.oTargetSE )\\n    if not res[\\'OK\\']:\\n      self.log.error( \"FTSRequest failed to get SRM Space Token for TargetSE\", res[\\'Message\\'] )\\n      return S_ERROR( \"TargetSE does not support FTS transfers\" )\\n\\n    # # check checksum types\\n    if self.__cksmTest:\\n      res = self.oTargetSE.getChecksumType()\\n      if not res[\"OK\"]:\\n        self.log.error( \"Unable to get checksum type for TargetSE\", \\n                        \"%s: %s\" % ( self.targetSE, res[\"Message\"] ) )\\n        cksmType = res[\"Value\"]\\n        if cksmType in ( \"NONE\", \"NULL\" ):\\n          self.log.warn( \"Checksum type set to %s at TargetSE %s, disabling checksum test\" % ( cksmType,\\n                                                                                              self.targetSE ) )\\n          self.__cksmTest = False\\n        elif cksmType != self.__cksmType:\\n          self.log.warn( \"Checksum type mismatch, disabling checksum test\" )\\n          self.__cksmTest = False\\n\\n    self.targetToken = res[\\'Value\\']\\n    self.targetValid = True\\n    return S_OK()', 'def __getSESpaceToken( oSE ):\\n    \"\"\" get space token from StorageElement instance\\n\\n    :param self: self reference\\n    :param StorageElement oSE: StorageElement instance\\n    \"\"\"\\n    res = oSE.getStorageParameters( \"SRM2\" )\\n    if not res[\\'OK\\']:\\n      return res\\n    return S_OK( res[\\'Value\\'].get( \\'SpaceToken\\' ) )', 'def setFTSGUID( self, guid ):\\n    \"\"\" FTS job GUID setter\\n\\n    :param self: self reference\\n    :param str guid: string containg GUID\\n    \"\"\"\\n    if not checkGuid( guid ):\\n      return S_ERROR( \"Incorrect GUID format\" )\\n    self.ftsGUID = guid\\n    return S_OK()', 'def isRequestTerminal( self ):\\n    \"\"\" check if FTS job has terminated\\n\\n    :param self: self reference\\n    \"\"\"\\n    if self.requestStatus in self.finalStates:\\n      self.isTerminal = True\\n    return S_OK( self.isTerminal )', 'def setLFN( self, lfn ):\\n    \"\"\" add LFN :lfn: to :fileDict:\\n\\n    :param self: self reference\\n    :param str lfn: LFN to add to\\n    \"\"\"\\n    self.fileDict.setdefault( lfn, {\\'Status\\':\\'Waiting\\'} )\\n    return S_OK()', 'def getSourceSURL( self, lfn ):\\n    \"\"\" get source SURL for LFN :lfn:\\n\\n    :param self: self reference\\n    :param str lfn: LFN\\n    \"\"\"\\n    return self.__getFileParameter( lfn, \\'Source\\' )', 'def getFailReason( self, lfn ):\\n    \"\"\" get fail reason for file :lfn:\\n\\n    :param self: self reference\\n    :param str lfn: LFN\\n    \"\"\"\\n    return self.__getFileParameter( lfn, \\'Reason\\' )', 'def getTransferTime( self, lfn ):\\n    \"\"\" get duration of transfer for file :lfn:\\n\\n    :param self: self reference\\n    :param str lfn: LFN\\n    \"\"\"\\n    return self.__getFileParameter( lfn, \\'Duration\\' )', 'def getStaging( self ):\\n    \"\"\" get files set for prestaging \"\"\"\\n    return S_OK( [lfn for lfn in self.fileDict\\n                  if self.fileDict[lfn].get( \\'Status\\', \\'\\' ) == \\'Staging\\'] )', 'def __setFileParameter( self, lfn, paramName, paramValue ):\\n    \"\"\" set :paramName: to :paramValue: for :lfn: file\\n\\n    :param self: self reference\\n    :param str lfn: LFN\\n    :param str paramName: parameter name\\n    :param mixed paramValue: a new parameter value\\n    \"\"\"\\n    self.setLFN( lfn )\\n    self.fileDict[lfn][paramName] = paramValue\\n    return S_OK()', 'def submit( self, monitor = False, printOutput = True ):\\n    \"\"\" submit FTS job\\n\\n    :param self: self reference\\n    :param bool monitor: flag to monitor progress of FTS job\\n    :param bool printOutput: flag to print output of execution to stdout\\n    \"\"\"\\n    res = self.__prepareForSubmission()\\n    if not res[\\'OK\\']:\\n      return res\\n    res = self.__submitFTSTransfer()\\n    if not res[\\'OK\\']:\\n      return res\\n    resDict = { \\'ftsGUID\\' : self.ftsGUID, \\'ftsServer\\' : self.ftsServer, \\'submittedFiles\\' : self.submittedFiles }\\n    if monitor or printOutput:\\n      gLogger.always( \"Submitted %s@%s\" % ( self.ftsGUID, self.ftsServer ) )\\n      if monitor:\\n        self.monitor( untilTerminal = True, printOutput = printOutput, full = False )\\n    return S_OK( resDict )', 'def __getCatalogObject( self ):\\n    \"\"\" CatalogInterface instance facade\\n\\n    :param self: self reference\\n    \"\"\"\\n    try:\\n      if not self.oCatalog:\\n        self.oCatalog = FileCatalog()\\n      return S_OK()\\n    except:\\n      return S_ERROR()', 'def __updateMetadataCache( self, lfns = None ):\\n    \"\"\" update metadata cache for list of LFNs\\n\\n    :param self: self reference\\n    :param list lnfs: list of LFNs\\n    \"\"\"\\n    if not lfns:\\n      lfns = self.fileDict.keys()\\n    toUpdate = [ lfn for lfn in lfns if lfn not in self.catalogMetadata ]\\n    if not toUpdate:\\n      return S_OK()\\n    res = self.__getCatalogObject()\\n    if not res[\\'OK\\']:\\n      return res\\n    res = self.oCatalog.getFileMetadata( toUpdate )\\n    if not res[\\'OK\\']:\\n      return S_ERROR( \"Failed to get source catalog metadata: %s\" % res[\\'Message\\'] )\\n    for lfn, error in res[\\'Value\\'][\\'Failed\\'].items():\\n      self.__setFileParameter( lfn, \\'Reason\\', error )\\n      self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n    for lfn, metadata in res[\\'Value\\'][\\'Successful\\'].items():\\n      self.catalogMetadata[lfn] = metadata\\n    return S_OK()', 'def resolveTarget( self ):\\n    \"\"\" find target SE eligible for submission\\n\\n    :param self: self reference\\n    \"\"\"\\n    toResolve = [ lfn for lfn in self.fileDict\\n                 if self.fileDict[lfn].get( \\'Status\\' ) not in self.noSubmitStatus ]\\n    if not toResolve:\\n      return S_OK()\\n    res = self.__updateReplicaCache( toResolve )\\n    if not res[\\'OK\\']:\\n      return res\\n    for lfn in toResolve:\\n      res = returnSingleResult( self.oTargetSE.getURL( lfn, protocol = \\'srm\\' ) )\\n      if not res[\\'OK\\']:\\n        reason = res.get( \\'Message\\', res[\\'Message\\'] )\\n        gLogger.warn( \"resolveTarget: skipping %s - %s\" % ( lfn, reason ) )\\n        self.__setFileParameter( lfn, \\'Reason\\', reason )\\n        self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n        continue\\n\\n      res = self.setTargetSURL( lfn, res[\\'Value\\'] )\\n      if not res[\\'OK\\']:\\n        gLogger.warn( \"resolveTarget: skipping %s - %s\" % ( lfn, res[\"Message\"] ) )\\n        self.__setFileParameter( lfn, \\'Reason\\', res[\\'Message\\'] )\\n        self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n        continue\\n    toResolve = []\\n    for lfn in self.fileDict:\\n      if \"Target\" in self.fileDict[lfn]:\\n        toResolve.append( lfn )\\n    if not toResolve:\\n      return S_ERROR( \"No eligible Target files\" )\\n    res = self.oTargetSE.exists( toResolve )\\n    if not res[\\'OK\\']:\\n      return S_ERROR( \"Failed to check target existence\" )\\n    for lfn, error in res[\\'Value\\'][\\'Failed\\'].items():\\n      self.__setFileParameter( lfn, \\'Reason\\', error )\\n      self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n    toRemove = []\\n    for lfn, exists in res[\\'Value\\'][\\'Successful\\'].items():\\n      if exists:\\n        res = self.getSourceSURL( lfn )\\n        if not res[\\'OK\\']:\\n          gLogger.warn( \"resolveTarget: skipping %s - target exists\" % lfn )\\n          self.__setFileParameter( lfn, \\'Reason\\', \"Target exists\" )\\n          self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n        elif res[\\'Value\\'] == self.fileDict[lfn][\\'Target\\']:\\n          gLogger.warn( \"resolveTarget: skipping %s - source and target pfns are the same\" % lfn )\\n          self.__setFileParameter( lfn, \\'Reason\\', \"Source and Target the same\" )\\n          self.__setFileParameter( lfn, \\'Status\\', \\'Failed\\' )\\n        else:\\n          toRemove.append( lfn )\\n    if toRemove:\\n      self.oTargetSE.removeFile( toRemove )\\n    return S_OK()', 'def __createFTSFiles( self ):\\n    \"\"\" create LFNs file for glite-transfer-submit command\\n\\n    This file consists one line for each fiel to be transferred:\\n\\n    sourceSURL targetSURL [CHECKSUMTYPE:CHECKSUM]\\n\\n    :param self: self reference\\n    \"\"\"\\n    self.__updateMetadataCache()\\n    for lfn in self.fileDict:\\n      lfnStatus = self.fileDict[lfn].get( \\'Status\\' )\\n      if lfnStatus not in self.noSubmitStatus:\\n        cksmStr = \"\"\\n        # # add chsmType:cksm only if cksmType is specified, else let FTS decide by itself\\n        if self.__cksmTest and self.__cksmType:\\n          checkSum = self.catalogMetadata.get( lfn, {} ).get( \\'Checksum\\' )\\n          if checkSum:\\n            cksmStr = \" %s:%s\" % ( self.__cksmType, intAdlerToHex( hexAdlerToInt( checkSum ) ) )\\n        ftsFile = FTSFile()\\n        ftsFile.LFN = lfn\\n        ftsFile.SourceSURL = self.fileDict[lfn].get( \\'Source\\' )\\n        ftsFile.TargetSURL = self.fileDict[lfn].get( \\'Target\\' )\\n        ftsFile.SourceSE = self.sourceSE\\n        ftsFile.TargetSE = self.targetSE\\n        ftsFile.Status = self.fileDict[lfn].get( \\'Status\\' )\\n        ftsFile.Checksum = cksmStr\\n        ftsFile.Size = self.catalogMetadata.get( lfn, {} ).get( \\'Size\\' )\\n        self.ftsFiles.append( ftsFile )\\n        self.submittedFiles += 1\\n    return S_OK()', 'def __submitFTSTransfer( self ):\\n    \"\"\" create and execute glite-transfer-submit CLI command\\n\\n    :param self: self reference\\n    \"\"\"\\n    log = gLogger.getSubLogger( \\'Submit\\' )\\n    self.__createFTSJob()\\n\\n    submit = self.ftsJob.submitFTS2( command = self.submitCommand )\\n    if not submit[\"OK\"]:\\n      log.error( \"unable to submit FTSJob: %s\" % submit[\"Message\"] )\\n      return submit\\n\\n    log.info( \"FTSJob \\'%s\\'@\\'%s\\' has been submitted\" % ( self.ftsJob.FTSGUID, self.ftsJob.FTSServer ) )\\n\\n    # # update statuses for job files\\n    for ftsFile in self.ftsJob:\\n      ftsFile.FTSGUID = self.ftsJob.FTSGUID\\n      ftsFile.Status = \"Submitted\"\\n      ftsFile.Attempt += 1\\n\\n    log.info( \"FTSJob \\'%s\\'@\\'%s\\' has been submitted\" % ( self.ftsJob.FTSGUID, self.ftsJob.FTSServer ) )\\n    self.ftsGUID = self.ftsJob.FTSGUID\\n    return S_OK()', 'def summary( self, untilTerminal = False, printOutput = False ):\\n    \"\"\" summary of FTS job\\n\\n    :param self: self reference\\n    :param bool untilTerminal: flag to monitor FTS job to its final state\\n    :param bool printOutput: flag to print out monitoring information to the stdout\\n    \"\"\"\\n    res = self.__isSummaryValid()\\n    if not res[\\'OK\\']:\\n      return res\\n    while not self.isTerminal:\\n      res = self.__parseOutput( full = True )\\n      if not res[\\'OK\\']:\\n        return res\\n      if untilTerminal:\\n        self.__print()\\n      self.isRequestTerminal()\\n      if res[\\'Value\\'] or ( not untilTerminal ):\\n        break\\n      time.sleep( 1 )\\n    if untilTerminal:\\n      print \"\"\\n    if printOutput and ( not untilTerminal ):\\n      return self.dumpSummary( printOutput = printOutput )\\n    return S_OK()', 'def dumpSummary( self, printOutput = False ):\\n    \"\"\" get FTS job summary as str\\n\\n    :param self: self reference\\n    :param bool printOutput: print summary to stdout\\n    \"\"\"\\n\\n    outStr = \\'\\'\\n    for status in sorted( self.statusSummary ):\\n      if self.statusSummary[status]:\\n        outStr = \\'%s\\\\t%-10s : %-10s\\\\n\\' % ( outStr, status, str( self.statusSummary[status] ) )\\n    outStr = outStr.rstrip( \\'\\\\n\\' )\\n    if printOutput:\\n      print outStr\\n    return S_OK( outStr )', 'def dump( self ):\\n    \"\"\" print FTS job parameters and files to stdout\\n\\n    :param self: self reference\\n    \"\"\"\\n    print \"%-10s : %-10s\" % ( \"Status\", self.requestStatus )\\n    print \"%-10s : %-10s\" % ( \"Source\", self.sourceSE )\\n    print \"%-10s : %-10s\" % ( \"Target\", self.targetSE )\\n    print \"%-10s : %-128s\" % ( \"Server\", self.ftsServer )\\n    print \"%-10s : %-128s\" % ( \"GUID\", self.ftsGUID )\\n    for lfn in sorted( self.fileDict ):\\n      print \"\\\\n  %-15s : %-128s\" % ( \\'LFN\\', lfn )\\n      for key in [\\'Source\\', \\'Target\\', \\'Status\\', \\'Reason\\', \\'Duration\\']:\\n        print \"  %-15s : %-128s\" % ( key, str( self.fileDict[lfn].get( key ) ) )\\n    return S_OK()', 'def __parseOutput( self, full = False ):\\n    \"\"\" execute glite-transfer-status command and parse its output\\n\\n    :param self: self reference\\n    :param bool full: glite-transfer-status verbosity level, when set, collect information of files as well\\n    \"\"\"\\n    monitor = self.ftsJob.monitorFTS2( command = self.monitorCommand, full = full )\\n    if not monitor[\\'OK\\']:\\n      return monitor\\n    self.percentageComplete = self.ftsJob.Completeness\\n    self.requestStatus = self.ftsJob.Status\\n    self.submitTime = self.ftsJob.SubmitTime\\n\\n    statusSummary = monitor[\\'Value\\']\\n    if statusSummary:\\n      for state in statusSummary:\\n        self.statusSummary[state] = statusSummary[state]\\n\\n    self.transferTime = 0\\n    for ftsFile in self.ftsJob:\\n      lfn = ftsFile.LFN\\n      self.__setFileParameter( lfn, \\'Status\\', ftsFile.Status )\\n      self.__setFileParameter( lfn, \\'Reason\\', ftsFile.Error )\\n      self.__setFileParameter( lfn, \\'Duration\\', ftsFile._duration )\\n      targetURL = self.__getFileParameter( lfn, \\'Target\\' )\\n      if not targetURL[\\'OK\\']:\\n        self.__setFileParameter( lfn, \\'Target\\', ftsFile.TargetSURL )\\n      self.transferTime += int( ftsFile._duration )\\n    return S_OK()', 'def finalize( self ):\\n    \"\"\" finalize FTS job\\n\\n    :param self: self reference\\n    \"\"\"\\n    self.__updateMetadataCache()\\n    transEndTime = dateTime()\\n    regStartTime = time.time()\\n    res = self.getTransferStatistics()\\n    transDict = res[\\'Value\\']\\n\\n    res = self.__registerSuccessful( transDict[\\'transLFNs\\'] )\\n\\n    regSuc, regTotal = res[\\'Value\\']\\n    regTime = time.time() - regStartTime\\n    if self.sourceSE and self.targetSE:\\n      self.__sendAccounting( regSuc, regTotal, regTime, transEndTime, transDict )\\n    return S_OK()', 'def getFailedRegistrations( self ):\\n    \"\"\" get failed registrations dict\\n\\n    :param self: self reference\\n    \"\"\"\\n    return S_OK( self.failedRegistrations )']}, {'features': [], 'snippets': ['def __init__(self):\\n        self.message = \"Downloading..\"\\n        self.total_bytes = 0\\n        self.current_bytes = 0\\n        self.completed = False\\n        self.error_occured = False\\n        self.start_time = 0\\n        self.file_name = \"\"\\n        self.kbps = 0\\n        self.assembly = False\\n        self.assembly_percent = 0', 'def __init__(self, nzbFile, save_to, nntpServer, nntpPort, nntpUser=None, nntpPassword=None, nntpSSL=False, nntpConnections=5, cache_path=\"\"):\\n\\n        # Settings\\n        self.save_to = save_to\\n        self.nntpServer = nntpServer\\n        self.nntpUser = nntpUser\\n        self.nntpPort = nntpPort\\n        self.nntpPassword = nntpPassword\\n        self.nntpSSL = nntpSSL\\n        self.nntpConnections = nntpConnections\\n        self.threads = []\\n        self.running = False\\n\\n        # setup our cache folder.\\n        self.cache_path = cache_path\\n        if ( self.cache_path == \"\" ): self.cache_path = \"packages/dlmanager/cache/\"\\n        self.clearCache()\\n\\n        # ensure both directorys exist\\n        mt.utils.mkdir(self.save_to)\\n        mt.utils.mkdir(self.cache_path)\\n\\n        # Open the NZB, get this show started.\\n        realFile = urllib.urlopen(nzbFile)\\n        self.nzb = NZBParser.parse(realFile)\\n        self.all_decoded = False\\n        self.connection_count = 0\\n\\n        # used to track status.\\n        self.status = StatusReport()\\n        self.status.file_name = nzbFile\\n        self.status.total_bytes = self.nzb.size\\n\\n        # Segment tracking.\\n        self.cache = []\\n        self.segment_list = []\\n        self.segments_finished = []\\n        self.segments_aborted = []\\n\\n        # Queues.\\n        self.segment_queue = []\\n        self.failed_queue = []\\n\\n        # Used to track the speed.\\n        self.speedTime = 0\\n        self.speedCounter = 0', 'def getStatus(self):\\n        return self.status', \"def decodeNextSeg(self):\\n        # if we're not running send an instant kill switch.\\n        if ( not self.running ): return -1\\n\\n        # try to grab a segment from the cache to decode.\\n        seg = None\\n        try:\\n            seg = self.cache.pop()\\n        except:\\n            pass\\t\\n\\n        if ( seg == None ) and ( self.all_decoded ):\\n            return -1\\n        return seg\", 'def decodeFinished(self):\\n        self.status.completed = True', 'def decodeSuccess(self, seg):\\n        self.status.current_bytes += seg.size\\n        self.segments_finished.append(seg.msgid)\\n        if ( (len(self.segments_finished)+len(self.segments_aborted)) >= len(self.segment_list) ):\\n            self.all_decoded = True', 'def decodeFailed(self, seg):\\n        if ( seg == None ): return\\n        mt.log.debug(\"Segment failed to decode: \" + seg.msgid)\\n        self.segFailed(seg)', 'def assemblyStatus(self, percent):\\n        self.status.assembly = True\\n        self.status.assembly_percent = percent', 'def threadStopped(self, thread_num):\\n        self.connection_count -= 1', 'def segComplete(self, seg):\\n        if ( seg == None ): return\\n\\n        if ( seg.data ): \\n            data_size = len(\"\".join(seg.data))\\n\\n            current_time = time.time()\\n            if ( (current_time - self.speedTime) > 1 ):\\n                self.status.kbps = self.speedCounter\\n                self.speedCounter = 0\\n                self.speedTime = current_time\\n            else:\\n                self.speedCounter += (data_size/1024)\\n\\n            self.cache.append(seg)\\n        #mt.log.debug(\"Segment Complete: \" + seg.msgid)', 'def segFailed(self, seg):\\n        if ( seg == None ): return\\n\\n        if ( seg.aborted() ):\\n            mt.log.error(\"Segment Aborted: \" + seg.msgid + \" after \" + str(seg.retries) + \" attempts.\")\\n            self.segments_aborted.append(seg.msgid)\\n            seg.data = []\\n            if ( (len(self.segments_finished)+len(self.segments_aborted)) >= len(self.segment_list) ):\\n                self.all_decoded = True\\n            return\\n\\n        seg.retries += 1\\n\\n        mt.log.error(\"Segment Failed: \" + seg.msgid + \" Attempt #\" + str(seg.retries) + \".\")\\n        self.failed_queue.append(seg)', \"def nextSeg(self):\\n        # if we're not running send an instant kill switch.\\n        if ( not self.running ): return -1\\n\\n        # try to get a segment from main queue or failed queue.\\n        queue_empty = False\\n        seg = None\\n        try:\\n            seg = self.segment_queue.pop()\\n        except:\\n            try:\\n                seg = self.failed_queue.pop()\\n            except:\\n                queue_empty = True\\n                pass\\n            pass\\n\\n        # We're all outta segments, if they're done decoding, kill the threads.\\n        if ( queue_empty ) and ( self.all_decoded ):\\n            return -1\\n\\n        return seg\", 'def clearCache(self):\\n        mt.utils.rmdir(self.cache_path)', 'def stop(self):\\n        self.running = False\\n        self.articleDecoder.stop()\\n        for thread in self.threads:\\n            thread.stop()\\n        self.clearCache()', 'def __init__(self, connection_number, server, port, username, password, ssl, nextSegFunc, onSegComplete = None, onSegFailed = None, onThreadStop = None):\\n        mt.threads.Thread.__init__(self)\\n\\n        # Settings\\n        self.connection = None\\n        self.connection_number = connection_number\\n        self.server = server\\n        self.port = port\\n        self.username = username\\n        self.password = password\\n        self.ssl = ssl\\n\\n        # Events.\\n        self.nextSegFunc = nextSegFunc\\n        self.onSegComplete = onSegComplete\\n        self.onSegFailed = onSegFailed\\n        self.onThreadStop = onThreadStop', 'def disconnect(self):\\n        if ( self.connection ):\\n            try:\\n                self.connection.quit()\\n            except:\\n                pass\\n        self.connection = None']}, {'features': [], 'snippets': ['def test_exists():\\n    \"\"\"`fix.with_fixture` function exists\"\"\"\\n    assert isinstance(with_fixture, FunctionType)', 'def setup_only(context):\\n        \"\"\"A fixture with no `teardown()`.\"\"\"\\n\\n        def setup():\\n            \"\"\"Add something to the context.\"\"\"\\n            assert context == {}\\n            context.squee = \"kapow\"\\n\\n        return setup', 'def case(context):\\n        \"\"\"Check that the context has been set up.\"\"\"\\n        assert context == {\"squee\": \"kapow\"}', 'def test_setup_teardown():\\n    \"\"\"`setup_teardown` fixture works as expected\"\"\"\\n\\n    def setup_teardown(context):\\n        \"\"\"A fixture with both `setup()` and `teardown()`.\"\"\"\\n\\n        def setup():\\n            \"\"\"Add something to the context.\"\"\"\\n            assert context == {}\\n            context.squee = \"kapow\"\\n\\n        def teardown():\\n            \"\"\"Check that `context.squee` has changed.\"\"\"\\n            assert context == {\"squee\": \"boing\"}\\n\\n        return setup, teardown\\n\\n    @with_fixture(setup_teardown)\\n    def case(context):\\n        \"\"\"Alter the context.\"\"\"\\n        assert context == {\"squee\": \"kapow\"}\\n        context.squee = \"boing\"\\n\\n    case()  # pylint: disable=E1120', 'def multiple(context):\\n        \"\"\"A fixture to be invoked multiple times.\"\"\"\\n\\n        def setup():\\n            \"\"\"Add something to the context.\"\"\"\\n            assert context == {}\\n            context.squee = \"kapow\"\\n\\n        def teardown():\\n            \"\"\"Check that `context.squee` has changed.\"\"\"\\n            assert context == {\"squee\": \"kapow\", \"boing\": \"thunk\"}\\n\\n        return setup, teardown', 'def case(context):\\n        \"\"\"Add to the context.\"\"\"\\n        assert context == {\"squee\": \"kapow\"}\\n        context.boing = \"thunk\"']}, {'features': [], 'snippets': ['def __init__(self, system, capForce, particleGroup = None):\\n        if not (pmi._PMIComm and pmi._PMIComm.isActive()) or pmi._MPIcomm.rank in pmi._PMIComm.getMPIcpugroup():\\n            if (particleGroup == None) or (particleGroup.size() == 0):\\n                cxxinit(self, integrator_CapForce, system, capForce)\\n            else:\\n                cxxinit(self, integrator_CapForce, system, capForce, particleGroup)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, node, emailMessage, references=list(), children=dict(), slotted=bool(\"false\")):\\n        self.node = node\\n        self.children = dict(children)\\n        self.references = references[:]\\n        self.slotted = slotted\\n        self.emailMessage = emailMessage', 'def get_children(self):\\n        return self.children', 'def set_children(self, children):\\n        self.children = children', 'def is_slotted(self):\\n        return self.slotted', 'def get_message(self):\\n        return self.emailMessage', 'def handleNode(currentNodeInAction, referenceNodeNow, referencesToCheck, patchMessageReferenceNode):\\n    for reference in referencesToCheck[:] :\\n        if reference in referenceNodeNow.get_children() :\\n            referencesToCheck.remove(reference)\\n            return patchMessageReferenceNode[reference]\\n    if len(referencesToCheck) == 0 :\\n        referenceNodeNow.get_children()[currentNodeInAction.get_node()] = currentNodeInAction', 'def makeChildren(patchMessageReferenceNode) :\\n    ref_keys = patchMessageReferenceNode.keys()\\n    ref_keys.sort()\\n    for messageId in ref_keys:\\n        referenceNode = patchMessageReferenceNode[messageId]\\n        utils.verboseOutput(verbose, \"Managing Message Id:\", referenceNode.get_node())\\n        referenceIds = referenceNode.get_references()\\n        referenceIdsClone = referenceIds[:]\\n        utils.verboseOutput(verbose, \"Cloned References: \", referenceIdsClone)\\n        if len(referenceIds) > 0 :\\n            nextNode = patchMessageReferenceNode[referenceIdsClone[0]]\\n            referenceIdsClone.remove(referenceIdsClone[0])\\n            while nextNode != None :\\n                utils.verboseOutput(verbose, \"Next Node: \", nextNode.get_node())\\n                utils.verboseOutput(verbose, \"Curent Node: \", referenceNode.get_node())\\n                utils.verboseOutput(verbose, \"REF: \", referenceIdsClone)\\n                nextNode = handleNode(referenceNode, nextNode, referenceIdsClone, patchMessageReferenceNode)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setup_class(cls):\\n        cls.K = nx.krackhardt_kite_graph()\\n        cls.P3 = nx.path_graph(3)\\n        cls.P4 = nx.path_graph(4)\\n        cls.K5 = nx.complete_graph(5)\\n\\n        cls.C4 = nx.cycle_graph(4)\\n        cls.T = nx.balanced_tree(r=2, h=2)\\n        cls.Gb = nx.Graph()\\n        cls.Gb.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (2, 4), (4, 5), (3, 5)])\\n\\n        F = nx.florentine_families_graph()\\n        cls.F = F\\n\\n        cls.LM = nx.les_miserables_graph()\\n\\n        # Create random undirected, unweighted graph for testing incremental version\\n        cls.undirected_G = nx.fast_gnp_random_graph(n=100, p=0.6, seed=123)\\n        cls.undirected_G_cc = nx.closeness_centrality(cls.undirected_G)', 'def test_digraph(self):\\n        G = nx.path_graph(3, create_using=nx.DiGraph())\\n        c = nx.closeness_centrality(G)\\n        cr = nx.closeness_centrality(G.reverse())\\n        d = {0: 0.0, 1: 0.500, 2: 0.667}\\n        dr = {0: 0.667, 1: 0.500, 2: 0.0}\\n        for n in sorted(self.P3):\\n            assert almost_equal(c[n], d[n], places=3)\\n            assert almost_equal(cr[n], dr[n], places=3)', 'def test_p3_closeness(self):\\n        c = nx.closeness_centrality(self.P3)\\n        d = {0: 0.667, 1: 1.000, 2: 0.667}\\n        for n in sorted(self.P3):\\n            assert almost_equal(c[n], d[n], places=3)', 'def test_florentine_families_closeness(self):\\n        c = nx.closeness_centrality(self.F)\\n        d = {\\n            \"Acciaiuoli\": 0.368,\\n            \"Albizzi\": 0.483,\\n            \"Barbadori\": 0.4375,\\n            \"Bischeri\": 0.400,\\n            \"Castellani\": 0.389,\\n            \"Ginori\": 0.333,\\n            \"Guadagni\": 0.467,\\n            \"Lamberteschi\": 0.326,\\n            \"Medici\": 0.560,\\n            \"Pazzi\": 0.286,\\n            \"Peruzzi\": 0.368,\\n            \"Ridolfi\": 0.500,\\n            \"Salviati\": 0.389,\\n            \"Strozzi\": 0.4375,\\n            \"Tornabuoni\": 0.483,\\n        }\\n        for n in sorted(self.F):\\n            assert almost_equal(c[n], d[n], places=3)', 'def test_weighted_closeness(self):\\n        edges = [\\n            (\"s\", \"u\", 10),\\n            (\"s\", \"x\", 5),\\n            (\"u\", \"v\", 1),\\n            (\"u\", \"x\", 2),\\n            (\"v\", \"y\", 1),\\n            (\"x\", \"u\", 3),\\n            (\"x\", \"v\", 5),\\n            (\"x\", \"y\", 2),\\n            (\"y\", \"s\", 7),\\n            (\"y\", \"v\", 6),\\n        ]\\n        XG = nx.Graph()\\n        XG.add_weighted_edges_from(edges)\\n        c = nx.closeness_centrality(XG, distance=\"weight\")\\n        d = {\"y\": 0.200, \"x\": 0.286, \"s\": 0.138, \"u\": 0.235, \"v\": 0.200}\\n        for n in sorted(XG):\\n            assert almost_equal(c[n], d[n], places=3)', 'def pick_add_edge(g):\\n        u = nx.utils.arbitrary_element(g)\\n        possible_nodes = set(g.nodes())\\n        neighbors = list(g.neighbors(u)) + [u]\\n        possible_nodes.difference_update(neighbors)\\n        v = nx.utils.arbitrary_element(possible_nodes)\\n        return (u, v)', 'def pick_remove_edge(g):\\n        u = nx.utils.arbitrary_element(g)\\n        possible_nodes = list(g.neighbors(u))\\n        v = nx.utils.arbitrary_element(possible_nodes)\\n        return (u, v)', 'def test_wrong_size_prev_cc_raises(self):\\n        with pytest.raises(nx.NetworkXError):\\n            G = self.undirected_G.copy()\\n            edge = self.pick_add_edge(G)\\n            insert = True\\n            prev_cc = self.undirected_G_cc.copy()\\n            prev_cc.pop(0)\\n            nx.incremental_closeness_centrality(G, edge, prev_cc, insert)', 'def test_zero_centrality(self):\\n        G = nx.path_graph(3)\\n        prev_cc = nx.closeness_centrality(G)\\n        edge = self.pick_remove_edge(G)\\n        test_cc = nx.incremental_closeness_centrality(G, edge, prev_cc, insertion=False)\\n        G.remove_edges_from([edge])\\n        real_cc = nx.closeness_centrality(G)\\n        shared_items = set(test_cc.items()) & set(real_cc.items())\\n        assert len(shared_items) == len(real_cc)\\n        assert 0 in test_cc.values()']}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Adding unique constraint on 'Vendeur', fields ['code_permanent']\\n        db.create_unique(u'encefal_vendeur', ['code_permanent'])\"]}, {'features': [], 'snippets': ['def loader(config_dict, engine):\\n    return WS28xxDriver(**config_dict[DRIVER_NAME])', 'def confeditor_loader():\\n    return WS28xxConfEditor()', \"def logmsg(dst, msg):\\n    syslog.syslog(dst, 'ws28xx: %s: %s' %\\n                  (threading.currentThread().getName(), msg))\", 'def loginf(msg):\\n    logmsg(syslog.LOG_INFO, msg)', 'def logerr(msg):\\n    logmsg(syslog.LOG_ERR, msg)', \"def log_frame(n, buf):\\n    logdbg('frame length is %d' % n)\\n    strbuf = ''\\n    for i in xrange(0,n):\\n        strbuf += str('%02x ' % buf[i])\\n        if (i + 1) % 16 == 0:\\n            logdbg(strbuf)\\n            strbuf = ''\\n    if strbuf:\\n        logdbg(strbuf)\", 'def get_datum_match(v, np, ofl):\\n    if np == v or ofl == v:\\n        return None\\n    return v', 'def get_next_index(idx):\\n    return get_index(idx + 1)', 'def tstr_to_ts(tstr):\\n    try:\\n        return int(time.mktime(time.strptime(tstr, \"%Y-%m-%d %H:%M:%S\")))\\n    except (OverflowError, ValueError, TypeError):\\n        pass\\n    return None', 'def addr_to_index(addr):\\n    return (addr - 416) / 18', \"def print_dict(data):\\n    for x in sorted(data.keys()):\\n        if x == 'dateTime':\\n            print '%s: %s' % (x, weeutil.weeutil.timestamp_to_string(data[x]))\\n        else:\\n            print '%s: %s' % (x, data[x])\", 'def default_stanza(self):\\n        return \"\"\"', 'def prompt_for_settings(self):\\n        print \"Specify the frequency used between the station and the\"\\n        print \"transceiver, either \\'US\\' (915 MHz) or \\'EU\\' (868.3 MHz).\"\\n        freq = self._prompt(\\'frequency\\', \\'US\\', [\\'US\\', \\'EU\\'])\\n        return {\\'transceiver_frequency\\': freq}', 'def add_options(self, parser):\\n        super(WS28xxConfigurator, self).add_options(parser)\\n        parser.add_option(\"--check-transceiver\", dest=\"check\",\\n                          action=\"store_true\",\\n                          help=\"check USB transceiver\")\\n        parser.add_option(\"--pair\", dest=\"pair\", action=\"store_true\",\\n                          help=\"pair the USB transceiver with station console\")\\n        parser.add_option(\"--info\", dest=\"info\", action=\"store_true\",\\n                          help=\"display weather station configuration\")\\n        parser.add_option(\"--set-interval\", dest=\"interval\",\\n                          type=int, metavar=\"N\",\\n                          help=\"set logging interval to N minutes\")\\n        parser.add_option(\"--current\", dest=\"current\", action=\"store_true\",\\n                          help=\"get the current weather conditions\")\\n        parser.add_option(\"--history\", dest=\"nrecords\", type=int, metavar=\"N\",\\n                          help=\"display N history records\")\\n        parser.add_option(\"--history-since\", dest=\"recmin\",\\n                          type=int, metavar=\"N\",\\n                          help=\"display history records since N minutes ago\")\\n        parser.add_option(\"--maxtries\", dest=\"maxtries\", type=int,\\n                          help=\"maximum number of retries, 0 indicates no max\")', 'def check_transceiver(self, maxtries):\\n        \"\"\"See if the transceiver is installed and operational.\"\"\"\\n        print \\'Checking for transceiver...\\'\\n        ntries = 0\\n        while ntries < maxtries:\\n            ntries += 1\\n            if self.station.transceiver_is_present():\\n                print \\'Transceiver is present\\'\\n                sn = self.station.get_transceiver_serial()\\n                print \\'serial: %s\\' % sn\\n                tid = self.station.get_transceiver_id()\\n                print \\'id: %d (0x%04x)\\' % (tid, tid)\\n                break\\n            print \\'Not found (attempt %d of %d) ...\\' % (ntries, maxtries)\\n            time.sleep(5)\\n        else:\\n            print \\'Transceiver not responding.\\'', \"def get_interval(self, maxtries):\\n        cfg = self.get_config(maxtries)\\n        if cfg is None:\\n            return None\\n        return getHistoryInterval(cfg['history_interval'])\", 'def set_interval(self, maxtries, interval, prompt):\\n        \"\"\"Set the station archive interval\"\"\"\\n        print \"This feature is not yet implemented\"', 'def show_current(self, maxtries):\\n        \"\"\"Get current weather observation.\"\"\"\\n        print \\'Querying the station for current weather data...\\'\\n        start_ts = None\\n        ntries = 0\\n        while ntries < maxtries or maxtries == 0:\\n            packet = self.station.get_observation()\\n            if packet is not None:\\n                print_dict(packet)\\n                break\\n            ntries += 1\\n            if start_ts is None:\\n                start_ts = int(time.time())\\n            else:\\n                dur = int(time.time()) - start_ts\\n                print \\'No data after %d seconds (press SET to sync)\\' % dur\\n            time.sleep(30)', 'def __init__(self, **stn_dict) :\\n        \"\"\"Initialize the station object.\\n\\n        model: Which station model is this?\\n        [Optional. Default is \\'LaCrosse WS28xx\\']\\n\\n        transceiver_frequency: Frequency for transceiver-to-console.  Specify\\n        either US or EU.\\n        [Required. Default is US]\\n\\n        polling_interval: How often to sample the USB interface for data.\\n        [Optional. Default is 30 seconds]\\n\\n        comm_interval: Communications mode interval\\n        [Optional.  Default is 3]\\n\\n        device_id: The USB device ID for the transceiver.  If there are\\n        multiple devices with the same vendor and product IDs on the bus,\\n        each will have a unique device identifier.  Use this identifier\\n        to indicate which device should be used.\\n        [Optional. Default is None]\\n\\n        serial: The transceiver serial number.  If there are multiple\\n        devices with the same vendor and product IDs on the bus, each will\\n        have a unique serial number.  Use the serial number to indicate which\\n        transceiver should be used.\\n        [Optional. Default is None]\\n        \"\"\"\\n\\n        self.model            = stn_dict.get(\\'model\\', \\'LaCrosse WS28xx\\')\\n        self.polling_interval = int(stn_dict.get(\\'polling_interval\\', 30))\\n        self.comm_interval    = int(stn_dict.get(\\'comm_interval\\', 3))\\n        self.frequency        = stn_dict.get(\\'transceiver_frequency\\', \\'US\\')\\n        self.device_id        = stn_dict.get(\\'device_id\\', None)\\n        self.serial           = stn_dict.get(\\'serial\\', None)\\n\\n        self.vendor_id        = 0x6666\\n        self.product_id       = 0x5555\\n\\n        now = int(time.time())\\n        self._service = None\\n        self._last_rain = None\\n        self._last_obs_ts = None\\n        self._last_nodata_log_ts = now\\n        self._nodata_interval = 300 # how often to check for no data\\n        self._last_contact_log_ts = now\\n        self._nocontact_interval = 300 # how often to check for no contact\\n        self._log_interval = 600 # how often to log\\n\\n        global DEBUG_COMM\\n        DEBUG_COMM = int(stn_dict.get(\\'debug_comm\\', 0))\\n        global DEBUG_CONFIG_DATA\\n        DEBUG_CONFIG_DATA = int(stn_dict.get(\\'debug_config_data\\', 0))\\n        global DEBUG_WEATHER_DATA\\n        DEBUG_WEATHER_DATA = int(stn_dict.get(\\'debug_weather_data\\', 0))\\n        global DEBUG_HISTORY_DATA\\n        DEBUG_HISTORY_DATA = int(stn_dict.get(\\'debug_history_data\\', 0))\\n        global DEBUG_DUMP_FORMAT\\n        DEBUG_DUMP_FORMAT = stn_dict.get(\\'debug_dump_format\\', \\'auto\\')\\n\\n        loginf(\\'driver version is %s\\' % DRIVER_VERSION)\\n        loginf(\\'frequency is %s\\' % self.frequency)\\n\\n        self.startUp()', 'def hardware_name(self):\\n        return self.model', 'def closePort(self):\\n        self.shutDown()', 'def genStartupRecords(self, ts):\\n        loginf(\\'Scanning historical records\\')\\n        maxtries = 65\\n        ntries = 0\\n        last_n = n = nrem = None\\n        last_ts = now = int(time.time())\\n        self.start_caching_history(since_ts=ts)\\n        while nrem is None or nrem > 0:\\n            if ntries >= maxtries:\\n                logerr(\\'No historical data after %d tries\\' % ntries)\\n                return\\n            time.sleep(60)\\n            ntries += 1\\n            now = int(time.time())\\n            n = self.get_num_history_scanned()\\n            if n == last_n:\\n                dur = now - last_ts\\n                loginf(\\'No data after %d seconds (press SET to sync)\\' % dur)\\n            else:\\n                ntries = 0\\n                last_ts = now\\n            last_n = n\\n            nrem = self.get_uncached_history_count()\\n            ni = self.get_next_history_index()\\n            li = self.get_latest_history_index()\\n            loginf(\"Scanned %s records: current=%s latest=%s remaining=%s\" %\\n                   (n, ni, li, nrem))\\n        self.stop_caching_history()\\n        records = self.get_history_cache_records()\\n        self.clear_history_cache()\\n        loginf(\\'Found %d historical records\\' % len(records))\\n        last_ts = None\\n        for r in records:\\n            if last_ts is not None and r[\\'dateTime\\'] is not None:\\n                r[\\'usUnits\\'] = weewx.METRIC\\n                r[\\'interval\\'] = (r[\\'dateTime\\'] - last_ts) / 60\\n                yield r\\n            last_ts = r[\\'dateTime\\']', 'def startUp(self):\\n        if self._service is not None:\\n            return\\n        self._service = CCommunicationService()\\n        self._service.setup(self.frequency,\\n                            self.vendor_id, self.product_id, self.device_id,\\n                            self.serial, comm_interval=self.comm_interval)\\n        self._service.startRFThread()', 'def transceiver_is_present(self):\\n        return self._service.DataStore.getTransceiverPresent()', 'def get_transceiver_serial(self):\\n        return self._service.DataStore.getTransceiverSerNo()', 'def get_last_contact(self):\\n        return self._service.getLastStat().last_seen_ts', \"def get_config(self):\\n        logdbg('get station configuration')\\n        cfg = self._service.getConfigData().asDict()\\n        cs = cfg.get('checksum_out')\\n        if cs is None or cs == 0:\\n            return None\\n        return cfg\", 'def stop_caching_history(self):\\n        self._service.stopCachingHistory()', 'def get_next_history_index(self):\\n        return self._service.getNextHistoryIndex()', 'def get_num_history_scanned(self):\\n        return self._service.getNumHistoryScanned()', 'def clear_history_cache(self):\\n        self._service.clearHistoryCache()', 'def testBit(int_type, offset):\\n        mask = 1 << offset\\n        return int_type & mask', 'def setBit(int_type, offset):\\n        mask = 1 << offset\\n        return int_type | mask', 'def setBitVal(int_type, offset, val):\\n        mask = val << offset\\n        return int_type | mask', 'def clearBit(int_type, offset):\\n        mask = ~(1 << offset)\\n        return int_type & mask', 'def toggleBit(int_type, offset):\\n        mask = 1 << offset\\n        return int_type ^ mask', 'def getWindDir(wdir, wspeed):\\n    if wspeed is None or wspeed == 0:\\n        return None\\n    if wdir < 0 or wdir >= 16:\\n        return None\\n    return wdir * 360 / 16', 'def getFrequency(standard):\\n    if standard == EFrequency.fsUS:\\n        return EFrequency.tfUS\\n    elif standard == EFrequency.fsEU:\\n        return EFrequency.tfEU\\n    logerr(\"unknown frequency standard \\'%s\\', using US\" % standard)\\n    return EFrequency.tfUS', 'def getBatteryStatus(status, flag):\\n    \"\"\"Return 1 if bit is set, 0 otherwise\"\"\"\\n    bit = batterybits.get(flag)\\n    if bit is None:\\n        return None\\n    if BitHandling.testBit(status, bit):\\n        return 1\\n    return 0', 'def getHistoryInterval(i):\\n    return history_intervals.get(i)', 'def TemperatureNP():\\n        return 81.099998', 'def TemperatureOFL():\\n        return 136.0', 'def PressureNP():\\n        return 10101010.0', 'def PressureOFL():\\n        return 16666.5', 'def HumidityNP():\\n        return 110.0', 'def HumidityOFL():\\n        return 121.0', 'def RainNP():\\n        return -0.2', 'def RainOFL():\\n        return 16666.664', 'def WindNP():\\n        return 183.6 # km/h = 51.0 m/s', 'def WindOFL():\\n        return 183.96 # km/h = 51.099998 m/s', 'def TemperatureOffset():\\n        return 40.0', 'def Reset(self):\\n        self._Value = 0.0\\n        self._ResetFlag = 23\\n        self._IsError = 1\\n        self._IsOverflow = 1', 'def __init__(self):\\n        self._Min = CMeasurement()\\n        self._Max = CMeasurement()', 'def isOFL2(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) == 15 \\\\\\n                or (buf[0][start+0] & 0xF) == 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) == 15 \\\\\\n                or (buf[0][start+1] >>  4) == 15\\n        return result', 'def isOFL3(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) == 15 \\\\\\n                or (buf[0][start+0] & 0xF) == 15 \\\\\\n                or (buf[0][start+1] >>  4) == 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) == 15 \\\\\\n                or (buf[0][start+1] >>  4) == 15 \\\\\\n                or (buf[0][start+1] & 0xF) == 15\\n        return result', 'def isOFL5(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) == 15 \\\\\\n                or (buf[0][start+0] & 0xF) == 15 \\\\\\n                or (buf[0][start+1] >>  4) == 15 \\\\\\n                or (buf[0][start+1] & 0xF) == 15 \\\\\\n                or (buf[0][start+2] >>  4) == 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) == 15 \\\\\\n                or (buf[0][start+1] >>  4) == 15 \\\\\\n                or (buf[0][start+1] & 0xF) == 15 \\\\\\n                or (buf[0][start+2] >>  4) == 15 \\\\\\n                or (buf[0][start+2] & 0xF) == 15\\n        return result', 'def isErr2(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) >= 10 \\\\\\n                and (buf[0][start+0] >>  4) != 15 \\\\\\n                or  (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15 \\\\\\n                or  (buf[0][start+1] >>  4) >= 10 \\\\\\n                and (buf[0][start+1] >>  4) != 15\\n        return result', 'def isErr3(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) >= 10 \\\\\\n                and (buf[0][start+0] >>  4) != 15 \\\\\\n                or  (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15 \\\\\\n                or  (buf[0][start+1] >>  4) >= 10 \\\\\\n                and (buf[0][start+1] >>  4) != 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15 \\\\\\n                or  (buf[0][start+1] >>  4) >= 10 \\\\\\n                and (buf[0][start+1] >>  4) != 15 \\\\\\n                or  (buf[0][start+1] & 0xF) >= 10 \\\\\\n                and (buf[0][start+1] & 0xF) != 15\\n        return result', 'def isErr5(buf, start, StartOnHiNibble):\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4) >= 10 \\\\\\n                and (buf[0][start+0] >>  4) != 15 \\\\\\n                or  (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15 \\\\\\n                or  (buf[0][start+1] >>  4) >= 10 \\\\\\n                and (buf[0][start+1] >>  4) != 15 \\\\\\n                or  (buf[0][start+1] & 0xF) >= 10 \\\\\\n                and (buf[0][start+1] & 0xF) != 15 \\\\\\n                or  (buf[0][start+2] >>  4) >= 10 \\\\\\n                and (buf[0][start+2] >>  4) != 15\\n        else:\\n            result = (buf[0][start+0] & 0xF) >= 10 \\\\\\n                and (buf[0][start+0] & 0xF) != 15 \\\\\\n                or  (buf[0][start+1] >>  4) >= 10 \\\\\\n                and (buf[0][start+1] >>  4) != 15 \\\\\\n                or  (buf[0][start+1] & 0xF) >= 10 \\\\\\n                and (buf[0][start+1] & 0xF) != 15 \\\\\\n                or  (buf[0][start+2] >>  4) >= 10 \\\\\\n                and (buf[0][start+2] >>  4) != 15 \\\\\\n                or  (buf[0][start+2] & 0xF) >= 10 \\\\\\n                and (buf[0][start+2] & 0xF) != 15\\n        return result', 'def reverseByteOrder(buf, start, Count):\\n        nbuf=buf[0]\\n        for i in xrange(0, Count >> 1):\\n            tmp = nbuf[start + i]\\n            nbuf[start + i] = nbuf[start + Count - i - 1]\\n            nbuf[start + Count - i - 1 ] = tmp\\n        buf[0]=nbuf', 'def readWindDirectionShared(buf, start):\\n        return (buf[0][0+start] & 0xF, buf[0][start] >> 4)', 'def toInt_2(buf, start, StartOnHiNibble):\\n        \"\"\"read 2 nibbles\"\"\"\\n        if StartOnHiNibble:\\n            rawpre  = (buf[0][start+0] >>  4)* 10 \\\\\\n                + (buf[0][start+0] & 0xF)* 1\\n        else:\\n            rawpre  = (buf[0][start+0] & 0xF)* 10 \\\\\\n                + (buf[0][start+1] >>  4)* 1\\n        return rawpre', 'def toRain_7_3(buf, start, StartOnHiNibble):\\n        \"\"\"read 7 nibbles, presentation with 3 decimals; units of mm\"\"\"\\n        if (USBHardware.isErr2(buf, start+0, StartOnHiNibble) or\\n            USBHardware.isErr5(buf, start+1, StartOnHiNibble)):\\n            result = CWeatherTraits.RainNP()\\n        elif (USBHardware.isOFL2(buf, start+0, StartOnHiNibble) or\\n              USBHardware.isOFL5(buf, start+1, StartOnHiNibble)):\\n            result = CWeatherTraits.RainOFL()\\n        elif StartOnHiNibble:\\n            result  = (buf[0][start+0] >>  4)*  1000 \\\\\\n                + (buf[0][start+0] & 0xF)* 100    \\\\\\n                + (buf[0][start+1] >>  4)*  10    \\\\\\n                + (buf[0][start+1] & 0xF)*   1    \\\\\\n                + (buf[0][start+2] >>  4)*   0.1  \\\\\\n                + (buf[0][start+2] & 0xF)*   0.01 \\\\\\n                + (buf[0][start+3] >>  4)*   0.001\\n        else:\\n            result  = (buf[0][start+0] & 0xF)*  1000 \\\\\\n                + (buf[0][start+1] >>  4)* 100    \\\\\\n                + (buf[0][start+1] & 0xF)*  10    \\\\\\n                + (buf[0][start+2] >>  4)*   1    \\\\\\n                + (buf[0][start+2] & 0xF)*   0.1  \\\\\\n                + (buf[0][start+3] >>  4)*   0.01 \\\\\\n                + (buf[0][start+3] & 0xF)*   0.001\\n        return result', \"def toRain_6_2(buf, start, StartOnHiNibble):\\n        '''read 6 nibbles, presentation with 2 decimals; units of mm'''\\n        if (USBHardware.isErr2(buf, start+0, StartOnHiNibble) or\\n            USBHardware.isErr2(buf, start+1, StartOnHiNibble) or\\n            USBHardware.isErr2(buf, start+2, StartOnHiNibble) ):\\n            result = CWeatherTraits.RainNP()\\n        elif (USBHardware.isOFL2(buf, start+0, StartOnHiNibble) or\\n              USBHardware.isOFL2(buf, start+1, StartOnHiNibble) or\\n              USBHardware.isOFL2(buf, start+2, StartOnHiNibble)):\\n            result = CWeatherTraits.RainOFL()\\n        elif StartOnHiNibble:\\n            result  = (buf[0][start+0] >>  4)*  1000 \\\\\\n                + (buf[0][start+0] & 0xF)* 100   \\\\\\n                + (buf[0][start+1] >>  4)*  10   \\\\\\n                + (buf[0][start+1] & 0xF)*   1   \\\\\\n                + (buf[0][start+2] >>  4)*   0.1 \\\\\\n                + (buf[0][start+2] & 0xF)*   0.01\\n        else:\\n            result  = (buf[0][start+0] & 0xF)*  1000 \\\\\\n                + (buf[0][start+1] >>  4)* 100   \\\\\\n                + (buf[0][start+1] & 0xF)*  10   \\\\\\n                + (buf[0][start+2] >>  4)*   1   \\\\\\n                + (buf[0][start+2] & 0xF)*   0.1 \\\\\\n                + (buf[0][start+3] >>  4)*   0.01\\n        return result\", 'def toRain_3_1(buf, start, StartOnHiNibble):\\n        \"\"\"read 3 nibbles, presentation with 1 decimal; units of 0.1 inch\"\"\"\\n        if StartOnHiNibble:\\n            hibyte = buf[0][start+0]\\n            lobyte = (buf[0][start+1] >> 4) & 0xF\\n        else:\\n            hibyte = 16*(buf[0][start+0] & 0xF) + ((buf[0][start+1] >> 4) & 0xF)\\n            lobyte = buf[0][start+1] & 0xF            \\n        if hibyte == 0xFF and lobyte == 0xE :\\n            result = CWeatherTraits.RainNP()\\n        elif hibyte == 0xFF and lobyte == 0xF :\\n            result = CWeatherTraits.RainOFL()\\n        else:\\n            val = USBHardware.toFloat_3_1(buf, start, StartOnHiNibble) # 0.1 inch\\n            result = val * 2.54 # mm\\n        return result', 'def toFloat_3_1(buf, start, StartOnHiNibble):\\n        \"\"\"read 3 nibbles, presentation with 1 decimal\"\"\"\\n        if StartOnHiNibble:\\n            result = (buf[0][start+0] >>  4)*16**2 \\\\\\n                + (buf[0][start+0] & 0xF)*   16**1 \\\\\\n                + (buf[0][start+1] >>  4)*   16**0\\n        else:\\n            result = (buf[0][start+0] & 0xF)*16**2 \\\\\\n                + (buf[0][start+1] >>  4)*   16**1 \\\\\\n                + (buf[0][start+1] & 0xF)*   16**0\\n        result = result / 10.0\\n        return result', 'def toDateTime(buf, start, StartOnHiNibble, label):\\n        \"\"\"read 10 nibbles, presentation as DateTime\"\"\"\\n        result = None\\n        if (USBHardware.isErr2(buf, start+0, StartOnHiNibble)\\n            or USBHardware.isErr2(buf, start+1, StartOnHiNibble)\\n            or USBHardware.isErr2(buf, start+2, StartOnHiNibble)\\n            or USBHardware.isErr2(buf, start+3, StartOnHiNibble)\\n            or USBHardware.isErr2(buf, start+4, StartOnHiNibble)):\\n            logerr(\\'ToDateTime: bogus date for %s: error status in buffer\\' %\\n                   label)\\n        else:\\n            year    = USBHardware.toInt_2(buf, start+0, StartOnHiNibble) + 2000\\n            month   = USBHardware.toInt_2(buf, start+1, StartOnHiNibble)\\n            days    = USBHardware.toInt_2(buf, start+2, StartOnHiNibble)\\n            hours   = USBHardware.toInt_2(buf, start+3, StartOnHiNibble)\\n            minutes = USBHardware.toInt_2(buf, start+4, StartOnHiNibble)\\n            try:\\n                result = datetime(year, month, days, hours, minutes)\\n            except ValueError:\\n                if label not in _bad_labels:\\n                    logerr((\\'ToDateTime: bogus date for %s:\\'\\n                            \\' bad date conversion from\\'\\n                            \\' %s %s %s %s %s\\') %\\n                           (label, minutes, hours, days, month, year))\\n        if result is None:\\n            # FIXME: use None instead of a really old date to indicate invalid\\n            result = datetime(1900, 01, 01, 00, 00)\\n        return result', 'def toHumidity_2_0(buf, start, StartOnHiNibble):\\n        \"\"\"read 2 nibbles, presentation with 0 decimal\"\"\"\\n        if USBHardware.isErr2(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.HumidityNP()\\n        elif USBHardware.isOFL2(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.HumidityOFL()\\n        else:\\n            result = USBHardware.toInt_2(buf, start, StartOnHiNibble)\\n        return result', 'def toTemperature_5_3(buf, start, StartOnHiNibble):\\n        \"\"\"read 5 nibbles, presentation with 3 decimals; units of degree C\"\"\"\\n        if USBHardware.isErr5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.TemperatureNP()\\n        elif USBHardware.isOFL5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.TemperatureOFL()\\n        else:\\n            if StartOnHiNibble:\\n                rawtemp = (buf[0][start+0] >>  4)* 10 \\\\\\n                    + (buf[0][start+0] & 0xF)*  1     \\\\\\n                    + (buf[0][start+1] >>  4)*  0.1   \\\\\\n                    + (buf[0][start+1] & 0xF)*  0.01  \\\\\\n                    + (buf[0][start+2] >>  4)*  0.001\\n            else:\\n                rawtemp = (buf[0][start+0] & 0xF)* 10 \\\\\\n                    + (buf[0][start+1] >>  4)*  1     \\\\\\n                    + (buf[0][start+1] & 0xF)*  0.1   \\\\\\n                    + (buf[0][start+2] >>  4)*  0.01  \\\\\\n                    + (buf[0][start+2] & 0xF)*  0.001\\n            result = rawtemp - CWeatherTraits.TemperatureOffset()\\n        return result', 'def toTemperature_3_1(buf, start, StartOnHiNibble):\\n        \"\"\"read 3 nibbles, presentation with 1 decimal; units of degree C\"\"\"\\n        if USBHardware.isErr3(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.TemperatureNP()\\n        elif USBHardware.isOFL3(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.TemperatureOFL()\\n        else:\\n            if StartOnHiNibble :\\n                rawtemp   =  (buf[0][start+0] >>  4)*  10 \\\\\\n                    +  (buf[0][start+0] & 0xF)*  1   \\\\\\n                    +  (buf[0][start+1] >>  4)*  0.1\\n            else:\\n                rawtemp   =  (buf[0][start+0] & 0xF)*  10 \\\\\\n                    +  (buf[0][start+1] >>  4)*  1   \\\\\\n                    +  (buf[0][start+1] & 0xF)*  0.1 \\n            result = rawtemp - CWeatherTraits.TemperatureOffset()\\n        return result', 'def toWindspeed_6_2(buf, start):\\n        \"\"\"read 6 nibbles, presentation with 2 decimals; units of km/h\"\"\"\\n        result = (buf[0][start+0] >> 4)* 16**5 \\\\\\n            + (buf[0][start+0] & 0xF)*   16**4 \\\\\\n            + (buf[0][start+1] >>  4)*   16**3 \\\\\\n            + (buf[0][start+1] & 0xF)*   16**2 \\\\\\n            + (buf[0][start+2] >>  4)*   16**1 \\\\\\n            + (buf[0][start+2] & 0xF)\\n        result /= 256.0\\n        result /= 100.0             # km/h\\n        return result', 'def toWindspeed_3_1(buf, start, StartOnHiNibble):\\n        \"\"\"read 3 nibbles, presentation with 1 decimal; units of m/s\"\"\"\\n        if StartOnHiNibble :\\n            hibyte = buf[0][start+0]\\n            lobyte = (buf[0][start+1] >> 4) & 0xF\\n        else:\\n            hibyte = 16*(buf[0][start+0] & 0xF) + ((buf[0][start+1] >> 4) & 0xF)\\n            lobyte = buf[0][start+1] & 0xF            \\n        if hibyte == 0xFF and lobyte == 0xE:\\n            result = CWeatherTraits.WindNP()\\n        elif hibyte == 0xFF and lobyte == 0xF:\\n            result = CWeatherTraits.WindOFL()\\n        else:\\n            result = USBHardware.toFloat_3_1(buf, start, StartOnHiNibble) # m/s\\n            result *= 3.6 # km/h\\n        return result', 'def readPressureShared(buf, start, StartOnHiNibble):\\n        return (USBHardware.toPressure_hPa_5_1(buf,start+2,1-StartOnHiNibble),\\n                USBHardware.toPressure_inHg_5_2(buf,start,StartOnHiNibble))', 'def toPressure_hPa_5_1(buf, start, StartOnHiNibble):\\n        \"\"\"read 5 nibbles, presentation with 1 decimal; units of hPa (mbar)\"\"\"\\n        if USBHardware.isErr5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.PressureNP()\\n        elif USBHardware.isOFL5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.PressureOFL()\\n        elif StartOnHiNibble :\\n            result = (buf[0][start+0] >> 4)* 1000 \\\\\\n                + (buf[0][start+0] & 0xF)* 100  \\\\\\n                + (buf[0][start+1] >>  4)*  10  \\\\\\n                + (buf[0][start+1] & 0xF)*  1   \\\\\\n                + (buf[0][start+2] >>  4)*  0.1\\n        else:\\n            result = (buf[0][start+0] & 0xF)* 1000 \\\\\\n                + (buf[0][start+1] >>  4)* 100  \\\\\\n                + (buf[0][start+1] & 0xF)*  10  \\\\\\n                + (buf[0][start+2] >>  4)*  1   \\\\\\n                + (buf[0][start+2] & 0xF)*  0.1\\n        return result', 'def toPressure_inHg_5_2(buf, start, StartOnHiNibble):\\n        \"\"\"read 5 nibbles, presentation with 2 decimals; units of inHg\"\"\"\\n        if USBHardware.isErr5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.PressureNP()\\n        elif USBHardware.isOFL5(buf, start+0, StartOnHiNibble):\\n            result = CWeatherTraits.PressureOFL()\\n        elif StartOnHiNibble :\\n            result = (buf[0][start+0] >> 4)* 100 \\\\\\n                + (buf[0][start+0] & 0xF)* 10   \\\\\\n                + (buf[0][start+1] >>  4)*  1   \\\\\\n                + (buf[0][start+1] & 0xF)*  0.1 \\\\\\n                + (buf[0][start+2] >>  4)*  0.01\\n        else:\\n            result = (buf[0][start+0] & 0xF)* 100 \\\\\\n                + (buf[0][start+1] >>  4)* 10   \\\\\\n                + (buf[0][start+1] & 0xF)*  1   \\\\\\n                + (buf[0][start+2] >>  4)*  0.1 \\\\\\n                + (buf[0][start+2] & 0xF)*  0.01\\n        return result', 'def __init__(self):\\n        self._timestamp = None\\n        self._checksum = None\\n        self._PressureRelative_hPa = CWeatherTraits.PressureNP()\\n        self._PressureRelative_hPaMinMax = CMinMaxMeasurement()\\n        self._PressureRelative_inHg = CWeatherTraits.PressureNP()\\n        self._PressureRelative_inHgMinMax = CMinMaxMeasurement()\\n        self._WindSpeed = CWeatherTraits.WindNP()\\n        self._WindDirection = EWindDirection.wdNone\\n        self._WindDirection1 = EWindDirection.wdNone\\n        self._WindDirection2 = EWindDirection.wdNone\\n        self._WindDirection3 = EWindDirection.wdNone\\n        self._WindDirection4 = EWindDirection.wdNone\\n        self._WindDirection5 = EWindDirection.wdNone\\n        self._Gust = CWeatherTraits.WindNP()\\n        self._GustMax = CMinMaxMeasurement()\\n        self._GustDirection = EWindDirection.wdNone\\n        self._GustDirection1 = EWindDirection.wdNone\\n        self._GustDirection2 = EWindDirection.wdNone\\n        self._GustDirection3 = EWindDirection.wdNone\\n        self._GustDirection4 = EWindDirection.wdNone\\n        self._GustDirection5 = EWindDirection.wdNone\\n        self._Rain1H = CWeatherTraits.RainNP()\\n        self._Rain1HMax = CMinMaxMeasurement()\\n        self._Rain24H = CWeatherTraits.RainNP()\\n        self._Rain24HMax = CMinMaxMeasurement()\\n        self._RainLastWeek = CWeatherTraits.RainNP()\\n        self._RainLastWeekMax = CMinMaxMeasurement()\\n        self._RainLastMonth = CWeatherTraits.RainNP()\\n        self._RainLastMonthMax = CMinMaxMeasurement()\\n        self._RainTotal = CWeatherTraits.RainNP()\\n        self._LastRainReset = None\\n        self._TempIndoor = CWeatherTraits.TemperatureNP()\\n        self._TempIndoorMinMax = CMinMaxMeasurement()\\n        self._TempOutdoor = CWeatherTraits.TemperatureNP()\\n        self._TempOutdoorMinMax = CMinMaxMeasurement()\\n        self._HumidityIndoor = CWeatherTraits.HumidityNP()\\n        self._HumidityIndoorMinMax = CMinMaxMeasurement()\\n        self._HumidityOutdoor = CWeatherTraits.HumidityNP()\\n        self._HumidityOutdoorMinMax = CMinMaxMeasurement()\\n        self._Dewpoint = CWeatherTraits.TemperatureNP()\\n        self._DewpointMinMax = CMinMaxMeasurement()\\n        self._Windchill = CWeatherTraits.TemperatureNP()\\n        self._WindchillMinMax = CMinMaxMeasurement()\\n        self._WeatherState = EWeatherState.WEATHER_ERR\\n        self._WeatherTendency = EWeatherTendency.TREND_ERR\\n        self._AlarmRingingFlags = 0\\n        self._AlarmMarkedFlags = 0\\n        self._PresRel_hPa_Max = 0.0\\n        self._PresRel_inHg_Max = 0.0', 'def calcChecksum(buf):\\n        return calc_checksum(buf, 6)', \"def read(self, buf):\\n        self._timestamp = int(time.time() + 0.5)\\n        self._checksum = CCurrentWeatherData.calcChecksum(buf)\\n\\n        nbuf = [0]\\n        nbuf[0] = buf[0]\\n        self._StartBytes = nbuf[0][6]*0xF + nbuf[0][7] # FIXME: what is this?\\n        self._WeatherTendency = (nbuf[0][8] >> 4) & 0xF\\n        if self._WeatherTendency > 3:\\n            self._WeatherTendency = 3 \\n        self._WeatherState = nbuf[0][8] & 0xF\\n        if self._WeatherState > 3:\\n            self._WeatherState = 3 \\n\\n        self._TempIndoorMinMax._Max._Value = USBHardware.toTemperature_5_3(nbuf, 19, 0)\\n        self._TempIndoorMinMax._Min._Value = USBHardware.toTemperature_5_3(nbuf, 22, 1)\\n        self._TempIndoor = USBHardware.toTemperature_5_3(nbuf, 24, 0)\\n        self._TempIndoorMinMax._Min._IsError = (self._TempIndoorMinMax._Min._Value == CWeatherTraits.TemperatureNP())\\n        self._TempIndoorMinMax._Min._IsOverflow = (self._TempIndoorMinMax._Min._Value == CWeatherTraits.TemperatureOFL())\\n        self._TempIndoorMinMax._Max._IsError = (self._TempIndoorMinMax._Max._Value == CWeatherTraits.TemperatureNP())\\n        self._TempIndoorMinMax._Max._IsOverflow = (self._TempIndoorMinMax._Max._Value == CWeatherTraits.TemperatureOFL())\\n        self._TempIndoorMinMax._Max._Time = None if self._TempIndoorMinMax._Max._IsError or self._TempIndoorMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 9, 0, 'TempIndoorMax')\\n        self._TempIndoorMinMax._Min._Time = None if self._TempIndoorMinMax._Min._IsError or self._TempIndoorMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 14, 0, 'TempIndoorMin')\\n\\n        self._TempOutdoorMinMax._Max._Value = USBHardware.toTemperature_5_3(nbuf, 37, 0)\\n        self._TempOutdoorMinMax._Min._Value = USBHardware.toTemperature_5_3(nbuf, 40, 1)\\n        self._TempOutdoor = USBHardware.toTemperature_5_3(nbuf, 42, 0)\\n        self._TempOutdoorMinMax._Min._IsError = (self._TempOutdoorMinMax._Min._Value == CWeatherTraits.TemperatureNP())\\n        self._TempOutdoorMinMax._Min._IsOverflow = (self._TempOutdoorMinMax._Min._Value == CWeatherTraits.TemperatureOFL())\\n        self._TempOutdoorMinMax._Max._IsError = (self._TempOutdoorMinMax._Max._Value == CWeatherTraits.TemperatureNP())\\n        self._TempOutdoorMinMax._Max._IsOverflow = (self._TempOutdoorMinMax._Max._Value == CWeatherTraits.TemperatureOFL())\\n        self._TempOutdoorMinMax._Max._Time = None if self._TempOutdoorMinMax._Max._IsError or self._TempOutdoorMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 27, 0, 'TempOutdoorMax')\\n        self._TempOutdoorMinMax._Min._Time = None if self._TempOutdoorMinMax._Min._IsError or self._TempOutdoorMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 32, 0, 'TempOutdoorMin')\\n\\n        self._WindchillMinMax._Max._Value = USBHardware.toTemperature_5_3(nbuf, 55, 0)\\n        self._WindchillMinMax._Min._Value = USBHardware.toTemperature_5_3(nbuf, 58, 1)\\n        self._Windchill = USBHardware.toTemperature_5_3(nbuf, 60, 0)\\n        self._WindchillMinMax._Min._IsError = (self._WindchillMinMax._Min._Value == CWeatherTraits.TemperatureNP())\\n        self._WindchillMinMax._Min._IsOverflow = (self._WindchillMinMax._Min._Value == CWeatherTraits.TemperatureOFL())\\n        self._WindchillMinMax._Max._IsError = (self._WindchillMinMax._Max._Value == CWeatherTraits.TemperatureNP())\\n        self._WindchillMinMax._Max._IsOverflow = (self._WindchillMinMax._Max._Value == CWeatherTraits.TemperatureOFL())\\n        self._WindchillMinMax._Max._Time = None if self._WindchillMinMax._Max._IsError or self._WindchillMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 45, 0, 'WindchillMax')\\n        self._WindchillMinMax._Min._Time = None if self._WindchillMinMax._Min._IsError or self._WindchillMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 50, 0, 'WindchillMin')\\n\\n        self._DewpointMinMax._Max._Value = USBHardware.toTemperature_5_3(nbuf, 73, 0)\\n        self._DewpointMinMax._Min._Value = USBHardware.toTemperature_5_3(nbuf, 76, 1)\\n        self._Dewpoint = USBHardware.toTemperature_5_3(nbuf, 78, 0)\\n        self._DewpointMinMax._Min._IsError = (self._DewpointMinMax._Min._Value == CWeatherTraits.TemperatureNP())\\n        self._DewpointMinMax._Min._IsOverflow = (self._DewpointMinMax._Min._Value == CWeatherTraits.TemperatureOFL())\\n        self._DewpointMinMax._Max._IsError = (self._DewpointMinMax._Max._Value == CWeatherTraits.TemperatureNP())\\n        self._DewpointMinMax._Max._IsOverflow = (self._DewpointMinMax._Max._Value == CWeatherTraits.TemperatureOFL())\\n        self._DewpointMinMax._Min._Time = None if self._DewpointMinMax._Min._IsError or self._DewpointMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 68, 0, 'DewpointMin')\\n        self._DewpointMinMax._Max._Time = None if self._DewpointMinMax._Max._IsError or self._DewpointMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 63, 0, 'DewpointMax')\\n\\n        self._HumidityIndoorMinMax._Max._Value = USBHardware.toHumidity_2_0(nbuf, 91, 1)\\n        self._HumidityIndoorMinMax._Min._Value = USBHardware.toHumidity_2_0(nbuf, 92, 1)\\n        self._HumidityIndoor = USBHardware.toHumidity_2_0(nbuf, 93, 1)\\n        self._HumidityIndoorMinMax._Min._IsError = (self._HumidityIndoorMinMax._Min._Value == CWeatherTraits.HumidityNP())\\n        self._HumidityIndoorMinMax._Min._IsOverflow = (self._HumidityIndoorMinMax._Min._Value == CWeatherTraits.HumidityOFL())\\n        self._HumidityIndoorMinMax._Max._IsError = (self._HumidityIndoorMinMax._Max._Value == CWeatherTraits.HumidityNP())\\n        self._HumidityIndoorMinMax._Max._IsOverflow = (self._HumidityIndoorMinMax._Max._Value == CWeatherTraits.HumidityOFL())\\n        self._HumidityIndoorMinMax._Max._Time = None if self._HumidityIndoorMinMax._Max._IsError or self._HumidityIndoorMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 81, 1, 'HumidityIndoorMax')\\n        self._HumidityIndoorMinMax._Min._Time = None if self._HumidityIndoorMinMax._Min._IsError or self._HumidityIndoorMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 86, 1, 'HumidityIndoorMin')\\n\\n        self._HumidityOutdoorMinMax._Max._Value = USBHardware.toHumidity_2_0(nbuf, 104, 1)\\n        self._HumidityOutdoorMinMax._Min._Value = USBHardware.toHumidity_2_0(nbuf, 105, 1)\\n        self._HumidityOutdoor = USBHardware.toHumidity_2_0(nbuf, 106, 1)\\n        self._HumidityOutdoorMinMax._Min._IsError = (self._HumidityOutdoorMinMax._Min._Value == CWeatherTraits.HumidityNP())\\n        self._HumidityOutdoorMinMax._Min._IsOverflow = (self._HumidityOutdoorMinMax._Min._Value == CWeatherTraits.HumidityOFL())\\n        self._HumidityOutdoorMinMax._Max._IsError = (self._HumidityOutdoorMinMax._Max._Value == CWeatherTraits.HumidityNP())\\n        self._HumidityOutdoorMinMax._Max._IsOverflow = (self._HumidityOutdoorMinMax._Max._Value == CWeatherTraits.HumidityOFL())\\n        self._HumidityOutdoorMinMax._Max._Time = None if self._HumidityOutdoorMinMax._Max._IsError or self._HumidityOutdoorMinMax._Max._IsOverflow else USBHardware.toDateTime(nbuf, 94, 1, 'HumidityOutdoorMax')\\n        self._HumidityOutdoorMinMax._Min._Time = None if self._HumidityOutdoorMinMax._Min._IsError or self._HumidityOutdoorMinMax._Min._IsOverflow else USBHardware.toDateTime(nbuf, 99, 1, 'HumidityOutdoorMin')\\n\\n        self._RainLastMonthMax._Max._Time = USBHardware.toDateTime(nbuf, 107, 1, 'RainLastMonthMax')\\n        self._RainLastMonthMax._Max._Value = USBHardware.toRain_6_2(nbuf, 112, 1)\\n        self._RainLastMonth = USBHardware.toRain_6_2(nbuf, 115, 1)\\n\\n        self._RainLastWeekMax._Max._Time = USBHardware.toDateTime(nbuf, 118, 1, 'RainLastWeekMax')\\n        self._RainLastWeekMax._Max._Value = USBHardware.toRain_6_2(nbuf, 123, 1)\\n        self._RainLastWeek = USBHardware.toRain_6_2(nbuf, 126, 1)\\n\\n        self._Rain24HMax._Max._Time = USBHardware.toDateTime(nbuf, 129, 1, 'Rain24HMax')\\n        self._Rain24HMax._Max._Value = USBHardware.toRain_6_2(nbuf, 134, 1)\\n        self._Rain24H = USBHardware.toRain_6_2(nbuf, 137, 1)\", 'def toLog(self):\\n        logdbg(\"_WeatherState=%s _WeatherTendency=%s _AlarmRingingFlags %04x\" % (CWeatherTraits.forecastMap[self._WeatherState], CWeatherTraits.trendMap[self._WeatherTendency], self._AlarmRingingFlags))\\n        logdbg(\"_TempIndoor=     %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._TempIndoor, self._TempIndoorMinMax._Min._Value, self._TempIndoorMinMax._Min._Time, self._TempIndoorMinMax._Max._Value, self._TempIndoorMinMax._Max._Time))\\n        logdbg(\"_HumidityIndoor= %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._HumidityIndoor, self._HumidityIndoorMinMax._Min._Value, self._HumidityIndoorMinMax._Min._Time, self._HumidityIndoorMinMax._Max._Value, self._HumidityIndoorMinMax._Max._Time))\\n        logdbg(\"_TempOutdoor=    %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._TempOutdoor, self._TempOutdoorMinMax._Min._Value, self._TempOutdoorMinMax._Min._Time, self._TempOutdoorMinMax._Max._Value, self._TempOutdoorMinMax._Max._Time))\\n        logdbg(\"_HumidityOutdoor=%8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._HumidityOutdoor, self._HumidityOutdoorMinMax._Min._Value, self._HumidityOutdoorMinMax._Min._Time, self._HumidityOutdoorMinMax._Max._Value, self._HumidityOutdoorMinMax._Max._Time))\\n        logdbg(\"_Windchill=      %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._Windchill, self._WindchillMinMax._Min._Value, self._WindchillMinMax._Min._Time, self._WindchillMinMax._Max._Value, self._WindchillMinMax._Max._Time))\\n        logdbg(\"_Dewpoint=       %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s)\" % (self._Dewpoint, self._DewpointMinMax._Min._Value, self._DewpointMinMax._Min._Time, self._DewpointMinMax._Max._Value, self._DewpointMinMax._Max._Time))\\n        logdbg(\"_WindSpeed=      %8.3f\" % self._WindSpeed)\\n        logdbg(\"_Gust=           %8.3f                                      _Max=%8.3f (%s)\" % (self._Gust, self._GustMax._Max._Value, self._GustMax._Max._Time))\\n        logdbg(\\'_WindDirection=    %3s    _GustDirection=    %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection],  CWeatherTraits.windDirMap[self._GustDirection]))\\n        logdbg(\\'_WindDirection1=   %3s    _GustDirection1=   %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection1], CWeatherTraits.windDirMap[self._GustDirection1]))\\n        logdbg(\\'_WindDirection2=   %3s    _GustDirection2=   %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection2], CWeatherTraits.windDirMap[self._GustDirection2]))\\n        logdbg(\\'_WindDirection3=   %3s    _GustDirection3=   %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection3], CWeatherTraits.windDirMap[self._GustDirection3]))\\n        logdbg(\\'_WindDirection4=   %3s    _GustDirection4=   %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection4], CWeatherTraits.windDirMap[self._GustDirection4]))\\n        logdbg(\\'_WindDirection5=   %3s    _GustDirection5=   %3s\\' % (CWeatherTraits.windDirMap[self._WindDirection5], CWeatherTraits.windDirMap[self._GustDirection5]))\\n        if (self._RainLastMonth > 0) or (self._RainLastWeek > 0):\\n            logdbg(\"_RainLastMonth=  %8.3f                                      _Max=%8.3f (%s)\" % (self._RainLastMonth, self._RainLastMonthMax._Max._Value, self._RainLastMonthMax._Max._Time))\\n            logdbg(\"_RainLastWeek=   %8.3f                                      _Max=%8.3f (%s)\" % (self._RainLastWeek, self._RainLastWeekMax._Max._Value, self._RainLastWeekMax._Max._Time))\\n        logdbg(\"_Rain24H=        %8.3f                                      _Max=%8.3f (%s)\" % (self._Rain24H, self._Rain24HMax._Max._Value, self._Rain24HMax._Max._Time))\\n        logdbg(\"_Rain1H=         %8.3f                                      _Max=%8.3f (%s)\" % (self._Rain1H, self._Rain1HMax._Max._Value, self._Rain1HMax._Max._Time))\\n        logdbg(\"_RainTotal=      %8.3f                            _LastRainReset=         (%s)\" % (self._RainTotal,  self._LastRainReset))\\n        logdbg(\"PressureRel_hPa= %8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s) \" % (self._PressureRelative_hPa, self._PressureRelative_hPaMinMax._Min._Value, self._PressureRelative_hPaMinMax._Min._Time, self._PressureRelative_hPaMinMax._Max._Value, self._PressureRelative_hPaMinMax._Max._Time))                       \\n        logdbg(\"PressureRel_inHg=%8.3f _Min=%8.3f (%s)  _Max=%8.3f (%s) \" % (self._PressureRelative_inHg, self._PressureRelative_inHgMinMax._Min._Value, self._PressureRelative_inHgMinMax._Min._Time, self._PressureRelative_inHgMinMax._Max._Value, self._PressureRelative_inHgMinMax._Max._Time))                       \\n        ###logdbg(\\'(* Bug in Weather Station: PressureRelative._Min._Time is written to location of _PressureRelative._Max._Time\\')\\n        ###logdbg(\\'Instead of PressureRelative._Min._Time we get: _PresRel_hPa_Max= %8.3f, _PresRel_inHg_max =%8.3f;\\' % (self._PresRel_hPa_Max, self._PresRel_inHg_Max))', 'def __init__(self):\\n        self._InBufCS = 0  # checksum of received config\\n        self._OutBufCS = 0 # calculated config checksum from outbuf config\\n        self._ClockMode = 0\\n        self._TemperatureFormat = 0\\n        self._PressureFormat = 0\\n        self._RainFormat = 0\\n        self._WindspeedFormat = 0\\n        self._WeatherThreshold = 0\\n        self._StormThreshold = 0\\n        self._LCDContrast = 0\\n        self._LowBatFlags = 0\\n        self._WindDirAlarmFlags = 0\\n        self._OtherAlarmFlags = 0\\n        self._ResetMinMaxFlags = 0 # output only\\n        self._HistoryInterval = 0\\n        self._TempIndoorMinMax = CMinMaxMeasurement()\\n        self._TempOutdoorMinMax = CMinMaxMeasurement()\\n        self._HumidityIndoorMinMax = CMinMaxMeasurement()\\n        self._HumidityOutdoorMinMax = CMinMaxMeasurement()\\n        self._Rain24HMax = CMinMaxMeasurement()\\n        self._GustMax = CMinMaxMeasurement()\\n        self._PressureRelative_hPaMinMax = CMinMaxMeasurement()\\n        self._PressureRelative_inHgMinMax = CMinMaxMeasurement()', \"def setHums(self,InHumLo,InHumHi,OutHumLo,OutHumHi):\\n        h1 = InHumLo\\n        h2 = InHumHi\\n        h3 = OutHumLo\\n        h4 = OutHumHi\\n        if h1 < 1 or h1 > 99 or h2 < 1 or h2 > 99 or \\\\\\n                h3 < 1 or h3 > 99 or h4 < 1 or h4 > 99:\\n            logerr('setHums: one or more values out of range')\\n            return 0\\n        self._HumidityIndoorMinMax._Min._Value = h1\\n        self._HumidityIndoorMinMax._Max._Value = h2\\n        self._HumidityOutdoorMinMax._Min._Value = h3\\n        self._HumidityOutdoorMinMax._Max._Value = h4\\n        return 1\", \"def setRain24H(self,RainFormat,Rain24hHi):\\n        f1 = RainFormat\\n        r1 = Rain24hHi \\n        if f1 not in [ERainFormat.rfMm, ERainFormat.rfInch]:\\n            logerr('setRain24: unknown format %s' % RainFormat)\\n            return 0\\n        if r1 < 0.0 or r1 > 9999.9:\\n            logerr('setRain24: value outside range')\\n            return 0\\n        self._RainFormat = f1\\n        self._Rain24HMax._Max._Value = r1\\n        return 1\", \"def setGust(self,WindSpeedFormat,GustHi):\\n        # When the units of a max gust alarm are changed in the weather\\n        # station itself, automatically the value is converted to the new\\n        # unit and rounded to a whole number.  Weewx receives a value\\n        # converted to km/h.\\n        #\\n        # It is too much trouble to sort out what exactly the internal\\n        # conversion algoritms are for the other wind units.\\n        #\\n        # Setting a value in km/h units is tested and works, so this will\\n        # be the only option available.  \\n        f1 = WindSpeedFormat\\n        g1 = GustHi\\n        if f1 < EWindspeedFormat.wfMs or f1 > EWindspeedFormat.wfMph:\\n            logerr('setGust: unknown format %s' % WindSpeedFormat)\\n            return 0\\n        if f1 != EWindspeedFormat.wfKmh:\\n            logerr('setGust: only units of km/h are supported')\\n            return 0\\n        if g1 < 0.0 or g1 > 180.0:\\n            logerr('setGust: value outside range')\\n            return 0 \\n        self._WindSpeedFormat = f1\\n        self._GustMax._Max._Value = int(g1) # apparently gust value is always an integer\\n        return 1\", \"def setPresRels(self,PressureFormat,PresRelhPaLo,PresRelhPaHi,PresRelinHgLo,PresRelinHgHi):\\n        f1 = PressureFormat\\n        p1 = PresRelhPaLo\\n        p2 = PresRelhPaHi\\n        p3 = PresRelinHgLo\\n        p4 = PresRelinHgHi\\n        if f1 not in [EPressureFormat.pfinHg, EPressureFormat.pfHPa]:\\n            logerr('setPresRel: unknown format %s' % PressureFormat)\\n            return 0\\n        if p1 < 920.0 or p1 > 1080.0 or p2 < 920.0 or p2 > 1080.0 or \\\\\\n                p3 < 27.10 or p3 > 31.90 or p4 < 27.10 or p4 > 31.90:\\n            logerr('setPresRel: value outside range')\\n            return 0\\n        self._RainFormat = f1\\n        self._PressureRelative_hPaMinMax._Min._Value = p1\\n        self._PressureRelative_hPaMinMax._Max._Value = p2\\n        self._PressureRelative_inHgMinMax._Min._Value = p3\\n        self._PressureRelative_inHgMinMax._Max._Value = p4\\n        return 1\", 'def getOutBufCS(self):\\n        return self._OutBufCS', 'def getInBufCS(self):\\n        return self._InBufCS', \"def setResetMinMaxFlags(self, resetMinMaxFlags):\\n        logdbg('setResetMinMaxFlags: %s' % resetMinMaxFlags)\\n        self._ResetMinMaxFlags = resetMinMaxFlags\", \"def parseWind_6(self, number, buf, start):\\n        '''Parse float number to 6 bytes'''\\n        num = int(number*100*256)\\n        parsebuf=[0]*6\\n        for i in xrange(0,6):\\n            parsebuf[i] = num%16\\n            num = num//16\\n        buf[0][0+start] = parsebuf[5]*16 + parsebuf[4]\\n        buf[0][1+start] = parsebuf[3]*16 + parsebuf[2]\\n        buf[0][2+start] = parsebuf[1]*16 + parsebuf[0]\", \"def parse_0(self, number, buf, start, StartOnHiNibble, numbytes):\\n        '''Parse 5-digit number with 0 decimals'''\\n        num = int(number)\\n        nbuf=[0]*5\\n        for i in xrange(5-numbytes,5):\\n            nbuf[i] = num%10\\n            num = num//10\\n        if StartOnHiNibble:\\n            buf[0][0+start] = nbuf[4]*16 + nbuf[3]\\n            buf[0][1+start] = nbuf[2]*16 + nbuf[1]\\n            buf[0][2+start] = nbuf[0]*16 + (buf[0][2+start] & 0x0F)\\n        else:\\n            buf[0][0+start] = (buf[0][0+start] & 0xF0) + nbuf[4]\\n            buf[0][1+start] = nbuf[3]*16 + nbuf[2]\\n            buf[0][2+start] = nbuf[1]*16 + nbuf[0]\", \"def parse_2(self, number, buf, start, StartOnHiNibble, numbytes):\\n        '''Parse 5 digit number with 2 decimals'''\\n        self.parse_0(number*100.0, buf, start, StartOnHiNibble, numbytes)\", \"def parse_3(self, number, buf, start, StartOnHiNibble, numbytes):\\n        '''Parse 5 digit number with 3 decimals'''\\n        self.parse_0(number*1000.0, buf, start, StartOnHiNibble, numbytes)\", \"def testConfigChanged(self,buf):\\n        nbuf = [0]\\n        nbuf[0] = buf[0]\\n        nbuf[0][0] = 16*(self._WindspeedFormat & 0xF) + 8*(self._RainFormat & 1) + 4*(self._PressureFormat & 1) + 2*(self._TemperatureFormat & 1) + (self._ClockMode & 1)\\n        nbuf[0][1] = self._WeatherThreshold & 0xF | 16 * self._StormThreshold & 0xF0\\n        nbuf[0][2] = self._LCDContrast & 0xF | 16 * self._LowBatFlags & 0xF0\\n        nbuf[0][3] = (self._OtherAlarmFlags >> 0) & 0xFF\\n        nbuf[0][4] = (self._OtherAlarmFlags >> 8) & 0xFF\\n        nbuf[0][5] = (self._WindDirAlarmFlags >> 0) & 0xFF\\n        nbuf[0][6] = (self._WindDirAlarmFlags >> 8) & 0xFF\\n        # reverse buf from here\\n        self.parse_2(self._PressureRelative_inHgMinMax._Max._Value, nbuf, 7, 1, 5)\\n        self.parse_1(self._PressureRelative_hPaMinMax._Max._Value, nbuf, 9, 0, 5)\\n        self.parse_2(self._PressureRelative_inHgMinMax._Min._Value, nbuf, 12, 1, 5)\\n        self.parse_1(self._PressureRelative_hPaMinMax._Min._Value, nbuf, 14, 0, 5)\\n        self.parseWind_6(self._GustMax._Max._Value, nbuf, 17)\\n        nbuf[0][20] = self._HistoryInterval & 0xF\\n        self.parseRain_3(self._Rain24HMax._Max._Value, nbuf, 21, 0, 7)\\n        self.parse_0(self._HumidityOutdoorMinMax._Max._Value, nbuf, 25, 1, 2)\\n        self.parse_0(self._HumidityOutdoorMinMax._Min._Value, nbuf, 26, 1, 2)\\n        self.parse_0(self._HumidityIndoorMinMax._Max._Value, nbuf, 27, 1, 2)\\n        self.parse_0(self._HumidityIndoorMinMax._Min._Value, nbuf, 28, 1, 2)\\n        self.parse_3(self._TempOutdoorMinMax._Max._Value + CWeatherTraits.TemperatureOffset(), nbuf, 29, 1, 5)\\n        self.parse_3(self._TempOutdoorMinMax._Min._Value + CWeatherTraits.TemperatureOffset(), nbuf, 31, 0, 5)\\n        self.parse_3(self._TempIndoorMinMax._Max._Value + CWeatherTraits.TemperatureOffset(), nbuf, 34, 1, 5)\\n        self.parse_3(self._TempIndoorMinMax._Min._Value + CWeatherTraits.TemperatureOffset(), nbuf, 36, 0, 5)\\n        # reverse buf to here\\n        USBHardware.reverseByteOrder(nbuf, 7, 32)\\n        # do not include the ResetMinMaxFlags bytes when calculating checksum\\n        nbuf[0][39] = (self._ResetMinMaxFlags >> 16) & 0xFF\\n        nbuf[0][40] = (self._ResetMinMaxFlags >>  8) & 0xFF\\n        nbuf[0][41] = (self._ResetMinMaxFlags >>  0) & 0xFF\\n        self._OutBufCS = calc_checksum(nbuf, 0, end=39) + 7\\n        nbuf[0][42] = (self._OutBufCS >> 8) & 0xFF\\n        nbuf[0][43] = (self._OutBufCS >> 0) & 0xFF\\n        buf[0] = nbuf[0]   \\n        if self._OutBufCS == self._InBufCS and self._ResetMinMaxFlags == 0:\\n            if DEBUG_CONFIG_DATA > 2:\\n                logdbg('testConfigChanged: checksum not changed: OutBufCS=%04x' % self._OutBufCS)\\n            changed = 0\\n        else:\\n            if DEBUG_CONFIG_DATA > 0:\\n                logdbg('testConfigChanged: checksum or resetMinMaxFlags changed: OutBufCS=%04x InBufCS=%04x _ResetMinMaxFlags=%06x' % (self._OutBufCS, self._InBufCS, self._ResetMinMaxFlags))\\n            if DEBUG_CONFIG_DATA > 1:\\n                self.toLog()\\n            changed = 1\\n        return changed\", \"def asDict(self):\\n        return {\\n            'checksum_in': self._InBufCS,\\n            'checksum_out': self._OutBufCS,\\n            'format_clock': self._ClockMode,\\n            'format_temperature': self._TemperatureFormat,\\n            'format_pressure': self._PressureFormat,\\n            'format_rain': self._RainFormat,\\n            'format_windspeed': self._WindspeedFormat,\\n            'threshold_weather': self._WeatherThreshold,\\n            'threshold_storm': self._StormThreshold,\\n            'lcd_contrast': self._LCDContrast,\\n            'low_battery_flags': self._LowBatFlags,\\n            'alarm_flags_wind_dir': self._WindDirAlarmFlags,\\n            'alarm_flags_other': self._OtherAlarmFlags,\", 'def __init__(self):\\n        self.Time = None\\n        self.TempIndoor = CWeatherTraits.TemperatureNP()\\n        self.HumidityIndoor = CWeatherTraits.HumidityNP()\\n        self.TempOutdoor = CWeatherTraits.TemperatureNP()\\n        self.HumidityOutdoor = CWeatherTraits.HumidityNP()\\n        self.PressureRelative = None\\n        self.RainCounterRaw = 0\\n        self.WindSpeed = CWeatherTraits.WindNP()\\n        self.WindDirection = EWindDirection.wdNone\\n        self.Gust = CWeatherTraits.WindNP()\\n        self.GustDirection = EWindDirection.wdNone', 'def toLog(self):\\n        \"\"\"emit raw historical data\"\"\"\\n        logdbg(\"Time              %s\"    % self.Time)\\n        logdbg(\"TempIndoor=       %7.1f\" % self.TempIndoor)\\n        logdbg(\"HumidityIndoor=   %7.0f\" % self.HumidityIndoor)\\n        logdbg(\"TempOutdoor=      %7.1f\" % self.TempOutdoor)\\n        logdbg(\"HumidityOutdoor=  %7.0f\" % self.HumidityOutdoor)\\n        logdbg(\"PressureRelative= %7.1f\" % self.PressureRelative)\\n        logdbg(\"RainCounterRaw=   %7.3f\" % self.RainCounterRaw)\\n        logdbg(\"WindSpeed=        %7.3f\" % self.WindSpeed)\\n        logdbg(\"WindDirection=    % 3s\" % CWeatherTraits.windDirMap[self.WindDirection])\\n        logdbg(\"Gust=             %7.3f\" % self.Gust)\\n        logdbg(\"GustDirection=    % 3s\" % CWeatherTraits.windDirMap[self.GustDirection])', 'def __init__(self):\\n        self.clear_records()', 'def __init__(self):\\n            self.VendorId       = 0x6666\\n            self.ProductId      = 0x5555\\n            self.VersionNo      = 1\\n            self.manufacturer   = \"LA CROSSE TECHNOLOGY\"\\n            self.product        = \"Weather Direct Light Wireless Device\"\\n            self.FrequencyStandard = EFrequency.fsUS\\n            self.Frequency      = getFrequency(self.FrequencyStandard)\\n            self.SerialNumber   = None\\n            self.DeviceID       = None', 'def __init__(self):\\n            self.LastBatteryStatus = None\\n            self.LastLinkQuality = None\\n            self.LastHistoryIndex = None\\n            self.LatestHistoryIndex = None\\n            self.last_seen_ts = None\\n            self.last_weather_ts = 0\\n            self.last_history_ts = 0\\n            self.last_config_ts = 0', 'def getFrequencyStandard(self):\\n        return self.TransceiverSettings.FrequencyStandard', 'def getDeviceID(self):\\n        return self.TransceiverSettings.DeviceID', 'def getRegisteredDeviceID(self):\\n        return self.registeredDeviceID', 'def getTransceiverPresent(self):\\n        return self.transceiverPresent', \"def setLastStatCache(self, seen_ts=None,\\n                         quality=None, battery=None,\\n                         weather_ts=None,\\n                         history_ts=None,\\n                         config_ts=None):\\n        if DEBUG_COMM > 1:\\n            logdbg('setLastStatCache: seen=%s quality=%s battery=%s weather=%s history=%s config=%s' %\\n                   (seen_ts, quality, battery, weather_ts, history_ts, config_ts))\\n        if seen_ts is not None:\\n            self.LastStat.last_seen_ts = seen_ts\\n        if quality is not None:\\n            self.LastStat.LastLinkQuality = quality\\n        if battery is not None:\\n            self.LastStat.LastBatteryStatus = battery\\n        if weather_ts is not None:\\n            self.LastStat.last_weather_ts = weather_ts\\n        if history_ts is not None:\\n            self.LastStat.last_history_ts = history_ts\\n        if config_ts is not None:\\n            self.LastStat.last_config_ts = config_ts\", 'def getLastHistoryIndex(self):\\n        return self.LastStat.LastHistoryIndex', 'def getLatestHistoryIndex(self):\\n        return self.LastStat.LatestHistoryIndex', 'def getDeviceRegistered(self):\\n        if ( self.registeredDeviceID is None\\n             or self.TransceiverSettings.DeviceID is None\\n             or self.registeredDeviceID != self.TransceiverSettings.DeviceID ):\\n            return False\\n        return True', 'def setCommModeInterval(self,val):\\n        logdbg(\"setCommModeInterval to %x\" % val)\\n        self.commModeInterval = val', 'def getTransceiverSerNo(self):\\n        return self.TransceiverSettings.SerialNumber', 'def __init__(self):\\n        self.devh = None\\n        self.timeout = 1000\\n        self.last_dump = None', 'def close(self):\\n        self._close_device()', \"def _open_device(self, dev, interface=0):\\n        self.devh = dev.open()\\n        if not self.devh:\\n            raise weewx.WeeWxIOError('Open USB device failed')\\n\\n        loginf('manufacturer: %s' % self.devh.getString(dev.iManufacturer,30))\\n        loginf('product: %s' % self.devh.getString(dev.iProduct,30))\\n        loginf('interface: %d' % interface)\\n\\n        # be sure kernel does not claim the interface\\n        try:\\n            self.devh.detachKernelDriver(interface)\\n        except Exception:\\n            pass\\n\\n        # attempt to claim the interface\\n        try:\\n            logdbg('claiming USB interface %d' % interface)\\n            self.devh.claimInterface(interface)\\n            self.devh.setAltInterface(interface)\\n        except usb.USBError, e:\\n            self._close_device()\\n            logcrt('Unable to claim USB interface %s: %s' % (interface, e))\\n            raise weewx.WeeWxIOError(e)\\n\\n        # FIXME: this seems to be specific to ws28xx?\\n        # FIXME: check return values\\n        usbWait = 0.05\\n        self.devh.getDescriptor(0x1, 0, 0x12)\\n        time.sleep(usbWait)\\n        self.devh.getDescriptor(0x2, 0, 0x9)\\n        time.sleep(usbWait)\\n        self.devh.getDescriptor(0x2, 0, 0x22)\\n        time.sleep(usbWait)\\n        self.devh.controlMsg(usb.TYPE_CLASS + usb.RECIP_INTERFACE,\\n                             0xa, [], 0x0, 0x0, 1000)\\n        time.sleep(usbWait)\\n        self.devh.getDescriptor(0x22, 0, 0x2a9)\\n        time.sleep(usbWait)\", \"def setTX(self):\\n        buf = [0]*0x15\\n        buf[0] = 0xD1\\n        if DEBUG_COMM > 1:\\n            self.dump('setTX', buf, fmt=DEBUG_DUMP_FORMAT)\\n        self.devh.controlMsg(usb.TYPE_CLASS + usb.RECIP_INTERFACE,\\n                             request=0x0000009,\\n                             buffer=buf,\\n                             value=0x00003d1,\\n                             index=0x0000000,\\n                             timeout=self.timeout)\", \"def getState(self,StateBuffer):\\n        buf = self.devh.controlMsg(requestType=usb.TYPE_CLASS |\\n                                   usb.RECIP_INTERFACE | usb.ENDPOINT_IN,\\n                                   request=usb.REQ_CLEAR_FEATURE,\\n                                   buffer=0x0a,\\n                                   value=0x00003de,\\n                                   index=0x0000000,\\n                                   timeout=self.timeout)\\n        if DEBUG_COMM > 1:\\n            self.dump('getState', buf, fmt=DEBUG_DUMP_FORMAT)\\n        StateBuffer[0]=[0]*0x2\\n        StateBuffer[0][0]=buf[1]\\n        StateBuffer[0][1]=buf[2]\", \"def setState(self,state):\\n        buf = [0]*0x15\\n        buf[0] = 0xd7\\n        buf[1] = state\\n        if DEBUG_COMM > 1:\\n            self.dump('setState', buf, fmt=DEBUG_DUMP_FORMAT)\\n        self.devh.controlMsg(usb.TYPE_CLASS + usb.RECIP_INTERFACE,\\n                             request=0x0000009,\\n                             buffer=buf,\\n                             value=0x00003d7,\\n                             index=0x0000000,\\n                             timeout=self.timeout)\", \"def getFrame(self,data,numBytes):\\n        buf = self.devh.controlMsg(requestType=usb.TYPE_CLASS |\\n                                   usb.RECIP_INTERFACE |\\n                                   usb.ENDPOINT_IN,\\n                                   request=usb.REQ_CLEAR_FEATURE,\\n                                   buffer=0x111,\\n                                   value=0x00003d6,\\n                                   index=0x0000000,\\n                                   timeout=self.timeout)\\n        new_data=[0]*0x131\\n        new_numBytes=(buf[1] << 8 | buf[2])& 0x1ff\\n        for i in xrange(0, new_numBytes):\\n            new_data[i] = buf[i+3]\\n        if DEBUG_COMM == 1:\\n            self.dump('getFrame', buf, 'short')\\n        elif DEBUG_COMM > 1:\\n            self.dump('getFrame', buf, fmt=DEBUG_DUMP_FORMAT)\\n        data[0] = new_data\\n        numBytes[0] = new_numBytes\", \"def execute(self, command):\\n        buf = [0]*0x0f #*0x15\\n        buf[0] = 0xd9\\n        buf[1] = command\\n        if DEBUG_COMM > 1:\\n            self.dump('execute', buf, fmt=DEBUG_DUMP_FORMAT)\\n        self.devh.controlMsg(usb.TYPE_CLASS + usb.RECIP_INTERFACE,\\n                             request=0x0000009,\\n                             buffer=buf,\\n                             value=0x00003d9,\\n                             index=0x0000000,\\n                             timeout=self.timeout)\", \"def dump(self, cmd, buf, fmt='auto'):\\n        strbuf = ''\\n        msglen = None\\n        if fmt == 'auto':\\n            if buf[0] in [0xd5, 0x00]:\\n                msglen = buf[2] + 3        # use msg length for set/get frame\\n            else:\\n                msglen = 16                # otherwise do same as short format\\n        elif fmt == 'short':\\n            msglen = 16\\n        for i,x in enumerate(buf):\\n            strbuf += str('%02x ' % x)\\n            if (i+1) % 16 == 0:\\n                self.dumpstr(cmd, strbuf)\\n                strbuf = ''\\n            if msglen is not None and i+1 >= msglen:\\n                break\\n        if strbuf:\\n            self.dumpstr(cmd, strbuf)\", \"def dumpstr(self, cmd, strbuf):\\n        pad = ' ' * (15-len(cmd))\\n        # de15 is idle, de14 is intermediate\\n        if strbuf in ['de 15 00 00 00 00 ','de 14 00 00 00 00 ']:\\n            if strbuf != self.last_dump or DEBUG_COMM > 2:\\n                logdbg('%s: %s%s' % (cmd, pad, strbuf))\\n            self.last_dump = strbuf\\n        else:\\n            logdbg('%s: %s%s' % (cmd, pad, strbuf))\\n            self.last_dump = None\", \"def __init__(self):\\n        logdbg('CCommunicationService.init')\\n\\n        self.shid = sHID()\\n        self.DataStore = CDataStore()\\n\\n        self.firstSleep = 1\\n        self.nextSleep = 1\\n        self.pollCount = 0\\n\\n        self.running = False\\n        self.child = None\\n        self.thread_wait = 60.0 # seconds\\n\\n        self.command = None\\n        self.history_cache = HistoryCache()\\n        # do not set time when offset to whole hour is <= _a3_offset\\n        self._a3_offset = 3\", 'def buildConfigFrame(self, Buffer):\\n        logdbg(\"buildConfigFrame\")\\n        newBuffer = [0]\\n        newBuffer[0] = [0]*48\\n        cfgBuffer = [0]\\n        cfgBuffer[0] = [0]*44\\n        changed = self.DataStore.StationConfig.testConfigChanged(cfgBuffer)\\n        if changed:\\n            self.shid.dump(\\'OutBuf\\', cfgBuffer[0], fmt=\\'long\\')\\n            newBuffer[0][0] = Buffer[0][0]\\n            newBuffer[0][1] = Buffer[0][1]\\n            newBuffer[0][2] = EAction.aSendConfig # 0x40 # change this value if we won\\'t store config\\n            newBuffer[0][3] = Buffer[0][3]\\n            for i in xrange(0,44):\\n                newBuffer[0][i+4] = cfgBuffer[0][i]\\n            Buffer[0] = newBuffer[0]\\n            Length = 48 # 0x30\\n        else: # current config not up to date; do not write yet\\n            Length = 0\\n        return Length', 'def buildACKFrame(self, Buffer, action, cs, hidx=None):\\n        if DEBUG_COMM > 1:\\n            logdbg(\"buildACKFrame: action=%x cs=%04x historyIndex=%s\" %\\n                   (action, cs, hidx))\\n        newBuffer = [0]\\n        newBuffer[0] = [0]*9\\n        for i in xrange(0,2):\\n            newBuffer[0][i] = Buffer[0][i]\\n\\n        comInt = self.DataStore.getCommModeInterval()\\n\\n        # When last weather is stale, change action to get current weather\\n        # This is only needed during long periods of history data catchup\\n        if self.command == EAction.aGetHistory:\\n            now = int(time.time())\\n            age = now - self.DataStore.LastStat.last_weather_ts\\n            # Morphing action only with GetHistory requests, \\n            # and stale data after a period of twice the CommModeInterval,\\n            # but not with init GetHistory requests (0xF0)\\n            if action == EAction.aGetHistory and age >= (comInt +1) * 2 and newBuffer[0][1] != 0xF0:\\n                if DEBUG_COMM > 0:\\n                    logdbg(\\'buildACKFrame: morphing action from %d to 5 (age=%s)\\' % (action, age))\\n                action = EAction.aGetCurrent\\n\\n        if hidx is None:\\n            if self.command == EAction.aGetHistory:\\n                hidx = self.history_cache.next_index\\n            elif self.DataStore.getLastHistoryIndex() is not None:\\n                hidx = self.DataStore.getLastHistoryIndex()\\n        if hidx is None or hidx < 0 or hidx >= WS28xxDriver.max_records:\\n            haddr = 0xffffff\\n        else:\\n            haddr = index_to_addr(hidx)\\n        if DEBUG_COMM > 1:\\n            logdbg(\\'buildACKFrame: idx: %s addr: 0x%04x\\' % (hidx, haddr))\\n\\n        newBuffer[0][2] = action & 0xF\\n        newBuffer[0][3] = (cs >> 8) & 0xFF\\n        newBuffer[0][4] = (cs >> 0) & 0xFF\\n        newBuffer[0][5] = (comInt >> 4) & 0xFF\\n        newBuffer[0][6] = (haddr >> 16) & 0x0F | 16 * (comInt & 0xF)\\n        newBuffer[0][7] = (haddr >> 8 ) & 0xFF\\n        newBuffer[0][8] = (haddr >> 0 ) & 0xFF\\n\\n        #d5 00 09 f0 f0 03 00 32 00 3f ff ff\\n        Buffer[0]=newBuffer[0]\\n        return 9', \"def handleConfig(self,Buffer,Length):\\n        logdbg('handleConfig: %s' % self.timing())\\n        if DEBUG_CONFIG_DATA > 2:\\n            self.shid.dump('InBuf', Buffer[0], fmt='long')\\n        newBuffer=[0]\\n        newBuffer[0] = Buffer[0]\\n        newLength = [0]\\n        now = int(time.time())\\n        self.DataStore.StationConfig.read(newBuffer)\\n        if DEBUG_CONFIG_DATA > 1:\\n            self.DataStore.StationConfig.toLog()\\n        self.DataStore.setLastStatCache(seen_ts=now,\\n                                        quality=(Buffer[0][3] & 0x7f), \\n                                        battery=(Buffer[0][2] & 0xf),\\n                                        config_ts=now)\\n        cs = newBuffer[0][47] | (newBuffer[0][46] << 8)\\n        self.setSleep(0.300,0.010)\\n        newLength[0] = self.buildACKFrame(newBuffer, EAction.aGetHistory, cs)\\n\\n        Buffer[0] = newBuffer[0]\\n        Length[0] = newLength[0]\", \"def handleHistoryData(self, buf, buflen):\\n        if DEBUG_HISTORY_DATA > 0:\\n            logdbg('handleHistoryData: %s' % self.timing())\\n\\n        now = int(time.time())\\n        self.DataStore.setLastStatCache(seen_ts=now,\\n                                        quality=(buf[0][3] & 0x7f),\\n                                        battery=(buf[0][2] & 0xf),\\n                                        history_ts=now)\\n\\n        newbuf = [0]\\n        newbuf[0] = buf[0]\\n        newlen = [0]\\n        data = CHistoryData()\\n        data.read(newbuf)\\n        if DEBUG_HISTORY_DATA > 1:\\n            data.toLog()\\n\\n        cs = newbuf[0][5] | (newbuf[0][4] << 8)\\n        latestAddr = bytes_to_addr(buf[0][6], buf[0][7], buf[0][8])\\n        thisAddr = bytes_to_addr(buf[0][9], buf[0][10], buf[0][11])\\n        latestIndex = addr_to_index(latestAddr)\\n        thisIndex = addr_to_index(thisAddr)\\n        ts = tstr_to_ts(str(data.Time))\\n\\n        nrec = get_index(latestIndex - thisIndex)\\n        logdbg('handleHistoryData: time=%s'\\n               ' this=%d (0x%04x) latest=%d (0x%04x) nrec=%d' %\\n               (data.Time, thisIndex, thisAddr, latestIndex, latestAddr, nrec))\\n\\n        # track the latest history index\\n        self.DataStore.setLastHistoryIndex(thisIndex)\\n        self.DataStore.setLatestHistoryIndex(latestIndex)\\n\\n        nextIndex = None\\n        if self.command == EAction.aGetHistory:\\n            if self.history_cache.start_index is None:\\n                nreq = 0\\n                if self.history_cache.num_rec > 0:\\n                    loginf('handleHistoryData: request for %s records' %\\n                           self.history_cache.num_rec)\\n                    nreq = self.history_cache.num_rec\\n                else:\\n                    loginf('handleHistoryData: request records since %s' %\\n                           weeutil.weeutil.timestamp_to_string(self.history_cache.since_ts))\\n                    span = int(time.time()) - self.history_cache.since_ts\\n                    # FIXME: what if we do not have config data yet?\\n                    cfg = self.getConfigData().asDict()\\n                    arcint = 60 * getHistoryInterval(cfg['history_interval'])\\n                    # FIXME: this assumes a constant archive interval for all\\n                    # records in the station history\\n                    nreq = int(span / arcint) + 5 # FIXME: punt 5\\n                if nreq > nrec:\\n                    loginf('handleHistoryData: too many records requested (%d)'\\n                           ', clipping to number stored (%d)' % (nreq, nrec))\\n                    nreq = nrec\\n                idx = get_index(latestIndex - nreq)\\n                self.history_cache.start_index = idx\\n                self.history_cache.next_index = idx\\n                self.DataStore.setLastHistoryIndex(idx)\\n                self.history_cache.num_outstanding_records = nreq\\n                logdbg('handleHistoryData: start_index=%s'\\n                       ' num_outstanding_records=%s' % (idx, nreq))\\n                nextIndex = idx\\n            elif self.history_cache.next_index is not None:\\n                # thisIndex should be the next record after next_index\\n                thisIndexTst = get_next_index(self.history_cache.next_index)\\n                if thisIndexTst == thisIndex:\\n                    self.history_cache.num_scanned += 1\\n                    # get the next history record\\n                    if ts is not None and self.history_cache.since_ts <= ts:\\n                        # Check if two records in a row with the same ts\\n                        if self.history_cache.last_ts == ts:\\n                            logdbg('handleHistoryData: remove previous record'\\n                                   ' with duplicate timestamp: %s' %\\n                                   weeutil.weeutil.timestamp_to_string(ts))\\n                            self.history_cache.records.pop()\\n                        self.history_cache.last_ts = ts\\n                        # append to the history\\n                        logdbg('handleHistoryData: appending history record'\\n                               ' %s: %s' % (thisIndex, data.asDict()))\\n                        self.history_cache.records.append(data.asDict())\\n                        self.history_cache.num_outstanding_records = nrec\\n                    elif ts is None:\\n                        logerr('handleHistoryData: skip record: this_ts=None')\\n                    else:\\n                        logdbg('handleHistoryData: skip record: since_ts=%s this_ts=%s' % (weeutil.weeutil.timestamp_to_string(self.history_cache.since_ts), weeutil.weeutil.timestamp_to_string(ts)))\\n                    self.history_cache.next_index = thisIndex\\n                else:\\n                    loginf('handleHistoryData: index mismatch: %s != %s' %\\n                           (thisIndexTst, thisIndex))\\n                nextIndex = self.history_cache.next_index\\n\\n        logdbg('handleHistoryData: next=%s' % nextIndex)\\n        self.setSleep(0.300,0.010)\\n        newlen[0] = self.buildACKFrame(newbuf, EAction.aGetHistory, cs, nextIndex)\\n\\n        buflen[0] = newlen[0]\\n        buf[0] = newbuf[0]\", 'def generateResponse(self, Buffer, Length):\\n        if DEBUG_COMM > 1:\\n            logdbg(\\'generateResponse: %s\\' % self.timing())\\n        newBuffer = [0]\\n        newBuffer[0] = Buffer[0]\\n        newLength = [0]\\n        newLength[0] = Length[0]\\n        if Length[0] == 0:\\n            raise BadResponse(\\'zero length buffer\\')\\n\\n        bufferID = (Buffer[0][0] <<8) | Buffer[0][1]\\n        respType = (Buffer[0][2] & 0xE0)\\n        if DEBUG_COMM > 1:\\n            logdbg(\"generateResponse: id=%04x resp=%x length=%x\" %\\n                   (bufferID, respType, Length[0]))\\n        deviceID = self.DataStore.getDeviceID()\\n        if bufferID != 0xF0F0:\\n            self.DataStore.setRegisteredDeviceID(bufferID)\\n\\n        if bufferID == 0xF0F0:\\n            loginf(\\'generateResponse: console not paired, attempting to pair to 0x%04x\\' % deviceID)\\n            newLength[0] = self.buildACKFrame(newBuffer, EAction.aGetConfig, deviceID, 0xFFFF)\\n        elif bufferID == deviceID:\\n            if respType == EResponseType.rtDataWritten:\\n                #    00000000: 00 00 06 00 32 20\\n                if Length[0] == 0x06:\\n                    self.DataStore.StationConfig.setResetMinMaxFlags(0)\\n                    self.shid.setRX()\\n                    raise DataWritten()\\n                else:\\n                    raise BadResponse(\\'len=%x resp=%x\\' % (Length[0], respType))\\n            elif respType == EResponseType.rtGetConfig:\\n                #    00000000: 00 00 30 00 32 40\\n                if Length[0] == 0x30:\\n                    self.handleConfig(newBuffer, newLength)\\n                else:\\n                    raise BadResponse(\\'len=%x resp=%x\\' % (Length[0], respType))\\n            elif respType == EResponseType.rtGetCurrentWeather:\\n                #    00000000: 00 00 d7 00 32 60\\n                if Length[0] == 0xd7: #215\\n                    self.handleCurrentData(newBuffer, newLength)\\n                else:\\n                    raise BadResponse(\\'len=%x resp=%x\\' % (Length[0], respType))\\n            elif respType == EResponseType.rtGetHistory:\\n                #    00000000: 00 00 1e 00 32 80\\n                if Length[0] == 0x1e:\\n                    self.handleHistoryData(newBuffer, newLength)\\n                else:\\n                    raise BadResponse(\\'len=%x resp=%x\\' % (Length[0], respType))\\n            elif respType == EResponseType.rtRequest:\\n                #    00000000: 00 00 06 f0 f0 a1\\n                #    00000000: 00 00 06 00 32 a3\\n                #    00000000: 00 00 06 00 32 a2\\n                if Length[0] == 0x06:\\n                    self.handleNextAction(newBuffer, newLength)\\n                else:\\n                    raise BadResponse(\\'len=%x resp=%x\\' % (Length[0], respType))\\n            else:\\n                raise BadResponse(\\'unexpected response type %x\\' % respType)\\n        elif respType not in [0x20,0x40,0x60,0x80,0xa1,0xa2,0xa3]:\\n            # message is probably corrupt\\n            raise BadResponse(\\'unknown response type %x\\' % respType)\\n        else:\\n            msg = \\'message from console contains unknown device ID (id=%04x resp=%x)\\' % (bufferID, respType)\\n            logdbg(msg)\\n            log_frame(Length[0],Buffer[0])\\n            raise BadResponse(msg)\\n\\n        Buffer[0] = newBuffer[0]\\n        Length[0] = newLength[0]', 'def initTransceiver(self, frequency_standard):\\n        logdbg(\\'initTransceiver: frequency_standard=%s\\' % frequency_standard)\\n\\n        self.DataStore.setFrequencyStandard(frequency_standard)\\n        self.configureRegisterNames()\\n\\n        # calculate the frequency then set frequency registers\\n        freq = self.DataStore.TransceiverSettings.Frequency\\n        loginf(\\'base frequency: %d\\' % freq)\\n        freqVal =  long(freq / 16000000.0 * 16777216.0)\\n        corVec = [None]\\n        self.shid.readConfigFlash(0x1F5, 4, corVec)\\n        corVal = corVec[0][0] << 8\\n        corVal |= corVec[0][1]\\n        corVal <<= 8\\n        corVal |= corVec[0][2]\\n        corVal <<= 8\\n        corVal |= corVec[0][3]\\n        loginf(\\'frequency correction: %d (0x%x)\\' % (corVal,corVal))\\n        freqVal += corVal\\n        if not (freqVal % 2):\\n            freqVal += 1\\n        loginf(\\'adjusted frequency: %d (0x%x)\\' % (freqVal,freqVal))\\n        self.reg_names[self.AX5051RegisterNames.FREQ3] = (freqVal >>24) & 0xFF\\n        self.reg_names[self.AX5051RegisterNames.FREQ2] = (freqVal >>16) & 0xFF\\n        self.reg_names[self.AX5051RegisterNames.FREQ1] = (freqVal >>8)  & 0xFF\\n        self.reg_names[self.AX5051RegisterNames.FREQ0] = (freqVal >>0)  & 0xFF\\n        logdbg(\\'frequency registers: %x %x %x %x\\' % (\\n                self.reg_names[self.AX5051RegisterNames.FREQ3],\\n                self.reg_names[self.AX5051RegisterNames.FREQ2],\\n                self.reg_names[self.AX5051RegisterNames.FREQ1],\\n                self.reg_names[self.AX5051RegisterNames.FREQ0]))\\n\\n        # figure out the transceiver id\\n        buf = [None]\\n        self.shid.readConfigFlash(0x1F9, 7, buf)\\n        tid  = buf[0][5] << 8\\n        tid += buf[0][6]\\n        loginf(\\'transceiver identifier: %d (0x%04x)\\' % (tid,tid))\\n        self.DataStore.setDeviceID(tid)\\n\\n        # figure out the transceiver serial number\\n        sn  = str(\"%02d\"%(buf[0][0]))\\n        sn += str(\"%02d\"%(buf[0][1]))\\n        sn += str(\"%02d\"%(buf[0][2]))\\n        sn += str(\"%02d\"%(buf[0][3]))\\n        sn += str(\"%02d\"%(buf[0][4]))\\n        sn += str(\"%02d\"%(buf[0][5]))\\n        sn += str(\"%02d\"%(buf[0][6]))\\n        loginf(\\'transceiver serial: %s\\' % sn)\\n        self.DataStore.setTransceiverSerNo(sn)', 'def setup(self, frequency_standard,\\n              vendor_id, product_id, device_id, serial,\\n              comm_interval=3):\\n        self.DataStore.setCommModeInterval(comm_interval)\\n        self.shid.open(vendor_id, product_id, device_id, serial)\\n        self.initTransceiver(frequency_standard)\\n        self.DataStore.setTransceiverPresent(True)', 'def getWeatherData(self):\\n        return self.DataStore.CurrentWeather', 'def getLastStat(self):\\n        return self.DataStore.LastStat', 'def getConfigData(self):\\n        return self.DataStore.StationConfig', 'def stopCachingHistory(self):\\n        self.command = None', 'def getNextHistoryIndex(self):\\n        return self.history_cache.next_index', 'def getLatestHistoryIndex(self):\\n        return self.DataStore.LastStat.LatestHistoryIndex', 'def clearHistoryCache(self):\\n        self.history_cache.clear_records()', \"def stopRFThread(self):\\n        self.running = False\\n        logdbg('stopRFThread: waiting for RF thread to terminate')\\n        self.child.join(self.thread_wait)\\n        if self.child.isAlive():\\n            logerr('unable to terminate RF thread after %d seconds' %\\n                   self.thread_wait)\\n        else:\\n            self.child = None\", \"def doRF(self):\\n        try:\\n            logdbg('setting up rf communication')\\n            self.doRFSetup()\\n            logdbg('starting rf communication')\\n            while self.running:\\n                self.doRFCommunication()\\n        except Exception, e:\\n            logerr('exception in doRF: %s' % e)\\n            if weewx.debug:\\n                log_traceback(dst=syslog.LOG_DEBUG)\\n            self.running = False\\n            raise\\n        finally:\\n            logdbg('stopping rf communication')\", 'def doRFSetup(self):\\n        self.shid.execute(5)\\n        self.shid.setPreamblePattern(0xaa)\\n        self.shid.setState(0)\\n        time.sleep(1)\\n        self.shid.setRX()\\n\\n        self.shid.setPreamblePattern(0xaa)\\n        self.shid.setState(0x1e)\\n        time.sleep(1)\\n        self.shid.setRX()\\n        self.setSleep(0.085,0.005)', 'def setSleep(self, firstsleep, nextsleep):\\n        self.firstSleep = firstsleep\\n        self.nextSleep = nextsleep']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def normalize_zdata(self,z_data,cal_z_data):\\n\\t\\treturn z_data/cal_z_data', 'def normalize_amplitude(self,z_data,cal_ampdata):\\n\\t\\treturn z_data/cal_ampdata', 'def normalize_phase(self,z_data,cal_phase):\\n\\t\\treturn z_data*np.exp(-1j*cal_phase)', 'def normalize_by_func(self,f_data,z_data,func):\\n\\t\\treturn z_data/func(f_data)', 'def _baseline_als(self,y, lam, p, niter=10):\\n\\t\\t\\'\\'\\'\\n\\t\\tsee http://zanran_storage.s3.amazonaws.com/www.science.uva.nl/ContentPages/443199618.pdf\\n\\t\\t\"Asymmetric Least Squares Smoothing\" by P. Eilers and H. Boelens in 2005.\\n\\t\\thttp://stackoverflow.com/questions/29156532/python-baseline-correction-library\\n\\t\\t\"There are two parameters: p for asymmetry and lambda for smoothness. Both have to be\\n\\t\\ttuned to the data at hand. We found that generally 0.001<=p<=0.1 is a good choice\\n\\t\\t(for a trace with positive peaks) and 10e2<=lambda<=10e9, but exceptions may occur.\"\\n\\t\\t\\'\\'\\'\\n\\t\\tL = len(y)\\n\\t\\tD = sparse.csc_matrix(np.diff(np.eye(L), 2))\\n\\t\\tw = np.ones(L)\\n\\t\\tfor i in range(niter):\\n\\t\\t\\tW = sparse.spdiags(w, 0, L, L)\\n\\t\\t\\tZ = W + lam * D.dot(D.transpose())\\n\\t\\t\\tz = sparse.linalg.spsolve(Z, w*y)\\n\\t\\t\\tw = p * (y > z) + (1-p) * (y < z)\\n\\t\\treturn z', \"def fit_baseline_amp(self,z_data,lam,p,niter=10):\\n\\t\\t'''\\n\\t\\tfor this to work, you need to analyze a large part of the baseline\\n\\t\\ttune lam and p until you get the desired result\\n\\t\\t'''\\n\\t\\treturn self._baseline_als(np.absolute(z_data),lam,p,niter=niter)\", \"def baseline_func_amp(self,z_data,f_data,lam,p,niter=10):\\n\\t\\t'''\\n\\t\\tfor this to work, you need to analyze a large part of the baseline\\n\\t\\ttune lam and p until you get the desired result\\n\\t\\treturns the baseline as a function\\n\\t\\tthe points in between the datapoints are computed by cubic interpolation\\n\\t\\t'''\\n\\t\\treturn interp1d(f_data, self._baseline_als(np.absolute(z_data),lam,p,niter=niter), kind='cubic')\", \"def baseline_func_phase(self,z_data,f_data,lam,p,niter=10):\\n\\t\\t'''\\n\\t\\tfor this to work, you need to analyze a large part of the baseline\\n\\t\\ttune lam and p until you get the desired result\\n\\t\\treturns the baseline as a function\\n\\t\\tthe points in between the datapoints are computed by cubic interpolation\\n\\t\\t'''\\n\\t\\treturn interp1d(f_data, self._baseline_als(np.angle(z_data),lam,p,niter=niter), kind='cubic')\", \"def fit_baseline_phase(self,z_data,lam,p,niter=10):\\n\\t\\t'''\\n\\t\\tfor this to work, you need to analyze a large part of the baseline\\n\\t\\ttune lam and p until you get the desired result\\n\\t\\t'''\\n\\t\\treturn self._baseline_als(np.angle(z_data),lam,p,niter=niter)\", 'def update(val):\\n\\t\\t\\tself.__lam = 10**sSmooth.val\\n\\t\\t\\tself.__p = sAsym.val\\n\\t\\t\\tself.__baseline = sbcorr.val*self._baseline_als(np.absolute(self.z_data_raw),self.__lam,self.__p,niter=niter)\\n\\t\\t\\tl0.set_ydata(np.absolute(self.z_data_raw))\\n\\t\\t\\tl0b.set_ydata(np.absolute(self.__baseline))\\n\\t\\t\\tl1.set_ydata(np.absolute(self.z_data_raw/self.__baseline))\\n\\t\\t\\tfig.canvas.draw_idle()']}, {'features': [], 'snippets': [\"def words(text): return re.findall('[a-z]+', text)\", 'def edits1(word):\\n   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\\n   deletes    = [a + b[1:] for a, b in splits if b]\\n   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\\n   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\\n   inserts    = [a + c + b     for a, b in splits for c in alphabet]\\n   return set(deletes + transposes + replaces + inserts)', 'def known(words): return set(w for w in words if w in NWORDS)']}, {'features': [], 'snippets': ['def __init__(self):\\n        self.comminfo = dict()\\n        self.init()', 'def getcash(self):\\n        return self.cash', 'def getcommissioninfo(self, data):\\n        if data._name in self.comminfo:\\n            return self.comminfo[data._name]\\n\\n        return self.comminfo[None]', 'def addcommissioninfo(self, comminfo, name=None):\\n        self.comminfo[name] = comminfo', 'def stop(self):\\n        pass', 'def getvalue(self, datas=None):\\n        pos_value = 0.0\\n        for data in datas or self.positions.keys():\\n            comminfo = self.getcommissioninfo(data)\\n            position = self.positions[data]\\n            pos_value += comminfo.getvalue(position, data.close[0])\\n\\n        return self.cash + pos_value', 'def submit(self, order):\\n        # FIXME: When an order is submitted, a margin check\\n        # requirement has to be done before it can be accepted. This implies\\n        # going over the entire list of pending orders for all datas and\\n        # existing positions, simulating order execution and ending up\\n        # with a \"cash\" figure that can be used to check the margin requirement\\n        # of the order. If not met, the order can be immediately rejected\\n        order.pannotated = None\\n        order.plen = len(order.data)\\n        order.accept()\\n        self.orders.append(order)\\n        self.pending.append(order)\\n        self.notify(order)\\n\\n        return order', 'def sell(self, owner, data,\\n             size, price=None, plimit=None,\\n             exectype=None, valid=None):\\n\\n        order = SellOrder(owner=owner, data=data,\\n                          size=size, price=price, pricelimit=plimit,\\n                          exectype=exectype, valid=valid)\\n\\n        return self.submit(order)', 'def notify(self, order):\\n        self.notifs.append(order.clone())', 'def _try_exec_close(self, order, pclose):\\n        if len(order.data) > order.plen:\\n\\n            dt0 = order.data.datetime[0]\\n\\n            if dt0 > order.dteos:\\n                if order.pannotated:\\n                    execdt = order.data.datetime[-1]\\n                    execprice = pannotated\\n                else:\\n                    execdt = dt0\\n                    execprice = pclose\\n\\n                self._execute(order, execdt, price=execprice)\\n\\n                return\\n\\n        # If no exexcution has taken place ... annotate the closing price\\n        order.pannotated = pclose', 'def _try_exec_stop(self, order, popen, phigh, plow, pcreated):\\n        if order.isbuy():\\n            if popen >= pcreated:\\n                # price penetrated with an open gap - use open\\n                self._execute(order, order.data.datetime[0], price=popen)\\n            elif phigh >= pcreated:\\n                # price penetrated during the session - use trigger price\\n                self._execute(order, order.data.datetime[0], price=pcreated)\\n\\n        else:  # Sell\\n            if popen <= pcreated:\\n                # price penetrated with an open gap - use open\\n                self._execute(order, order.data.datetime[0], price=popen)\\n            elif plow <= pcreated:\\n                # price penetrated during the session - use trigger price\\n                self._execute(order, order.data.datetime[0], price=pcreated)']}, {'features': [], 'snippets': ['def __init__ (self, canvas, objects):\\n    self.canvas = canvas\\n    self.objects = objects']}, {'features': [], 'snippets': ['def Preconditions (self, Reception):\\n        self.Start_Time = clock ()\\n        self.Next_Step  = 1\\n\\n        self.Log (\"Incoming calls test case: Setting up preconditions...\")\\n\\n        self.Log (\"Requesting a customer (caller)...\")\\n        self.Caller = Customers.request ()\\n\\n        self.Log (\"Requesting a receptionist...\")\\n        self.Receptionist = Receptionists.request ()\\n\\n        self.Log (\"Requesting a second receptionist...\")\\n        self.Receptionist_2 = Receptionists.request ()\\n\\n        self.Log (\"Requesting a customer (callee)...\")\\n        self.Callee = Customers.request ()\\n\\n        self.Log (\"Select which reception to test...\")\\n        self.Reception    = Reception\\n\\n        self.Log (\"Select a reception database connection...\")\\n        self.Reception_Database = Database_Reception (uri       = config.reception_server_uri,\\n                                                      authtoken = self.Receptionist.call_control.authtoken)', 'def Step (self,\\n              Message,\\n              Delay_In_Seconds = 0.0):\\n        if self.Next_Step is None:\\n            self.Next_Step = 1\\n        if self.Start_Time is None:\\n            self.Start_Time = clock ()\\n\\n        logging.info (\"Step \" + str (self.Next_Step) + \": \" + Message)\\n        sleep (Delay_In_Seconds)\\n        self.Next_Step += 1', 'def Caller_Places_Call (self, Number):\\n        self.Step (Message = \"Caller places call to \" + str (Number) + \"...\")\\n\\n        self.Log (Message = \"Dialling through caller agent...\")\\n        self.Caller.dial (Number)', 'def Caller_Hears_Dialtone (self):\\n        self.Step (Message = \"Caller hears dial-tone...\")\\n\\n        self.Log (Message = \"Caller agent waits for dial-tone...\")\\n        self.Caller.sip_phone.Wait_For_Dialtone ()', 'def Call_Announced (self):\\n        self.Step (Message = \"Receptionist\\'s client waits for \\'call_offer\\'...\")\\n\\n        try:\\n            self.Receptionist.event_stack.WaitFor (\"call_offer\")\\n        except TimeOutReached:\\n            logging.critical (self.Receptionist.event_stack.dump_stack ())\\n            self.fail (\"Call offer didn\\'t arrive from Call-Flow-Control.\")\\n\\n        if not self.Receptionist.event_stack.stack_contains (event_type=\"call_offer\",\\n                                                             destination=self.Reception):\\n            logging.critical (self.Receptionist.event_stack.dump_stack ())\\n            self.fail (\"The arrived call offer was not for the expected reception (destination).\")\\n\\n        return self.Receptionist.event_stack.Get_Latest_Event (Event_Type=\"call_offer\", Destination=self.Reception)[\\'call\\'][\\'id\\'],\\\\\\n               self.Receptionist.event_stack.Get_Latest_Event (Event_Type=\"call_offer\", Destination=self.Reception)[\\'call\\'][\\'reception_id\\']', 'def Call_Announced_As_Unlocked (self, Call_ID):\\n        self.Step (Message = \"Call-Flow-Control sends out \\'call_unlock\\'...\")\\n\\n        try:\\n            self.Receptionist.event_stack.WaitFor (event_type = \"call_unlock\",\\n                                                   call_id    = Call_ID)\\n        except TimeOutReached:\\n            logging.critical (self.Receptionist.event_stack.dump_stack ())\\n            self.fail (\"No \\'call_unlock\\' event arrived from Call-Flow-Control.\")\\n\\n        if not self.Receptionist.event_stack.stack_contains (event_type  = \"call_unlock\",\\n                                                             destination = self.Reception,\\n                                                             call_id     = Call_ID):\\n            logging.critical (self.Receptionist.event_stack.dump_stack ())\\n            self.fail (\"The arrived \\'call_unlock\\' event was not for the expected reception (destination).\")', 'def Offer_To_Pick_Up_Call (self, Call_Flow_Control, Call_ID):\\n        self.Step (Message = \"Client offers to answer call...\")\\n\\n        try:\\n            Call_Flow_Control.PickupCall (call_id = Call_ID)\\n        except:\\n            self.Log (Message = \"Pick-up call returned an error of some kind.\")']}, {'features': [], 'snippets': ['def __init__(self, p_str):\\n        TodoBase.__init__(self, p_str)\\n        self.attributes = {}', 'def start_date(self):\\n        \"\"\" Returns a date object of the todo\\'s start date. \"\"\"\\n        return self.get_date(config().tag_start())', 'def is_active(self):\\n        \"\"\"\\n        Returns True when the start date is today or in the past and the\\n        task has not yet been completed.\\n        \"\"\"\\n        start = self.start_date()\\n        return not self.is_completed() and (not start or start <= date.today())', 'def days_till_due(self):\\n        \"\"\"\\n        Returns the number of days till the due date. Returns a negative number\\n        of days when the due date is in the past.\\n        Returns 0 when the task has no due date.\\n        \"\"\"\\n        due = self.due_date()\\n        if due:\\n            diff = due - date.today()\\n            return diff.days\\n        return 0']}, {'features': [], 'snippets': [\"def get_organizations(self):\\n        legis = Organization('Minnesota Legislature', classification='legislature')\\n\\n        upper = Organization('Minnesota Senate', classification='upper',\\n                             parent_id=legis._id)\\n        lower = Organization('Minnesota House of Representatives',\\n                             classification='lower', parent_id=legis._id)\\n\\n        for n in range(1, 68):\\n            upper.add_post(label=str(n), role='Senator',\\n                           division_id='ocd-division/country:us/state:mn/sldu:{}'.format(n))\\n            lower.add_post(label=str(n) + 'A', role='Representative',\\n                           division_id='ocd-division/country:us/state:mn/sldl:{}a'.format(n))\\n            lower.add_post(label=str(n) + 'B', role='Representative',\\n                           division_id='ocd-division/country:us/state:mn/sldl:{}b'.format(n))\\n\\n        yield legis\\n        yield upper\\n        yield lower\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def alertPopup(title, msg):\\n    popup = Popup(title = title,\\n                      content=Label(text = msg),\\n                      size_hint=(None, None), size=(dp(600), dp(200)))\\n    popup.open()', 'def confirmPopup(title, msg, answerCallback):\\n    content = ConfirmPopup(text=msg)\\n    content.bind(on_answer=answerCallback)\\n    popup = Popup(title=title,\\n                    content=content,\\n                    size_hint=(None, None),\\n                    size=(dp(600),dp(200)),\\n                    auto_dismiss= False)\\n    popup.open()\\n    return popup', \"def __init__(self,**kwargs):\\n        self.register_event_type('on_answer')\\n        super(ConfirmPopup,self).__init__(**kwargs)\", 'def on_answer(self, *args):\\n        pass', \"def __init__(self,**kwargs):\\n        self.register_event_type('on_answer')\\n        super(EditorPopup,self).__init__(**kwargs)\", 'def on_content(self, instance, value):\\n        Clock.schedule_once(lambda dt: self.ids.content.add_widget(value))', 'def on_answer(self, *args):\\n        pass', \"def __init__(self,**kwargs):\\n        self.register_event_type('on_ok')\\n        super(OkPopup,self).__init__(**kwargs)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def scale(cur, res, num):']}, {'features': [], 'snippets': ['def _fromUtf8(s):\\r\\n        return s', 'def _translate(context, text, disambig):\\r\\n        return QtGui.QApplication.translate(context, text, disambig, _encoding)', 'def _translate(context, text, disambig):\\r\\n        return QtGui.QApplication.translate(context, text, disambig)', 'def __init__(self, parent=None, datafolder=None):\\r\\n        \"\"\"\\r\\n        Constructor\\r\\n        \"\"\"\\r\\n        QtGui.QDialog.__init__(self, parent)', 'def setupUi(self, Dialog):\\r\\n        Dialog.setObjectName(_fromUtf8(\"Dialog\"))\\r\\n        Dialog.resize(1000, 400)', 'def load_data(self):\\r\\n        print(self.datafolder)\\r\\n        self.samplefile = glob.glob(os.path.join(self.datafolder, \"*_SAMPLES.csv\"))[0]\\r\\n        if os.path.isfile(self.samplefile):\\r\\n            self.samplesdf = pd.read_csv(self.samplefile, encoding=\\'ISO-8859-1\\')\\r\\n        else:\\r\\n            print(\"File not found: \", self.samplefile)\\r\\n            self.samplesdf = None', 'def prepare_form(self, Dialog):\\r\\n        # load or reload data\\r\\n        self.load_data()', 'def ask_delete_confirm1(self, args):\\r\\n        sid = args[0]\\r\\n        Dialog = args[1]', \"def delete_confirmed(self, sid):\\r\\n        # sample file\\r\\n        filename = self.samplesdf.loc[sid, 'filename']\", 'def ask_apply_changes(self, args):\\r\\n        sid = args[0]\\r\\n        Dialog = args[1]', 'def apply_changes_confirmed(self, sid, newdata):\\r\\n        # rename files\\r\\n        newdata[\\'filename\\'] = str(newdata[\\'date\\']) + \"_\" + str(newdata[\\'samplename\\']) + \".csv\"\\r\\n        os.rename(os.path.join(self.datafolder, str(self.samplesdf.at[sid, \\'filename\\'])),\\r\\n                  os.path.join(self.datafolder, str(newdata[\\'filename\\'])))\\r\\n        os.rename(os.path.join(self.datafolder, \"rawdata\", str(self.samplesdf.at[sid, \\'filename\\'])),\\r\\n                  os.path.join(self.datafolder, \"rawdata\", str(newdata[\\'filename\\'])))', 'def update_form(self, Dialog):\\r\\n        # empty variables\\r\\n        self.edits = None\\r\\n        self.combobox = None\\r\\n        self.buttons = None\\r\\n        self.radios = None\\r\\n        self.labs = None\\r\\n        self.labels = None']}, {'features': [], 'snippets': ['def _handle(self, *args, **options):\\n        if len(args) != 2:\\n            raise CommandError(\"Need to specify (only) branch and changeset\")\\n\\n        (project, changeset) = args\\n\\n        # get reference to repo\\n        rdm = RefDataManager()\\n        repos = filter(lambda x: x[\\'name\\'] == project,\\n                       rdm.get_all_repository_info())\\n        if not repos:\\n            raise CommandError(\"No project found named \\'%s\\'\" % project)\\n        repo = repos[0]\\n\\n        # make sure all tasks are run synchronously / immediately\\n        settings.CELERY_ALWAYS_EAGER = True\\n\\n        # get hg pushlog\\n        pushlog_url = \\'%s/json-pushes/?full=1&version=2\\' % repo[\\'url\\']\\n\\n        # ingest this particular revision for this project\\n        process = HgPushlogProcess()\\n        # Use the actual push SHA, in case the changeset specified was a tag\\n        # or branch name (eg tip). HgPushlogProcess returns the full SHA, but\\n        # job ingestion expects the short version, so we truncate it.\\n        push_sha = process.run(pushlog_url, project, changeset=changeset)[0:12]\\n\\n        Builds4hJobsProcess().run(filter_to_project=project,\\n                                  filter_to_revision=push_sha,\\n                                  filter_to_job_group=options[\\'filter_job_group\\'])\\n        PendingJobsProcess().run(filter_to_project=project,\\n                                 filter_to_revision=push_sha,\\n                                 filter_to_job_group=options[\\'filter_job_group\\'])\\n        RunningJobsProcess().run(filter_to_project=project,\\n                                 filter_to_revision=push_sha,\\n                                 filter_to_job_group=options[\\'filter_job_group\\'])']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        super(IntegrationTestAnalyzers, self).setUp()\\n\\n        self.api = SuperSearchWithFields(config=self.config)\\n        self.now = datetimeutil.utc_now()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def test_data(request):\\n    data, choosers, spec, probabilities = request.param\\n    return {\\n        'data': data,\\n        'choosers': choosers,\\n        'spec': spec,\\n        'probabilities': probabilities\\n    }\", \"def choosers(test_data):\\n    filen = os.path.join(\\n        os.path.dirname(__file__), 'data', test_data['choosers'])\\n    return pd.read_csv(filen)\", \"def spec(test_data):\\n    return test_data['spec']\", 'def choosers_dm(choosers, spec):\\n    return eval_variables(spec.index, choosers)', \"def utilities(choosers_dm, spec, test_data):\\n    utils = choosers_dm.dot(spec).astype('float')\\n    return pd.DataFrame(\\n        utils.as_matrix().reshape(test_data['probabilities'].shape),\\n        columns=test_data['probabilities'].columns)\", 'def test_utils_to_probs_raises():\\n    with pytest.raises(RuntimeError):\\n        mnl.utils_to_probs(\\n            pd.DataFrame([[1, 2, np.inf, 3]]))', 'def test_make_choices_real_probs(random_seed, utilities):\\n    probs = mnl.utils_to_probs(utilities)\\n    choices = mnl.make_choices(probs)\\n\\n    pdt.assert_series_equal(\\n        choices,\\n        pd.Series([1, 2], index=[0, 1]))', \"def interaction_choosers():\\n    return pd.DataFrame({\\n        'attr': ['a', 'b', 'c', 'b']},\\n        index=['w', 'x', 'y', 'z'])\", \"def interaction_alts():\\n    return pd.DataFrame({\\n        'prop': [10, 20, 30, 40]},\\n        index=[1, 2, 3, 4])\"]}, {'features': [], 'snippets': ['def __init__(self, value):\\n        self.value = value', 'def __init__(self, exception):\\n        self.exception = exception', 'def _check_value(value):\\n    \"\"\" Return ``value``, or call its getter if ``value`` is a :class:`SpecialValue`. \"\"\"\\n    return value.get() if isinstance(value, SpecialValue) else value', 'def __new__(meta, name, bases, attrs):\\n        \"\"\" Combine the ``_slots`` dict from parent classes, and determine\\n        ``__slots__`` for them on the new class.\\n        \"\"\"\\n        base_slots = {}\\n        for base in reversed(bases):\\n            base_slots.update(getattr(base, \\'_slots\\', ()))\\n\\n        slots = dict(base_slots)\\n        slots.update(attrs.get(\\'_slots\\', ()))\\n\\n        attrs[\\'__slots__\\'] = set(slots) - set(base_slots)\\n        attrs[\\'_slots\\'] = slots\\n        return type.__new__(meta, name, bases, attrs)', 'def _compute_upper(self):\\n                for rec in self:\\n                    rec.upper = rec.name.upper() if rec.name else False', \"def _search_upper(self, operator, value):\\n                if operator == 'like':\\n                    operator = 'ilike'\\n                return [('name', operator, value)]\", \"def __init__(self, string=None, **kwargs):\\n        kwargs['string'] = string\\n        attrs = {key: val for key, val in kwargs.iteritems() if val is not None}\\n        self._attrs = attrs or EMPTY_DICT\", 'def __setattr__(self, name, value):\\n        \"\"\" Set slot or non-slot field attribute. \"\"\"\\n        try:\\n            object.__setattr__(self, name, value)\\n        except AttributeError:\\n            if self._attrs:\\n                self._attrs[name] = value\\n            else:\\n                self._attrs = {name: value}     # replace EMPTY_DICT', 'def new(self, **kwargs):\\n        \"\"\" Return a field of the same type as ``self``, with its own parameters. \"\"\"\\n        return type(self)(**kwargs)', 'def _determine_default(self, cls, name):\\n        \"\"\" Retrieve the default value for ``self`` in the hierarchy of ``cls``, and\\n            determine ``self.default`` and ``cls._defaults`` accordingly.\\n        \"\"\"\\n        self.default = None\\n\\n        # traverse the class hierarchy upwards, and take the first field\\n        # definition with a default or _defaults for self\\n        for klass in cls.__mro__:\\n            if name in klass.__dict__:\\n                field = klass.__dict__[name]\\n                if not isinstance(field, type(self)):\\n                    # klass contains another value overridden by self\\n                    return\\n\\n                if \\'default\\' in field._attrs:\\n                    # take the default in field, and adapt it for cls._defaults\\n                    value = field._attrs[\\'default\\']\\n                    if callable(value):\\n                        from openerp import api\\n                        self.default = value\\n                        cls._defaults[name] = api.model(\\n                            lambda recs: self.convert_to_write(value(recs))\\n                        )\\n                    else:\\n                        self.default = lambda recs: value\\n                        cls._defaults[name] = value\\n                    return\\n\\n            defaults = klass.__dict__.get(\\'_defaults\\') or {}\\n            if name in defaults:\\n                # take the value from _defaults, and adapt it for self.default\\n                value = defaults[name]\\n                if callable(value):\\n                    func = lambda recs: value(recs._model, recs._cr, recs._uid, recs._context)\\n                else:\\n                    func = lambda recs: value\\n                self.default = lambda recs: self.convert_to_cache(\\n                    func(recs), recs, validate=False,\\n                )\\n                cls._defaults[name] = value\\n                return', 'def __repr__(self):\\n        return \"%s.%s\" % (self.model_name, self.name)', 'def setup(self, env):\\n        \"\"\" Make sure that ``self`` is set up, except for recomputation triggers. \"\"\"\\n        if not self.setup_done:\\n            if self.related:\\n                self._setup_related(env)\\n            else:\\n                self._setup_regular(env)\\n            self.setup_done = True', 'def _setup_regular(self, env):\\n        \"\"\" Setup the attributes of a non-related field. \"\"\"\\n        recs = env[self.model_name]\\n\\n        def make_depends(deps):\\n            return tuple(deps(recs) if callable(deps) else deps)\\n\\n        # convert compute into a callable and determine depends\\n        if isinstance(self.compute, basestring):\\n            # if the compute method has been overridden, concatenate all their _depends\\n            self.depends = ()\\n            for method in resolve_all_mro(type(recs), self.compute, reverse=True):\\n                self.depends += make_depends(getattr(method, \\'_depends\\', ()))\\n            self.compute = getattr(type(recs), self.compute)\\n        else:\\n            self.depends = make_depends(getattr(self.compute, \\'_depends\\', ()))\\n\\n        # convert inverse and search into callables\\n        if isinstance(self.inverse, basestring):\\n            self.inverse = getattr(type(recs), self.inverse)\\n        if isinstance(self.search, basestring):\\n            self.search = getattr(type(recs), self.search)', 'def _setup_related(self, env):\\n        \"\"\" Setup the attributes of a related field. \"\"\"\\n        # fix the type of self.related if necessary\\n        if isinstance(self.related, basestring):\\n            self.related = tuple(self.related.split(\\'.\\'))\\n\\n        # determine the chain of fields, and make sure they are all set up\\n        recs = env[self.model_name]\\n        fields = []\\n        for name in self.related:\\n            field = recs._fields[name]\\n            field.setup(env)\\n            recs = recs[name]\\n            fields.append(field)\\n\\n        self.related_field = field\\n\\n        # check type consistency\\n        if self.type != field.type:\\n            raise Warning(\"Type of related field %s is inconsistent with %s\" % (self, field))\\n\\n        # determine dependencies, compute, inverse, and search\\n        self.depends = (\\'.\\'.join(self.related),)\\n        self.compute = self._compute_related\\n        if not (self.readonly or field.readonly):\\n            self.inverse = self._inverse_related\\n        if field._description_searchable:\\n            # allow searching on self only if the related field is searchable\\n            self.search = self._search_related\\n\\n        # copy attributes from field to self (string, help, etc.)\\n        for attr, prop in self.related_attrs:\\n            if not getattr(self, attr):\\n                setattr(self, attr, getattr(field, prop))\\n\\n        for attr, value in field._attrs.iteritems():\\n            if attr not in self._attrs:\\n                setattr(self, attr, value)\\n\\n        # special case for states: copy it only for inherited fields\\n        if not self.states and self.inherited:\\n            self.states = field.states\\n\\n        # special case for required: check if all fields are required\\n        if not self.store and not self.required:\\n            self.required = all(field.required for field in fields)', 'def _inverse_related(self, records):\\n        \"\"\" Inverse the related field ``self`` on ``records``. \"\"\"\\n        # store record values, otherwise they may be lost by cache invalidation!\\n        record_value = {record: record[self.name] for record in records}\\n        for record in records:\\n            other = record\\n            # traverse the intermediate fields, and keep at most one record\\n            for name in self.related[:-1]:\\n                other = other[name][:1]\\n            if other:\\n                other[self.related[-1]] = record_value[record]', 'def base_field(self):\\n        \"\"\" Return the base field of an inherited field, or ``self``. \"\"\"\\n        return self.related_field.base_field if self.inherited else self', 'def add_trigger(self, trigger):\\n        \"\"\" Add a recomputation trigger on ``self``. \"\"\"\\n        if trigger not in self._triggers:\\n            self._triggers += (trigger,)', 'def _setup_dependency(self, path0, model, path1):\\n        \"\"\" Make ``self`` depend on ``model``; `path0 + path1` is a dependency of\\n            ``self``, and ``path0`` is the sequence of field names from ``self.model``\\n            to ``model``.\\n        \"\"\"\\n        env = model.env\\n        head, tail = path1[0], path1[1:]\\n\\n        if head == \\'*\\':\\n            # special case: add triggers on all fields of model (except self)\\n            fields = set(model._fields.itervalues()) - set([self])\\n        else:\\n            fields = [model._fields[head]]\\n\\n        for field in fields:\\n            if field == self:\\n                _logger.debug(\"Field %s is recursively defined\", self)\\n                self.recursive = True\\n                continue\\n\\n            #_logger.debug(\"Add trigger on %s to recompute %s\", field, self)\\n            field.add_trigger((self, \\'.\\'.join(path0 or [\\'id\\'])))\\n\\n            # add trigger on inverse fields, too\\n            for invf in field.inverse_fields:\\n                #_logger.debug(\"Add trigger on %s to recompute %s\", invf, self)\\n                invf.add_trigger((self, \\'.\\'.join(path0 + [head])))\\n\\n            # recursively traverse the dependency\\n            if tail:\\n                comodel = env[field.comodel_name]\\n                self._setup_dependency(path0 + [head], comodel, tail)', 'def dependents(self):\\n        \"\"\" Return the computed fields that depend on ``self``. \"\"\"\\n        return (field for field, path in self._triggers)', 'def get_description(self, env):\\n        \"\"\" Return a dictionary that describes the field ``self``. \"\"\"\\n        desc = {\\'type\\': self.type}\\n        for attr, prop in self.description_attrs:\\n            value = getattr(self, prop)\\n            if callable(value):\\n                value = value(env)\\n            if value is not None:\\n                desc[attr] = value\\n\\n        return desc', 'def _description_searchable(self):\\n        return bool(self.store or self.search or (self.column and self.column._fnct_search))', 'def _description_sortable(self):\\n        return self.store or (self.inherited and self.related_field._description_sortable)', 'def _description_help(self, env):\\n        if self.help and env.lang:\\n            name = \"%s,%s\" % (self.model_name, self.name)\\n            trans = env[\\'ir.translation\\']._get_source(name, \\'help\\', env.lang)\\n            return trans or self.help\\n        return self.help', 'def to_column(self):\\n        \"\"\" Return a column object corresponding to ``self``, or ``None``. \"\"\"\\n        if not self.store and self.compute:\\n            # non-stored computed fields do not have a corresponding column\\n            self.column = None\\n            return None\\n\\n        # determine column parameters\\n        #_logger.debug(\"Create fields._column for Field %s\", self)\\n        args = {}\\n        for attr, prop in self.column_attrs:\\n            args[attr] = getattr(self, prop)\\n        for attr, value in self._attrs.iteritems():\\n            args[attr] = value\\n\\n        if self.company_dependent:\\n            # company-dependent fields are mapped to former property fields\\n            args[\\'type\\'] = self.type\\n            args[\\'relation\\'] = self.comodel_name\\n            self.column = fields.property(**args)\\n        elif self.column:\\n            # let the column provide a valid column for the given parameters\\n            self.column = self.column.new(_computed_field=bool(self.compute), **args)\\n        else:\\n            # create a fresh new column of the right type\\n            self.column = getattr(fields, self.type)(**args)\\n\\n        return self.column', 'def null(self, env):\\n        \"\"\" return the null value for this field in the given environment \"\"\"\\n        return False', 'def convert_to_read(self, value, use_name_get=True):\\n        \"\"\" convert ``value`` from the cache to a value as returned by method\\n            :meth:`BaseModel.read`\\n\\n            :param bool use_name_get: when True, value\\'s diplay name will\\n                be computed using :meth:`BaseModel.name_get`, if relevant\\n                for the field\\n        \"\"\"\\n        return False if value is None else value', 'def convert_to_onchange(self, value):\\n        \"\"\" convert ``value`` from the cache to a valid value for an onchange\\n            method v7.\\n        \"\"\"\\n        return self.convert_to_write(value)', 'def convert_to_display_name(self, value, record=None):\\n        \"\"\" convert ``value`` from the cache to a suitable display name. \"\"\"\\n        return ustr(value)', 'def __get__(self, record, owner):\\n        \"\"\" return the value of field ``self`` on ``record`` \"\"\"\\n        if record is None:\\n            return self         # the field is accessed through the owner class\\n\\n        if not record:\\n            # null record -> return the null value for this field\\n            return self.null(record.env)\\n\\n        # only a single record may be accessed\\n        record.ensure_one()\\n\\n        try:\\n            return record._cache[self]\\n        except KeyError:\\n            pass\\n\\n        # cache miss, retrieve value\\n        if record.id:\\n            # normal record -> read or compute value for this field\\n            self.determine_value(record)\\n        else:\\n            # draft record -> compute the value or let it be null\\n            self.determine_draft_value(record)\\n\\n        # the result should be in cache now\\n        return record._cache[self]', 'def _compute_value(self, records):\\n        \"\"\" Invoke the compute method on ``records``. \"\"\"\\n        # initialize the fields to their corresponding null value in cache\\n        for field in self.computed_fields:\\n            records._cache[field] = field.null(records.env)\\n            records.env.computed[field].update(records._ids)\\n        self.compute(records)\\n        for field in self.computed_fields:\\n            records.env.computed[field].difference_update(records._ids)', 'def determine_value(self, record):\\n        \"\"\" Determine the value of ``self`` for ``record``. \"\"\"\\n        env = record.env\\n\\n        if self.column and not (self.depends and env.in_draft):\\n            # this is a stored field or an old-style function field\\n            if self.depends:\\n                # this is a stored computed field, check for recomputation\\n                recs = record._recompute_check(self)\\n                if recs:\\n                    # recompute the value (only in cache)\\n                    self.compute_value(recs)\\n                    # HACK: if result is in the wrong cache, copy values\\n                    if recs.env != env:\\n                        for source, target in zip(recs, recs.with_env(env)):\\n                            try:\\n                                values = target._convert_to_cache({\\n                                    f.name: source[f.name] for f in self.computed_fields\\n                                }, validate=False)\\n                            except MissingError as e:\\n                                values = FailedValue(e)\\n                            target._cache.update(values)\\n                    # the result is saved to database by BaseModel.recompute()\\n                    return\\n\\n            # read the field from database\\n            record._prefetch_field(self)\\n\\n        elif self.compute:\\n            # this is either a non-stored computed field, or a stored computed\\n            # field in draft mode\\n            if self.recursive:\\n                self.compute_value(record)\\n            else:\\n                recs = record._in_cache_without(self)\\n                self.compute_value(recs)\\n\\n        else:\\n            # this is a non-stored non-computed field\\n            record._cache[self] = self.null(env)', 'def determine_inverse(self, records):\\n        \"\"\" Given the value of ``self`` on ``records``, inverse the computation. \"\"\"\\n        if self.inverse:\\n            self.inverse(records)', 'def modified(self, records):\\n        \"\"\" Notify that field ``self`` has been modified on ``records``: prepare the\\n            fields/records to recompute, and return a spec indicating what to\\n            invalidate.\\n        \"\"\"\\n        # invalidate the fields that depend on self, and prepare recomputation\\n        spec = [(self, records._ids)]\\n        for field, path in self._triggers:\\n            if path and field.store:\\n                # don\\'t move this line to function top, see log\\n                env = records.env(user=SUPERUSER_ID, context={\\'active_test\\': False})\\n                target = env[field.model_name].search([(path, \\'in\\', records.ids)])\\n                if target:\\n                    spec.append((field, target._ids))\\n                    # recompute field on target in the environment of records,\\n                    # and as user admin if required\\n                    if field.compute_sudo:\\n                        target = target.with_env(records.env(user=SUPERUSER_ID))\\n                    else:\\n                        target = target.with_env(records.env)\\n                    target._recompute_todo(field)\\n            else:\\n                spec.append((field, None))\\n\\n        return spec', 'def convert_to_cache(self, value, record, validate=True):\\n        return bool(value)', \"def convert_to_cache(self, value, record, validate=True):\\n        if isinstance(value, dict):\\n            # special case, when an integer field is used as inverse for a one2many\\n            return value.get('id', False)\\n        return int(value or 0)\", 'def _update(self, records, value):\\n        # special case, when an integer field is used as inverse for a one2many\\n        records._cache[self] = value.id or 0', 'def __init__(self, string=None, digits=None, **kwargs):\\n        super(Float, self).__init__(string=string, _digits=digits, **kwargs)', 'def digits(self):\\n        if callable(self._digits):\\n            with fields._get_cursor() as cr:\\n                return self._digits(cr)\\n        else:\\n            return self._digits', 'def _setup_regular(self, env):\\n        super(Float, self)._setup_regular(env)\\n        self._setup_digits(env)', 'def convert_to_cache(self, value, record, validate=True):\\n        # apply rounding here, otherwise value in cache may be wrong!\\n        value = float(value or 0.0)\\n        digits = self.digits\\n        return float_round(value, precision_digits=digits[1]) if digits else value', 'def _setup_regular(self, env):\\n        super(Char, self)._setup_regular(env)\\n        assert isinstance(self.size, (NoneType, int)), \\\\\\n            \"Char field %s with non-integer size %r\" % (self, self.size)', 'def convert_to_cache(self, value, record, validate=True):\\n        if value is None or value is False:\\n            return False\\n        return ustr(value)', 'def convert_to_cache(self, value, record, validate=True):\\n        if value is None or value is False:\\n            return False\\n        if validate and self.sanitize:\\n            return html_sanitize(value, strip_style=self.strip_style)\\n        return value', 'def today(*args):\\n        \"\"\" Return the current day in the format expected by the ORM.\\n            This function may be used to compute default values.\\n        \"\"\"\\n        return date.today().strftime(DATE_FORMAT)', 'def context_today(record, timestamp=None):\\n        \"\"\" Return the current date as seen in the client\\'s timezone in a format\\n            fit for date fields. This method may be used to compute default\\n            values.\\n\\n            :param datetime timestamp: optional datetime value to use instead of\\n                the current date and time (must be a datetime, regular dates\\n                can\\'t be converted between timezones.)\\n            :rtype: str\\n        \"\"\"\\n        today = timestamp or datetime.now()\\n        context_today = None\\n        tz_name = record._context.get(\\'tz\\') or record.env.user.tz\\n        if tz_name:\\n            try:\\n                today_utc = pytz.timezone(\\'UTC\\').localize(today, is_dst=False)  # UTC = no DST\\n                context_today = today_utc.astimezone(pytz.timezone(tz_name))\\n            except Exception:\\n                _logger.debug(\"failed to compute context/client-specific today date, using UTC value for `today`\",\\n                              exc_info=True)\\n        return (context_today or today).strftime(DATE_FORMAT)', 'def from_string(value):\\n        \"\"\" Convert an ORM ``value`` into a :class:`date` value. \"\"\"\\n        if not value:\\n            return None\\n        value = value[:DATE_LENGTH]\\n        return datetime.strptime(value, DATE_FORMAT).date()', 'def to_string(value):\\n        \"\"\" Convert a :class:`date` value into the format expected by the ORM. \"\"\"\\n        return value.strftime(DATE_FORMAT) if value else False', \"def convert_to_export(self, value, env):\\n        if not value:\\n            return ''\\n        return self.from_string(value) if env.context.get('export_raw_data') else ustr(value)\", 'def now(*args):\\n        \"\"\" Return the current day and time in the format expected by the ORM.\\n            This function may be used to compute default values.\\n        \"\"\"\\n        return datetime.now().strftime(DATETIME_FORMAT)', 'def context_timestamp(record, timestamp):\\n        \"\"\"Returns the given timestamp converted to the client\\'s timezone.\\n           This method is *not* meant for use as a _defaults initializer,\\n           because datetime fields are automatically converted upon\\n           display on client side. For _defaults you :meth:`fields.datetime.now`\\n           should be used instead.\\n\\n           :param datetime timestamp: naive datetime value (expressed in UTC)\\n                                      to be converted to the client timezone\\n           :rtype: datetime\\n           :return: timestamp converted to timezone-aware datetime in context\\n                    timezone\\n        \"\"\"\\n        assert isinstance(timestamp, datetime), \\'Datetime instance expected\\'\\n        tz_name = record._context.get(\\'tz\\') or record.env.user.tz\\n        utc_timestamp = pytz.utc.localize(timestamp, is_dst=False)  # UTC = no DST\\n        if tz_name:\\n            try:\\n                context_tz = pytz.timezone(tz_name)\\n                return utc_timestamp.astimezone(context_tz)\\n            except Exception:\\n                _logger.debug(\"failed to compute context/client-specific timestamp, \"\\n                              \"using the UTC value\",\\n                              exc_info=True)\\n        return utc_timestamp', 'def from_string(value):\\n        \"\"\" Convert an ORM ``value`` into a :class:`datetime` value. \"\"\"\\n        if not value:\\n            return None\\n        value = value[:DATETIME_LENGTH]\\n        if len(value) == DATE_LENGTH:\\n            value += \" 00:00:00\"\\n        return datetime.strptime(value, DATETIME_FORMAT)', 'def to_string(value):\\n        \"\"\" Convert a :class:`datetime` value into the format expected by the ORM. \"\"\"\\n        return value.strftime(DATETIME_FORMAT) if value else False', \"def convert_to_export(self, value, env):\\n        if not value:\\n            return ''\\n        return self.from_string(value) if env.context.get('export_raw_data') else ustr(value)\", 'def __init__(self, selection=None, string=None, **kwargs):\\n        if callable(selection):\\n            from openerp import api\\n            selection = api.expected(api.model, selection)\\n        super(Selection, self).__init__(selection=selection, string=string, **kwargs)', 'def _setup_related(self, env):\\n        super(Selection, self)._setup_related(env)\\n        # selection must be computed on related field\\n        field = self.related_field\\n        self.selection = lambda model: field._description_selection(model.env)', 'def _description_selection(self, env):\\n        \"\"\" return the selection list (pairs (value, label)); labels are\\n            translated according to context language\\n        \"\"\"\\n        selection = self.selection\\n        if isinstance(selection, basestring):\\n            return getattr(env[self.model_name], selection)()\\n        if callable(selection):\\n            return selection(env[self.model_name])\\n\\n        # translate selection labels\\n        if env.lang:\\n            name = \"%s,%s\" % (self.model_name, self.name)\\n            translate = partial(\\n                env[\\'ir.translation\\']._get_source, name, \\'selection\\', env.lang)\\n            return [(value, translate(label) if label else label) for value, label in selection]\\n        else:\\n            return selection', 'def _column_selection(self):\\n        if isinstance(self.selection, basestring):\\n            method = self.selection\\n            return lambda self, *a, **kw: getattr(self, method)(*a, **kw)\\n        else:\\n            return self.selection', 'def convert_to_cache(self, value, record, validate=True):\\n        if not validate:\\n            return value or False\\n        if value in self.get_values(record.env):\\n            return value\\n        elif not value:\\n            return False\\n        raise ValueError(\"Wrong value for %s: %r\" % (self, value))', 'def _setup_regular(self, env):\\n        super(Reference, self)._setup_regular(env)\\n        assert isinstance(self.size, (NoneType, int)), \\\\\\n            \"Reference field %s with non-integer size %r\" % (self, self.size)', 'def convert_to_read(self, value, use_name_get=True):\\n        return \"%s,%s\" % (value._name, value.id) if value else False', 'def convert_to_display_name(self, value, record=None):\\n        return ustr(value and value.display_name)', 'def _setup_regular(self, env):\\n        super(_Relational, self)._setup_regular(env)\\n        if self.comodel_name not in env.registry:\\n            _logger.warning(\"Field %s with unknown comodel_name %r\"\\n                            % (self, self.comodel_name))\\n            self.comodel_name = \\'_unknown\\'', \"def _related_domain(self):\\n        if callable(self.domain):\\n            # will be called with another model than self's\\n            return lambda recs: self.domain(recs.env[self.model_name])\\n        else:\\n            # maybe not correct if domain is a string...\\n            return self.domain\", 'def _description_domain(self, env):\\n        return self.domain(env[self.model_name]) if callable(self.domain) else self.domain', 'def null(self, env):\\n        return env[self.comodel_name]', 'def __init__(self, comodel_name=None, string=None, **kwargs):\\n        super(Many2one, self).__init__(comodel_name=comodel_name, string=string, **kwargs)', 'def _update(self, records, value):\\n        \"\"\" Update the cached value of ``self`` for ``records`` with ``value``. \"\"\"\\n        records._cache[self] = value', \"def convert_to_read(self, value, use_name_get=True):\\n        if use_name_get and value:\\n            # evaluate name_get() as superuser, because the visibility of a\\n            # many2one field value (id and name) depends on the current record's\\n            # access rights, and not the value's access rights.\\n            try:\\n                value_sudo = value.sudo()\\n                # performance trick: make sure that all records of the same\\n                # model as value in value.env will be prefetched in value_sudo.env\\n                value_sudo.env.prefetch[value._name].update(value.env.prefetch[value._name])\\n                return value_sudo.name_get()[0]\\n            except MissingError:\\n                # Should not happen, unless the foreign key is missing.\\n                return False\\n        else:\\n            return value.id\", 'def convert_to_onchange(self, value):\\n        return value.id', 'def convert_to_display_name(self, value, record=None):\\n        return ustr(value.display_name)', 'def __init__(self, field, record, value):\\n        self.args = (field, record, value)', 'def _update(self, records, value):\\n        \"\"\" Update the cached value of ``self`` for ``records`` with ``value``. \"\"\"\\n        for record in records:\\n            if self in record._cache:\\n                record._cache[self] = record[self.name] | value\\n            else:\\n                record._cache[self] = UnionUpdate(self, record, value)', 'def convert_to_read(self, value, use_name_get=True):\\n        return value.ids', \"def convert_to_export(self, value, env):\\n        return ','.join(name for id, name in value.name_get()) if value else ''\", 'def _compute_related(self, records):\\n        \"\"\" Compute the related field ``self`` on ``records``. \"\"\"\\n        for record in records:\\n            value = record\\n            # traverse the intermediate fields, and keep at most one record\\n            for name in self.related[:-1]:\\n                value = value[name][:1]\\n            record[self.name] = value[self.related[-1]]', 'def __init__(self, comodel_name=None, inverse_name=None, string=None, **kwargs):\\n        super(One2many, self).__init__(\\n            comodel_name=comodel_name,\\n            inverse_name=inverse_name,\\n            string=string,\\n            **kwargs\\n        )', 'def __init__(self, comodel_name=None, relation=None, column1=None, column2=None,\\n                 string=None, **kwargs):\\n        super(Many2many, self).__init__(\\n            comodel_name=comodel_name,\\n            relation=relation,\\n            column1=column1,\\n            column2=column2,\\n            string=string,\\n            **kwargs\\n        )', 'def convert_to_cache(self, value, record, validate=True):\\n        return value or {}', 'def to_column(self):\\n        self.column = fields.integer(self.string)\\n        return self.column', 'def __set__(self, record, value):\\n        raise TypeError(\"field \\'id\\' cannot be assigned\")']}, {'features': [], 'snippets': ['def _get_datasets_settings(self):\\n    return { \\n        \"nipa-section1-10101-a\": {\\n            \\'dataset_code\\': \\'nipa-section1-10101-a\\',\\n            \\'name\\': \\'Table 1.1.1. Percent Change From Preceding Period in Real Gross Domestic Product - Annually\\',\\n            \\'last_update\\': None,\\n            \\'metadata\\': {\\n                \\'filename\\': \\'nipa-section1.xls.zip\\',\\n                \\'sheet_name\\': \\'10101 Ann\\',\\n                \\'url\\': \\'http://www.bea.gov/national/nipaweb/GetCSV.asp?GetWhat=SS_Data/Section1All_xls.zip&Section=2\\'\\n            },\\n        }\\n    }', 'def _load_files(self, dataset_code):\\n        url = \"http://www.bea.gov/national/nipaweb/GetCSV.asp?GetWhat=SS_Data/Section1All_xls.zip&Section=2\"\\n        self.register_url(url, \\n                          self.DATASETS[dataset_code][\"filepath\"])', 'def test_load_datasets_first(self):\\n\\n        dataset_code = \"nipa-section1-10101-a\"\\n        self._load_files(dataset_code)\\n        self.assertLoadDatasetsFirst([dataset_code])', 'def test_load_datasets_update(self):\\n\\n        dataset_code = \"nipa-section1-10101-a\"\\n        self._load_files(dataset_code)\\n        self.assertLoadDatasetsUpdate([dataset_code])', 'def test_build_data_tree(self):\\n\\n        dataset_code = \"nipa-section1-10101-a\"\\n        self.assertDataTree(dataset_code)', 'def test_upsert_dataset_10101(self):\\n\\n        # nosetests -s -v dlstats.tests.fetchers.test_bea:FetcherTestCase.test_upsert_dataset_10101']}, {'features': [], 'snippets': ['def clean(c):\\n    \"\"\"Remove generated files\"\"\"\\n    if os.path.isdir(CONFIG[\\'deploy_path\\']):\\n        shutil.rmtree(CONFIG[\\'deploy_path\\'])\\n        os.makedirs(CONFIG[\\'deploy_path\\'])', 'def build(c):\\n    \"\"\"Build local version of site\"\"\"\\n    c.run(\\'pelican -s pelicanconf.py\\')', 'def rebuild(c):\\n    \"\"\"`build` with the delete switch\"\"\"\\n    c.run(\\'pelican -d -s pelicanconf.py\\')', 'def regenerate(c):\\n    \"\"\"Automatically regenerate site upon file modification\"\"\"\\n    c.run(\\'pelican -r -s pelicanconf.py\\')', 'def serve(c):\\n    \"\"\"Serve site at http://localhost:8000/\"\"\"\\n\\n    class AddressReuseTCPServer(RootedHTTPServer):\\n        allow_reuse_address = True\\n\\n    server = AddressReuseTCPServer(\\n        CONFIG[\\'deploy_path\\'],\\n        (\\'\\', CONFIG[\\'port\\']),\\n        ComplexHTTPRequestHandler)\\n\\n    sys.stderr.write(\\'Serving on port {port} ...\\\\n\\'.format(**CONFIG))\\n    server.serve_forever()', 'def reserve(c):\\n    \"\"\"`build`, then `serve`\"\"\"\\n    build(c)\\n    serve(c)', 'def preview(c):\\n    \"\"\"Build production version of site\"\"\"\\n    c.run(\\'pelican -s publishconf.py\\')', 'def publish(c):\\n    \"\"\"Publish to production via rsync\"\"\"\\n    c.run(\\'pelican -s publishconf.py\\')\\n    c.run(\\n        \\'rsync --delete --exclude \".DS_Store\" -pthrvz -c \\'\\n        \\'{} {production}:{dest_path}\\'.format(\\n            CONFIG[\\'deploy_path\\'].rstrip(\\'/\\') + \\'/\\',\\n            **CONFIG))']}, {'features': [], 'snippets': [\"def make_arg_parser():\\n    parser = argparse.ArgumentParser(description='This is the commandline interface for shi7_learning',\\n                                     usage='shi7_learning v{version}\\\\nshi7_learning.py -i <input> -o <output> ...'.format(version=__version__))\\n    parser.add_argument('-i', '--input', help='Set the directory path of the fastq directory OR oligos.txt if splitting', required=True)\\n    parser.add_argument('-o', '--output', help='Set the directory path of the output (default: cwd)', default=os.getcwd())\\n    parser.add_argument('--debug', help='Retain all intermediate files (default: Disabled)', dest='debug', action='store_true')\\n    parser.add_argument('-t', '--threads', help='Set the number of threads (default: %(default)s)',\\n                        default=min(multiprocessing.cpu_count(), 16))\\n    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\\n    parser.set_defaults()\\n    return parser\", 'def limit_fastq(fastq_gen, num_sequences=1000):\\n    for i in range(num_sequences):\\n        try:\\n            yield next(fastq_gen)\\n        except StopIteration:\\n            return', 'def count_num_lines(path):\\n    with open(path) as path_inf:\\n        return sum(1 for line in path_inf)', 'def check_sequence_name(path_R1, path_R2):\\n    with open(path_R1) as path_inf_R1, open(path_R2) as path_inf_R2:\\n        fastq_gen_R1 = read_fastq(path_inf_R1)\\n        fastq_gen_R2 = read_fastq(path_inf_R2)\\n        for gen_R1, gen_R2 in zip(fastq_gen_R1,fastq_gen_R2):\\n            title_R1, title_R2 = gen_R1[0], gen_R2[0]\\n            if len(title_R1) != len(title_R2):\\n                return False\\n            diff_idx = [i for i in range(len(title_R1)) if title_R1[i] != title_R2[i]]\\n            if len(diff_idx) != 1:\\n                return False\\n            if int(title_R2[diff_idx[0]]) - int(title_R1[diff_idx[0]]) != 1:\\n                return False\\n    return True', 'def get_directory_size(path):\\n    return sum([get_file_size(os.path.join(path, fastq)) for fastq in os.listdir(path)])', 'def choose_axe_adaptors(path_subsampled_fastqs, paired_end, output_path, threads):\\n    adapters = [\\'TruSeq2\\', \\'TruSeq3\\', \\'TruSeq3-2\\', \\'Nextera\\']\\n    threads = min(threads, multiprocessing.cpu_count(), 16)\\n    original_size = get_directory_size(os.path.dirname(path_subsampled_fastqs[0]))\\n    logging.info(\\'Original size of the subsampled_fastqs = \\' + str(original_size))\\n    best_size = original_size\\n    best_adap = None\\n    for adapter in adapters:\\n        if paired_end:\\n            axe_adaptors_paired_end(path_subsampled_fastqs, output_path, adapter, threads, shell=False)\\n        else:\\n            axe_adaptors_single_end(path_subsampled_fastqs, output_path, adapter, threads, shell=False)\\n        fastqs_path_size = get_directory_size(output_path)\\n        logging.info(\"Adapters: {adapter}\\\\tFile Size: {filesize}\".format(adapter=adapter, filesize=fastqs_path_size))\\n        if fastqs_path_size <= best_size:\\n            best_size = fastqs_path_size\\n            best_adap = adapter\\n\\n    if best_size < 0.995*original_size:\\n        # Actually write the best files again for use in later steps\\n        logging.info(\"Best Adapters: {adapter}\\\\tFile Size: {filesize}\".format(adapter=best_adap, filesize=best_size))\\n\\n        if paired_end:\\n            files = axe_adaptors_paired_end(path_subsampled_fastqs, output_path, best_adap, threads, shell=False)\\n        else:\\n            files = axe_adaptors_single_end(path_subsampled_fastqs, output_path, best_adap, threads, shell=False)\\n        return best_adap, best_size, files\\n    else:\\n        return None, original_size, path_subsampled_fastqs', 'def flash_check_cv(flash_output_path):\\n    hist_files = [os.path.join(flash_output_path, f) for f in os.listdir(flash_output_path) if f.endswith(\\'.hist\\')]\\n    total_cv = total_mean = 0\\n    for f in hist_files:\\n        with open(f) as inf:\\n            csv_inf = csv.reader(inf, delimiter=\"\\\\t\")\\n            x2f = 0\\n            sum = 0\\n            cnt = 0\\n            for row in csv_inf:\\n                row = [int(r) for r in row]\\n                cnt = cnt + row[1]\\n                sum = sum + row[0] * row[1]\\n                x2f = x2f + row[0] * row[0] * row[1]\\n            mean = sum/cnt\\n            std = math.sqrt((x2f - sum*sum/cnt)/(cnt-1))\\n            cv = std/mean\\n            total_cv = total_cv + cv\\n            total_mean = total_mean + mean\\n    total_files = len(hist_files)\\n    return total_cv/total_files, total_mean/total_files', 'def template_input(input):\\n    input = os.path.abspath(input)\\n    # input, input_cmd\\n    return \"input\\\\t{}\".format(input), [\"--input\", input]', 'def template_trim(filt_q, trim_q):\\n    return \"filt_q: %d, trim_q: %d\" % (filt_q, trim_q), [\"--filter_qual\", str(filt_q), \"--trim_qual\", str(trim_q)]', 'def template_output(output):\\n    # output, output_cmd\\n    output = os.path.abspath(output)\\n    return \"output\\\\t{}\".format(output), [\"--output\", output]', 'def template_flash(stitches, do_outies):\\n    return \"stitches: %s, outies: %s\" % (stitches, do_outies), [\"--flash\", str(stitches), \"--allow_outies\", str(do_outies)]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, pluginname, account=None):\\n        self.cookies = {}\\n        self.plugin = pluginname\\n        self.account = account', 'def get_cookies(self):\\n        return list(self.cookies.values())', 'def get_cookie(self, name):\\n        return self.parse_cookie(name)']}, {'features': [], 'snippets': [\"def setUpClass(cls):\\n        # Nose runs setUpClass methods even if a class decorator says to skip\\n        # the class: https://github.com/nose-devs/nose/issues/946\\n        # So, skip the test class here if we are not in the LMS.\\n        if settings.ROOT_URLCONF != 'lms.urls':\\n            raise unittest.SkipTest('Test only valid in lms')\\n\\n        super(TestReviewXBlock, cls).setUpClass()\\n\\n        # Set up for the actual course\\n        cls.course_actual = CourseFactory.create(\\n            display_name='Review_Test_Course_ACTUAL',\\n            org='DillonX',\\n            number='DAD101x',\\n            run='3T2017'\\n        )\\n        # There are multiple sections so the learner can load different\\n        # problems, but should only be shown review problems from what they have loaded\\n        with cls.store.bulk_operations(cls.course_actual.id, emit_signals=False):\\n            cls.chapter_actual = ItemFactory.create(\\n                parent=cls.course_actual, display_name='Overview'\\n            )\\n            cls.section1_actual = ItemFactory.create(\\n                parent=cls.chapter_actual, display_name='Section 1'\\n            )\\n            cls.unit1_actual = ItemFactory.create(\\n                parent=cls.section1_actual, display_name='New Unit 1'\\n            )\\n            cls.xblock1_actual = ItemFactory.create(\\n                parent=cls.unit1_actual,\\n                category='problem',\\n                display_name='Problem 1'\\n            )\\n            cls.xblock2_actual = ItemFactory.create(\\n                parent=cls.unit1_actual,\\n                category='problem',\\n                display_name='Problem 2'\\n            )\\n            cls.xblock3_actual = ItemFactory.create(\\n                parent=cls.unit1_actual,\\n                category='problem',\\n                display_name='Problem 3'\\n            )\\n            cls.xblock4_actual = ItemFactory.create(\\n                parent=cls.unit1_actual,\\n                category='problem',\\n                display_name='Problem 4'\\n            )\\n            cls.section2_actual = ItemFactory.create(\\n                parent=cls.chapter_actual, display_name='Section 2'\\n            )\\n            cls.unit2_actual = ItemFactory.create(\\n                parent=cls.section2_actual, display_name='New Unit 2'\\n            )\\n            cls.xblock5_actual = ItemFactory.create(\\n                parent=cls.unit2_actual,\\n                category='problem',\\n                display_name='Problem 5'\\n            )\\n            cls.section3_actual = ItemFactory.create(\\n                parent=cls.chapter_actual, display_name='Section 3'\\n            )\\n            cls.unit3_actual = ItemFactory.create(\\n                parent=cls.section3_actual, display_name='New Unit 3'\\n            )\\n            cls.xblock6_actual = ItemFactory.create(\\n                parent=cls.unit3_actual,\\n                category='problem',\\n                display_name='Problem 6'\\n            )\\n\\n        cls.course_actual_url = reverse(\\n            'courseware_section',\\n            kwargs={\\n                'course_id': unicode(cls.course_actual.id),\\n                'chapter': 'Overview',\\n                'section': 'Welcome',\\n            }\\n        )\\n\\n        # Set up for the review course where the review problems are hosted\\n        cls.course_review = CourseFactory.create(\\n            display_name='Review_Test_Course_REVIEW',\\n            org='DillonX',\\n            number='DAD101x_review',\\n            run='3T2017'\\n        )\\n        with cls.store.bulk_operations(cls.course_review.id, emit_signals=True):\\n            cls.chapter_review = ItemFactory.create(\\n                parent=cls.course_review, display_name='Overview'\\n            )\\n            cls.section_review = ItemFactory.create(\\n                parent=cls.chapter_review, display_name='Welcome'\\n            )\\n            cls.unit1_review = ItemFactory.create(\\n                parent=cls.section_review, display_name='New Unit 1'\\n            )\\n            cls.xblock1_review = ItemFactory.create(\\n                parent=cls.unit1_review,\\n                category='problem',\\n                display_name='Problem 1'\\n            )\\n            cls.xblock2_review = ItemFactory.create(\\n                parent=cls.unit1_review,\\n                category='problem',\\n                display_name='Problem 2'\\n            )\\n            cls.xblock3_review = ItemFactory.create(\\n                parent=cls.unit1_review,\\n                category='problem',\\n                display_name='Problem 3'\\n            )\\n            cls.xblock4_review = ItemFactory.create(\\n                parent=cls.unit1_review,\\n                category='problem',\\n                display_name='Problem 4'\\n            )\\n            cls.unit2_review = ItemFactory.create(\\n                parent=cls.section_review, display_name='New Unit 2'\\n            )\\n            cls.xblock5_review = ItemFactory.create(\\n                parent=cls.unit2_review,\\n                category='problem',\\n                display_name='Problem 5'\\n            )\\n            cls.unit3_review = ItemFactory.create(\\n                parent=cls.section_review, display_name='New Unit 3'\\n            )\\n            cls.xblock6_review = ItemFactory.create(\\n                parent=cls.unit3_review,\\n                category='problem',\\n                display_name='Problem 6'\\n            )\\n\\n        cls.course_review_url = reverse(\\n            'courseware_section',\\n            kwargs={\\n                'course_id': unicode(cls.course_review.id),\\n                'chapter': 'Overview',\\n                'section': 'Welcome',\\n            }\\n        )\", 'def enroll_student(self, email, password, course):\\n        \"\"\"\\n        Student login and enroll for the course\\n        \"\"\"\\n        self.login(email, password)\\n        self.enroll(course, verify=True)', 'def test_no_review_problems(self):\\n        \"\"\"\\n        If a user has not seen any problems, they should\\n        receive a response to go out and try more problems so they have\\n        material to review.\\n        \"\"\"\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_actual)\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_review)\\n\\n        with self.store.bulk_operations(self.course_actual.id, emit_signals=False):\\n            review_section_actual = ItemFactory.create(\\n                parent=self.chapter_actual, display_name=\\'Review Subsection\\'\\n            )\\n            review_unit_actual = ItemFactory.create(\\n                parent=review_section_actual, display_name=\\'Review Unit\\'\\n            )\\n\\n            review_xblock_actual = ItemFactory.create(  # pylint: disable=unused-variable\\n                parent=review_unit_actual,\\n                category=\\'review\\',\\n                display_name=\\'Review Tool\\'\\n            )\\n\\n        # Loading the review section\\n        response = self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': review_section_actual.location.name,\\n            }\\n        ))\\n\\n        expected_h2 = \\'Nothing to review\\'\\n        self.assertIn(expected_h2, response.content)', 'def test_too_few_review_problems(self, num_desired):\\n        \"\"\"\\n        If a user does not have enough problems to review, they should\\n        receive a response to go out and try more problems so they have\\n        material to review.\\n\\n        Testing loading 4 problems and asking for 5 and then loading every\\n        problem and asking for more than that.\\n        \"\"\"\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_actual)\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_review)\\n\\n        # Want to load fewer problems than num_desired\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section1_actual.location.name,\\n            }\\n        ))\\n        if num_desired > 6:\\n            self.client.get(reverse(\\n                \\'courseware_section\\',\\n                kwargs={\\n                    \\'course_id\\': self.course_actual.id,\\n                    \\'chapter\\': self.chapter_actual.location.name,\\n                    \\'section\\': self.section2_actual.location.name,\\n                }\\n            ))\\n            self.client.get(reverse(\\n                \\'courseware_section\\',\\n                kwargs={\\n                    \\'course_id\\': self.course_actual.id,\\n                    \\'chapter\\': self.chapter_actual.location.name,\\n                    \\'section\\': self.section3_actual.location.name,\\n                }\\n            ))\\n\\n        with self.store.bulk_operations(self.course_actual.id, emit_signals=False):\\n            review_section_actual = ItemFactory.create(\\n                parent=self.chapter_actual, display_name=\\'Review Subsection\\'\\n            )\\n            review_unit_actual = ItemFactory.create(\\n                parent=review_section_actual, display_name=\\'Review Unit\\'\\n            )\\n\\n            review_xblock_actual = ItemFactory.create(  # pylint: disable=unused-variable\\n                parent=review_unit_actual,\\n                category=\\'review\\',\\n                display_name=\\'Review Tool\\',\\n                num_desired=num_desired\\n            )\\n\\n        # Loading the review section\\n        response = self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': review_section_actual.location.name,\\n            }\\n        ))\\n\\n        expected_h2 = \\'Nothing to review\\'\\n\\n        self.assertIn(expected_h2, response.content)', 'def test_review_problems(self, num_desired):\\n        \"\"\"\\n        If a user has enough problems to review, they should\\n        receive a response where there are review problems for them to try.\\n        \"\"\"\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_actual)\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_review)\\n\\n        # Loading problems so the learner has enough problems in the CSM\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section1_actual.location.name,\\n            }\\n        ))\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section2_actual.location.name,\\n            }\\n        ))\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section3_actual.location.name,\\n            }\\n        ))\\n\\n        with self.store.bulk_operations(self.course_actual.id, emit_signals=False):\\n            review_section_actual = ItemFactory.create(\\n                parent=self.chapter_actual, display_name=\\'Review Subsection\\'\\n            )\\n            review_unit_actual = ItemFactory.create(\\n                parent=review_section_actual, display_name=\\'Review Unit\\'\\n            )\\n\\n            review_xblock_actual = ItemFactory.create(  # pylint: disable=unused-variable\\n                parent=review_unit_actual,\\n                category=\\'review\\',\\n                display_name=\\'Review Tool\\',\\n                num_desired=num_desired\\n            )\\n\\n        # Loading the review section\\n        response = self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': review_section_actual.location.name,\\n            }\\n        ))\\n\\n        expected_header_text = \\'Review Problems\\'\\n        # The problems are defaulted to correct upon load\\n        # This happens because the problems \"raw_possible\" field is 0 and the\\n        # \"raw_earned\" field is also 0.\\n        expected_correctness_text = \\'correct\\'\\n        expected_problems = [\\'Review Problem 1\\', \\'Review Problem 2\\', \\'Review Problem 3\\',\\n                             \\'Review Problem 4\\', \\'Review Problem 5\\', \\'Review Problem 6\\']\\n\\n        self.assertIn(expected_header_text, response.content)\\n        self.assertEqual(response.content.count(expected_correctness_text), num_desired)\\n        # Since the problems are randomly selected, we have to check\\n        # the correct number of problems are returned.\\n        count = 0\\n        for problem in expected_problems:\\n            if problem in response.content:\\n                count += 1\\n        self.assertEqual(count, num_desired)\\n        self.assertEqual(response.content.count(self.URL_BEGINNING), num_desired)', 'def test_review_problem_urls(self, num_desired):\\n        \"\"\"\\n        Verify that the URLs returned from the Review xBlock are valid and\\n        correct URLs for the problems the learner has seen.\\n        \"\"\"\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_actual)\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_review)\\n\\n        # Loading problems so the learner has enough problems in the CSM\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section1_actual.location.name,\\n            }\\n        ))\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section2_actual.location.name,\\n            }\\n        ))\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section3_actual.location.name,\\n            }\\n        ))\\n\\n        user = User.objects.get(email=self.STUDENTS[0][\\'email\\'])\\n        crum.set_current_user(user)\\n        result_urls = get_review_ids.get_problems(num_desired, self.course_actual.id)\\n\\n        expected_urls = [\\n            (self.URL_BEGINNING + \\'problem+block@Problem_1\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_2\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_3\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_4\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_5\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_6\\', True, 0)\\n        ]\\n\\n        # Since the problems are randomly selected, we have to check\\n        # the correct number of urls are returned.\\n        count = 0\\n        for url in expected_urls:\\n            if url in result_urls:\\n                count += 1\\n        self.assertEqual(count, num_desired)', 'def test_review_problem_urls_unique_problem(self, num_desired):\\n        \"\"\"\\n        Verify that the URLs returned from the Review xBlock are valid and\\n        correct URLs for the problems the learner has seen. This test will give\\n        a unique problem to a learner and verify only that learner sees\\n        it as a review. It will also ensure that if a learner has not loaded a\\n        problem, it should never show up as a review problem\\n        \"\"\"\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_actual)\\n        self.enroll_student(self.STUDENTS[0][\\'email\\'], self.STUDENTS[0][\\'password\\'], self.course_review)\\n\\n        # Loading problems so the learner has enough problems in the CSM\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section1_actual.location.name,\\n            }\\n        ))\\n        self.client.get(reverse(\\n            \\'courseware_section\\',\\n            kwargs={\\n                \\'course_id\\': self.course_actual.id,\\n                \\'chapter\\': self.chapter_actual.location.name,\\n                \\'section\\': self.section3_actual.location.name,\\n            }\\n        ))\\n\\n        user = User.objects.get(email=self.STUDENTS[0][\\'email\\'])\\n        crum.set_current_user(user)\\n        result_urls = get_review_ids.get_problems(num_desired, self.course_actual.id)\\n\\n        expected_urls = [\\n            (self.URL_BEGINNING + \\'problem+block@Problem_1\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_2\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_3\\', True, 0),\\n            (self.URL_BEGINNING + \\'problem+block@Problem_4\\', True, 0),\\n            # This is the unique problem when num_desired == 5\\n            (self.URL_BEGINNING + \\'problem+block@Problem_6\\', True, 0)\\n        ]\\n        expected_not_loaded_problem = (self.URL_BEGINNING + \\'problem+block@Problem_5\\', True, 0)\\n\\n        # Since the problems are randomly selected, we have to check\\n        # the correct number of urls are returned.\\n        count = 0\\n        for url in expected_urls:\\n            if url in result_urls:\\n                count += 1\\n        self.assertEqual(count, num_desired)\\n        self.assertNotIn(expected_not_loaded_problem, result_urls)']}, {'features': [], 'snippets': ['def _action_compute_lines(self, cr, uid, ids, properties=None, context=None):\\n        \"\"\" Computes bills of material of a product.\\n        @param properties: List containing dictionaries of properties.\\n        @return: No. of products.\\n        \"\"\"\\n        if properties is None:\\n            properties = []\\n        results = []\\n        bom_obj = self.pool.get(\\'mrp.bom\\')\\n        uom_obj = self.pool.get(\\'product.uom\\')\\n        prod_line_obj = self.pool.get(\\'mrp.production.product.line\\')\\n        workcenter_line_obj = self.pool.get(\\'mrp.production.workcenter.line\\')\\n        for production in self.browse(cr, uid, ids):\\n            #unlink product_lines\\n            prod_line_obj.unlink(cr, SUPERUSER_ID, [line.id for line in production.product_lines], context=context)', 'def action_ready(self, cr, uid, ids, context=None):\\n        \"\"\" Changes the production state to Ready and location id of stock move.\\n        @return: True\\n        \"\"\"\\n        move_obj = self.pool.get(\\'stock.move\\')\\n        self.write(cr, uid, ids, {\\'state\\': \\'ready\\'})\\n\\n        for production in self.browse(cr, uid, ids, context=context):            \\n            if not production.bom_id:\\n                produce_move_id = self._make_production_produce_line(cr, uid, production, context=context)\\n\\n        for (production_id,name) in self.name_get(cr, uid, ids):\\n            production = self.browse(cr, uid, production_id)\\n            if production.move_prod_id and production.move_prod_id.location_id.id != production.location_dest_id.id:\\n                move_obj.write(cr, uid, [production.move_prod_id.id],\\n                        {\\'location_id\\': production.location_dest_id.id})\\n        return True']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def minute_to_hm(minute):\\n    if isinstance(minute, int):\\n        return \"%d:%02d\" % (divmod(minute, 60))\\n    return None']}, {'features': [], 'snippets': ['def error(status, message=\"\"):\\n    if message:\\n        data = json.dumps(dict(message=message))\\n    else:\\n        data=\"\"\\n    return make_response(data, status)', 'def dequeue():\\n    try:\\n        dequeue_task()\\n    except Exception as e:\\n        return make_response(dumps(dict(status=e.message)), 202)\\n\\n    return make_response(dumps(dict(status=\"ok\")), 202)', 'def post_election():\\n    \\'\\'\\'\\n    POST /election\\n\\n    Creates an election, with the given input data. This involves communicating\\n    with the different election authorities to generate the joint public key.\\n\\n    Example request:\\n    POST /election\\n    {\\n      \"id\": 1110,\\n      \"title\": \"Votación de candidatos\",\\n      \"description\": \"Selecciona los documentos polí\\xadtico, ético y organizativo con los que Podemos\",\\n      \"director\": \"wadobo-auth1\",\\n      \"authorities\": \"openkratio-authority\",\\n      \"layout\": \"pcandidates-election\",\\n      \"presentation\": {\\n        \"share_text\": \"lo que sea\",\\n        \"theme\": \"foo\",\\n        \"urls\": [\\n          {\\n            \"title\": \"\",\\n            \"url\": \"\"\\n          }\\n        ],\\n        \"theme_css\": \"whatever\"\\n      },\\n      \"end_date\": \"2013-12-09T18:17:14.457000\",\\n      \"start_date\": \"2013-12-06T18:17:14.457000\",\\n      \"questions\": [\\n          {\\n              \"description\": \"\",\\n              \"layout\": \"pcandidates-election\",\\n              \"max\": 1,\\n              \"min\": 0,\\n              \"num_winners\": 1,\\n              \"title\": \"Secretarí\\xada General\",\\n              \"randomize_answer_order\": true,\\n              \"tally_type\": \"plurality-at-large\",\\n              \"answer_total_votes_percentage\": \"over-total-valid-votes\",\\n              \"answers\": [\\n                {\\n                  \"id\": 0,\\n                  \"category\": \"Equipo de Enfermeras\",\\n                  \"details\": \"\",\\n                  \"sort_order\": 1,\\n                  \"urls\": [\\n                    {\\n                      \"title\": \"\",\\n                      \"url\": \"\"\\n                    }\\n                  ],\\n                  \"text\": \"Fulanita de tal\",\\n                }\\n              ]\\n          }\\n      ],\\n      \"authorities\": [\\n        {\\n          \"name\": \"Asociación Sugus GNU/Linux\",\\n          \"orchestra_url\": \"https://sugus.eii.us.es/orchestra\",\\n          \"ssl_cert\": \"-----BEGIN CERTIFICATE-----\\\\nMIIFATCCA+mgAwIBAgIQAOli4NZQEWpKZeYX25jjwDANBgkqhkiG9w0BAQUFADBz\\\\n8YOltJ6QfO7jNHU9jh/AxeiRf6MibZn6fvBHvFCrVBvDD43M0gdhMkVEDVNkPaak\\\\nC7AHA/waXZ2EwW57Chr2hlZWAkwkFvsWxNt9BgJAJJt4CIVhN/iau/SaXD0l0t1N\\\\nT0ye54QPYl38Eumvc439Yd1CeVS/HYbP0ISIfpNkkFA5TiQdoA==\\\\n-----END CERTIFICATE-----\"\\n        },\\n        {\\n          \"name\": \"Agora Ciudadana\",\\n          \"orchestra_url\": \"https://agoravoting.com:6874/orchestra\",\\n          \"ssl_cert\": \"-----BEGIN CERTIFICATE-----\\\\nMIIFATCCA+mgAwIBAgIQAOli4NZQEWpKZeYX25jjwDANBgkqhkiG9w0BAQUFADBz\\\\n8YOltJ6QfO7jNHU9jh/AxeiRf6MibZn6fvBHvFCrVBvDD43M0gdhMkVEDVNkPaak\\\\nC7AHA/waXZ2EwW57Chr2hlZWAkwkFvsWxNt9BgJAJJt4CIVhN/iau/SaXD0l0t1N\\\\nT0ye54QPYl38Eumvc439Yd1CeVS/HYbP0ISIfpNkkFA5TiQdoA==\\\\n-----END CERTIFICATE-----\"\\n        },\\n        {\\n          \"name\": \"Wadobo Labs\",\\n          \"orchestra_url\": \"https://wadobo.com:6874/orchestra\",\\n          \"ssl_cert\": \"-----BEGIN CERTIFICATE-----\\\\nMIIFATCCA+mgAwIBAgIQAOli4NZQEWpKZeYX25jjwDANBgkqhkiG9w0BAQUFADBz\\\\n8YOltJ6QfO7jNHU9jh/AxeiRf6MibZn6fvBHvFCrVBvDD43M0gdhMkVEDVNkPaak\\\\nC7AHA/waXZ2EwW57Chr2hlZWAkwkFvsWxNt9BgJAJJt4CIVhN/iau/SaXD0l0t1N\\\\nT0ye54QPYl38Eumvc439Yd1CeVS/HYbP0ISIfpNkkFA5TiQdoA==\\\\n-----END CERTIFICATE-----\"\\n        }\\n      ]\\n    }\\n\\n\\n    On success, response is empty with status 202 Accepted and returns something\\n    like:\\n\\n    {\\n        \"task_id\": \"ba83ee09-aa83-1901-bb11-e645b52fc558\",\\n    }\\n    When the election finally gets processed, the callback_url is called with a\\n    POST containing the protInfo.xml file generated jointly by each\\n    authority, following this example response:\\n\\n    {\\n        \"status\": \"finished\",\\n        \"reference\": {\\n            \"election_id\": \"d9e5ee09-03fa-4890-aa83-2fc558e645b5\",\\n            \"action\": \"POST /election\"\\n        },\\n        \"session_data\": [{\\n            \"session_id\": \"deadbeef-03fa-4890-aa83-2fc558e645b5\",\\n            \"publickey\": [\"<pubkey codified in hexadecimal>\"]\\n        }]\\n    }\\n\\n    Note that this protInfo.xml will contain the election public key, but\\n    also some other information. In particular, it\\'s worth noting that\\n    the http and hint servers\\' urls for each authority could change later,\\n    if election-orchestra needs it.\\n\\n    If there was an error, then the callback will be called following this\\n    example format:\\n\\n    {\\n        \"status\": \"error\",\\n        \"reference\": {\\n            \"session_id\": \"d9e5ee09-03fa-4890-aa83-2fc558e645b5\",\\n            \"action\": \"POST /election\"\\n        },\\n        \"data\": {\\n            \"message\": \"error message\"\\n        }\\n    }\\n    \\'\\'\\'\\n\\n    data = request.get_json(force=True, silent=True)\\n    d = base64.b64encode(pickle.dumps(data)).decode(\\'utf-8\\')\\n    queueid = queue_task(task=\\'election\\', data=d)\\n\\n    return make_response(dumps(dict(queue_id=queueid)), 202)', 'def post_tally():\\n    \\'\\'\\'\\n    POST /tally\\n\\n    Tallies an election, with the given input data. This involves communicating\\n    with the different election authorities to do the tally.\\n\\n    Example request:\\n    POST /tally\\n    {\\n        \"election_id\": 111,\\n        \"callback_url\": \"https://127.0.0.1:5000/public_api/receive_tally\",\\n        \"votes_url\": \"https://127.0.0.1:5000/public_data/vota4/encrypted_ciphertexts\",\\n        \"votes_hash\": \"ni:///sha-256;f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk\"\\n    }\\n\\n    On success, response is empty with status 202 Accepted and returns something\\n    like:\\n\\n    {\\n        \"task_id\": \"ba83ee09-aa83-1901-bb11-e645b52fc558\",\\n    }\\n\\n    When the election finally gets processed, the callback_url is called with POST\\n    similar to the following example:\\n\\n    {\\n        \"status\": \"finished\",\\n        \"reference\": {\\n            \"election_id\": \"d9e5ee09-03fa-4890-aa83-2fc558e645b5\",\\n            \"action\": \"POST /tally\"\\n        },\\n        \"data\": {\\n            \"votes_url\": \"https://127.0.0.1:5000/public_data/vota4/tally.tar.bz2\",\\n            \"votes_hash\": \"ni:///sha-256;f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk\"\\n        }\\n    }\\n\\n    If there was an error, then the callback will be called following this\\n    example format:\\n\\n    {\\n        \"status\": \"error\",\\n        \"reference\": {\\n            \"election_id\": \"d9e5ee09-03fa-4890-aa83-2fc558e645b5\",\\n            \"action\": \"POST /tally\"\\n        },\\n        \"data\": {\\n            \"message\": \"error message\"\\n        }\\n    }\\n    \\'\\'\\'\\n\\n    # first of all, parse input data\\n    data = request.get_json(force=True, silent=True)\\n    d = base64.b64encode(pickle.dumps(data)).decode(\\'utf-8\\')\\n    queueid = queue_task(task=\\'tally\\', data=d)\\n    return make_response(dumps(dict(queue_id=queueid)), 202)', 'def receive_election():\\n    \\'\\'\\'\\n    This is a test route to be able to test that callbacks are correctly sent\\n    \\'\\'\\'\\n    print(\"ATTENTION received election callback: \")\\n    print(request.get_json(force=True, silent=True))\\n    return make_response(\"\", 202)']}, {'features': [], 'snippets': [\"def forwards(self, orm):\\n        # Adding field 'UserProject.drive_auth'\\n        db.add_column(u'user_project', 'drive_auth',\\n                      self.gf('django.db.models.fields.BooleanField')(default=False),\\n                      keep_default=False)\"]}, {'features': [], 'snippets': ['def __init__(self):\\n        self.rebus_controller = None  # type: Optional[RebusController]', \"def send_config_change_event(msg, error=EventError.ErrorTypes.NO_ERROR, pubsub=INJECTED):\\n        # type: (str, Dict[str, Any], PubSub) -> None\\n        event = EsafeEvent(EsafeEvent.Types.CONFIG_CHANGE, {'type': 'apartment', 'msg': msg}, error=error)\\n        pubsub.publish_esafe_event(PubSub.EsafeTopics.CONFIG, event)\", 'def load_apartment(apartment_id):\\n        # type: (int) -> Optional[ApartmentDTO]\\n        apartment_orm = Apartment.select().where(Apartment.id == apartment_id).first()\\n        if apartment_orm is None:\\n            return None\\n        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)\\n        return apartment_dto', 'def load_apartment_by_mailbox_id(mailbox_id):\\n        # type: (int) -> Optional[ApartmentDTO]\\n        apartment_orm = Apartment.select().where(Apartment.mailbox_rebus_id == mailbox_id).first()\\n        if apartment_orm is None:\\n            return None\\n        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)\\n        return apartment_dto', 'def load_apartment_by_doorbell_id(doorbell_id):\\n        # type: (int) -> Optional[ApartmentDTO]\\n        apartment_orm = Apartment.select().where(Apartment.doorbell_rebus_id == doorbell_id).first()\\n        if apartment_orm is None:\\n            return None\\n        apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)\\n        return apartment_dto', 'def load_apartments():\\n        # type: () -> List[ApartmentDTO]\\n        apartments = []\\n        for apartment_orm in Apartment.select():\\n            apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)\\n            apartments.append(apartment_dto)\\n        return apartments', 'def get_apartment_count():\\n        # type: () -> int\\n        return Apartment.select().count()', 'def apartment_id_exists(apartment_id):\\n        # type: (int) -> bool\\n        apartments = ApartmentController.load_apartments()\\n        ids = (x.id for x in apartments)\\n        return apartment_id in ids', \"def save_apartment(self, apartment_dto, send_event=True):\\n        # type: (ApartmentDTO, bool) -> ApartmentDTO\\n        self._check_rebus_ids(apartment_dto)\\n        apartment_orm = ApartmentMapper.dto_to_orm(apartment_dto)\\n        apartment_orm.save()\\n        if send_event:\\n            ApartmentController.send_config_change_event('save')\\n        return ApartmentMapper.orm_to_dto(apartment_orm)\", \"def update_apartment(self, apartment_dto, send_event=True):\\n        # type: (ApartmentDTO, bool) -> ApartmentDTO\\n        self._check_rebus_ids(apartment_dto)\\n        if 'id' not in apartment_dto.loaded_fields or apartment_dto.id is None:\\n            raise RuntimeError('cannot update an apartment without the id being set')\\n        try:\\n            apartment_orm = Apartment.get_by_id(apartment_dto.id)\\n            loaded_apartment_dto = ApartmentMapper.orm_to_dto(apartment_orm)\\n            for field in apartment_dto.loaded_fields:\\n                if field == 'id':\\n                    continue\\n                if hasattr(apartment_dto, field):\\n                    setattr(loaded_apartment_dto, field, getattr(apartment_dto, field))\\n            apartment_orm = ApartmentMapper.dto_to_orm(loaded_apartment_dto)\\n            apartment_orm.save()\\n            if send_event:\\n                ApartmentController.send_config_change_event('update')\\n            return ApartmentMapper.orm_to_dto(apartment_orm)\\n        except Exception as e:\\n            raise RuntimeError('Could not update the user: {}'.format(e))\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def default_error_handler(ec, *args):\\n        return ValueError(_mpv_error_string(ec).decode('utf-8'), ec, *args)\", 'def raise_for_ec(kls, ec, func, *args):\\n        ec = 0 if ec > 0 else ec\\n        ex = kls.EXCEPTION_DICT.get(ec , kls.default_error_handler)\\n        if ex:\\n            raise ex(ec, *args)', 'def __eq__(self, other):\\n        return self is other or self.value == other or self.value == int(other)', \"def __repr__(self):\\n        return ['NONE', 'SHUTDOWN', 'LOG_MESSAGE', 'GET_PROPERTY_REPLY', 'SET_PROPERTY_REPLY', 'COMMAND_REPLY',\\n                'START_FILE', 'END_FILE', 'FILE_LOADED', 'TRACKS_CHANGED', 'TRACK_SWITCHED', 'IDLE', 'PAUSE', 'UNPAUSE',\\n                'TICK', 'SCRIPT_INPUT_DISPATCH', 'CLIENT_MESSAGE', 'VIDEO_RECONFIG', 'AUDIO_RECONFIG',\\n                'METADATA_UPDATE', 'SEEK', 'PLAYBACK_RESTART', 'PROPERTY_CHANGE', 'CHAPTER_CHANGE'][self.value]\", 'def array_value(self, decode_str=False):\\n        return [ self.values[i].node_value(decode_str) for i in range(self.num) ]', 'def node_value(self, decode_str=False):\\n        return MpvNode.node_cast_value(byref(c_void_p(self.val)), self.format.value, decode_str)', \"def node_cast_value(v, fmt, decode_str=False):\\n        dwrap = lambda s: s.decode('utf-8') if decode_str else s\\n        return {\\n            MpvFormat.NONE:         lambda v: None,\\n            MpvFormat.STRING:       lambda v: dwrap(cast(v, POINTER(c_char_p)).contents.value),\\n            MpvFormat.OSD_STRING:   lambda v: cast(v, POINTER(c_char_p)).contents.value.decode('utf-8'),\\n            MpvFormat.FLAG:         lambda v: bool(cast(v, POINTER(c_int)).contents.value),\\n            MpvFormat.INT64:        lambda v: cast(v, POINTER(c_longlong)).contents.value,\\n            MpvFormat.DOUBLE:       lambda v: cast(v, POINTER(c_double)).contents.value,\\n            MpvFormat.NODE:         lambda v: cast(v, POINTER(MpvNode)).contents.node_value(decode_str),\\n            MpvFormat.NODE_ARRAY:   lambda v: cast(v, POINTER(POINTER(MpvNodeList))).contents.contents.array_value(decode_str),\\n            MpvFormat.NODE_MAP:     lambda v: cast(v, POINTER(POINTER(MpvNodeList))).contents.contents.dict_value(decode_str),\\n            MpvFormat.BYTE_ARRAY:   lambda v: cast(v, POINTER(c_char_p)).contents.value,\\n            }[fmt](v)\", \"def as_dict(self):\\n        dtype = {MpvEventID.END_FILE:               MpvEventEndFile,\\n                MpvEventID.PROPERTY_CHANGE:         MpvEventProperty,\\n                MpvEventID.GET_PROPERTY_REPLY:      MpvEventProperty,\\n                MpvEventID.LOG_MESSAGE:             MpvEventLogMessage,\\n                MpvEventID.SCRIPT_INPUT_DISPATCH:   MpvEventScriptInputDispatch,\\n                MpvEventID.CLIENT_MESSAGE:          MpvEventClientMessage\\n            }.get(self.event_id.value, None)\\n        return {'event_id': self.event_id.value,\\n                'error': self.error,\\n                'reply_userdata': self.reply_userdata,\\n                'event': cast(self.data, POINTER(dtype)).contents.as_dict() if dtype else None}\", \"def as_dict(self):\\n        if self.format.value == MpvFormat.STRING:\\n            proptype, _access = ALL_PROPERTIES.get(self.name, (str, None))\\n            return {'name': self.name.decode('utf-8'),\\n                    'format': self.format,\\n                    'data': self.data,\\n                    'value': proptype(cast(self.data, POINTER(c_char_p)).contents.value.decode('utf-8'))}\\n        else:\\n            return {'name': self.name.decode('utf-8'),\\n                    'format': self.format,\\n                    'data': self.data}\", \"def as_dict(self):\\n        return { 'prefix': self.prefix.decode('utf-8'),\\n                 'level':  self.level.decode('utf-8'),\\n                 'text':   self.text.decode('utf-8').rstrip() }\", \"def as_dict(self):\\n        return {'reason': self.value}\", 'def as_dict(self):\\n        pass # TODO', \"def as_dict(self):\\n        return { 'args': [ self.args[i].decode('utf-8') for i in range(self.num_args) ] }\", \"def _handle_func(name, args, restype, errcheck, ctx=MpvHandle):\\n    func = getattr(backend, name)\\n    func.argtypes = [ctx] + args if ctx else args\\n    if restype is not None:\\n        func.restype = restype\\n    if errcheck is not None:\\n        func.errcheck = errcheck\\n    globals()['_'+name] = func\", \"def notnull_errcheck(res, func, *args):\\n    if res is None:\\n        raise RuntimeError('Underspecified error in MPV when calling {} with args {!r}: NULL pointer returned.'\\\\\\n                'Please consult your local debugger.'.format(func.__name__, args))\\n    return res\", 'def _handle_gl_func(name, args=[], restype=None):\\n    _handle_func(name, args, restype, errcheck=None, ctx=MpvOpenGLCbContext)', 'def _mpv_client_api_version():\\n    ver = backend.mpv_client_api_version()\\n    return ver>>16, ver&0xFFFF', \"def _ensure_encoding(possibly_bytes):\\n    return possibly_bytes.decode('utf-8') if type(possibly_bytes) is bytes else possibly_bytes\", 'def load_lua():\\n    \"\"\" Use this function if you intend to use mpv\\'s built-in lua interpreter. This is e.g. needed for playback of\\n    youtube urls. \"\"\"\\n    CDLL(\\'liblua.so\\', mode=RTLD_GLOBAL)', 'def __init__(self, *extra_mpv_flags, log_handler=None, start_event_thread=True, **extra_mpv_opts):\\n        \"\"\" Create an MPV instance.\\n\\n        Extra arguments and extra keyword arguments will be passed to mpv as options. \"\"\"\\n\\n        self._event_thread = None\\n        self.handle = _mpv_create()\\n\\n        _mpv_set_option_string(self.handle, b\\'audio-display\\', b\\'no\\')\\n        istr = lambda o: (\\'yes\\' if o else \\'no\\') if type(o) is bool else str(o)\\n        try:\\n            for flag in extra_mpv_flags:\\n                _mpv_set_option_string(self.handle, flag.encode(\\'utf-8\\'), b\\'\\')\\n            for k,v in extra_mpv_opts.items():\\n                _mpv_set_option_string(self.handle, k.replace(\\'_\\', \\'-\\').encode(\\'utf-8\\'), istr(v).encode(\\'utf-8\\'))\\n        except AttributeError as e:\\n            _mpv_initialize(self.handle)\\n            raise e\\n        _mpv_initialize(self.handle)\\n\\n        self._event_callbacks = []\\n        self._property_handlers = collections.defaultdict(lambda: [])\\n        self._message_handlers = {}\\n        self._key_binding_handlers = {}\\n        self._playback_cond = threading.Condition()\\n        self._event_handle = _mpv_create_client(self.handle, b\\'py_event_handler\\')\\n        self._loop = partial(_event_loop, self._event_handle, self._playback_cond, self._event_callbacks,\\n                self._message_handlers, self._property_handlers, log_handler)\\n        if start_event_thread:\\n            self._event_thread = threading.Thread(target=self._loop, name=\\'MPVEventHandlerThread\\')\\n            self._event_thread.setDaemon(True)\\n            self._event_thread.start()\\n        else:\\n            self._event_thread = None\\n\\n        if log_handler is not None:\\n            self.set_loglevel(\\'terminal-default\\')', \"def wait_for_property(self, name, cond=lambda val: val, level_sensitive=True):\\n        sema = threading.Semaphore(value=0)\\n        def observer(val):\\n            if cond(val):\\n                sema.release()\\n        self.observe_property(name, observer)\\n        if not level_sensitive or not cond(getattr(self, name.replace('-', '_'))):\\n            sema.acquire()\\n        self.unobserve_property(name, observer)\", 'def terminate(self):\\n        self.handle, handle = None, self.handle\\n        if threading.current_thread() is self._event_thread:\\n            # Handle special case to allow event handle to be detached.\\n            # This is necessary since otherwise the event thread would deadlock itself.\\n            grim_reaper = threading.Thread(target=lambda: _mpv_terminate_destroy(handle))\\n            grim_reaper.start()\\n        else:\\n            _mpv_terminate_destroy(handle)\\n            if self._event_thread:\\n                self._event_thread.join()', 'def command(self, name, *args):\\n        \"\"\" Execute a raw command \"\"\"\\n        args = [name.encode(\\'utf-8\\')] + [ (arg if type(arg) is bytes else str(arg).encode(\\'utf-8\\'))\\n                for arg in args if arg is not None ] + [None]\\n        _mpv_command(self.handle, (c_char_p*len(args))(*args))', \"def revert_seek(self):\\n        self.command('revert_seek');\", \"def frame_back_step(self):\\n        self.command('frame_back_step')\", \"def _cycle_property(self, name, direction='up'):\\n        self.command('cycle_property', name, direction)\", \"def screenshot(self, includes='subtitles', mode='single'):\\n        self.command('screenshot', includes, mode)\", \"def playlist_next(self, mode='weak'):\\n        self.command('playlist_next', mode)\", \"def _encode_options(options):\\n        return ','.join('{}={}'.format(str(key), str(val)) for key, val in options.items())\", \"def loadlist(self, playlist, mode='replace'):\\n        self.command('loadlist', playlist.encode(fs_enc), mode)\", \"def playlist_remove(self, index='current'):\\n        self.command('playlist_remove', index)\", \"def run(self, command, *args):\\n        self.command('run', command, *args)\", \"def quit_watch_later(self, code=None):\\n        self.command('quit_watch_later', code)\", \"def sub_remove(self, sub_id=None):\\n        self.command('sub_remove', sub_id)\", \"def sub_step(self, skip):\\n        self.command('sub_step', skip)\", \"def toggle_osd(self):\\n        self.command('osd')\", \"def show_progress(self):\\n        self.command('show_progress')\", \"def write_watch_later_config(self):\\n        self.command('write_watch_later_config')\", \"def overlay_remove(self, overlay_id):\\n        self.command('overlay_remove', overlay_id)\", \"def script_message_to(self, target, *args):\\n        self.command('script_message_to', target, *args)\", 'def unobserve_property(self, name, handler):\\n        handlers = self._property_handlers[name]\\n        handlers.remove(handler)\\n        if not handlers:\\n            _mpv_unobserve_property(self._event_handle, hash(name)&0xffffffffffffffff)', 'def unregister_message_handler(self, target):\\n        del self._message_handlers[target]', 'def unregister_event_callback(self, callback):\\n        self._event_callbacks.remove(callback)', \"def _binding_name(callback_or_cmd):\\n        return 'py_kb_{:016x}'.format(hash(callback_or_cmd)&0xffffffffffffffff)\", 'def _handle_key_binding_message(self, binding_name, key_state, key_name):\\n        self._key_binding_handlers[binding_name](key_state, key_name)', 'def play(self, filename):\\n        self.loadfile(filename)', \"def _get_property(self, name, proptype=str, decode_str=False):\\n        fmt = {int:         MpvFormat.INT64,\\n               float:       MpvFormat.DOUBLE,\\n               bool:        MpvFormat.FLAG,\\n               str:         MpvFormat.STRING,\\n               bytes:       MpvFormat.STRING,\\n               commalist:   MpvFormat.STRING,\\n               MpvFormat.NODE: MpvFormat.NODE}[proptype]\\n\\n        out = cast(create_string_buffer(sizeof(c_void_p)), c_void_p)\\n        outptr = byref(out)\\n        try:\\n            cval = _mpv_get_property(self.handle, name.encode('utf-8'), fmt, outptr)\\n            rv = MpvNode.node_cast_value(outptr, fmt, decode_str or proptype in (str, commalist))\\n\\n            if proptype is commalist:\\n                rv = proptype(rv)\\n\\n            if proptype is str:\\n                _mpv_free(out)\\n            elif proptype is MpvFormat.NODE:\\n                _mpv_free_node_contents(outptr)\\n\\n            return rv\\n        except PropertyUnavailableError as ex:\\n            return None\", 'def __getitem__(self, name, file_local=False):\\n        \"\"\" Get an option value \"\"\"\\n        prefix = \\'file-local-options/\\' if file_local else \\'options/\\'\\n        return self._get_property(prefix+name)', 'def __iter__(self):\\n        return iter(self.options)', \"def commalist(propval=''):\\n    return str(propval).split(',')\", \"def bindproperty(MPV, name, proptype, access, decode_str=False):\\n    getter = lambda self: self._get_property(name, proptype, decode_str)\\n    setter = lambda self, value: self._set_property(name, value, proptype)\\n\\n    def barf(*args):\\n        raise NotImplementedError('Access denied')\\n\\n    setattr(MPV, name.replace('-', '_'), property(getter if 'r' in access else barf, setter if 'w' in access else barf))\"]}, {'features': [], 'snippets': ['def wrapper%(signature)s:\\n    with ldap3mock:\\n        return func%(funcargs)s', 'def _convert_objectGUID(item):\\n    item = uuid.UUID(\"{{{0!s}}}\".format(item)).bytes_le\\n    item = escape_bytes(item)\\n    return item', 'def __init__(self):\\n        self._calls = []', 'def __len__(self):\\n        return len(self._calls)', 'def setdata(self, request, response):\\n        self._calls.append(Call(request, response))', 'def __init__(self, connection):\\n                self.connection = connection', 'def __init__(self, connection):\\n            self.standard = self.Standard(connection)', 'def set_directory(self, directory):\\n        self.directory = directory', 'def open(read_server_info=True):\\n        return', 'def start_tls(self, read_server_info=True):\\n        self.start_tls_called = True', 'def delete(self, dn, controls=None):\\n\\n        self.result = { \\'dn\\' : \\'\\',\\n                        \\'referrals\\' : None,\\n                        \\'description\\' : \\'success\\',\\n                        \\'result\\' : 0,\\n                        \\'message\\' : \\'\\',\\n                        \\'type\\' : \\'addResponse\\'}\\n\\n        # Check to see if the user exists in the directory\\n        try:\\n            index = self._find_user(dn)\\n        except StopIteration:\\n            # If we get here the user doesn\\'t exist so continue\\n            self.result[\"description\"] = \"failure\"\\n            self.result[\"result\"] = 32\\n            self.result[\"message\"] = \"Error no such object: {0}\".format(dn)\\n            return False\\n\\n        # Delete the entry object for the user\\n        self.directory.pop(index)\\n\\n        # Attempt to write changes to disk\\n        with open(DIRECTORY, \\'w+\\') as f:\\n            f.write(str(self.directory))\\n\\n        return True', 'def _match_greater_than_or_equal(search_base, attribute, value, candidates):\\n        matches = list()\\n        for entry in candidates:\\n            dn = entry.get(\"dn\")\\n            if not dn.endswith(search_base):\\n                continue\\n\\n            value_from_directory = entry.get(\"attributes\").get(attribute)\\n            if str(value_from_directory) >= str(value):\\n                entry[\"type\"] = \"searchResEntry\"\\n                matches.append(entry)\\n\\n        return matches', 'def _match_greater_than(search_base, attribute, value, candidates):\\n        matches = list()\\n        for entry in candidates:\\n            dn = entry.get(\"dn\")\\n            if not dn.endswith(search_base):\\n                continue\\n\\n            value_from_directory = entry.get(\"attributes\").get(attribute)\\n            if str(value_from_directory) > str(value):\\n                entry[\"type\"] = \"searchResEntry\"\\n                matches.append(entry)\\n\\n        return matches', 'def _match_less_than_or_equal(search_base, attribute, value, candidates):\\n        matches = list()\\n        for entry in candidates:\\n            dn = entry.get(\"dn\")\\n            if not dn.endswith(search_base):\\n                continue\\n\\n            value_from_directory = entry.get(\"attributes\").get(attribute)\\n            if str(value_from_directory) <= str(value):\\n                entry[\"type\"] = \"searchResEntry\"\\n                matches.append(entry)\\n\\n        return matches', 'def _match_less_than(search_base, attribute, value, candidates):\\n        matches = list()\\n        for entry in candidates:\\n            dn = entry.get(\"dn\")\\n            if not dn.endswith(search_base):\\n                continue\\n\\n            value_from_directory = entry.get(\"attributes\").get(attribute)\\n            if str(value_from_directory) < str(value):\\n                entry[\"type\"] = \"searchResEntry\"\\n                matches.append(entry)\\n\\n        return matches', 'def _match_equal_to(search_base, attribute, value, candidates):\\n        matches = list()\\n        match_using_regex = False\\n\\n        if \"*\" in value:\\n            match_using_regex = True\\n            #regex = check_escape(value)\\n            regex = value.replace(\\'*\\', \\'.*\\')\\n            regex = \"^{0}$\".format(regex)\\n\\n        for entry in candidates:\\n            dn = to_unicode(entry.get(\"dn\"))\\n\\n            if attribute not in entry.get(\"attributes\") or not dn.endswith(search_base):\\n                continue\\n\\n            values_from_directory = entry.get(\"attributes\").get(attribute)\\n            if isinstance(values_from_directory, list):\\n                for item in values_from_directory:\\n                    if attribute == \"objectGUID\":\\n                        item = _convert_objectGUID(item)\\n\\n                    if match_using_regex:\\n                        m = re.match(regex, str(item), re.I)\\n                        if m:\\n                            entry[\"type\"] = \"searchResEntry\"\\n                            matches.append(entry)\\n                    else:\\n                        if item == value:\\n                            entry[\"type\"] = \"searchResEntry\"\\n                            matches.append(entry)\\n\\n            else:\\n                if attribute == \"objectGUID\":\\n                    values_from_directory = _convert_objectGUID(values_from_directory)\\n\\n                if match_using_regex:\\n                    m = re.match(regex, str(values_from_directory), re.I)\\n                    if m:\\n                        entry[\"type\"] = \"searchResEntry\"\\n                        matches.append(entry)\\n                else:\\n                    # The value, which we compare is unicode, so we convert\\n                    # the values_from_directory to unicode rather than str.\\n                    if isinstance(values_from_directory, bytes):\\n                        values_from_directory = values_from_directory.decode(\\n                            \"utf-8\")\\n                    elif type(values_from_directory) == int:\\n                        values_from_directory = u\"{0!s}\".format(values_from_directory)\\n                    if value == values_from_directory:\\n                        entry[\"type\"] = \"searchResEntry\"\\n                        matches.append(entry)\\n\\n        return matches', 'def _match_notequal_to(search_base, attribute, value, candidates):\\n        matches = list()\\n        match_using_regex = False\\n\\n        if \"*\" in value:\\n            match_using_regex = True\\n            #regex = check_escape(value)\\n            regex = value.replace(\\'*\\', \\'.*\\')\\n            regex = \"^{0}$\".format(regex)\\n\\n        for entry in candidates:\\n            found = False\\n            dn = entry.get(\"dn\")\\n\\n            if not dn.endswith(search_base):\\n                continue\\n\\n            values_from_directory = entry.get(\"attributes\").get(attribute)\\n            if isinstance(values_from_directory, list):\\n                for item in values_from_directory:\\n                    if attribute == \"objectGUID\":\\n                        item = _convert_objectGUID(item)\\n\\n                    if match_using_regex:\\n                        m = re.match(regex, str(item), re.I)\\n                        if m:\\n                            found = True\\n                    else:\\n                        if item == value:\\n                            found = True\\n                if found is False:\\n                    entry[\"type\"] = \"searchResEntry\"\\n                    matches.append(entry)\\n            else:\\n                if attribute == \"objectGUID\":\\n                    values_from_directory = _convert_objectGUID(values_from_directory)\\n\\n                if match_using_regex:\\n                    m = re.match(regex, str(values_from_directory), re.I)\\n                    if not m:\\n                        entry[\"type\"] = \"searchResEntry\"\\n                        matches.append(entry)\\n                else:\\n                    if str(value) != str(values_from_directory):\\n                        entry[\"type\"] = \"searchResEntry\"\\n                        matches.append(entry)\\n\\n        return matches', 'def _parse_filter():\\n        op = pyparsing.oneOf(\\'! & |\\')\\n        lpar  = pyparsing.Literal(\\'(\\').suppress()\\n        rpar  = pyparsing.Literal(\\')\\').suppress()\\n\\n        k = pyparsing.Word(pyparsing.alphanums)\\n        # NOTE: We may need to expand on this list, but as this is not a real\\n        # LDAP server we should be OK.\\n        # Value to contain:\\n        #   numbers, upper/lower case letters, astrisk, at symbol, minus, full\\n        #   stop, backslash or a space\\n        v = pyparsing.Word(pyparsing.alphanums + \"-*@.\\\\\\\\ äöü\")\\n        rel = pyparsing.oneOf(\"= ~= >= <=\")\\n\\n        expr = pyparsing.Forward()\\n        atom = pyparsing.Group(lpar + op + expr + rpar) \\\\\\n                            | pyparsing.Combine(lpar + k + rel + v + rpar)\\n        expr << atom + pyparsing.ZeroOrMore( expr )\\n\\n        return expr', 'def _deDuplicate(results):\\n        found = dict()\\n        deDuped = list()\\n        for entry in results:\\n            dn = entry.get(\"dn\")\\n            if not dn in found:\\n                found[dn] = 1\\n                deDuped.append(entry)\\n\\n        return deDuped', 'def _search_not(self, base, search_filter, candidates=None):\\n        # Create empty candidates list as we need to use self.directory for\\n        # each search\\n        candidates = list()\\n        this_filter = list()\\n\\n        index = 0\\n        search_filter.remove(\"!\")\\n        for condition in search_filter:\\n            if not isinstance(condition, list):\\n                this_filter.append(condition)\\n            index +=1\\n\\n        # Remove this_filter items from search_filter list\\n        for condition in this_filter:\\n            search_filter.remove(condition)\\n\\n        try:\\n            search_filter = list(search_filter[0])\\n            for sub_filter in search_filter:\\n                if not isinstance(sub_filter, list):\\n                    candidates = self.operation.get(sub_filter)(base,\\n                                                                search_filter,\\n                                                                candidates)\\n                else:\\n                    candidates = self.operation.get(sub_filter[0])(base,\\n                                                                   sub_filter,\\n                                                                   candidates)\\n        except IndexError:\\n            pass\\n\\n        candidates = self._invert_results(candidates)\\n\\n        for item in this_filter:\\n            if \">=\" in item:\\n                k, v = item.split(\">=\")\\n                candidates = Connection._match_less_than(base, k, v,\\n                                                            self.directory)\\n            elif \"<=\" in item:\\n                k, v = item.split(\"<=\")\\n                candidates = Connection._match_greater_than(base, k, v,\\n                                                         self.directory)\\n            # Emulate AD functionality, same as \"=\"\\n            elif \"~=\" in item:\\n                k, v = item.split(\"~=\")\\n                candidates = Connection._match_notequal_to(base, k, v,\\n                                                         self.directory)\\n            elif \"=\" in item:\\n                k, v = item.split(\"=\")\\n                candidates = Connection._match_notequal_to(base, k, v,\\n                                                         self.directory)\\n        return candidates', 'def _search_or(self, base, search_filter, candidates=None):\\n        # Create empty candidates list as we need to use self.directory for\\n        # each search\\n        candidates = list()\\n        this_filter = list()\\n\\n        index = 0\\n        search_filter.remove(\"|\")\\n        for condition in search_filter:\\n            if not isinstance(condition, list):\\n                this_filter.append(condition)\\n            index +=1\\n\\n        # Remove this_filter items from search_filter list\\n        for condition in this_filter:\\n            search_filter.remove(condition)\\n\\n        try:\\n            search_filter = list(search_filter[0])\\n            for sub_filter in search_filter:\\n                if not isinstance(sub_filter, list):\\n                    candidates += self.operation.get(sub_filter)(base,\\n                                                                 search_filter,\\n                                                                 candidates)\\n                else:\\n                    candidates += self.operation.get(sub_filter[0])(base,\\n                                                                    sub_filter,\\n                                                                    candidates)\\n        except IndexError:\\n            pass\\n\\n        for item in this_filter:\\n            if \">=\" in item:\\n                k, v = item.split(\">=\")\\n                candidates += Connection._match_greater_than_or_equal(base, k, v,\\n                                                             self.directory)\\n            elif \"<=\" in item:\\n                k, v = item.split(\"<=\")\\n                candidates += Connection._match_less_than_or_equal(base, k, v,\\n                                                          self.directory)\\n            # Emulate AD functionality, same as \"=\"\\n            elif \"~=\" in item:\\n                k, v = item.split(\"~=\")\\n                candidates += Connection._match_equal_to(base, k, v,\\n                                                         self.directory)\\n            elif \"=\" in item:\\n                k, v = item.split(\"=\")\\n                candidates += Connection._match_equal_to(base, k, v,\\n                                                         self.directory)\\n        return candidates', 'def unbind(self):\\n        return True', 'def __init__(self):\\n        self._calls = CallList()\\n        self._server_mock = None\\n        self.directory = []\\n        self.exception = None\\n        self.reset()', \"def setLDAPDirectory(self, directory=None):\\n        if directory is None:\\n                self.directory = []\\n        else:\\n            try:\\n                with open(DIRECTORY, 'w+') as f:\\n                    f.write(str(directory))\\n                    self.directory = directory\\n            except OSError as e:\\n                raise\", \"def _load_data(self, directory):\\n        try:\\n            with open(directory, 'r') as f:\\n                data = f.read()\\n                return literal_eval(data)\\n        except OSError as e:\\n            raise\", 'def calls(self):\\n        return self._calls', 'def __exit__(self, *args):\\n        self.stop()\\n        self.reset()', 'def _on_Server(self, host, port, use_ssl, connect_timeout, get_info=None,\\n                   tls=None):\\n        # mangle request packet\\n\\n        return \"FakeServerObject\"', \"def start(self):\\n        import mock\\n\\n        def unbound_on_Server(host, port,\\n                              use_ssl,\\n                              connect_timeout, *a, **kwargs):\\n            return self._on_Server(host, port,\\n                              use_ssl,\\n                              connect_timeout, *a, **kwargs)\\n        self._server_mock = mock.MagicMock()\\n        self._server_mock.side_effect = unbound_on_Server\\n        self._patcher = mock.patch('ldap3.Server',\\n                                   self._server_mock)\\n        self._patcher.start()\\n\\n        def unbound_on_Connection(server, user,\\n                                  password,\\n                                  auto_bind,\\n                                  client_strategy,\\n                                  authentication,\\n                                  check_names,\\n                                  auto_referrals, *a, **kwargs):\\n            return self._on_Connection(server, user,\\n                                       password,\\n                                       auto_bind,\\n                                       client_strategy,\\n                                       authentication,\\n                                       check_names,\\n                                       auto_referrals, *a,\\n                                       **kwargs)\\n\\n        self._patcher2 = mock.patch('ldap3.Connection',\\n                                    unbound_on_Connection)\\n        self._patcher2.start()\", 'def get_server_mock(self):\\n        return self._server_mock']}, {'features': [], 'snippets': ['def generateUUID():  # pylint: disable=invalid-name\\n    \"\"\" Utility function; generates UUIDs \"\"\"\\n    return str(uuid.uuid4())', 'def status_before_must_be(*valid_start_statuses):\\n    \"\"\"\\n    Helper decorator with arguments to make sure that an object with a `status`\\n    attribute is in one of a list of acceptable status states before a method\\n    is called. You could use it in a class definition like:\\n\\n        @status_before_must_be(\"submitted\", \"approved\", \"denied\")\\n        def refund_user(self, user_id):\\n            # Do logic here...\\n\\n    If the object has a status that is not listed when the `refund_user` method\\n    is invoked, it will throw a `VerificationException`. This is just to avoid\\n    distracting boilerplate when looking at a Model that needs to go through a\\n    workflow process.\\n    \"\"\"\\n    def decorator_func(func):\\n        \"\"\"\\n        Decorator function that gets returned\\n        \"\"\"\\n        @functools.wraps(func)\\n        def with_status_check(obj, *args, **kwargs):\\n            if obj.status not in valid_start_statuses:\\n                exception_msg = (\\n                    u\"Error calling {} {}: status is \\'{}\\', must be one of: {}\"\\n                ).format(func, obj, obj.status, valid_start_statuses)\\n                raise VerificationException(exception_msg)\\n            return func(obj, *args, **kwargs)\\n\\n        return with_status_check\\n\\n    return decorator_func', 'def expiration_datetime(self):\\n        \"\"\"Datetime that the verification will expire. \"\"\"\\n        days_good_for = settings.VERIFY_STUDENT[\"DAYS_GOOD_FOR\"]\\n        return self.created_at + timedelta(days=days_good_for)', 'def active_at_datetime(self, deadline):\\n        \"\"\"Check whether the verification was active at a particular datetime.\\n\\n        Arguments:\\n            deadline (datetime): The date at which the verification was active\\n                (created before and expiration datetime is after today).\\n\\n        Returns:\\n            bool\\n\\n        \"\"\"\\n        return (\\n            self.created_at < deadline and\\n            self.expiration_datetime > now()\\n        )', \"def __unicode__(self):\\n        return 'ManualIDVerification for {name}, status: {status}'.format(\\n            name=self.name,\\n            status=self.status,\\n        )\", \"def __unicode__(self):\\n        return 'SSOIDVerification for {name}, status: {status}'.format(\\n            name=self.name,\\n            status=self.status,\\n        )\", 'def parsed_error_msg(self):\\n        \"\"\"\\n        Sometimes, the error message we\\'ve received needs to be parsed into\\n        something more human readable\\n\\n        The default behavior is to return the current error message as is.\\n        \"\"\"\\n        return self.error_msg', 'def upload_face_image(self, img):\\n        raise NotImplementedError', 'def upload_photo_id_image(self, img):\\n        raise NotImplementedError', 'def mark_ready(self):\\n        \"\"\"\\n        Mark that the user data in this attempt is correct. In order to\\n        succeed, the user must have uploaded the necessary images\\n        (`face_image_url`, `photo_id_image_url`). This method will also copy\\n        their name from their user profile. Prior to marking it ready, we read\\n        this value directly from their profile, since they\\'re free to change it.\\n        This often happens because people put in less formal versions of their\\n        name on signup, but realize they want something different to go on a\\n        formal document.\\n\\n        Valid attempt statuses when calling this method:\\n            `created`\\n\\n        Status after method completes: `ready`\\n\\n        Other fields that will be set by this method:\\n            `name`\\n\\n        State Transitions:\\n\\n        `created` → `ready`\\n            This is what happens when the user confirms to us that the pictures\\n            they uploaded are good. Note that we don\\'t actually do a submission\\n            anywhere yet.\\n        \"\"\"\\n        # At any point prior to this, they can change their names via their\\n        # student dashboard. But at this point, we lock the value into the\\n        # attempt.\\n        self.name = self.user.profile.name\\n        self.status = \"ready\"\\n        self.save()', 'def approve(self, user_id=None, service=\"\"):\\n        \"\"\"\\n        Approve this attempt. `user_id`\\n\\n        Valid attempt statuses when calling this method:\\n            `submitted`, `approved`, `denied`\\n\\n        Status after method completes: `approved`\\n\\n        Other fields that will be set by this method:\\n            `reviewed_by_user_id`, `reviewed_by_service`, `error_msg`\\n\\n        State Transitions:\\n\\n        `submitted` → `approved`\\n            This is the usual flow, whether initiated by a staff user or an\\n            external validation service.\\n        `approved` → `approved`\\n            No-op. First one to approve it wins.\\n        `denied` → `approved`\\n            This might happen if a staff member wants to override a decision\\n            made by an external service or another staff member (say, in\\n            response to a support request). In this case, the previous values\\n            of `reviewed_by_user_id` and `reviewed_by_service` will be changed\\n            to whoever is doing the approving, and `error_msg` will be reset.\\n            The only record that this record was ever denied would be in our\\n            logs. This should be a relatively rare occurence.\\n        \"\"\"\\n        # If someone approves an outdated version of this, the first one wins\\n        if self.status == \"approved\":\\n            return\\n\\n        log.info(u\"Verification for user \\'{user_id}\\' approved by \\'{reviewer}\\'.\".format(\\n            user_id=self.user, reviewer=user_id\\n        ))\\n        self.error_msg = \"\"  # reset, in case this attempt was denied before\\n        self.error_code = \"\"  # reset, in case this attempt was denied before\\n        self.reviewing_user = user_id\\n        self.reviewing_service = service\\n        self.status = \"approved\"\\n        self.save()\\n        # Emit signal to find and generate eligible certificates\\n        LEARNER_NOW_VERIFIED.send_robust(\\n            sender=PhotoVerification,\\n            user=self.user\\n        )', 'def deny(self,\\n             error_msg,\\n             error_code=\"\",\\n             reviewing_user=None,\\n             reviewing_service=\"\"):\\n        \"\"\"\\n        Deny this attempt.\\n\\n        Valid attempt statuses when calling this method:\\n            `submitted`, `approved`, `denied`\\n\\n        Status after method completes: `denied`\\n\\n        Other fields that will be set by this method:\\n            `reviewed_by_user_id`, `reviewed_by_service`, `error_msg`,\\n            `error_code`\\n\\n        State Transitions:\\n\\n        `submitted` → `denied`\\n            This is the usual flow, whether initiated by a staff user or an\\n            external validation service.\\n        `approved` → `denied`\\n            This might happen if a staff member wants to override a decision\\n            made by an external service or another staff member, or just correct\\n            a mistake made during the approval process. In this case, the\\n            previous values of `reviewed_by_user_id` and `reviewed_by_service`\\n            will be changed to whoever is doing the denying. The only record\\n            that this record was ever approved would be in our logs. This should\\n            be a relatively rare occurence.\\n        `denied` → `denied`\\n            Update the error message and reviewing_user/reviewing_service. Just\\n            lets you amend the error message in case there were additional\\n            details to be made.\\n        \"\"\"\\n        log.info(u\"Verification for user \\'{user_id}\\' denied by \\'{reviewer}\\'.\".format(\\n            user_id=self.user, reviewer=reviewing_user\\n        ))\\n        self.error_msg = error_msg\\n        self.error_code = error_code\\n        self.reviewing_user = reviewing_user\\n        self.reviewing_service = reviewing_service\\n        self.status = \"denied\"\\n        self.save()', 'def system_error(self,\\n                     error_msg,\\n                     error_code=\"\",\\n                     reviewing_user=None,\\n                     reviewing_service=\"\"):\\n        \"\"\"\\n        Mark that this attempt could not be completed because of a system error.\\n        Status should be moved to `must_retry`. For example, if Software Secure\\n        reported to us that they couldn\\'t process our submission because they\\n        couldn\\'t decrypt the image we sent.\\n        \"\"\"\\n        if self.status in [\"approved\", \"denied\"]:\\n            return  # If we were already approved or denied, just leave it.\\n\\n        self.error_msg = error_msg\\n        self.error_code = error_code\\n        self.reviewing_user = reviewing_user\\n        self.reviewing_service = reviewing_service\\n        self.status = \"must_retry\"\\n        self.save()', 'def retire_user(cls, user_id):\\n        \"\"\"\\n        Retire user as part of GDPR Phase I\\n        Returns \\'True\\' if records found\\n\\n        :param user_id: int\\n        :return: bool\\n        \"\"\"\\n        try:\\n            user_obj = User.objects.get(id=user_id)\\n        except User.DoesNotExist:\\n            return False\\n\\n        photo_objects = cls.objects.filter(\\n            user=user_obj\\n        ).update(\\n            name=\\'\\',\\n            face_image_url=\\'\\',\\n            photo_id_image_url=\\'\\',\\n            photo_id_key=\\'\\'\\n        )\\n        return photo_objects > 0', 'def approve(self, user_id=None, service=\"\"):\\n        \"\"\"\\n        Approve the verification attempt for user\\n\\n        Valid attempt statuses when calling this method:\\n            `submitted`, `approved`, `denied`\\n\\n        After method completes:\\n            status is set to `approved`\\n            expiry_date is set to one year from now\\n        \"\"\"\\n        self.expiry_date = now() + timedelta(\\n            days=settings.VERIFY_STUDENT[\"DAYS_GOOD_FOR\"]\\n        )\\n        super(SoftwareSecurePhotoVerification, self).approve(user_id, service)', 'def get_initial_verification(cls, user, earliest_allowed_date=None):\\n        \"\"\"Get initial verification for a user with the \\'photo_id_key\\'.\\n\\n        Arguments:\\n            user(User): user object\\n            earliest_allowed_date(datetime): override expiration date for initial verification\\n\\n        Return:\\n            SoftwareSecurePhotoVerification (object) or None\\n        \"\"\"\\n        init_verification = cls.objects.filter(\\n            user=user,\\n            status__in=[\"submitted\", \"approved\"],\\n            created_at__gte=(\\n                earliest_allowed_date or earliest_allowed_verification_date()\\n            )\\n        ).exclude(photo_id_key=\\'\\')\\n\\n        return init_verification.latest(\\'created_at\\') if init_verification.exists() else None', 'def upload_face_image(self, img_data):\\n        \"\"\"\\n        Upload an image of the user\\'s face. `img_data` should be a raw\\n        bytestream of a PNG image. This method will take the data, encrypt it\\n        using our FACE_IMAGE_AES_KEY, encode it with base64 and save it to the\\n        storage backend.\\n\\n        Yes, encoding it to base64 adds compute and disk usage without much real\\n        benefit, but that\\'s what the other end of this API is expecting to get.\\n        \"\"\"\\n        # Skip this whole thing if we\\'re running acceptance tests or if we\\'re\\n        # developing and aren\\'t interested in working on student identity\\n        # verification functionality. If you do want to work on it, you have to\\n        # explicitly enable these in your private settings.\\n        if settings.FEATURES.get(\\'AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING\\'):\\n            return\\n\\n        aes_key_str = settings.VERIFY_STUDENT[\"SOFTWARE_SECURE\"][\"FACE_IMAGE_AES_KEY\"]\\n        aes_key = aes_key_str.decode(\"hex\")\\n\\n        path = self._get_path(\"face\")\\n        buff = ContentFile(encrypt_and_encode(img_data, aes_key))\\n        self._storage.save(path, buff)', 'def upload_photo_id_image(self, img_data):\\n        \"\"\"\\n        Upload an the user\\'s photo ID image. `img_data` should be a raw\\n        bytestream of a PNG image. This method will take the data, encrypt it\\n        using a randomly generated AES key, encode it with base64 and save it\\n        to the storage backend. The random key is also encrypted using Software\\n        Secure\\'s public RSA key and stored in our `photo_id_key` field.\\n\\n        Yes, encoding it to base64 adds compute and disk usage without much real\\n        benefit, but that\\'s what the other end of this API is expecting to get.\\n        \"\"\"\\n        # Skip this whole thing if we\\'re running acceptance tests or if we\\'re\\n        # developing and aren\\'t interested in working on student identity\\n        # verification functionality. If you do want to work on it, you have to\\n        # explicitly enable these in your private settings.\\n        if settings.FEATURES.get(\\'AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING\\'):\\n            # fake photo id key is set only for initial verification\\n            self.photo_id_key = \\'fake-photo-id-key\\'\\n            self.save()\\n            return\\n\\n        aes_key = random_aes_key()\\n        rsa_key_str = settings.VERIFY_STUDENT[\"SOFTWARE_SECURE\"][\"RSA_PUBLIC_KEY\"]\\n        rsa_encrypted_aes_key = rsa_encrypt(aes_key, rsa_key_str)\\n\\n        # Save this to the storage backend\\n        path = self._get_path(\"photo_id\")\\n        buff = ContentFile(encrypt_and_encode(img_data, aes_key))\\n        self._storage.save(path, buff)\\n\\n        # Update our record fields\\n        self.photo_id_key = rsa_encrypted_aes_key.encode(\\'base64\\')\\n        self.save()', 'def submit(self, copy_id_photo_from=None):\\n        \"\"\"\\n        Submit our verification attempt to Software Secure for validation. This\\n        will set our status to \"submitted\" if the post is successful, and\\n        \"must_retry\" if the post fails.\\n\\n        Keyword Arguments:\\n            copy_id_photo_from (SoftwareSecurePhotoVerification): If provided, re-send the ID photo\\n                data from this attempt.  This is used for reverification, in which new face photos\\n                are sent with previously-submitted ID photos.\\n\\n        \"\"\"\\n        try:\\n            response = self.send_request(copy_id_photo_from=copy_id_photo_from)\\n            if response.ok:\\n                self.submitted_at = now()\\n                self.status = \"submitted\"\\n                self.save()\\n            else:\\n                self.status = \"must_retry\"\\n                self.error_msg = response.text\\n                self.save()\\n        except Exception:       # pylint: disable=broad-except\\n            log.exception(\\n                u\\'Software Secure submission failed for user %s, setting status to must_retry\\',\\n                self.user.username\\n            )\\n            self.status = \"must_retry\"\\n            self.save()', 'def image_url(self, name, override_receipt_id=None):\\n        \"\"\"\\n        We dynamically generate this, since we want it the expiration clock to\\n        start when the message is created, not when the record is created.\\n\\n        Arguments:\\n            name (str): Name of the image (e.g. \"photo_id\" or \"face\")\\n\\n        Keyword Arguments:\\n            override_receipt_id (str): If provided, use this receipt ID instead\\n                of the ID for this attempt.  This is useful for reverification\\n                where we need to construct a URL to a previously-submitted\\n                photo ID image.\\n\\n        Returns:\\n            string: The expiring URL for the image.\\n\\n        \"\"\"\\n        path = self._get_path(name, override_receipt_id=override_receipt_id)\\n        return self._storage.url(path)', 'def _storage(self):\\n        \"\"\"\\n        Return the configured django storage backend.\\n        \"\"\"\\n        config = settings.VERIFY_STUDENT[\"SOFTWARE_SECURE\"]\\n\\n        # Default to the S3 backend for backward compatibility\\n        storage_class = config.get(\"STORAGE_CLASS\", \"storages.backends.s3boto.S3BotoStorage\")\\n        storage_kwargs = config.get(\"STORAGE_KWARGS\", {})\\n\\n        # Map old settings to the parameters expected by the storage backend\\n        if \"AWS_ACCESS_KEY\" in config:\\n            storage_kwargs[\"access_key\"] = config[\"AWS_ACCESS_KEY\"]\\n        if \"AWS_SECRET_KEY\" in config:\\n            storage_kwargs[\"secret_key\"] = config[\"AWS_SECRET_KEY\"]\\n        if \"S3_BUCKET\" in config:\\n            storage_kwargs[\"bucket\"] = config[\"S3_BUCKET\"]\\n            storage_kwargs[\"querystring_expire\"] = self.IMAGE_LINK_DURATION\\n\\n        return get_storage(storage_class, **storage_kwargs)', 'def _encrypted_user_photo_key_str(self):\\n        \"\"\"\\n        Software Secure needs to have both UserPhoto and PhotoID decrypted in\\n        the same manner. So even though this is going to be the same for every\\n        request, we\\'re also using RSA encryption to encrypt the AES key for\\n        faces.\\n        \"\"\"\\n        face_aes_key_str = settings.VERIFY_STUDENT[\"SOFTWARE_SECURE\"][\"FACE_IMAGE_AES_KEY\"]\\n        face_aes_key = face_aes_key_str.decode(\"hex\")\\n        rsa_key_str = settings.VERIFY_STUDENT[\"SOFTWARE_SECURE\"][\"RSA_PUBLIC_KEY\"]\\n        rsa_encrypted_face_aes_key = rsa_encrypt(face_aes_key, rsa_key_str)\\n\\n        return rsa_encrypted_face_aes_key.encode(\"base64\")', 'def request_message_txt(self):\\n        \"\"\"\\n        This is the body of the request we send across. This is never actually\\n        used in the code, but exists for debugging purposes -- you can call\\n        `print attempt.request_message_txt()` on the console and get a readable\\n        rendering of the request that would be sent across, without actually\\n        sending anything.\\n        \"\"\"\\n        headers, body = self.create_request()\\n\\n        header_txt = \"\\\\n\".join(\\n            u\"{}: {}\".format(h, v) for h, v in sorted(headers.items())\\n        )\\n        body_txt = json.dumps(body, indent=2, sort_keys=True, ensure_ascii=False).encode(\\'utf-8\\')\\n\\n        return header_txt + \"\\\\n\\\\n\" + body_txt', 'def should_display_status_to_user(self):\\n        \"\"\"Whether or not the status from this attempt should be displayed to the user.\"\"\"\\n        return True', 'def set_deadline(cls, course_key, deadline, is_explicit=False):\\n        \"\"\"\\n        Configure the verification deadline for a course.\\n\\n        If `deadline` is `None`, then the course will have no verification\\n        deadline.  In this case, users will be able to verify for the course\\n        at any time.\\n\\n        Arguments:\\n            course_key (CourseKey): Identifier for the course.\\n            deadline (datetime or None): The verification deadline.\\n\\n        \"\"\"\\n        if deadline is None:\\n            VerificationDeadline.objects.filter(course_key=course_key).delete()\\n        else:\\n            record, created = VerificationDeadline.objects.get_or_create(\\n                course_key=course_key,\\n                defaults={\"deadline\": deadline, \"deadline_is_explicit\": is_explicit}\\n            )\\n\\n            if not created:\\n                record.deadline = deadline\\n                record.deadline_is_explicit = is_explicit\\n                record.save()', 'def deadlines_for_courses(cls, course_keys):\\n        \"\"\"\\n        Retrieve verification deadlines for particular courses.\\n\\n        Arguments:\\n            course_keys (list): List of `CourseKey`s.\\n\\n        Returns:\\n            dict: Map of course keys to datetimes (verification deadlines)\\n\\n        \"\"\"\\n        all_deadlines = cache.get(cls.ALL_DEADLINES_CACHE_KEY)\\n        if all_deadlines is None:\\n            all_deadlines = {\\n                deadline.course_key: deadline.deadline\\n                for deadline in VerificationDeadline.objects.all()\\n            }\\n            cache.set(cls.ALL_DEADLINES_CACHE_KEY, all_deadlines)\\n\\n        return {\\n            course_key: all_deadlines[course_key]\\n            for course_key in course_keys\\n            if course_key in all_deadlines\\n        }', 'def deadline_for_course(cls, course_key):\\n        \"\"\"\\n        Retrieve the verification deadline for a particular course.\\n\\n        Arguments:\\n            course_key (CourseKey): The identifier for the course.\\n\\n        Returns:\\n            datetime or None\\n\\n        \"\"\"\\n        try:\\n            deadline = cls.objects.get(course_key=course_key)\\n            return deadline.deadline\\n        except cls.DoesNotExist:\\n            return None']}, {'features': [], 'snippets': [\"def name_get(self, cr, uid, ids, context=None):\\n        # always return the full hierarchical name\\n        res = self._complete_name(cr, uid, ids, 'complete_name', None, context=context)\\n        return res.items()\", 'def _get_sublocations(self, cr, uid, ids, context=None):\\n        \"\"\" return all sublocations of the given stock locations (included) \"\"\"\\n        return self.search(cr, uid, [(\\'id\\', \\'child_of\\', ids)], context=context)', 'def chained_location_get(self, cr, uid, location, partner=None, product=None, context=None):\\n        \"\"\" Finds chained location\\n        @param location: Location id\\n        @param partner: Partner id\\n        @param product: Product id\\n        @return: List of values\\n        \"\"\"\\n        result = None\\n        if location.chained_location_type == \\'customer\\':\\n            if partner:\\n                result = partner.property_stock_customer\\n            else:\\n                loc_id = self.pool[\\'res.partner\\'].default_get(cr, uid, [\\'property_stock_customer\\'], context=context)[\\'property_stock_customer\\']\\n                result = self.pool[\\'stock.location\\'].browse(cr, uid, loc_id, context=context)\\n        elif location.chained_location_type == \\'fixed\\':\\n            result = location.chained_location_id\\n        if result:\\n            return result, location.chained_auto_packing, location.chained_delay, location.chained_journal_id and location.chained_journal_id.id or False, location.chained_company_id and location.chained_company_id.id or False, location.chained_picking_type, False\\n        return result', 'def _product_get_all_report(self, cr, uid, ids, product_ids=False, context=None):\\n        return self._product_get_report(cr, uid, ids, product_ids, context, recursive=True)', 'def _product_get_multi_location(self, cr, uid, ids, product_ids=False, context=None,\\n                                    states=[\\'done\\'], what=(\\'in\\', \\'out\\')):\\n        \"\"\"\\n        @param product_ids: Ids of product\\n        @param states: List of states\\n        @param what: Tuple of\\n        @return:\\n        \"\"\"\\n        product_obj = self.pool.get(\\'product.product\\')\\n        if context is None:\\n            context = {}\\n        context.update({\\n            \\'states\\': states,\\n            \\'what\\': what,\\n            \\'location\\': ids\\n        })\\n        return product_obj.get_product_available(cr, uid, product_ids, context=context)', \"def _product_all_get(self, cr, uid, id, product_ids=False, context=None, states=None):\\n        if states is None:\\n            states = ['done']\\n        # build the list of ids of children of the location given by id\\n        ids = id and [id] or []\\n        location_ids = self.search(cr, uid, [('location_id', 'child_of', ids)])\\n        return self._product_get_multi_location(cr, uid, location_ids, product_ids, context, states)\", 'def _product_reserve(self, cr, uid, ids, product_id, product_qty, context=None, lock=False):\\n        \"\"\"\\n        Attempt to find a quantity ``product_qty`` (in the product\\'s default uom or the uom passed in ``context``) of product ``product_id``\\n        in locations with id ``ids`` and their child locations. If ``lock`` is True, the stock.move lines\\n        of product with id ``product_id`` in the searched location will be write-locked using Postgres\\'s\\n        \"FOR UPDATE NOWAIT\" option until the transaction is committed or rolled back, to prevent reservin\\n        twice the same products.\\n        If ``lock`` is True and the lock cannot be obtained (because another transaction has locked some of\\n        the same stock.move lines), a log line will be output and False will be returned, as if there was\\n        not enough stock.\\n\\n        :param product_id: Id of product to reserve\\n        :param product_qty: Quantity of product to reserve (in the product\\'s default uom or the uom passed in ``context``)\\n        :param lock: if True, the stock.move lines of product with id ``product_id`` in all locations (and children locations) with ``ids`` will\\n                     be write-locked using postgres\\'s \"FOR UPDATE NOWAIT\" option until the transaction is committed or rolled back. This is\\n                     to prevent reserving twice the same products.\\n        :param context: optional context dictionary: if a \\'uom\\' key is present it will be used instead of the default product uom to\\n                        compute the ``product_qty`` and in the return value.\\n        :return: List of tuples in the form (qty, location_id) with the (partial) quantities that can be taken in each location to\\n                 reach the requested product_qty (``qty`` is expressed in the default uom of the product), of False if enough\\n                 products could not be found, or the lock could not be obtained (and ``lock`` was True).\\n        \"\"\"\\n        result = []\\n        amount = 0.0\\n        if context is None:\\n            context = {}\\n        uom_obj = self.pool.get(\\'product.uom\\')\\n        uom_rounding = self.pool.get(\\'product.product\\').browse(cr, uid, product_id, context=context).uom_id.rounding\\n        if context.get(\\'uom\\'):\\n            uom_rounding = uom_obj.browse(cr, uid, context.get(\\'uom\\'), context=context).rounding\\n\\n        locations_ids = self.search(cr, uid, [(\\'location_id\\', \\'child_of\\', ids)])\\n        if locations_ids:\\n            # Fetch only the locations in which this product has ever been processed (in or out)\\n            cr.execute(\"\"\"SELECT l.id FROM stock_location l WHERE l.id in %s AND\\n                        EXISTS (SELECT 1 FROM stock_move m WHERE m.product_id = %s\\n                                AND ((state = \\'done\\' AND m.location_dest_id = l.id)\\n                                    OR (state in (\\'done\\',\\'assigned\\') AND m.location_id = l.id)))\\n                       \"\"\", (tuple(locations_ids), product_id,))\\n            locations_ids = [i for (i,) in cr.fetchall()]\\n        for id in locations_ids:\\n            if lock:\\n                try:\\n                    # Must lock with a separate select query because FOR UPDATE can\\'t be used with\\n                    # aggregation/group by\\'s (when individual rows aren\\'t identifiable).\\n                    # We use a SAVEPOINT to be able to rollback this part of the transaction without\\n                    # failing the whole transaction in case the LOCK cannot be acquired.\\n                    cr.execute(\"SAVEPOINT stock_location_product_reserve\")\\n                    cr.execute(\"\"\"SELECT id FROM stock_move\\n                                  WHERE product_id=%s AND\\n                                          (\\n                                            (location_dest_id=%s AND\\n                                             location_id<>%s AND\\n                                             state=\\'done\\')\\n                                            OR\\n                                            (location_id=%s AND\\n                                             location_dest_id<>%s AND\\n                                             state in (\\'done\\', \\'assigned\\'))\\n                                          )\\n                                  FOR UPDATE of stock_move NOWAIT\"\"\", (product_id, id, id, id, id), log_exceptions=False)\\n                except Exception:\\n                    # Here it\\'s likely that the FOR UPDATE NOWAIT failed to get the LOCK,\\n                    # so we ROLLBACK to the SAVEPOINT to restore the transaction to its earlier\\n                    # state, we return False as if the products were not available, and log it:\\n                    cr.execute(\"ROLLBACK TO stock_location_product_reserve\")\\n                    _logger.warning(\"Failed attempt to reserve %s x product %s, likely due to another transaction already in progress. Next attempt is likely to work. Detailed error available at DEBUG level.\", product_qty, product_id)\\n                    _logger.debug(\"Trace of the failed product reservation attempt: \", exc_info=True)\\n                    return False\\n\\n            # XXX TODO: rewrite this with one single query, possibly even the quantity conversion\\n            cr.execute(\"\"\"SELECT product_uom, sum(product_qty) AS product_qty\\n                          FROM stock_move\\n                          WHERE location_dest_id=%s AND\\n                                location_id<>%s AND\\n                                product_id=%s AND\\n                                state=\\'done\\'\\n                          GROUP BY product_uom\\n                       \"\"\",\\n                       (id, id, product_id))\\n            results = cr.dictfetchall()\\n            cr.execute(\"\"\"SELECT product_uom,-sum(product_qty) AS product_qty\\n                          FROM stock_move\\n                          WHERE location_id=%s AND\\n                                location_dest_id<>%s AND\\n                                product_id=%s AND\\n                                state in (\\'done\\', \\'assigned\\')\\n                          GROUP BY product_uom\\n                       \"\"\",\\n                       (id, id, product_id))\\n            results += cr.dictfetchall()\\n            total = 0.0\\n            results2 = 0.0\\n            for r in results:\\n                amount = uom_obj._compute_qty(cr, uid, r[\\'product_uom\\'], r[\\'product_qty\\'], context.get(\\'uom\\', False))\\n                results2 += amount\\n                total += amount\\n            if total <= 0.0:\\n                continue\\n\\n            amount = results2\\n            compare_qty = float_compare(amount, 0, precision_rounding=uom_rounding)\\n            if compare_qty == 1:\\n                if amount > min(total, product_qty):\\n                    amount = min(product_qty, total)\\n                result.append((amount, id))\\n                product_qty -= amount\\n                total -= amount\\n                if product_qty <= 0.0:\\n                    return result\\n                if total <= 0.0:\\n                    continue\\n        return False', \"def checksum(sscc):\\n        salt = '31' * 8 + '3'\\n        sum = 0\\n        for sscc_part, salt_part in zip(sscc, salt):\\n            sum += int(sscc_part) * int(salt_part)\\n        return (10 - (sum % 10)) % 10\", \"def make_sscc(self, cr, uid, context=None):\\n        sequence = self.pool.get('ir.sequence').get(cr, uid, 'stock.lot.tracking')\\n        try:\\n            return sequence + str(self.checksum(sequence))\\n        except Exception:\\n            return sequence\", \"def name_search(self, cr, user, name, args=None, operator='ilike', context=None, limit=100):\\n        if not args:\\n            args = []\\n        ids = self.search(cr, user, [('serial', '=', name)]+ args, limit=limit, context=context)\\n        ids += self.search(cr, user, [('name', operator, name)]+ args, limit=limit, context=context)\\n        return self.name_get(cr, user, ids, context)\", \"def unlink(self, cr, uid, ids, context=None):\\n        raise osv.except_osv(_('Error!'), _('You cannot remove a lot line.'))\", 'def _set_maximum_date(self, cr, uid, ids, name, value, arg, context=None):\\n        \"\"\" Calculates planned date if it is greater than \\'value\\'.\\n        @param name: Name of field\\n        @param value: Value of field\\n        @param arg: User defined argument\\n        @return: True or False\\n        \"\"\"\\n        if not value:\\n            return False\\n        if isinstance(ids, (int, long)):\\n            ids = [ids]\\n        for pick in self.browse(cr, uid, ids, context=context):\\n            sql_str = \"\"\"update stock_move set\\n                    date_expected=\\'%s\\'\\n                where\\n                    picking_id=%d \"\"\" % (value, pick.id)\\n            if pick.max_date:\\n                sql_str += \" and (date_expected=\\'\" + pick.max_date + \"\\')\"\\n            cr.execute(sql_str)\\n        return True', 'def get_min_max_date(self, cr, uid, ids, field_name, arg, context=None):\\n        \"\"\" Finds minimum and maximum dates for picking.\\n        @return: Dictionary of values\\n        \"\"\"\\n        res = {}\\n        for id in ids:\\n            res[id] = {\\'min_date\\': False, \\'max_date\\': False}\\n        if not ids:\\n            return res\\n        cr.execute(\"\"\"select\\n                picking_id,\\n                min(date_expected),\\n                max(date_expected)\\n            from\\n                stock_move\\n            where\\n                picking_id IN %s\\n            group by\\n                picking_id\"\"\",(tuple(ids),))\\n        for pick, dt1, dt2 in cr.fetchall():\\n            res[pick][\\'min_date\\'] = dt1\\n            res[pick][\\'max_date\\'] = dt2\\n        return res', 'def action_process(self, cr, uid, ids, context=None):\\n        if context is None:\\n            context = {}\\n        \"\"\"Open the partial picking wizard\"\"\"\\n        context.update({\\n            \\'active_model\\': self._name,\\n            \\'active_ids\\': ids,\\n            \\'active_id\\': len(ids) and ids[0] or False\\n        })\\n        return {\\n            \\'view_type\\': \\'form\\',\\n            \\'view_mode\\': \\'form\\',\\n            \\'res_model\\': \\'stock.partial.picking\\',\\n            \\'type\\': \\'ir.actions.act_window\\',\\n            \\'target\\': \\'new\\',\\n            \\'context\\': context,\\n            \\'nodestroy\\': True,\\n        }', 'def fields_view_get(self, cr, uid, view_id=None, view_type=False, context=None, toolbar=False, submenu=False):\\n        if view_type == \\'form\\' and not view_id:\\n            mod_obj = self.pool.get(\\'ir.model.data\\')\\n            if self._name == \"stock.picking.in\":\\n                model, view_id = mod_obj.get_object_reference(cr, uid, \\'stock\\', \\'view_picking_in_form\\')\\n            if self._name == \"stock.picking.out\":\\n                model, view_id = mod_obj.get_object_reference(cr, uid, \\'stock\\', \\'view_picking_out_form\\')\\n        return super(stock_picking, self).fields_view_get(cr, uid, view_id=view_id, view_type=view_type, context=context, toolbar=toolbar, submenu=submenu)', 'def action_explode(self, cr, uid, moves, context=None):\\n        \"\"\"Hook to allow other modules to split the moves of a picking.\"\"\"\\n        return moves', 'def test_auto_picking(self, cr, uid, ids):\\n        # TODO: Check locations to see if in the same location ?\\n        return True', 'def force_assign(self, cr, uid, ids, *args):\\n        \"\"\" Changes state of picking to available if moves are confirmed or waiting.\\n        @return: True\\n        \"\"\"\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        for pick in self.browse(cr, uid, ids):\\n            move_ids = [x.id for x in pick.move_lines if x.state in [\\'confirmed\\',\\'waiting\\']]\\n            self.pool.get(\\'stock.move\\').force_assign(cr, uid, move_ids)\\n            wf_service.trg_write(uid, \\'stock.picking\\', pick.id, cr)\\n        return True', 'def draft_validate(self, cr, uid, ids, context=None):\\n        \"\"\" Validates picking directly from draft state.\\n        @return: True\\n        \"\"\"\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        self.draft_force_assign(cr, uid, ids)\\n        for pick in self.browse(cr, uid, ids, context=context):\\n            move_ids = [x.id for x in pick.move_lines]\\n            self.pool.get(\\'stock.move\\').force_assign(cr, uid, move_ids)\\n            wf_service.trg_write(uid, \\'stock.picking\\', pick.id, cr)\\n        return self.action_process(\\n            cr, uid, ids, context=context)', 'def action_assign_wkf(self, cr, uid, ids, context=None):\\n        \"\"\" Changes picking state to assigned.\\n        @return: True\\n        \"\"\"\\n        self.write(cr, uid, ids, {\\'state\\': \\'assigned\\'})\\n        return True', 'def test_assigned(self, cr, uid, ids):\\n        \"\"\" Tests whether the move is in assigned state or not.\\n        @return: True or False\\n        \"\"\"\\n        #TOFIX: assignment of move lines should be call before testing assigment otherwise picking never gone in assign state\\n        ok = True\\n        for pick in self.browse(cr, uid, ids):\\n            mt = pick.move_type\\n            # incomming shipments are always set as available if they aren\\'t chained\\n            if pick.type == \\'in\\':\\n                if all([x.state != \\'waiting\\' for x in pick.move_lines]):\\n                    return True\\n            for move in pick.move_lines:\\n                if (move.state in (\\'confirmed\\', \\'draft\\')) and (mt == \\'one\\'):\\n                    return False\\n                if (mt == \\'direct\\') and (move.state == \\'assigned\\') and (move.product_qty):\\n                    return True\\n                ok = ok and (move.state in (\\'cancel\\', \\'done\\', \\'assigned\\'))\\n        return ok', 'def action_done(self, cr, uid, ids, context=None):\\n        \"\"\"Changes picking state to done.', 'def action_move(self, cr, uid, ids, context=None):\\n        \"\"\"Process the Stock Moves of the Picking', 'def get_currency_id(self, cr, uid, picking):\\n        return False', 'def _get_comment_invoice(self, cr, uid, picking):\\n        \"\"\"\\n        @return: comment string for invoice\\n        \"\"\"\\n        return picking.note or \\'\\'', \"def _get_discount_invoice(self, cr, uid, move_line):\\n        '''Return the discount for the move line'''\\n        return 0.0\", 'def _get_account_analytic_invoice(self, cr, uid, picking, move_line):\\n        return False', \"def _invoice_hook(self, cr, uid, picking, invoice_id):\\n        '''Call after the creation of the invoice'''\\n        return\", 'def _prepare_invoice_group(self, cr, uid, picking, partner, invoice, context=None):\\n        \"\"\" Builds the dict for grouped invoices\\n            @param picking: picking object\\n            @param partner: object of the partner to invoice (not used here, but may be usefull if this function is inherited)\\n            @param invoice: object of the invoice that we are updating\\n            @return: dict that will be used to update the invoice\\n        \"\"\"\\n        comment = self._get_comment_invoice(cr, uid, picking)\\n        return {\\n            \\'name\\': (invoice.name or \\'\\') + \\', \\' + (picking.name or \\'\\'),\\n            \\'origin\\': (invoice.origin or \\'\\') + \\', \\' + (picking.name or \\'\\') + (picking.origin and (\\':\\' + picking.origin) or \\'\\'),\\n            \\'comment\\': (comment and (invoice.comment and invoice.comment + \"\\\\n\" + comment or comment)) or (invoice.comment and invoice.comment or \\'\\'),\\n            \\'date_invoice\\': context.get(\\'date_inv\\', False),\\n            \\'user_id\\': uid,\\n        }', 'def _prepare_invoice_line(self, cr, uid, group, picking, move_line, invoice_id,\\n        invoice_vals, context=None):\\n        \"\"\" Builds the dict containing the values for the invoice line\\n            @param group: True or False\\n            @param picking: picking object\\n            @param: move_line: move_line object\\n            @param: invoice_id: ID of the related invoice\\n            @param: invoice_vals: dict used to created the invoice\\n            @return: dict that will be used to create the invoice line\\n        \"\"\"\\n        if group:\\n            name = (picking.name or \\'\\') + \\'-\\' + move_line.name\\n        else:\\n            name = move_line.name\\n        origin = move_line.picking_id.name or \\'\\'\\n        if move_line.picking_id.origin:\\n            origin += \\':\\' + move_line.picking_id.origin\\n\\n        if invoice_vals[\\'type\\'] in (\\'out_invoice\\', \\'out_refund\\'):\\n            account_id = move_line.product_id.property_account_income.id\\n            if not account_id:\\n                account_id = move_line.product_id.categ_id.\\\\\\n                        property_account_income_categ.id\\n        else:\\n            account_id = move_line.product_id.property_account_expense.id\\n            if not account_id:\\n                account_id = move_line.product_id.categ_id.\\\\\\n                        property_account_expense_categ.id\\n        if invoice_vals[\\'fiscal_position\\']:\\n            fp_obj = self.pool.get(\\'account.fiscal.position\\')\\n            fiscal_position = fp_obj.browse(cr, uid, invoice_vals[\\'fiscal_position\\'], context=context)\\n            account_id = fp_obj.map_account(cr, uid, fiscal_position, account_id)\\n        # set UoS if it\\'s a sale and the picking doesn\\'t have one\\n        uos_id = move_line.product_uos and move_line.product_uos.id or False\\n        if not uos_id and invoice_vals[\\'type\\'] in (\\'out_invoice\\', \\'out_refund\\'):\\n            uos_id = move_line.product_uom.id\\n\\n        return {\\n            \\'name\\': name,\\n            \\'origin\\': origin,\\n            \\'invoice_id\\': invoice_id,\\n            \\'uos_id\\': uos_id,\\n            \\'product_id\\': move_line.product_id.id,\\n            \\'account_id\\': account_id,\\n            \\'price_unit\\': self._get_price_unit_invoice(cr, uid, move_line, invoice_vals[\\'type\\']),\\n            \\'discount\\': self._get_discount_invoice(cr, uid, move_line),\\n            \\'quantity\\': move_line.product_uos_qty or move_line.product_qty,\\n            \\'invoice_line_tax_id\\': [(6, 0, self._get_taxes_invoice(cr, uid, move_line, invoice_vals[\\'type\\']))],\\n            \\'account_analytic_id\\': self._get_account_analytic_invoice(cr, uid, picking, move_line),\\n        }', 'def test_done(self, cr, uid, ids, context=None):\\n        \"\"\" Test whether the move lines are done or not.\\n        @return: True or False\\n        \"\"\"\\n        ok = False\\n        for pick in self.browse(cr, uid, ids, context=context):\\n            if not pick.move_lines:\\n                return True\\n            for move in pick.move_lines:\\n                if move.state not in (\\'cancel\\',\\'done\\'):\\n                    return False\\n                if move.state==\\'done\\':\\n                    ok = True\\n        return ok', \"def allow_cancel(self, cr, uid, ids, context=None):\\n        for pick in self.browse(cr, uid, ids, context=context):\\n            if not pick.move_lines:\\n                return True\\n            for move in pick.move_lines:\\n                if move.state == 'done':\\n                    raise osv.except_osv(_('Error!'), _('You cannot cancel the picking as some moves have been done. You should cancel the picking lines.'))\\n        return True\", 'def do_partial(self, cr, uid, ids, partial_datas, context=None):\\n        \"\"\" Makes partial picking and moves done.\\n        @param partial_datas : Dictionary containing details of partial picking\\n                          like partner_id, partner_id, delivery_date,\\n                          delivery moves with product_id, product_qty, uom\\n        @return: Dictionary of values\\n        \"\"\"\\n        if context is None:\\n            context = {}\\n        else:\\n            context = dict(context)\\n        res = {}\\n        move_obj = self.pool.get(\\'stock.move\\')\\n        product_obj = self.pool.get(\\'product.product\\')\\n        currency_obj = self.pool.get(\\'res.currency\\')\\n        uom_obj = self.pool.get(\\'product.uom\\')\\n        sequence_obj = self.pool.get(\\'ir.sequence\\')\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        for pick in self.browse(cr, uid, ids, context=context):\\n            new_picking = None\\n            complete, too_many, too_few = [], [], []\\n            move_product_qty, prodlot_ids, product_avail, partial_qty, product_uoms = {}, {}, {}, {}, {}\\n            for move in pick.move_lines:\\n                if move.state in (\\'done\\', \\'cancel\\'):\\n                    continue\\n                partial_data = partial_datas.get(\\'move%s\\'%(move.id), {})\\n                product_qty = partial_data.get(\\'product_qty\\',0.0)\\n                move_product_qty[move.id] = product_qty\\n                product_uom = partial_data.get(\\'product_uom\\',False)\\n                product_price = partial_data.get(\\'product_price\\',0.0)\\n                product_currency = partial_data.get(\\'product_currency\\',False)\\n                prodlot_id = partial_data.get(\\'prodlot_id\\')\\n                prodlot_ids[move.id] = prodlot_id\\n                product_uoms[move.id] = product_uom\\n                partial_qty[move.id] = uom_obj._compute_qty(cr, uid, product_uoms[move.id], product_qty, move.product_uom.id)\\n                if move.product_qty == partial_qty[move.id]:\\n                    complete.append(move)\\n                elif move.product_qty > partial_qty[move.id]:\\n                    too_few.append(move)\\n                else:\\n                    too_many.append(move)\\n\\n                # Average price computation\\n                if (pick.type == \\'in\\') and (move.product_id.cost_method == \\'average\\'):\\n                    product = product_obj.browse(cr, uid, move.product_id.id)\\n                    move_currency_id = move.company_id.currency_id.id\\n                    context[\\'currency_id\\'] = move_currency_id\\n                    qty = uom_obj._compute_qty(cr, uid, product_uom, product_qty, product.uom_id.id)\\n\\n                    if product.id not in product_avail:\\n                        # keep track of stock on hand including processed lines not yet marked as done\\n                        product_avail[product.id] = product.qty_available\\n\\n                    if qty > 0:\\n                        new_price = currency_obj.compute(cr, uid, product_currency,\\n                                move_currency_id, product_price, round=False)\\n                        new_price = uom_obj._compute_price(cr, uid, product_uom, new_price,\\n                                product.uom_id.id)\\n                        if product_avail[product.id] <= 0:\\n                            product_avail[product.id] = 0\\n                            new_std_price = new_price\\n                        else:\\n                            # Get the standard price\\n                            amount_unit = product.price_get(\\'standard_price\\', context=context)[product.id]\\n                            new_std_price = ((amount_unit * product_avail[product.id])\\\\\\n                                + (new_price * qty))/(product_avail[product.id] + qty)\\n                        # Write the field according to price type field\\n                        product_obj.write(cr, uid, [product.id], {\\'standard_price\\': new_std_price})\\n\\n                        # Record the values that were chosen in the wizard, so they can be\\n                        # used for inventory valuation if real-time valuation is enabled.\\n                        move_obj.write(cr, uid, [move.id],\\n                                {\\'price_unit\\': product_price,\\n                                 \\'price_currency_id\\': product_currency})\\n\\n                        product_avail[product.id] += qty\\n\\n\\n\\n            for move in too_few:\\n                product_qty = move_product_qty[move.id]\\n                if not new_picking:\\n                    new_picking_name = pick.name\\n                    self.write(cr, uid, [pick.id], \\n                               {\\'name\\': sequence_obj.get(cr, uid,\\n                                            \\'stock.picking.%s\\'%(pick.type)),\\n                               })\\n                    new_picking = self.copy(cr, uid, pick.id,\\n                            {\\n                                \\'name\\': new_picking_name,\\n                                \\'move_lines\\' : [],\\n                                \\'state\\':\\'draft\\',\\n                            })\\n                if product_qty != 0:\\n                    defaults = {\\n                            \\'product_qty\\' : product_qty,\\n                            \\'product_uos_qty\\': product_qty, #TODO: put correct uos_qty\\n                            \\'picking_id\\' : new_picking,\\n                            \\'state\\': \\'assigned\\',\\n                            \\'move_dest_id\\': False,\\n                            \\'price_unit\\': move.price_unit,\\n                            \\'product_uom\\': product_uoms[move.id]\\n                    }\\n                    prodlot_id = prodlot_ids[move.id]\\n                    if prodlot_id:\\n                        defaults.update(prodlot_id=prodlot_id)\\n                    move_obj.copy(cr, uid, move.id, defaults)\\n                move_obj.write(cr, uid, [move.id],\\n                        {\\n                            \\'product_qty\\': move.product_qty - partial_qty[move.id],\\n                            \\'product_uos_qty\\': move.product_qty - partial_qty[move.id], #TODO: put correct uos_qty\\n                            \\'prodlot_id\\': False,\\n                            \\'tracking_id\\': False,\\n                        })\\n\\n            if new_picking:\\n                move_obj.write(cr, uid, [c.id for c in complete], {\\'picking_id\\': new_picking})\\n            for move in complete:\\n                defaults = {\\'product_uom\\': product_uoms[move.id], \\'product_qty\\': move_product_qty[move.id]}\\n                if prodlot_ids.get(move.id):\\n                    defaults.update({\\'prodlot_id\\': prodlot_ids[move.id]})\\n                move_obj.write(cr, uid, [move.id], defaults)\\n            for move in too_many:\\n                product_qty = move_product_qty[move.id]\\n                defaults = {\\n                    \\'product_qty\\' : product_qty,\\n                    \\'product_uos_qty\\': product_qty, #TODO: put correct uos_qty\\n                    \\'product_uom\\': product_uoms[move.id]\\n                }\\n                prodlot_id = prodlot_ids.get(move.id)\\n                if prodlot_ids.get(move.id):\\n                    defaults.update(prodlot_id=prodlot_id)\\n                if new_picking:\\n                    defaults.update(picking_id=new_picking)\\n                move_obj.write(cr, uid, [move.id], defaults)\\n\\n            # At first we confirm the new picking (if necessary)\\n            if new_picking:\\n                wf_service.trg_validate(uid, \\'stock.picking\\', new_picking, \\'button_confirm\\', cr)\\n                # Then we finish the good picking\\n                self.write(cr, uid, [pick.id], {\\'backorder_id\\': new_picking})\\n                self.action_move(cr, uid, [new_picking], context=context)\\n                wf_service.trg_validate(uid, \\'stock.picking\\', new_picking, \\'button_done\\', cr)\\n                wf_service.trg_write(uid, \\'stock.picking\\', pick.id, cr)\\n                delivered_pack_id = pick.id\\n                back_order_name = self.browse(cr, uid, delivered_pack_id, context=context).name\\n                self.message_post(cr, uid, new_picking, body=_(\"Back order <em>%s</em> has been <b>created</b>.\") % (back_order_name), context=context)\\n            else:\\n                self.action_move(cr, uid, [pick.id], context=context)\\n                wf_service.trg_validate(uid, \\'stock.picking\\', pick.id, \\'button_done\\', cr)\\n                delivered_pack_id = pick.id\\n\\n            delivered_pack = self.browse(cr, uid, delivered_pack_id, context=context)\\n            res[pick.id] = {\\'delivered_picking\\': delivered_pack.id or False}\\n\\n        return res', 'def _get_view_id(self, cr, uid, type):\\n        \"\"\"Get the view id suiting the given type', \"def name_get(self, cr, uid, ids, context=None):\\n        if not ids:\\n            return []\\n        reads = self.read(cr, uid, ids, ['name', 'prefix', 'ref'], context)\\n        res = []\\n        for record in reads:\\n            name = record['name']\\n            prefix = record['prefix']\\n            if prefix:\\n                name = prefix + '/' + name\\n            if record['ref']:\\n                name = '%s [%s]' % (name, record['ref'])\\n            res.append((record['id'], name))\\n        return res\", 'def _get_stock(self, cr, uid, ids, field_name, arg, context=None):\\n        \"\"\" Gets stock of products for locations\\n        @return: Dictionary of values\\n        \"\"\"\\n        if context is None:\\n            context = {}\\n        if \\'location_id\\' not in context:\\n            locations = self.pool.get(\\'stock.location\\').search(cr, uid, [(\\'usage\\', \\'=\\', \\'internal\\')], context=context)\\n        else:\\n            locations = context[\\'location_id\\'] and [context[\\'location_id\\']] or []\\n\\n        if isinstance(ids, (int, long)):\\n            ids = [ids]\\n\\n        res = {}.fromkeys(ids, 0.0)\\n        if locations:\\n            cr.execute(\\'\\'\\'select\\n                    prodlot_id,\\n                    sum(qty)\\n                from\\n                    stock_report_prodlots\\n                where\\n                    location_id IN %s and prodlot_id IN %s group by prodlot_id\\'\\'\\',(tuple(locations),tuple(ids),))\\n            res.update(dict(cr.fetchall()))\\n\\n        return res', 'def action_traceability(self, cr, uid, ids, context=None):\\n        \"\"\" It traces the information of a product\\n        @param self: The object pointer.\\n        @param cr: A database cursor\\n        @param uid: ID of the user currently logged in\\n        @param ids: List of IDs selected\\n        @param context: A standard dictionary\\n        @return: A dictionary of values\\n        \"\"\"\\n        value=self.pool.get(\\'action.traceability\\').action_traceability(cr,uid,ids,context)\\n        return value', \"def _getSSCC(self, cr, uid, context=None):\\n        cr.execute('select id from stock_tracking where create_uid=%s order by id desc limit 1', (uid,))\\n        res = cr.fetchone()\\n        return (res and res[0]) or False\", 'def action_partial_move(self, cr, uid, ids, context=None):\\n        if context is None: context = {}\\n        if context.get(\\'active_model\\') != self._name:\\n            context.update(active_ids=ids, active_model=self._name)\\n        partial_id = self.pool.get(\"stock.partial.move\").create(\\n            cr, uid, {}, context=context)\\n        return {\\n            \\'name\\':_(\"Products to Process\"),\\n            \\'view_mode\\': \\'form\\',\\n            \\'view_id\\': False,\\n            \\'view_type\\': \\'form\\',\\n            \\'res_model\\': \\'stock.partial.move\\',\\n            \\'res_id\\': partial_id,\\n            \\'type\\': \\'ir.actions.act_window\\',\\n            \\'nodestroy\\': True,\\n            \\'target\\': \\'new\\',\\n            \\'domain\\': \\'[]\\',\\n            \\'context\\': context\\n        }', 'def _check_tracking(self, cr, uid, ids, context=None):\\n        \"\"\" Checks if serial number is assigned to stock move or not.\\n        @return: True or False\\n        \"\"\"\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if not move.prodlot_id and \\\\\\n               (move.state == \\'done\\' and \\\\\\n               ( \\\\\\n                   (move.product_id.track_production and move.location_id.usage == \\'production\\') or \\\\\\n                   (move.product_id.track_production and move.location_dest_id.usage == \\'production\\') or \\\\\\n                   (move.product_id.track_incoming and move.location_id.usage == \\'supplier\\') or \\\\\\n                   (move.product_id.track_outgoing and move.location_dest_id.usage == \\'customer\\') or \\\\\\n                   (move.product_id.track_incoming and move.location_id.usage == \\'inventory\\') \\\\\\n               )):\\n                return False\\n        return True', \"def _check_location(self, cr, uid, ids, context=None):\\n        for record in self.browse(cr, uid, ids, context=context):\\n            if (record.state=='done') and (record.location_id.usage == 'view'):\\n                raise osv.except_osv(_('Error'), _('You cannot move product %s from a location of type view %s.')% (record.product_id.name, record.location_id.name))\\n            if (record.state=='done') and (record.location_dest_id.usage == 'view' ):\\n                raise osv.except_osv(_('Error'), _('You cannot move product %s to a location of type view %s.')% (record.product_id.name, record.location_dest_id.name))\\n        return True\", 'def _default_location_destination(self, cr, uid, context=None):\\n        \"\"\" Gets default address of partner for destination location\\n        @return: Address id or False\\n        \"\"\"\\n        mod_obj = self.pool.get(\\'ir.model.data\\')\\n        picking_type = context.get(\\'picking_type\\')\\n        location_id = False\\n        if context is None:\\n            context = {}\\n        if context.get(\\'move_line\\', []):\\n            if context[\\'move_line\\'][0]:\\n                if isinstance(context[\\'move_line\\'][0], (tuple, list)):\\n                    location_id = context[\\'move_line\\'][0][2] and context[\\'move_line\\'][0][2].get(\\'location_dest_id\\',False)\\n                else:\\n                    move_list = self.pool.get(\\'stock.move\\').read(cr, uid, context[\\'move_line\\'][0], [\\'location_dest_id\\'])\\n                    location_id = move_list and move_list[\\'location_dest_id\\'][0] or False\\n        elif context.get(\\'address_out_id\\', False):\\n            property_out = self.pool.get(\\'res.partner\\').browse(cr, uid, context[\\'address_out_id\\'], context).property_stock_customer\\n            location_id = property_out and property_out.id or False\\n        else:\\n            location_xml_id = False\\n            if picking_type in (\\'in\\', \\'internal\\'):\\n                location_xml_id = \\'stock_location_stock\\'\\n            elif picking_type == \\'out\\':\\n                location_xml_id = \\'stock_location_customers\\'\\n            if location_xml_id:\\n                try:\\n                    location_model, location_id = mod_obj.get_object_reference(cr, uid, \\'stock\\', location_xml_id)\\n                    with tools.mute_logger(\\'openerp.osv.orm\\'):\\n                        self.pool.get(\\'stock.location\\').check_access_rule(cr, uid, [location_id], \\'read\\', context=context)\\n                except (orm.except_orm, ValueError):\\n                    location_id = False\\n\\n        return location_id', \"def _default_destination_address(self, cr, uid, context=None):\\n        user = self.pool.get('res.users').browse(cr, uid, uid, context=context)\\n        return user.company_id.partner_id.id\", \"def write(self, cr, uid, ids, vals, context=None):\\n        if isinstance(ids, (int, long)):\\n            ids = [ids]\\n        if uid != 1:\\n            frozen_fields = set(['product_qty', 'product_uom', 'product_uos_qty', 'product_uos', 'location_id', 'location_dest_id', 'product_id'])\\n            for move in self.browse(cr, uid, ids, context=context):\\n                if move.state == 'done':\\n                    if frozen_fields.intersection(vals):\\n                        raise osv.except_osv(_('Operation Forbidden!'),\\n                                             _('Quantities, Units of Measure, Products and Locations cannot be modified on stock moves that have already been processed (except by the Administrator).'))\\n        return  super(stock_move, self).write(cr, uid, ids, vals, context=context)\", \"def _auto_init(self, cursor, context=None):\\n        res = super(stock_move, self)._auto_init(cursor, context=context)\\n        cursor.execute('SELECT indexname \\\\\\n                FROM pg_indexes \\\\\\n                WHERE indexname = \\\\'stock_move_location_id_location_dest_id_product_id_state\\\\'')\\n        if not cursor.fetchone():\\n            cursor.execute('CREATE INDEX stock_move_location_id_location_dest_id_product_id_state \\\\\\n                    ON stock_move (product_id, state, location_id, location_dest_id)')\\n        return res\", 'def onchange_quantity(self, cr, uid, ids, product_id, product_qty,\\n                          product_uom, product_uos):\\n        \"\"\" On change of product quantity finds UoM and UoS quantities\\n        @param product_id: Product id\\n        @param product_qty: Changed Quantity of product\\n        @param product_uom: Unit of measure of product\\n        @param product_uos: Unit of sale of product\\n        @return: Dictionary of values\\n        \"\"\"\\n        result = {\\n                  \\'product_uos_qty\\': 0.00\\n          }\\n        warning = {}\\n\\n        if (not product_id) or (product_qty <=0.0):\\n            result[\\'product_qty\\'] = 0.0\\n            return {\\'value\\': result}\\n\\n        product_obj = self.pool.get(\\'product.product\\')\\n        uos_coeff = product_obj.read(cr, uid, product_id, [\\'uos_coeff\\'])', 'def onchange_uos_quantity(self, cr, uid, ids, product_id, product_uos_qty,\\n                          product_uos, product_uom):\\n        \"\"\" On change of product quantity finds UoM and UoS quantities\\n        @param product_id: Product id\\n        @param product_uos_qty: Changed UoS Quantity of product\\n        @param product_uom: Unit of measure of product\\n        @param product_uos: Unit of sale of product\\n        @return: Dictionary of values\\n        \"\"\"\\n        result = {\\n                  \\'product_qty\\': 0.00\\n          }\\n        warning = {}\\n\\n        if (not product_id) or (product_uos_qty <=0.0):\\n            result[\\'product_uos_qty\\'] = 0.0\\n            return {\\'value\\': result}\\n\\n        product_obj = self.pool.get(\\'product.product\\')\\n        uos_coeff = product_obj.read(cr, uid, product_id, [\\'uos_coeff\\'])', 'def onchange_product_id(self, cr, uid, ids, prod_id=False, loc_id=False,\\n                            loc_dest_id=False, partner_id=False):\\n        \"\"\" On change of product id, if finds UoM, UoS, quantity and UoS quantity.\\n        @param prod_id: Changed Product id\\n        @param loc_id: Source location id\\n        @param loc_dest_id: Destination location id\\n        @param partner_id: Address id of partner\\n        @return: Dictionary of values\\n        \"\"\"\\n        if not prod_id:\\n            return {}\\n        user = self.pool.get(\\'res.users\\').browse(cr, uid, uid)\\n        lang = user and user.lang or False\\n        if partner_id:\\n            addr_rec = self.pool.get(\\'res.partner\\').browse(cr, uid, partner_id)\\n            if addr_rec:\\n                lang = addr_rec and addr_rec.lang or False\\n        ctx = {\\'lang\\': lang}\\n\\n        product = self.pool.get(\\'product.product\\').browse(cr, uid, [prod_id], context=ctx)[0]\\n        uos_id  = product.uos_id and product.uos_id.id or False\\n        result = {\\n            \\'product_uom\\': product.uom_id.id,\\n            \\'product_uos\\': uos_id,\\n            \\'product_qty\\': 1.00,\\n            \\'product_uos_qty\\' : self.pool.get(\\'stock.move\\').onchange_quantity(cr, uid, ids, prod_id, 1.00, product.uom_id.id, uos_id)[\\'value\\'][\\'product_uos_qty\\'],\\n            \\'prodlot_id\\' : False,\\n        }\\n        if not ids:\\n            result[\\'name\\'] = product.partner_ref\\n        if loc_id:\\n            result[\\'location_id\\'] = loc_id\\n        if loc_dest_id:\\n            result[\\'location_dest_id\\'] = loc_dest_id\\n        return {\\'value\\': result}', 'def onchange_date(self, cr, uid, ids, date, date_expected, context=None):\\n        \"\"\" On change of Scheduled Date gives a Move date.\\n        @param date_expected: Scheduled Date\\n        @param date: Move Date\\n        @return: Move Date\\n        \"\"\"\\n        if not date_expected:\\n            date_expected = time.strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n        return {\\'value\\':{\\'date\\': date_expected}}', 'def _prepare_chained_picking(self, cr, uid, picking_name, picking, picking_type, moves_todo, context=None):\\n        \"\"\"Prepare the definition (values) to create a new chained picking.\\n\\n           :param str picking_name: desired new picking name\\n           :param browse_record picking: source picking (being chained to)\\n           :param str picking_type: desired new picking type\\n           :param list moves_todo: specification of the stock moves to be later included in this\\n               picking, in the form::\\n\\n                   [[move, (dest_location, auto_packing, chained_delay, chained_journal,\\n                                  chained_company_id, chained_picking_type)],\\n                    ...\\n                   ]\\n\\n               See also :meth:`stock_location.chained_location_get`.\\n        \"\"\"\\n        res_company = self.pool.get(\\'res.company\\')\\n        return {\\n                    \\'name\\': picking_name,\\n                    \\'origin\\': tools.ustr(picking.origin or \\'\\'),\\n                    \\'type\\': picking_type,\\n                    \\'note\\': picking.note,\\n                    \\'move_type\\': picking.move_type,\\n                    \\'auto_picking\\': moves_todo[0][1][1] == \\'auto\\',\\n                    \\'stock_journal_id\\': moves_todo[0][1][3],\\n                    \\'company_id\\': moves_todo[0][1][4] or res_company._company_default_get(cr, uid, \\'stock.company\\', context=context),\\n                    \\'partner_id\\': picking.partner_id.id,\\n                    \\'invoice_state\\': \\'none\\',\\n                    \\'date\\': picking.date,\\n                }', 'def create_chained_picking(self, cr, uid, moves, context=None):\\n        res_obj = self.pool.get(\\'res.company\\')\\n        location_obj = self.pool.get(\\'stock.location\\')\\n        move_obj = self.pool.get(\\'stock.move\\')\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        new_moves = []\\n        if context is None:\\n            context = {}\\n        seq_obj = self.pool.get(\\'ir.sequence\\')\\n        for picking, todo in self._chain_compute(cr, uid, moves, context=context).items():\\n            ptype = todo[0][1][5] and todo[0][1][5] or location_obj.picking_type_get(cr, uid, todo[0][0].location_dest_id, todo[0][1][0])\\n            if picking:\\n                # name of new picking according to its type\\n                if ptype == \\'internal\\':\\n                    new_pick_name = seq_obj.get(cr, uid,\\'stock.picking\\')\\n                else :\\n                    new_pick_name = seq_obj.get(cr, uid, \\'stock.picking.\\' + ptype)\\n                pickid = self._create_chained_picking(cr, uid, new_pick_name, picking, ptype, todo, context=context)\\n                # Need to check name of old picking because it always considers picking as \"OUT\" when created from Sales Order\\n                old_ptype = location_obj.picking_type_get(cr, uid, picking.move_lines[0].location_id, picking.move_lines[0].location_dest_id)\\n                if old_ptype != picking.type:\\n                    old_pick_name = seq_obj.get(cr, uid, \\'stock.picking.\\' + old_ptype)\\n                    self.pool.get(\\'stock.picking\\').write(cr, uid, [picking.id], {\\'name\\': old_pick_name, \\'type\\': old_ptype}, context=context)\\n            else:\\n                pickid = False\\n            for move, (loc, dummy, delay, dummy, company_id, ptype, invoice_state) in todo:\\n                new_id = move_obj.copy(cr, uid, move.id, {\\n                    \\'location_id\\': move.location_dest_id.id,\\n                    \\'location_dest_id\\': loc.id,\\n                    \\'date\\': time.strftime(\\'%Y-%m-%d\\'),\\n                    \\'picking_id\\': pickid,\\n                    \\'state\\': \\'waiting\\',\\n                    \\'company_id\\': company_id or res_obj._company_default_get(cr, uid, \\'stock.company\\', context=context)  ,\\n                    \\'move_history_ids\\': [],\\n                    \\'date_expected\\': (datetime.strptime(move.date, \\'%Y-%m-%d %H:%M:%S\\') + relativedelta(days=delay or 0)).strftime(\\'%Y-%m-%d\\'),\\n                    \\'move_history_ids2\\': []}\\n                )\\n                move_obj.write(cr, uid, [move.id], {\\n                    \\'move_dest_id\\': new_id,\\n                    \\'move_history_ids\\': [(4, new_id)]\\n                })\\n                new_moves.append(self.browse(cr, uid, [new_id])[0])\\n            if pickid:\\n                wf_service.trg_validate(uid, \\'stock.picking\\', pickid, \\'button_confirm\\', cr)\\n        if new_moves:\\n            new_moves += self.create_chained_picking(cr, uid, new_moves, context)\\n        return new_moves', 'def action_assign(self, cr, uid, ids, *args):\\n        \"\"\" Changes state to confirmed or waiting.\\n        @return: List of values\\n        \"\"\"\\n        todo = []\\n        for move in self.browse(cr, uid, ids):\\n            if move.state in (\\'confirmed\\', \\'waiting\\'):\\n                todo.append(move.id)\\n        res = self.check_assign(cr, uid, todo)\\n        return res', 'def cancel_assign(self, cr, uid, ids, context=None):\\n        \"\"\" Changes the state to confirmed.\\n        @return: True\\n        \"\"\"\\n        self.write(cr, uid, ids, {\\'state\\': \\'confirmed\\'})\\n\\n        # fix for bug lp:707031\\n        # called write of related picking because changing move availability does\\n        # not trigger workflow of picking in order to change the state of picking\\n        wf_service = netsvc.LocalService(\\'workflow\\')\\n        for move in self.browse(cr, uid, ids, context):\\n            if move.picking_id:\\n                wf_service.trg_write(uid, \\'stock.picking\\', move.picking_id.id, cr)\\n        return True', 'def check_assign(self, cr, uid, ids, context=None):\\n        \"\"\" Checks the product type and accordingly writes the state.\\n        @return: No. of moves done\\n        \"\"\"\\n        done = []\\n        count = 0\\n        pickings = {}\\n        if context is None:\\n            context = {}\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.product_id.type == \\'consu\\' or move.location_id.usage == \\'supplier\\':\\n                if move.state in (\\'confirmed\\', \\'waiting\\'):\\n                    done.append(move.id)\\n                pickings[move.picking_id.id] = 1\\n                continue\\n            if move.state in (\\'confirmed\\', \\'waiting\\'):\\n                # Important: we must pass lock=True to _product_reserve() to avoid race conditions and double reservations\\n                res = self.pool.get(\\'stock.location\\')._product_reserve(cr, uid, [move.location_id.id], move.product_id.id, move.product_qty, {\\'uom\\': move.product_uom.id}, lock=True)\\n                if res:\\n                    #_product_available_test depends on the next status for correct functioning\\n                    #the test does not work correctly if the same product occurs multiple times\\n                    #in the same order. This is e.g. the case when using the button \\'split in two\\' of\\n                    #the stock outgoing form\\n                    self.write(cr, uid, [move.id], {\\'state\\':\\'assigned\\'})\\n                    done.append(move.id)\\n                    pickings[move.picking_id.id] = 1\\n                    r = res.pop(0)\\n                    product_uos_qty = self.pool.get(\\'stock.move\\').onchange_quantity(cr, uid, ids, move.product_id.id, r[0], move.product_id.uom_id.id, move.product_id.uos_id.id)[\\'value\\'][\\'product_uos_qty\\']\\n                    cr.execute(\\'update stock_move set location_id=%s, product_qty=%s, product_uos_qty=%s where id=%s\\', (r[1], r[0],product_uos_qty, move.id))\\n\\n                    while res:\\n                        r = res.pop(0)\\n                        product_uos_qty = self.pool.get(\\'stock.move\\').onchange_quantity(cr, uid, ids, move.product_id.id, r[0], move.product_id.uom_id.id, move.product_id.uos_id.id)[\\'value\\'][\\'product_uos_qty\\']\\n                        move_id = self.copy(cr, uid, move.id, {\\'product_uos_qty\\': product_uos_qty, \\'product_qty\\': r[0], \\'location_id\\': r[1]})\\n                        done.append(move_id)\\n        if done:\\n            count += len(done)\\n            self.write(cr, uid, done, {\\'state\\': \\'assigned\\'})\\n\\n        if count:\\n            for pick_id in pickings:\\n                wf_service = netsvc.LocalService(\"workflow\")\\n                wf_service.trg_write(uid, \\'stock.picking\\', pick_id, cr)\\n        return count', 'def action_cancel(self, cr, uid, ids, context=None):\\n        \"\"\" Cancels the moves and if all moves are cancelled it cancels the picking.\\n        @return: True\\n        \"\"\"\\n        if not len(ids):\\n            return True\\n        if context is None:\\n            context = {}\\n        pickings = set()\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.state in (\\'confirmed\\', \\'waiting\\', \\'assigned\\', \\'draft\\'):\\n                if move.picking_id:\\n                    pickings.add(move.picking_id.id)\\n            if move.move_dest_id and move.move_dest_id.state == \\'waiting\\':\\n                self.write(cr, uid, [move.move_dest_id.id], {\\'state\\': \\'confirmed\\'}, context=context)\\n                if context.get(\\'call_unlink\\',False) and move.move_dest_id.picking_id:\\n                    wf_service = netsvc.LocalService(\"workflow\")\\n                    wf_service.trg_write(uid, \\'stock.picking\\', move.move_dest_id.picking_id.id, cr)\\n        self.write(cr, uid, ids, {\\'state\\': \\'cancel\\', \\'move_dest_id\\': False}, context=context)\\n        if not context.get(\\'call_unlink\\',False):\\n            for pick in self.pool.get(\\'stock.picking\\').browse(cr, uid, list(pickings), context=context):\\n                if all(move.state == \\'cancel\\' for move in pick.move_lines):\\n                    self.pool.get(\\'stock.picking\\').write(cr, uid, [pick.id], {\\'state\\': \\'cancel\\'}, context=context)\\n\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        for id in ids:\\n            wf_service.trg_trigger(uid, \\'stock.move\\', id, cr)\\n        return True', 'def _get_reference_accounting_values_for_valuation(self, cr, uid, move, context=None):\\n        \"\"\"\\n        Return the reference amount and reference currency representing the inventory valuation for this move.\\n        These reference values should possibly be converted before being posted in Journals to adapt to the primary\\n        and secondary currencies of the relevant accounts.\\n        \"\"\"\\n        product_uom_obj = self.pool.get(\\'product.uom\\')\\n\\n        # by default the reference currency is that of the move\\'s company\\n        reference_currency_id = move.company_id.currency_id.id\\n\\n        default_uom = move.product_id.uom_id.id\\n        qty = product_uom_obj._compute_qty(cr, uid, move.product_uom.id, move.product_qty, default_uom)\\n\\n        # if product is set to average price and a specific value was entered in the picking wizard,\\n        # we use it\\n        if move.product_id.cost_method == \\'average\\' and move.price_unit:\\n            reference_amount = qty * move.price_unit\\n            reference_currency_id = move.price_currency_id.id or reference_currency_id\\n\\n        # Otherwise we default to the company\\'s valuation price type, considering that the values of the\\n        # valuation field are expressed in the default currency of the move\\'s company.\\n        else:\\n            if context is None:\\n                context = {}\\n            currency_ctx = dict(context, currency_id = move.company_id.currency_id.id)\\n            amount_unit = move.product_id.price_get(\\'standard_price\\', context=currency_ctx)[move.product_id.id]\\n            reference_amount = amount_unit * qty\\n\\n        return reference_amount, reference_currency_id', 'def action_done(self, cr, uid, ids, context=None):\\n        \"\"\" Makes the move done and if all moves are done, it will finish the picking.\\n        @return:\\n        \"\"\"\\n        picking_ids = []\\n        move_ids = []\\n        wf_service = netsvc.LocalService(\"workflow\")\\n        if context is None:\\n            context = {}\\n\\n        todo = []\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.state==\"draft\":\\n                todo.append(move.id)\\n        if todo:\\n            self.action_confirm(cr, uid, todo, context=context)\\n            todo = []\\n\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.state in [\\'done\\',\\'cancel\\']:\\n                continue\\n            move_ids.append(move.id)\\n\\n            if move.picking_id:\\n                picking_ids.append(move.picking_id.id)\\n            if move.move_dest_id.id and (move.state != \\'done\\'):\\n                # Downstream move should only be triggered if this move is the last pending upstream move\\n                other_upstream_move_ids = self.search(cr, uid, [(\\'id\\',\\'!=\\',move.id),(\\'state\\',\\'not in\\',[\\'done\\',\\'cancel\\']),\\n                                            (\\'move_dest_id\\',\\'=\\',move.move_dest_id.id)], context=context)\\n                if not other_upstream_move_ids:\\n                    self.write(cr, uid, [move.id], {\\'move_history_ids\\': [(4, move.move_dest_id.id)]})\\n                    if move.move_dest_id.state in (\\'waiting\\', \\'confirmed\\'):\\n                        self.force_assign(cr, uid, [move.move_dest_id.id], context=context)\\n                        if move.move_dest_id.picking_id:\\n                            wf_service.trg_write(uid, \\'stock.picking\\', move.move_dest_id.picking_id.id, cr)\\n                        if move.move_dest_id.auto_validate:\\n                            self.action_done(cr, uid, [move.move_dest_id.id], context=context)\\n\\n            self._create_product_valuation_moves(cr, uid, move, context=context)\\n            if move.state not in (\\'confirmed\\',\\'done\\',\\'assigned\\'):\\n                todo.append(move.id)\\n\\n        if todo:\\n            self.action_confirm(cr, uid, todo, context=context)\\n\\n        self.write(cr, uid, move_ids, {\\'state\\': \\'done\\', \\'date\\': time.strftime(DEFAULT_SERVER_DATETIME_FORMAT)}, context=context)\\n        for id in move_ids:\\n             wf_service.trg_trigger(uid, \\'stock.move\\', id, cr)\\n\\n        for pick_id in picking_ids:\\n            wf_service.trg_write(uid, \\'stock.picking\\', pick_id, cr)\\n\\n        return True', \"def unlink(self, cr, uid, ids, context=None):\\n        if context is None:\\n            context = {}\\n        ctx = context.copy()\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.state != 'draft' and not ctx.get('call_unlink', False):\\n                raise osv.except_osv(_('User Error!'), _('You can only delete draft moves.'))\\n        return super(stock_move, self).unlink(\\n            cr, uid, ids, context=ctx)\", 'def _create_lot(self, cr, uid, ids, product_id, prefix=False):\\n        \"\"\" Creates production lot\\n        @return: Production lot id\\n        \"\"\"\\n        prodlot_obj = self.pool.get(\\'stock.production.lot\\')\\n        prodlot_id = prodlot_obj.create(cr, uid, {\\'prefix\\': prefix, \\'product_id\\': product_id})\\n        return prodlot_id', 'def action_split(self, cr, uid, ids, quantity, split_by_qty=1, prefix=False, with_lot=True, context=None):\\n        \"\"\" Split Stock Move lines into production lot which specified split by quantity.\\n        @param cr: the database cursor\\n        @param uid: the user id\\n        @param ids: ids of stock move object to be splited\\n        @param split_by_qty : specify split by qty\\n        @param prefix : specify prefix of production lot\\n        @param with_lot : if true, prodcution lot will assign for split line otherwise not.\\n        @param context: context arguments\\n        @return: Splited move lines\\n        \"\"\"\\n\\n        if context is None:\\n            context = {}\\n        if quantity <= 0:\\n            raise osv.except_osv(_(\\'Warning!\\'), _(\\'Please provide proper quantity.\\'))\\n\\n        res = []\\n\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if split_by_qty <= 0 or quantity == 0:\\n                return res\\n\\n            uos_qty = split_by_qty / move.product_qty * move.product_uos_qty\\n\\n            quantity_rest = quantity % split_by_qty\\n            uos_qty_rest = split_by_qty / move.product_qty * move.product_uos_qty\\n\\n            update_val = {\\n                \\'product_qty\\': split_by_qty,\\n                \\'product_uos_qty\\': uos_qty,\\n            }\\n            for idx in range(int(quantity//split_by_qty)):\\n                if not idx and move.product_qty<=quantity:\\n                    current_move = move.id\\n                else:\\n                    current_move = self.copy(cr, uid, move.id, {\\'state\\': move.state})\\n                res.append(current_move)\\n                if with_lot:\\n                    update_val[\\'prodlot_id\\'] = self._create_lot(cr, uid, [current_move], move.product_id.id)\\n\\n                self.write(cr, uid, [current_move], update_val)\\n\\n\\n            if quantity_rest > 0:\\n                idx = int(quantity//split_by_qty)\\n                update_val[\\'product_qty\\'] = quantity_rest\\n                update_val[\\'product_uos_qty\\'] = uos_qty_rest\\n                if not idx and move.product_qty<=quantity:\\n                    current_move = move.id\\n                else:\\n                    current_move = self.copy(cr, uid, move.id, {\\'state\\': move.state})\\n\\n                res.append(current_move)\\n\\n\\n                if with_lot:\\n                    update_val[\\'prodlot_id\\'] = self._create_lot(cr, uid, [current_move], move.product_id.id)\\n\\n                self.write(cr, uid, [current_move], update_val)\\n        return res', 'def do_partial(self, cr, uid, ids, partial_datas, context=None):\\n        \"\"\" Makes partial pickings and moves done.\\n        @param partial_datas: Dictionary containing details of partial picking\\n                          like partner_id, delivery_date, delivery\\n                          moves with product_id, product_qty, uom\\n        \"\"\"\\n        res = {}\\n        picking_obj = self.pool.get(\\'stock.picking\\')\\n        product_obj = self.pool.get(\\'product.product\\')\\n        currency_obj = self.pool.get(\\'res.currency\\')\\n        uom_obj = self.pool.get(\\'product.uom\\')\\n        wf_service = netsvc.LocalService(\"workflow\")\\n\\n        if context is None:\\n            context = {}\\n\\n        complete, too_many, too_few = [], [], []\\n        move_product_qty = {}\\n        prodlot_ids = {}\\n        for move in self.browse(cr, uid, ids, context=context):\\n            if move.state in (\\'done\\', \\'cancel\\'):\\n                continue\\n            partial_data = partial_datas.get(\\'move%s\\'%(move.id), False)\\n            assert partial_data, _(\\'Missing partial picking data for move #%s.\\') % (move.id)\\n            product_qty = partial_data.get(\\'product_qty\\',0.0)\\n            move_product_qty[move.id] = product_qty\\n            product_uom = partial_data.get(\\'product_uom\\',False)\\n            product_price = partial_data.get(\\'product_price\\',0.0)\\n            product_currency = partial_data.get(\\'product_currency\\',False)\\n            prodlot_ids[move.id] = partial_data.get(\\'prodlot_id\\')\\n            if move.product_qty == product_qty:\\n                complete.append(move)\\n            elif move.product_qty > product_qty:\\n                too_few.append(move)\\n            else:\\n                too_many.append(move)\\n\\n            # Average price computation\\n            if (move.picking_id.type == \\'in\\') and (move.product_id.cost_method == \\'average\\'):\\n                product = product_obj.browse(cr, uid, move.product_id.id)\\n                move_currency_id = move.company_id.currency_id.id\\n                context[\\'currency_id\\'] = move_currency_id\\n                qty = uom_obj._compute_qty(cr, uid, product_uom, product_qty, product.uom_id.id)\\n                if qty > 0:\\n                    new_price = currency_obj.compute(cr, uid, product_currency,\\n                            move_currency_id, product_price, round=False)\\n                    new_price = uom_obj._compute_price(cr, uid, product_uom, new_price,\\n                            product.uom_id.id)\\n                    if product.qty_available <= 0:\\n                        new_std_price = new_price\\n                    else:\\n                        # Get the standard price\\n                        amount_unit = product.price_get(\\'standard_price\\', context=context)[product.id]\\n                        new_std_price = ((amount_unit * product.qty_available)\\\\\\n                            + (new_price * qty))/(product.qty_available + qty)\\n\\n                    product_obj.write(cr, uid, [product.id],{\\'standard_price\\': new_std_price})\\n\\n                    # Record the values that were chosen in the wizard, so they can be\\n                    # used for inventory valuation if real-time valuation is enabled.\\n                    self.write(cr, uid, [move.id],\\n                                {\\'price_unit\\': product_price,\\n                                 \\'price_currency_id\\': product_currency,\\n                                })\\n\\n        for move in too_few:\\n            product_qty = move_product_qty[move.id]\\n            if product_qty != 0:\\n                defaults = {\\n                            \\'product_qty\\' : product_qty,\\n                            \\'product_uos_qty\\': product_qty,\\n                            \\'picking_id\\' : move.picking_id.id,\\n                            \\'state\\': \\'assigned\\',\\n                            \\'move_dest_id\\': False,\\n                            \\'price_unit\\': move.price_unit,\\n                            }\\n                prodlot_id = prodlot_ids[move.id]\\n                if prodlot_id:\\n                    defaults.update(prodlot_id=prodlot_id)\\n                new_move = self.copy(cr, uid, move.id, defaults)\\n                complete.append(self.browse(cr, uid, new_move))\\n            self.write(cr, uid, [move.id],\\n                    {\\n                        \\'product_qty\\': move.product_qty - product_qty,\\n                        \\'product_uos_qty\\': move.product_qty - product_qty,\\n                        \\'prodlot_id\\': False,\\n                        \\'tracking_id\\': False,\\n                    })\\n\\n\\n        for move in too_many:\\n            self.write(cr, uid, [move.id],\\n                    {\\n                        \\'product_qty\\': move.product_qty,\\n                        \\'product_uos_qty\\': move.product_qty,\\n                    })\\n            complete.append(move)\\n\\n        for move in complete:\\n            if prodlot_ids.get(move.id):\\n                self.write(cr, uid, [move.id],{\\'prodlot_id\\': prodlot_ids.get(move.id)})\\n            self.action_done(cr, uid, [move.id], context=context)\\n            if  move.picking_id.id :\\n                # TOCHECK : Done picking if all moves are done\\n                cr.execute(\"\"\"\\n                    SELECT move.id FROM stock_picking pick\\n                    RIGHT JOIN stock_move move ON move.picking_id = pick.id AND move.state = %s\\n                    WHERE pick.id = %s\"\"\",\\n                            (\\'done\\', move.picking_id.id))\\n                res = cr.fetchall()\\n                if len(res) == len(move.picking_id.move_lines):\\n                    picking_obj.action_move(cr, uid, [move.picking_id.id])\\n                    wf_service.trg_validate(uid, \\'stock.picking\\', move.picking_id.id, \\'button_done\\', cr)\\n\\n        return [move.id for move in complete]', \"def copy(self, cr, uid, id, default=None, context=None):\\n        if default is None:\\n            default = {}\\n        default = default.copy()\\n        default.update({'move_ids': [], 'date_done': False})\\n        return super(stock_inventory, self).copy(cr, uid, id, default, context=context)\", 'def action_done(self, cr, uid, ids, context=None):\\n        \"\"\" Finish the inventory\\n        @return: True\\n        \"\"\"\\n        if context is None:\\n            context = {}\\n        move_obj = self.pool.get(\\'stock.move\\')\\n        for inv in self.browse(cr, uid, ids, context=context):\\n            move_obj.action_done(cr, uid, [x.id for x in inv.move_ids], context=context)\\n            self.write(cr, uid, [inv.id], {\\'state\\':\\'done\\', \\'date_done\\': time.strftime(\\'%Y-%m-%d %H:%M:%S\\')}, context=context)\\n        return True', 'def action_cancel_draft(self, cr, uid, ids, context=None):\\n        \"\"\" Cancels the stock move and change inventory state to draft.\\n        @return: True\\n        \"\"\"\\n        for inv in self.browse(cr, uid, ids, context=context):\\n            self.pool.get(\\'stock.move\\').action_cancel(cr, uid, [x.id for x in inv.move_ids], context=context)\\n            self.write(cr, uid, [inv.id], {\\'state\\':\\'draft\\'}, context=context)\\n        return True', \"def _default_stock_location(self, cr, uid, context=None):\\n        try:\\n            location_model, location_id = self.pool.get('ir.model.data').get_object_reference(cr, uid, 'stock', 'stock_location_stock')\\n            with tools.mute_logger('openerp.osv.orm'):\\n                self.pool.get('stock.location').check_access_rule(cr, uid, [location_id], 'read', context=context)\\n        except (orm.except_orm, ValueError):\\n            location_id = False\\n        return location_id\", 'def on_change_product_id(self, cr, uid, ids, location_id, product, uom=False, to_date=False):\\n        \"\"\" Changes UoM and name if product_id changes.\\n        @param location_id: Location id\\n        @param product: Changed product_id\\n        @param uom: UoM product\\n        @return:  Dictionary of changed values\\n        \"\"\"\\n        if not product:\\n            return {\\'value\\': {\\'product_qty\\': 0.0, \\'product_uom\\': False, \\'prod_lot_id\\': False}}\\n        obj_product = self.pool.get(\\'product.product\\').browse(cr, uid, product)\\n        uom = uom or obj_product.uom_id.id\\n        amount = self.pool.get(\\'stock.location\\')._product_get(cr, uid, location_id, [product], {\\'uom\\': uom, \\'to_date\\': to_date, \\'compute_child\\': False})[product]\\n        result = {\\'product_qty\\': amount, \\'product_uom\\': uom, \\'prod_lot_id\\': False}\\n        return {\\'value\\': result}', \"def _default_lot_input_stock_id(self, cr, uid, context=None):\\n        try:\\n            lot_input_stock_model, lot_input_stock_id = self.pool.get('ir.model.data').get_object_reference(cr, uid, 'stock', 'stock_location_stock')\\n            with tools.mute_logger('openerp.osv.orm'):\\n                self.pool.get('stock.location').check_access_rule(cr, uid, [lot_input_stock_id], 'read', context=context)\\n        except (ValueError, orm.except_orm):\\n            # the user does not have read access on the location or it does not exists\\n            lot_input_stock_id = False\\n        return lot_input_stock_id\", \"def search(self, cr, user, args, offset=0, limit=None, order=None, context=None, count=False):\\n        return self.pool.get('stock.picking').search(cr, user, args, offset, limit, order, context, count)\", \"def check_access_rights(self, cr, uid, operation, raise_exception=True):\\n        #override in order to redirect the check of acces rights on the stock.picking object\\n        return self.pool.get('stock.picking').check_access_rights(cr, uid, operation, raise_exception=raise_exception)\", \"def _workflow_trigger(self, cr, uid, ids, trigger, context=None):\\n        #override in order to trigger the workflow of stock.picking at the end of create, write and unlink operation\\n        #instead of it's own workflow (which is not existing)\\n        return self.pool.get('stock.picking')._workflow_trigger(cr, uid, ids, trigger, context=context)\", 'def message_post(self, *args, **kwargs):\\n        \"\"\"Post the message on stock.picking to be able to see it in the form view when using the chatter\"\"\"\\n        return self.pool.get(\\'stock.picking\\').message_post(*args, **kwargs)', 'def message_unsubscribe(self, *args, **kwargs):\\n        \"\"\"Send the unsubscribe action on stock.picking model to match with subscribe\"\"\"\\n        return self.pool.get(\\'stock.picking\\').message_unsubscribe(*args, **kwargs)', \"def search(self, cr, user, args, offset=0, limit=None, order=None, context=None, count=False):\\n        return self.pool.get('stock.picking').search(cr, user, args, offset, limit, order, context, count)\", \"def check_access_rights(self, cr, uid, operation, raise_exception=True):\\n        #override in order to redirect the check of acces rights on the stock.picking object\\n        return self.pool.get('stock.picking').check_access_rights(cr, uid, operation, raise_exception=raise_exception)\", \"def _workflow_trigger(self, cr, uid, ids, trigger, context=None):\\n        #override in order to trigger the workflow of stock.picking at the end of create, write and unlink operation\\n        #instead of it's own workflow (which is not existing)\\n        return self.pool.get('stock.picking')._workflow_trigger(cr, uid, ids, trigger, context=context)\", 'def message_post(self, *args, **kwargs):\\n        \"\"\"Post the message on stock.picking to be able to see it in the form view when using the chatter\"\"\"\\n        return self.pool.get(\\'stock.picking\\').message_post(*args, **kwargs)', 'def message_unsubscribe(self, *args, **kwargs):\\n        \"\"\"Send the unsubscribe action on stock.picking model to match with subscribe\"\"\"\\n        return self.pool.get(\\'stock.picking\\').message_unsubscribe(*args, **kwargs)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, requested_depth=None):\\n        self.requested_depth = requested_depth', 'def name(cls):\\n        return \"blocks_api:block_depth\"', 'def get_block_depth(cls, block_structure, block_key):\\n        \"\"\"\\n        Return the precalculated depth of a block within the block_structure:\\n\\n        Arguments:\\n            block_structure: a BlockStructure instance\\n            block_key: the key of the block whose depth we want to know\\n\\n        Returns:\\n            int\\n        \"\"\"\\n        return block_structure.get_transformer_block_field(\\n            block_key,\\n            cls,\\n            cls.BLOCK_DEPTH,\\n        )']}, {'features': [], 'snippets': ['def _get_priorities(self):\\n        \"\"\"\\n        Load priorities from parameters.\\n        :return: dict\\n        \"\"\"\\n        key = \\'mail.sending.job.priorities\\'\\n        try:\\n            priorities = ast.literal_eval(\\n                self.env[\\'ir.config_parameter\\'].sudo().get_param(\\n                    key, default=\\'{}\\'))\\n        # Catch exception to have a understandable error message\\n        except (ValueError, SyntaxError):\\n            raise exceptions.UserError(\\n                _(\"Error to load the system parameter (%s) \"\\n                  \"of priorities\") % key)\\n        # As literal_eval can transform str into any format, check if we\\n        # have a real dict\\n        if not isinstance(priorities, dict):\\n            raise exceptions.UserError(\\n                _(\"Error to load the system parameter (%s) of priorities.\\\\n\"\\n                  \"Invalid dictionary\") % key)\\n        return priorities']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, program, timeOfDay):\\n        super().__init__()\\n        self._program = program\\n        self._timeOfDay = timeOfDay', 'def setThreadStopEvent(self, threadStopEvent):\\n        self.threadStopEvent = threadStopEvent\\n        self._program.setThreadStopEvent(threadStopEvent)', 'def getCurrentColor(self):\\n        return self._program.getCurrentColor()']}, {'features': [], 'snippets': ['def main(*args):\\n    _args = ()\\n    for arg in args:\\n        if isinstance(arg, Path):\\n            _args += (str(arg),)\\n        else:\\n            _args += (arg,)\\n    _main(_args)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def testEmpty(self):\\r\\n        hpcp = HPCP()([], [])\\r\\n        self.assertEqualVector(hpcp, [0.]*12)', 'def testZeros(self):\\r\\n        hpcp = HPCP()([0]*10, [0]*10)\\r\\n        self.assertEqualVector(hpcp, [0.]*12)', \"def testSin440(self):\\r\\n        # Tests whether a real audio signal of one pure tone gets read as a\\r\\n        # single semitone activation, and gets read into the right pcp bin\\r\\n        sampleRate = 44100\\r\\n        audio = MonoLoader(filename = join(testdata.audio_dir, 'generated/synthesised/sin440_0db.wav'),\\r\\n                           sampleRate = sampleRate)()\\r\\n        speaks = SpectralPeaks(sampleRate = sampleRate,\\r\\n                               maxPeaks = 1,\\r\\n                               maxFrequency = sampleRate/2,\\r\\n                               minFrequency = 0,\\r\\n                               magnitudeThreshold = 0,\\r\\n                               orderBy = 'magnitude')\\r\\n        (freqs, mags) = speaks(Spectrum()(audio))\\r\\n        hpcp = HPCP()(freqs, mags)\\r\\n        self.assertEqualVector(hpcp, [1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])\", \"def testAllSemitones(self):\\r\\n        # Tests whether a spectral peak output of 12 consecutive semitones\\r\\n        # yields a HPCP of all 1's\\r\\n        tonic = 440\\r\\n        freqs = [(tonic * 2**(x/12.)) for x in range(12)]\\r\\n        mags = [1] * 12\\r\\n        hpcp = HPCP()(freqs, mags)\\r\\n        self.assertEqualVector(hpcp, [1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.])\", 'def testSubmediantPosition(self):\\r\\n        # Make sure that the submediant of a key based on 440 is in the\\r\\n        # correct location (submediant was randomly selected from all the\\r\\n        # tones)\\r\\n        tonic = 440\\r\\n        submediant = tonic * 2**(9./12.)\\r\\n        hpcp = HPCP()([submediant], [1])', 'def testMaxShifted(self):\\r\\n        # Tests whether a HPCP reading with only the dominant semitone\\r\\n        # activated is correctly shifted so that the dominant is at the\\r\\n        # position 0\\r\\n        tonic = 440\\r\\n        dominant = tonic * 2**(7./12.)\\r\\n        hpcp = HPCP(maxShifted=True)([dominant], [1])', 'def chordHelper(self, half_steps, tunning, strength):\\r\\n        notes = [tunning*(2.**(half_steps[i]/12.)) for i in range(len(half_steps))]\\r\\n        hpcp = HPCP(maxShifted=False)([notes[0], notes[1], notes[2]], strength)\\r\\n        for i in range(len(hpcp)):\\r\\n            if i in half_steps: self.assertTrue(hpcp[i]>0)\\r\\n            elif (i - 12) in half_steps: self.assertTrue(hpcp[i]>0)\\r\\n            else: self.assertEqual(hpcp[i], 0)', 'def testChord(self):\\r\\n        tunning = 440\\r\\n        AMajor = [0, 4, 7] # AMajor = A4-C#5-E5\\r\\n        self.chordHelper(AMajor, tunning, [1,1,1])\\r\\n        CMajor = [3, -4, -2] # CMajor = C5-F4-G4\\r\\n        self.chordHelper(CMajor, tunning, [1,1,1])\\r\\n        CMajor = [-4, 3, -2] # CMajor = C5-F4-G4\\r\\n        self.chordHelper(CMajor, tunning, [1,0.5,0.2])\\r\\n        CMajor = [-4, -2, 3] # CMajor = C5-F4-G4\\r\\n        self.chordHelper(CMajor, tunning, [1,0.5,0.2])\\r\\n        CMajor = [3, 8, 10] # CMajor = C5-F5-G5\\r\\n        self.chordHelper(CMajor, tunning, [1,0.5,0.2])\\r\\n        AMinor = [0, 3, 7] # AMinor = A4-C5-E5\\r\\n        self.chordHelper(AMinor, tunning, [1,0.5,0.2])\\r\\n        CMinor = [3, 6, 10] # CMinor = C5-E5-G5\\r\\n        self.chordHelper(CMinor, tunning, [1,0.5,0.2])', 'def testLowFrequency(self):\\r\\n        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([99], [1])\\r\\n        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])', 'def testHighFrequency(self):\\r\\n        hpcp = HPCP(minFrequency=100, maxFrequency=1000)([1001], [1])\\r\\n        self.assertEqualVector(hpcp, [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])', \"def testSmallMinRange(self):\\r\\n        self.assertConfigureFails(HPCP(), {'minFrequency':1, 'splitFrequency':200})\", \"def testSmallMaxRange(self):\\r\\n        self.assertConfigureFails(HPCP(), {'maxFrequency':1199, 'splitFrequency':1000})\", \"def testSmallMinMaxRange(self):\\r\\n        self.assertConfigureFails(HPCP(), {'bandPreset':False, 'maxFrequency':200, 'minFrequency':1})\", \"def testSizeNonmultiple12(self):\\r\\n        self.assertConfigureFails(HPCP(), {'size':13})\", \"def testHarmonics(self):\\r\\n        # Regression test for the 'harmonics' parameter\\r\\n        tone = 100. # arbitrary frequency [Hz]\\r\\n        freqs = [tone, tone*2, tone*3, tone*4]\\r\\n        mags = [1]*4\", 'def testRegression(self):\\r\\n        # Just makes sure algorithm does not crash on a real data source. This\\r\\n        # test is not really looking for correctness. Maybe consider revising\\r\\n        # it.\\r\\n        inputSize = 512\\r\\n        sampleRate = 44100']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def add_executive_group(apps, schema_editor):\\n    # create group\\n    db_alias = schema_editor.connection.alias\\n    emit_post_migrate_signal(1, False, db_alias)\\n    Group = apps.get_model('auth', 'Group')\\n    Permission = apps.get_model('auth', 'Permission')\\n    executive_group, created = Group.objects.get_or_create(name='executive')\\n    if created:\\n        # Learning unit\\n        can_access_learningunit = Permission.objects.get(codename='can_access_learningunit')\\n        executive_group.permissions.add(can_access_learningunit)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, req = None):\\n        if req is not None:\\n            self.req = req\\n            etalage_env = req.environ.get('etalage', {})\\n            for key in object.__getattribute__(self, 'env_keys'):\\n                value = etalage_env.get(key)\\n                if value is not None:\\n                    setattr(self, key, value)\", 'def _(self):\\n        return self.translator.ugettext', 'def get_containing(self, name, depth = 0):\\n        \"\"\"Return the n-th (n = ``depth``) context containing attribute named ``name``.\"\"\"\\n        ctx_dict = object.__getattribute__(self, \\'__dict__\\')\\n        if name in ctx_dict:\\n            if depth <= 0:\\n                return self\\n            depth -= 1\\n        parent = ctx_dict.get(\\'_parent\\')\\n        if parent is None:\\n            return None\\n        return parent.get_containing(name, depth = depth)', \"def iter(self):\\n        yield self\\n        parent = object.__getattribute__(self, '_parent')\\n        if parent is not None:\\n            for ancestor in parent.iter():\\n                yield ancestor\", 'def iter_inherited(self, name):\\n        for ctx in self.iter_containing(name):\\n            yield object.__getattribute__(ctx, name)', 'def lang_get(self):\\n        if self._lang is None:\\n            # self._lang = self.req.accept_language.best_matches(\\'en-US\\') if self.req is not None else []\\n            # Note: Don\\'t forget to add country-less language code when only a \"language-COUNTRY\" code is given.\\n            self._lang = [\\'fr-FR\\', \\'fr\\']\\n            if self.req is not None:\\n                self.req.environ.setdefault(\\'etalage\\', {})[\\'_lang\\'] = self._lang\\n        return self._lang', 'def new(self, **kwargs):\\n        ctx = Ctx()\\n        ctx._parent = self\\n        for name, value in kwargs.iteritems():\\n            setattr(ctx, name, value)\\n        return ctx', \"def parent(self):\\n        return object.__getattribute__(self, '_parent')\", 'def scopes_get(self):\\n        return self._scopes', \"def session(self):\\n        return self.req.environ.get('beaker.session') if self.req is not None else None\", 'def translator(self):\\n        \"\"\"Get a valid translator object from one or several languages names.\"\"\"\\n        if self._translator is None:\\n            languages = self.lang\\n            if not languages:\\n                return gettext.NullTranslations()\\n            if not isinstance(languages, list):\\n                languages = [languages]\\n            translator = gettext.NullTranslations()\\n            i18n_dir_by_plugin_name = conf[\\'i18n_dir_by_plugin_name\\'] or {}\\n            for name, i18n_dir in [\\n                    (\\'biryani\\', conf[\\'biryani_i18n_dir\\']),\\n                    (conf[\\'package_name\\'], conf[\\'i18n_dir\\']),\\n                    ] + sorted(i18n_dir_by_plugin_name.iteritems()):\\n                if name is not None and i18n_dir is not None:\\n                    translator = new_translator(name, i18n_dir, languages, fallback = translator)\\n            self._translator = translator\\n        return self._translator']}, {'features': [], 'snippets': [\"def endorse_user(request, recipient_username):\\n    recipient = get_object_or_404(Profile, user__username=recipient_username)\\n    if recipient == request.profile:\\n        raise Http404()\\n    try:\\n        endorsement = Endorsement.objects.get(\\n            endorser=request.profile, recipient=recipient)\\n    except Endorsement.DoesNotExist:\\n        endorsement = None\\n    if request.method == 'POST':\\n        if 'delete' in request.POST and endorsement:\\n            endorsement.delete()\\n            messages.info(request, MESSAGES['endorsement_deleted'])\\n            return HttpResponseRedirect(\\n                endorsement.recipient.get_absolute_url())\\n        form = EndorseForm(request.POST, instance=endorsement,\\n                           endorser=request.profile, recipient=recipient)\\n        if form.is_valid():\\n            is_new = endorsement is None\\n            endorsement = form.save()\\n            if is_new:\\n                send_endorsement_notification(endorsement)\\n            messages.info(request, MESSAGES['endorsement_saved'])\\n            return HttpResponseRedirect(endorsement.get_absolute_url())\\n    else:\\n        form = EndorseForm(instance=endorsement, endorser=request.profile,\\n                           recipient=recipient)\\n    profile = recipient  # For profile_base.html.\\n    return locals()\", 'def endorsement(request, endorsement_id):\\n    endorsement = get_object_or_404(Endorsement, pk=endorsement_id)\\n    return locals()', 'def relationships(request):\\n    accounts = ripple.get_user_accounts(request.profile)\\n    return locals()', \"def relationship(request, partner_username):\\n    partner = get_object_or_404(Profile, user__username=partner_username)\\n    if partner == request.profile:\\n        raise Http404  # Can't have relationship with yourself.\\n    account = request.profile.account(partner)\\n    if account:\\n        entries = account.entries \\n        balance = account.balance\\n    else:\\n        entries = []\\n        balance = 0\\n    profile = partner  # For profile_base.html.\\n    return locals()\", \"def acknowledge_user(request, recipient_username):\\n    recipient = get_object_or_404(Profile, user__username=recipient_username)\\n    if recipient == request.profile:\\n        raise Http404\\n    # TODO: Don't recompute max_amount on form submit?  Cache, or put in form\\n    # as hidden field?\\n    max_amount = ripple.max_payment(request.profile, recipient)\\n    if request.method == 'POST':\\n        form = AcknowledgementForm(request.POST, max_ripple=max_amount)\\n        if form.is_valid():\\n            acknowledgement = form.send_acknowledgement(\\n                request.profile, recipient)\\n            send_acknowledgement_notification(acknowledgement)\\n            messages.info(request, MESSAGES['acknowledgement_sent'])\\n            return HttpResponseRedirect(acknowledgement.get_absolute_url())\\n    else:\\n        form = AcknowledgementForm(max_ripple=max_amount, initial=request.GET)\\n    can_ripple = max_amount > 0\\n    profile = recipient  # For profile_base.html.\\n    return locals()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, request, policy):\\n\\t\\tself.request = request\\n\\t\\tself.policy = policy', 'def __init__(self, default, routes=None):\\n\\t\\tself._default = default\\n\\t\\tif routes is None:\\n\\t\\t\\troutes = {}\\n\\t\\tself._routes = routes', \"def match(self, request):\\n\\t\\tif hasattr(request, 'auth_policy'):\\n\\t\\t\\treturn request.auth_policy\\n\\t\\tcur = None\\n\\t\\tcur_len = 0\\n\\t\\tfor route, plug in self._routes.items():\\n\\t\\t\\tr_len = len(route)\\n\\t\\t\\tif r_len <= cur_len:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tpath = request.path\\n\\t\\t\\tif route == path[:r_len]:\\n\\t\\t\\t\\tif len(path) > r_len:\\n\\t\\t\\t\\t\\tif path[r_len:r_len + 1] != '/':\\n\\t\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tcur = plug\\n\\t\\t\\t\\tcur_len = r_len\\n\\t\\tif cur:\\n\\t\\t\\trequest.auth_policy = cur\\n\\t\\telse:\\n\\t\\t\\trequest.auth_policy = self._default\\n\\t\\trequest.registry.notify(PluginPolicySelected(request, request.auth_policy))\\n\\t\\treturn request.auth_policy\", 'def unauthenticated_userid(self, request):\\n\\t\\treturn self.match(request).unauthenticated_userid(request)', 'def remember(self, request, principal, **kw):\\n\\t\\treturn self.match(request).remember(request, principal, **kw)', 'def _filter_token(tok):\\n\\treturn str(tok).translate(_TOKEN_FILTER_MAP)', \"def _generate_nonce(ts, secret, salt=None, chars=string.hexdigits.upper()):\\n\\t# TODO: Add IP-address to nonce\\n\\tif not salt:\\n\\t\\ttry:\\n\\t\\t\\trng = random.SystemRandom()\\n\\t\\texcept NotImplementedError:\\n\\t\\t\\trng = random\\n\\t\\tsalt = ''.join(rng.choice(chars) for i in range(16))\\n\\tctx = hashlib.md5(('%s:%s:%s' % (ts, salt, secret)).encode())\\n\\treturn ('%s:%s:%s' % (ts, salt, ctx.hexdigest()))\", \"def _generate_digest_challenge(ts, secret, realm, opaque, stale=False):\\n\\tnonce = _generate_nonce(ts, secret)\\n\\treturn 'Digest %s' % (_format_kvpairs(\\n\\t\\trealm=realm,\\n\\t\\tqop='auth',\\n\\t\\tnonce=nonce,\\n\\t\\topaque=opaque,\\n\\t\\talgorithm='MD5',\\n\\t\\tstale='true' if stale else 'false'\\n\\t),)\", \"def _parse_authorization(request, secret, realm):\\n\\tauthz = request.authorization\\n\\tif (not authz) or (len(authz) != 2) or (authz[0] != 'Digest'):\\n\\t\\t_add_www_authenticate(request, secret, realm)\\n\\t\\treturn None\\n\\tparams = authz[1]\\n\\tif 'algorithm' not in params:\\n\\t\\tparams['algorithm'] = 'MD5'\\n\\tfor required in ('username', 'realm', 'nonce', 'uri', 'response', 'cnonce', 'nc', 'opaque'):\\n\\t\\tif (required not in params) or ((required == 'opaque') and (params['opaque'] != 'NPDIGEST')):\\n\\t\\t\\t_add_www_authenticate(request, secret, realm)\\n\\t\\t\\treturn None\\n\\treturn params\", \"def __init__(self, secret, callback, realm='Realm'):\\n\\t\\tself.secret = secret\\n\\t\\tself.callback = callback\\n\\t\\tself.realm = realm\", \"def unauthenticated_userid(self, request):\\n\\t\\tparams = _parse_authorization(request, self.secret, self.realm)\\n\\t\\tif params is None:\\n\\t\\t\\treturn None\\n\\t\\tif not _is_valid_nonce(params['nonce'], self.secret):\\n\\t\\t\\t_add_www_authenticate(request, self.secret, self.realm)\\n\\t\\t\\treturn None\\n\\t\\treturn 'u:%s' % params['username']\", 'def remember(self, request, principal, *kw):\\n\\t\\treturn []']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def testEmpty(self):\\r\\n        gen = VectorInput([])\\r\\n        tcToTotal = sTCToTotal()\\r\\n        p = Pool()', 'def testOneValue(self):\\r\\n        gen = VectorInput([1])\\r\\n        tcToTotal = sTCToTotal()\\r\\n        p = Pool()', 'def testRegression(self):\\r\\n        envelope = range(22050)\\r\\n        envelope.reverse()\\r\\n        envelope = range(22050) + envelope']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, parent_id = None):\\n        \"\"\"\\n\\n        \"\"\"\\n        self._id = None\\n        self._parent = parent_id\\n        self._values = {}', 'def drop_operation(cls,operation_id):\\n        \"\"\"\\n        Drops an Operation, identified by it\\'s Operation Id and\\n        it\\'s children recursively\\n        Drop deletes the Operations from Database\\n        \"\"\"\\n        db = Database()\\n\\n        stmnt = \"SELECT OPE_ID FROM OPERATIONS WHERE OPE_OPE_PARENT = ? AND OPE_STATUS IN (0, 2) ;\"\\n        cur = db.query(stmnt,(operation_id,))\\n        for row in cur.fetchallmap():\\n            cls.drop_operation(row[\"OPE_ID\"])\\n\\n        stmnt = \"DELETE FROM OPERATIONS WHERE OPE_ID = ? AND OPE_STATUS IN (0, 2) ;\"\\n        db.query(stmnt,(operation_id,),commit=True)', 'def retry_operation(cls,operation_id):\\n        \"\"\"\\n        Resets the state of an operation and it\\'s children recursively to 0 (PENDING)\\n        The operation is identified by a given operationId\\n        \"\"\"\\n        db = Database()\\n\\n        stmnt = \"SELECT OPE_ID FROM OPERATIONS WHERE OPE_OPE_PARENT = ? AND OPE_STATUS = 2 ;\"\\n        cur = db.query(stmnt,(operation_id,))\\n        for row in cur.fetchallmap():\\n            cls.retry_operation(row[\"OPE_ID\"])\\n\\n        stmnt = \"UPDATE OPERATIONS SET OPE_STATUS = 0 WHERE OPE_ID = ? AND OPE_STATUS = 2 ;\"\\n        db.query(stmnt,(operation_id,),commit=True)', 'def cancel_operation(cls,operation_id):\\n        \"\"\"\\n        Cancels an Operation, identified by it\\'s Operation Id and\\n        it\\'s children recursively\\n        Cancel Deletes the Operation from Database\\n        \"\"\"\\n        db = Database()\\n\\n        stmnt = \"SELECT OPE_ID FROM OPERATIONS WHERE OPE_OPE_PARENT = ? AND OPE_STATUS = 0 ;\"\\n        cur = db.query(stmnt,(operation_id,))\\n        for row in cur.fetchallmap():\\n            cls.cancel_operation(row[\"OPE_ID\"])\\n\\n        stmnt = \"DELETE FROM OPERATIONS WHERE OPE_ID = ? AND OPE_STATUS = 0 ;\"\\n        db.query(stmnt,(operation_id,),commit=True)', 'def restore_operation(cls, operation_record):\\n        \"\"\"\\n        Restore an Operationobject stored in the database by a Dataset consisting of\\n        the operation\\'s ID and the operation\\'s TYPE:\\n        For example:   {\"OPE_ID\": 100, \"OPE_TYPE\": \"TestOperation\"}\\n        Restores the Operationobject\\'s _values-attribute by the data saved\\n        in the DB-Table OPERATIONDATA\\n        \"\"\"\\n        classname = operation_record[\"OPE_TYPE\"]\\n        module = \"\" #TODO Implement modulename from database if Operation belongs to Module\\n        is_operation_of_module = False\\n        exec \"\"\"', 'def process_children(cls, operation):\\n        \"\"\"\\n        Recursively executes the workloads of Operation\\'s Childoperations\\n        It hereby catches exceptions in the workloads, sets the OPE_STATUS\\n        to 2 (FAILED) if a catch occurs, then passes the exception on to the \\n        higher layer.\\n        If an Operation succeeds, it\\'s entry in DB gets deleted\\n        \"\"\"\\n        db = Database()\\n\\n        stmnt = \"SELECT OPE_ID, OPE_TYPE FROM OPERATIONS WHERE OPE_OPE_PARENT = ? ORDER BY OPE_INVOKED ;\"\\n        stmnt_lock = \"UPDATE OPERATIONS SET OPE_STATUS = 1 WHERE OPE_ID = ? ;\"\\n        cur = db.query(stmnt,(operation.get_id(),))\\n        for row in cur.fetchallmap():\\n            child_operation = cls.restore_operation(row)\\n            db.query(stmnt_lock,(child_operation.get_id(),),commit=True)\\n            try:\\n                cls.process_children(child_operation)\\n                child_operation.do_workload()\\n            except Exception,e:\\n                stmnt_err = \"UPDATE OPERATIONS SET OPE_STATUS = 2 WHERE OPE_ID = ? ;\"\\n                db.query(stmnt_err,(int(row[\"OPE_ID\"]),),commit=True)\\n                #TODO GENERATE ERROR IN LOG\\n                raise e\\n            stmnt_delete = \"DELETE FROM OPERATIONS WHERE OPE_ID = ?;\"\\n            db.query(stmnt_delete,(child_operation.get_id(),),commit=True)', 'def process_next(cls):\\n        \"\"\"\\n        Sets the status of the next toplevel operation to 1 (ACTIVE)\\n        Fetches the next toplevel-operation from the database, applies a FILESYSTEMLOCK!\\n        Which is /tmp/scv_operating.lck !!! \\n        \"\"\"\\n        db = Database()\\n        configuration = Configuration()\\n        if os.path.exists(configuration.get_entry(\"core.webpath\")+\"/scv_operating.lck\"):\\n            return False\\n        lockfile = open(configuration.get_entry(\"core.webpath\")+\"/scv_operating.lck\",\"w\")\\n        lockfile.close()\\n        stmnt_lock = \"UPDATE OPERATIONS SET OPE_STATUS = 1 \\\\\\n                            WHERE OPE_ID IN ( \\\\\\n                              SELECT OPE_ID FROM OPERATIONS \\\\\\n                              WHERE OPE_OPE_PARENT IS NULL AND OPE_STATUS = 0 \\\\\\n                              AND OPE_INVOKED = ( \\\\\\n                                SELECT MIN(OPE_INVOKED) FROM OPERATIONS  \\\\\\n                                WHERE OPE_OPE_PARENT IS NULL AND OPE_STATUS = 0) \\\\\\n                            ) ;\"\\n        stmnt = \"SELECT OPE_ID, OPE_TYPE FROM OPERATIONS WHERE OPE_OPE_PARENT IS NULL AND OPE_STATUS = 1 ;\"\\n        db.query(stmnt_lock,commit=True)\\n        cur = db.query(stmnt)\\n        res = cur.fetchallmap()\\n        if len(res) > 0:\\n            operation = cls.restore_operation(res[0])\\n            try:\\n                cls.process_children(operation)\\n                operation.do_workload()\\n            except Exception, e:\\n                stmnt_err = \"UPDATE OPERATIONS SET OPE_STATUS = 2 WHERE OPE_ID = ? ;\"\\n                db.query(stmnt_err,(operation.get_id(),),commit=True)\\n                error = StringIO()\\n                print_exc(None,error)\\n                Core().log(error.getvalue())\\n            ret = True\\n        else:\\n            ret = False\\n        stmnt_delete = \"DELETE FROM OPERATIONS WHERE OPE_STATUS = 1 ;\"\\n        db.query(stmnt_delete,commit=True)\\n        db.commit()\\n        try:\\n            os.unlink(configuration.get_entry(\"core.webpath\")+\"/scv_operating.lck\")\\n        except OSError,e :\\n            raise OperationException(OperationException.get_msg(0))\\n        return ret', 'def get_current_operations_for_gui(cls, operation_types=None):\\n        \"\"\"\\n        Returns all Operations in an associative array.\\n        The array\\'s indices are the operationIDs\\n        The Objects contain all information about the operations,\\n        including the Data\\n        \"\"\"\\n        db = Database()\\n        #TODO CHECK HOW LISTS ARE HANDLED IN FDB\\n        if operation_types is not None and type(operation_types) == list:\\n            stmnt = \"SELECT OPE_ID, OPE_OPE_PARENT, OPE_INVOKED, OPE_TYPE, OPE_STATUS FROM OPERATIONS WHERE OPE_TYPE IN (?) ORDER BY OPE_INVOKED ;\"\\n            cur = db.query(stmnt,(operation_types))\\n        else:\\n            stmnt = \"SELECT OPE_ID, OPE_OPE_PARENT, OPE_INVOKED, OPE_TYPE, OPE_STATUS FROM OPERATIONS ORDER BY OPE_INVOKED ;\"\\n            cur = db.query(stmnt)\\n        ret = {}\\n        for row in cur.fetchallmap():\\n            operation = cls.restore_operation(row)\\n            custom_values = operation.get_values()\\n\\n            ret[row[\"OPE_ID\"]] = {\"id\":row[\"OPE_ID\"],\\n                                  \"parent\":row[\"OPE_OPE_PARENT\"],\\n                                  \"invoked\":str(row[\"OPE_INVOKED\"]),\\n                                  \"type\":row[\"OPE_TYPE\"],\\n                                  \"status\":row[\"OPE_STATUS\"],\\n                                  \"data\":custom_values}\\n        return ret', 'def get_value(self,key):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        return self._values(key)', 'def set_parent(self,parent_id):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        self._parent = parent_id', 'def set_db_id(self):\\n        \"\"\"\\n        Get a new Operation Id from the Database and assign it to this\\n        Operation if this Operation\\'s id is null. Afterwards return the \\n        new Id\\n        \"\"\"\\n        if self._id is None:\\n            self._id = Database().get_seq_next(\\'OPE_GEN\\')\\n        return self._id', 'def get_id(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        return self._id', 'def do_workload(self):\\n        \"\"\"\\n        This method must be overridden by inheriting classes.\\n        The code inside this method will be executed when the\\n        Operation is processed by Operation.processNext or \\n        Operation.processChild \\n        \"\"\"\\n        pass', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        Operation.__init__(self)', 'def get_meta(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        return self._values', 'def get_currently_processed_modules(cls):\\n        \"\"\"\\n        Returns an Array of ModuleOperation-Objects that are\\n        currently listedin the queue \\n        \"\"\"\\n        db = Database()\\n        stmnt = \"SELECT OPE_ID, OPE_OPE_PARENT, OPE_TYPE FROM OPERATIONS \\\\\\n                   WHERE OPE_TYPE = \\'ModuleInstallOperation\\' \\\\\\n                   or OPE_TYPE = \\'ModuleUninstallOperation\\' ;\"\\n        cur = db.query(stmnt);\\n        ret = []\\n        for row in cur.fetchallmap():\\n            ret.append(Operation.restore_operation(row).get_meta())\\n        return ret', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        ModuleOperation.__init__(self)', 'def optimize_queue(self):\\n        \"\"\"\\n        optimizes the queue. \\n        \"\"\"\\n        pass    #TODO Implement', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        ModuleOperation.__init__(self)', 'def optimize_queue(self):\\n        \"\"\"\\n        optimizes the queue. \\n        \"\"\"\\n        pass    #TODO Implement', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        ModuleOperation.__init__(self)', 'def optimize_queue(self):\\n        \"\"\"\\n        optimizes the queue. \\n        \"\"\"\\n        pass    #TODO Implement', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        Operation.__init__(self)', 'def __init__(self):\\n        \"\"\"\\n        trivial\\n        \"\"\"\\n        Operation.__init__(self)', 'def __init__(self, pidfile):\\n        \"\"\"\\n        Initialize the deamon\\n        \"\"\"\\n        Daemon.__init__(self,pidfile)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def mock_render_to_string(template_name, context):\\n    \"\"\"Return a string that encodes template_name and context\"\"\"\\n    return str((template_name, context))', 'def setUp(self):\\n        \"\"\" Test case setup \"\"\"\\n        super(CourseCreatorAdminTest, self).setUp()\\n        self.user = User.objects.create_user(\\'test_user\\', \\'test_user+courses@edx.org\\', \\'foo\\')\\n        self.table_entry = CourseCreator(user=self.user)\\n        self.table_entry.save()\\n\\n        self.admin = User.objects.create_user(\\'Mark\\', \\'admin+courses@edx.org\\', \\'foo\\')\\n        self.admin.is_staff = True\\n\\n        self.request = HttpRequest()\\n        self.request.user = self.admin\\n\\n        self.creator_admin = CourseCreatorAdmin(self.table_entry, AdminSite())\\n\\n        self.studio_request_email = \\'mark@marky.mark\\'\\n        self.enable_creator_group_patch = {\\n            \"ENABLE_CREATOR_GROUP\": True,\\n            \"STUDIO_REQUEST_EMAIL\": self.studio_request_email\\n        }', 'def test_change_status(self, email_user):\\n        \"\"\"\\n        Tests that updates to state impact the creator group maintained in authz.py and that e-mails are sent.\\n        \"\"\"\\n\\n        def change_state_and_verify_email(state, is_creator):\\n            \"\"\" Changes user state, verifies creator status, and verifies e-mail is sent based on transition \"\"\"\\n            self._change_state(state)\\n            self.assertEqual(is_creator, auth.user_has_role(self.user, CourseCreatorRole()))\\n\\n            context = {\\'studio_request_email\\': self.studio_request_email}\\n            if state == CourseCreator.GRANTED:\\n                template = \\'emails/course_creator_granted.txt\\'\\n            elif state == CourseCreator.DENIED:\\n                template = \\'emails/course_creator_denied.txt\\'\\n            else:\\n                template = \\'emails/course_creator_revoked.txt\\'\\n            email_user.assert_called_with(\\n                mock_render_to_string(\\'emails/course_creator_subject.txt\\', context),\\n                mock_render_to_string(template, context),\\n                self.studio_request_email\\n            )\\n\\n        with mock.patch.dict(\\'django.conf.settings.FEATURES\\', self.enable_creator_group_patch):\\n\\n            # User is initially unrequested.\\n            self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))\\n\\n            change_state_and_verify_email(CourseCreator.GRANTED, True)\\n\\n            change_state_and_verify_email(CourseCreator.DENIED, False)\\n\\n            change_state_and_verify_email(CourseCreator.GRANTED, True)\\n\\n            change_state_and_verify_email(CourseCreator.PENDING, False)\\n\\n            change_state_and_verify_email(CourseCreator.GRANTED, True)\\n\\n            change_state_and_verify_email(CourseCreator.UNREQUESTED, False)\\n\\n            change_state_and_verify_email(CourseCreator.DENIED, False)', 'def test_mail_admin_on_pending(self):\\n        \"\"\"\\n        Tests that the admin account is notified when a user is in the \\'pending\\' state.\\n        \"\"\"\\n\\n        def check_admin_message_state(state, expect_sent_to_admin, expect_sent_to_user):\\n            \"\"\" Changes user state and verifies e-mail sent to admin address only when pending. \"\"\"\\n            mail.outbox = []\\n            self._change_state(state)\\n\\n            # If a message is sent to the user about course creator status change, it will be the first\\n            # message sent. Admin message will follow.\\n            base_num_emails = 1 if expect_sent_to_user else 0\\n            if expect_sent_to_admin:\\n                context = {\\'user_name\\': \"test_user\", \\'user_email\\': u\\'test_user+courses@edx.org\\'}\\n                self.assertEquals(base_num_emails + 1, len(mail.outbox), \\'Expected admin message to be sent\\')\\n                sent_mail = mail.outbox[base_num_emails]\\n                self.assertEquals(\\n                    mock_render_to_string(\\'emails/course_creator_admin_subject.txt\\', context),\\n                    sent_mail.subject\\n                )\\n                self.assertEquals(\\n                    mock_render_to_string(\\'emails/course_creator_admin_user_pending.txt\\', context),\\n                    sent_mail.body\\n                )\\n                self.assertEquals(self.studio_request_email, sent_mail.from_email)\\n                self.assertEqual([self.studio_request_email], sent_mail.to)\\n            else:\\n                self.assertEquals(base_num_emails, len(mail.outbox))\\n\\n        with mock.patch.dict(\\'django.conf.settings.FEATURES\\', self.enable_creator_group_patch):\\n            # E-mail message should be sent to admin only when new state is PENDING, regardless of what\\n            # previous state was (unless previous state was already PENDING).\\n            # E-mail message sent to user only on transition into and out of GRANTED state.\\n            check_admin_message_state(CourseCreator.UNREQUESTED, expect_sent_to_admin=False, expect_sent_to_user=False)\\n            check_admin_message_state(CourseCreator.PENDING, expect_sent_to_admin=True, expect_sent_to_user=False)\\n            check_admin_message_state(CourseCreator.GRANTED, expect_sent_to_admin=False, expect_sent_to_user=True)\\n            check_admin_message_state(CourseCreator.DENIED, expect_sent_to_admin=False, expect_sent_to_user=True)\\n            check_admin_message_state(CourseCreator.GRANTED, expect_sent_to_admin=False, expect_sent_to_user=True)\\n            check_admin_message_state(CourseCreator.PENDING, expect_sent_to_admin=True, expect_sent_to_user=True)\\n            check_admin_message_state(CourseCreator.PENDING, expect_sent_to_admin=False, expect_sent_to_user=False)\\n            check_admin_message_state(CourseCreator.DENIED, expect_sent_to_admin=False, expect_sent_to_user=True)', 'def test_add_permission(self):\\n        \"\"\"\\n        Tests that staff cannot add entries\\n        \"\"\"\\n        self.assertFalse(self.creator_admin.has_add_permission(self.request))']}, {'features': [], 'snippets': [\"def test_read(self):\\n        config = read_config('config')\\n        self.assertEqual(config['cmus_host'], 'raspberry')\\n        self.assertEqual(config['cmus_passwd'], 'PaSsWd')\\n        self.assertEqual(config['app_host'], 'localhost')\\n        self.assertEqual(config['app_port'], '8080')\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_registry():\\n\\treg = telepathy.client.ManagerRegistry()\\n\\treg.LoadManagers()\\n\\treturn reg', 'def __init__(self):\\n\\t\\tself._action = None', 'def append_action(self, action):\\n\\t\\tassert self._action is None\\n\\t\\tself._action = action', 'def _on_done(self):\\n\\t\\tif self._action is None:\\n\\t\\t\\treturn\\n\\t\\tself._action.queue_action()', 'def _on_generic_message(self, *args):\\n\\t\\tpass', 'def queue_action(self):\\n\\t\\tgobject.idle_add(self._on_done)', 'def __init__(self, loop):\\n\\t\\tsuper(QuitLoop, self).__init__()\\n\\t\\tself._loop = loop', 'def __init__(self, cm):\\n\\t\\tsuper(DisplayParams, self).__init__()\\n\\t\\tself._cm = cm', 'def _on_done(self, params):\\n\\t\\tprint \"Connection Parameters:\"\\n\\t\\tfor name, flags, signature, default in params:\\n\\t\\t\\tprint \"\\\\t%s (%s)\" % (name, signature),\\n\\n\\t\\t\\tif flags & telepathy.constants.CONN_MGR_PARAM_FLAG_REQUIRED:\\n\\t\\t\\t\\tprint \"required\",\\n\\t\\t\\tif flags & telepathy.constants.CONN_MGR_PARAM_FLAG_REGISTER:\\n\\t\\t\\t\\tprint \"register\",\\n\\t\\t\\tif flags & telepathy.constants.CONN_MGR_PARAM_FLAG_SECRET:\\n\\t\\t\\t\\tprint \"secret\",\\n\\t\\t\\tif flags & telepathy.constants.CONN_MGR_PARAM_FLAG_DBUS_PROPERTY:\\n\\t\\t\\t\\tprint \"dbus-property\",\\n\\t\\t\\tif flags & telepathy.constants.CONN_MGR_PARAM_FLAG_HAS_DEFAULT:\\n\\t\\t\\t\\tprint \"has-default(%s)\" % default,\\n\\n\\t\\t\\tprint \"\"\\n\\t\\tsuper(DisplayParams, self)._on_done()', 'def __init__(self, cm, username, password, forward):\\n\\t\\tsuper(RequestConnection, self).__init__()\\n\\t\\tself._cm = cm\\n\\n\\t\\tself._conn = None\\n\\t\\tself._serviceName = None\\n\\n\\t\\tself._username = username\\n\\t\\tself._password = password\\n\\t\\tself._forward = forward', 'def conn(self):\\n\\t\\treturn self._conn', 'def serviceName(self):\\n\\t\\treturn self._serviceName', 'def _on_done(self, busName, objectPath):\\n\\t\\tself._serviceName = busName\\n\\t\\tself._conn = telepathy.client.Connection(busName, objectPath)\\n\\t\\tsuper(RequestConnection, self)._on_done()', 'def __init__(self, connAction):\\n\\t\\tsuper(Connect, self).__init__()\\n\\t\\tself._connAction = connAction', 'def _on_done(self):\\n\\t\\tsuper(Connect, self)._on_done()', 'def __init__(self, connAction):\\n\\t\\tsuper(SimplePresenceOptions, self).__init__()\\n\\t\\tself._connAction = connAction', 'def _on_done(self, statuses):\\n\\t\\tprint \"\\\\tAvailable Statuses\"\\n\\t\\tfor (key, value) in statuses.iteritems():\\n\\t\\t\\tprint \"\\\\t\\\\t - %s\" % key\\n\\t\\tsuper(SimplePresenceOptions, self)._on_done()', 'def handle(self):\\n\\t\\treturn 0', 'def handles(self):\\n\\t\\treturn []', 'def __init__(self, connAction):\\n\\t\\tsuper(UserHandle, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._handle = None', 'def handle(self):\\n\\t\\treturn self._handle', 'def handles(self):\\n\\t\\treturn [self._handle]', 'def _on_done(self, handle):\\n\\t\\tself._handle = handle\\n\\t\\tsuper(UserHandle, self)._on_done()', 'def __init__(self, connAction, handleType, handleNames):\\n\\t\\tsuper(RequestHandle, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._handle = None\\n\\t\\tself._handleType = handleType\\n\\t\\tself._handleNames = handleNames', 'def handle(self):\\n\\t\\treturn self._handle', 'def handles(self):\\n\\t\\treturn [self._handle]', 'def _on_done(self, handles):\\n\\t\\tself._handle = handles[0]\\n\\t\\tsuper(RequestHandle, self)._on_done()', 'def __init__(self, connAction, handleAction, channelType, handleType):\\n\\t\\tsuper(RequestChannel, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._handleAction = handleAction\\n\\t\\tself._channel = None\\n\\t\\tself._channelType = channelType\\n\\t\\tself._handleType = handleType', 'def channel(self):\\n\\t\\treturn self._channel', 'def _on_done(self, channelObjectPath):\\n\\t\\tself._channel = telepathy.client.Channel(self._connAction.serviceName, channelObjectPath)\\n\\t\\tsuper(RequestChannel, self)._on_done()', 'def __init__(self, connAction, channelType, handleType, handleId):\\n\\t\\tsuper(EnsureChannel, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._channel = None\\n\\t\\tself._channelType = channelType\\n\\t\\tself._handleType = handleType\\n\\t\\tself._handleId = handleId\\n\\t\\tself._handle = None', 'def channel(self):\\n\\t\\treturn self._channel', 'def handle(self):\\n\\t\\treturn self._handle', 'def handles(self):\\n\\t\\treturn [self._handle]', 'def _on_done(self, yours, channelObjectPath, properties):\\n\\t\\tprint \"Create?\", not not yours\\n\\t\\tprint \"Path:\", channelObjectPath\\n\\t\\tprint \"Properties:\", properties\\n\\t\\tself._channel = telepathy.client.Channel(self._connAction.serviceName, channelObjectPath)\\n\\t\\tself._handle = properties[telepathy.server.CHANNEL_INTERFACE+\".TargetHandle\"]\\n\\t\\tsuper(EnsureChannel, self)._on_done()', 'def __init__(self, connAction, chanAction):\\n\\t\\tsuper(CloseChannel, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._chanAction = chanAction\\n\\t\\tself._handles = []', 'def _on_done(self):\\n\\t\\tsuper(CloseChannel, self)._on_done()', 'def __init__(self, connAction, chanAction):\\n\\t\\tsuper(ContactHandles, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._chanAction = chanAction\\n\\t\\tself._handles = []', 'def handles(self):\\n\\t\\treturn self._handles', 'def _on_done(self, handles):\\n\\t\\tself._handles = list(handles)\\n\\t\\tsuper(ContactHandles, self)._on_done()', 'def __init__(self, connAction, handleAction):\\n\\t\\tsuper(SimplePresenceStatus, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._handleAction = handleAction', 'def _on_done(self, aliases):\\n\\t\\tprint \"\\\\tPresences:\"\\n\\t\\tfor hid, (presenceType, presence, presenceMessage) in aliases.iteritems():\\n\\t\\t\\tprint \"\\\\t\\\\t%s:\" % hid, presenceType, presence, presenceMessage\\n\\t\\tsuper(SimplePresenceStatus, self)._on_done()', 'def __init__(self, connAction, status, message):\\n\\t\\tsuper(SetSimplePresence, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._status = status\\n\\t\\tself._message = message', 'def _on_done(self):\\n\\t\\tsuper(SetSimplePresence, self)._on_done()', 'def __init__(self, connAction, handleAction):\\n\\t\\tsuper(Aliases, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._handleAction = handleAction', 'def _on_done(self, aliases):\\n\\t\\tprint \"\\\\tAliases:\"\\n\\t\\tfor h, alias in zip(self._handleAction.handles, aliases):\\n\\t\\t\\tprint \"\\\\t\\\\t\", h, alias\\n\\t\\tsuper(Aliases, self)._on_done()', 'def __init__(self, connAction, chanAction, handleAction):\\n\\t\\tsuper(Call, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._chanAction = chanAction\\n\\t\\tself._handleAction = handleAction', 'def _on_done(self, handle):\\n\\t\\tprint \"Call started\"\\n\\t\\tsuper(Call, self)._on_done()', 'def __init__(self, connAction, chanAction, handleAction, messageType, message):\\n\\t\\tsuper(SendText, self).__init__()\\n\\t\\tself._connAction = connAction\\n\\t\\tself._chanAction = chanAction\\n\\t\\tself._handleAction = handleAction\\n\\t\\tself._messageType = messageType\\n\\t\\tself._message = message', 'def _on_done(self,):\\n\\t\\tprint \"Message sent\"\\n\\t\\tsuper(SendText, self)._on_done()', 'def __init__(self, length):\\n\\t\\tsuper(Sleep, self).__init__()\\n\\t\\tself._length = length', 'def __init__(self):\\n\\t\\tsuper(Block, self).__init__()', 'def _on_done(self):\\n\\t\\t#super(SendText, self)._on_done()\\n\\t\\tpass', 'def __init__(self, connAction):\\n\\t\\tsuper(Disconnect, self).__init__()\\n\\t\\tself._connAction = connAction']}, {'features': [], 'snippets': ['def __init__(self, api_job):\\n        super(SetVolumeOption, self).__init__()\\n        self.api_job = api_job\\n        self.atom = SetVolumeOption']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, fileName, windowName, prefsDict):\\n        \"\"\"Initialize the Orca configuration GUI.\\n\\n        Arguments:\\n        - fileName: name of the GtkBuilder file.\\n        - windowName: name of the component to get from the GtkBuilder file.\\n        - prefsDict: dictionary of preferences to use during initialization\\n        \"\"\"\\n\\n        orca_gtkbuilder.GtkBuilderWrapper.__init__(self, fileName, windowName)\\n        self.prefsDict = prefsDict\\n\\n        self._defaultProfile = [\\'Default\\', \\'default\\']\\n\\n        # Initialize variables to None to keep pylint happy.\\n        #\\n        self.bbindings = None\\n        self.cellRendererText = None\\n        self.defaultVoice = None\\n        self.disableKeyGrabPref = None\\n        self.getTextAttributesView = None\\n        self.hyperlinkVoice = None\\n        self.initializingSpeech = None\\n        self.kbindings = None\\n        self.keyBindingsModel = None\\n        self.keyBindView = None\\n        self.newBinding = None\\n        self.pendingKeyBindings = None\\n        self.planeCellRendererText = None\\n        self.pronunciationModel = None\\n        self.pronunciationView = None\\n        self.screenHeight = None\\n        self.screenWidth = None\\n        self.speechFamiliesChoice = None\\n        self.speechFamiliesChoices = None\\n        self.speechFamiliesModel = None\\n        self.speechLanguagesChoice = None\\n        self.speechLanguagesChoices = None\\n        self.speechLanguagesModel = None\\n        self.speechFamilies = []\\n        self.speechServersChoice = None\\n        self.speechServersChoices = None\\n        self.speechServersModel = None\\n        self.speechSystemsChoice = None\\n        self.speechSystemsChoices = None\\n        self.speechSystemsModel = None\\n        self.systemVoice = None\\n        self.uppercaseVoice = None\\n        self.window = None\\n        self.workingFactories = None\\n        self.savedGain = None\\n        self.savedPitch = None\\n        self.savedRate = None\\n        self._isInitialSetup = False\\n        self.selectedFamilyChoices = {}\\n        self.selectedLanguageChoices = {}\\n        self.profilesCombo = None\\n        self.profilesComboModel = None\\n        self.startingProfileCombo = None\\n        self._capturedKey = []\\n        self.script = None', 'def _getACSSForVoiceType(self, voiceType):\\n        \"\"\"Return the ACSS value for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n\\n        Returns the voice dictionary for the given voice type.\\n        \"\"\"\\n\\n        if voiceType == DEFAULT:\\n            voiceACSS = self.defaultVoice\\n        elif voiceType == UPPERCASE:\\n            voiceACSS = self.uppercaseVoice\\n        elif voiceType == HYPERLINK:\\n            voiceACSS = self.hyperlinkVoice\\n        elif voiceType == SYSTEM:\\n            voiceACSS = self.systemVoice\\n        else:\\n            voiceACSS = self.defaultVoice\\n\\n        return voiceACSS', 'def _getKeyValueForVoiceType(self, voiceType, key, useDefault=True):\\n        \"\"\"Look for the value of the given key in the voice dictionary\\n           for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n        - key: the key to look for in the voice dictionary.\\n        - useDefault: if True, and the key isn\\'t found for the given voice\\n                      type, the look for it in the default voice dictionary\\n                      as well.\\n\\n        Returns the value of the given key, or None if it\\'s not set.\\n        \"\"\"\\n\\n        if voiceType == DEFAULT:\\n            voice = self.defaultVoice\\n        elif voiceType == UPPERCASE:\\n            voice = self.uppercaseVoice\\n            if key not in voice:\\n                if not useDefault:\\n                    return None\\n                voice = self.defaultVoice\\n        elif voiceType == HYPERLINK:\\n            voice = self.hyperlinkVoice\\n            if key not in voice:\\n                if not useDefault:\\n                    return None\\n                voice = self.defaultVoice\\n        elif voiceType == SYSTEM:\\n            voice = self.systemVoice\\n            if key not in voice:\\n                if not useDefault:\\n                    return None\\n                voice = self.defaultVoice\\n        else:\\n            voice = self.defaultVoice\\n\\n        if key in voice:\\n            return voice[key]\\n        else:\\n            return None', 'def _setFamilyNameForVoiceType(self, voiceType, name, language, dialect, variant):\\n        \"\"\"Sets the name of the voice family for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n        - name: the name of the voice family to set.\\n        - language: the locale of the voice family to set.\\n        - dialect: the dialect of the voice family to set.\\n        \"\"\"\\n\\n        family = self._getKeyValueForVoiceType(voiceType,\\n                                               acss.ACSS.FAMILY,\\n                                               False)\\n\\n        voiceACSS = self._getACSSForVoiceType(voiceType)\\n        if family:\\n            family[speechserver.VoiceFamily.NAME] = name\\n            family[speechserver.VoiceFamily.LANG] = language\\n            family[speechserver.VoiceFamily.DIALECT] = dialect\\n            family[speechserver.VoiceFamily.VARIANT] = variant\\n        else:\\n            voiceACSS[acss.ACSS.FAMILY] = {}\\n            voiceACSS[acss.ACSS.FAMILY][speechserver.VoiceFamily.NAME] = name\\n            voiceACSS[acss.ACSS.FAMILY][speechserver.VoiceFamily.LANG] = language\\n            voiceACSS[acss.ACSS.FAMILY][speechserver.VoiceFamily.DIALECT] = dialect\\n            voiceACSS[acss.ACSS.FAMILY][speechserver.VoiceFamily.VARIANT] = variant\\n        voiceACSS[\\'established\\'] = True\\n\\n        #settings.voices[voiceType] = voiceACSS', 'def _setRateForVoiceType(self, voiceType, value):\\n        \"\"\"Sets the speaking rate value for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n        - value: the rate value to set.\\n        \"\"\"\\n\\n        voiceACSS = self._getACSSForVoiceType(voiceType)\\n        voiceACSS[acss.ACSS.RATE] = value\\n        voiceACSS[\\'established\\'] = True\\n        #settings.voices[voiceType] = voiceACSS', 'def _setPitchForVoiceType(self, voiceType, value):\\n        \"\"\"Sets the pitch value for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n        - value: the pitch value to set.\\n        \"\"\"\\n\\n        voiceACSS = self._getACSSForVoiceType(voiceType)\\n        voiceACSS[acss.ACSS.AVERAGE_PITCH] = value\\n        voiceACSS[\\'established\\'] = True\\n        #settings.voices[voiceType] = voiceACSS', 'def _setVolumeForVoiceType(self, voiceType, value):\\n        \"\"\"Sets the volume (gain) value for the given voice type.\\n\\n        Arguments:\\n        - voiceType: one of DEFAULT, UPPERCASE, HYPERLINK, SYSTEM\\n        - value: the volume (gain) value to set.\\n        \"\"\"\\n\\n        voiceACSS = self._getACSSForVoiceType(voiceType)\\n        voiceACSS[acss.ACSS.GAIN] = value\\n        voiceACSS[\\'established\\'] = True\\n        #settings.voices[voiceType] = voiceACSS', 'def _setSpeechFamiliesChoice(self, familyName):\\n        \"\"\"Sets the active item in the families (\"Person:\") combo box\\n        to the given family name.\\n\\n        Arguments:\\n        - familyName: the family name to use to set the active combo box item.\\n        \"\"\"\\n\\n        if len(self.speechFamilies) == 0:\\n            return\\n\\n        languageSet = False\\n        familySet = False\\n        for family in self.speechFamilies:\\n            name = family[speechserver.VoiceFamily.NAME]\\n            if name == familyName:\\n                lang = family[speechserver.VoiceFamily.LANG]\\n                dialect = family[speechserver.VoiceFamily.DIALECT]\\n\\n                if dialect:\\n                    language = lang + \\'-\\' + dialect\\n                else:\\n                    language = lang\\n\\n                i = 0\\n                for languageChoice in self.speechLanguagesChoices:\\n                    if languageChoice == language:\\n                        self.get_widget(\"speechLanguages\").set_active(i)\\n                        self.speechLanguagesChoice = self.speechLanguagesChoices[i]\\n                        languageSet = True\\n\\n                        self._setupFamilies()\\n\\n                        i = 0\\n                        for familyChoice in self.speechFamiliesChoices:\\n                            name = familyChoice[speechserver.VoiceFamily.NAME]\\n                            if name == familyName:\\n                                self.get_widget(\"speechFamilies\").set_active(i)\\n                                self.speechFamiliesChoice = self.speechFamiliesChoices[i]\\n                                familySet = True\\n                                break\\n                            i += 1\\n\\n                        break\\n\\n                    i += 1\\n\\n                break\\n\\n        if not languageSet:\\n            debug.println(debug.LEVEL_FINEST,\\n                          \"Could not find speech language match for %s\" \\\\\\n                          % familyName)\\n            self.get_widget(\"speechLanguages\").set_active(0)\\n            self.speechLanguagesChoice = self.speechLanguagesChoices[0]\\n\\n        if languageSet:\\n            self.selectedLanguageChoices[self.speechServersChoice] = i\\n\\n        if not familySet:\\n            debug.println(debug.LEVEL_FINEST,\\n                          \"Could not find speech family match for %s\" \\\\\\n                          % familyName)\\n            self.get_widget(\"speechFamilies\").set_active(0)\\n            self.speechFamiliesChoice = self.speechFamiliesChoices[0]\\n\\n        if familySet:\\n            self.selectedFamilyChoices[self.speechServersChoice,\\n                    self.speechLanguagesChoice] = i', 'def _setSpeechLanguagesChoice(self, languageName):\\n        \"\"\"Sets the active item in the languages (\"Language:\") combo box\\n        to the given language name.\\n\\n        Arguments:\\n        - languageName: the language name to use to set the active combo box item.\\n        \"\"\"\\n\\n        print(\"setSpeechLanguagesChoice\")\\n\\n        if len(self.speechLanguagesChoices) == 0:\\n            return\\n\\n        valueSet = False\\n        i = 0\\n        for language in self.speechLanguagesChoices:\\n            if language == languageName:\\n                self.get_widget(\"speechLanguages\").set_active(i)\\n                self.speechLanguagesChoice = self.speechLanguagesChoices[i]\\n                valueSet = True\\n                break\\n            i += 1\\n\\n        if not valueSet:\\n            debug.println(debug.LEVEL_FINEST,\\n                          \"Could not find speech language match for %s\" \\\\\\n                          % languageName)\\n            self.get_widget(\"speechLanguages\").set_active(0)\\n            self.speechLanguagesChoice = self.speechLanguagesChoices[0]\\n\\n        if valueSet:\\n            self.selectedLanguageChoices[self.speechServersChoice] = i\\n\\n        self._setupFamilies()', 'def _setSpeechServersChoice(self, serverInfo):\\n        \"\"\"Sets the active item in the speech servers combo box to the\\n        given server.\\n\\n        Arguments:\\n        - serversChoices: the list of available speech servers.\\n        - serverInfo: the speech server to use to set the active combo\\n        box item.\\n        \"\"\"\\n\\n        if len(self.speechServersChoices) == 0:\\n            return\\n\\n        # We\\'ll fallback to whatever we happen to be using in the event\\n        # that this preference has never been set.\\n        #\\n        if not serverInfo:\\n            serverInfo = speech.getInfo()\\n\\n        valueSet = False\\n        i = 0\\n        for server in self.speechServersChoices:\\n            if serverInfo == server.getInfo():\\n                self.get_widget(\"speechServers\").set_active(i)\\n                self.speechServersChoice = server\\n                valueSet = True\\n                break\\n            i += 1\\n\\n        if not valueSet:\\n            debug.println(debug.LEVEL_FINEST,\\n                          \"Could not find speech server match for %s\" \\\\\\n                          %  repr(serverInfo))\\n            self.get_widget(\"speechServers\").set_active(0)\\n            self.speechServersChoice = self.speechServersChoices[0]\\n\\n        self._setupVoices()', 'def _setSpeechSystemsChoice(self, systemName):\\n        \"\"\"Set the active item in the speech systems combo box to the\\n        given system name.\\n\\n        Arguments:\\n        - factoryChoices: the list of available speech factories (systems).\\n        - systemName: the speech system name to use to set the active combo\\n        box item.\\n        \"\"\"\\n\\n        systemName = systemName.strip(\"\\'\")\\n\\n        if len(self.speechSystemsChoices) == 0:\\n            self.speechSystemsChoice = None\\n            return\\n\\n        valueSet = False\\n        i = 0\\n        for speechSystem in self.speechSystemsChoices:\\n            name = speechSystem.__name__\\n            if name.endswith(systemName):\\n                self.get_widget(\"speechSystems\").set_active(i)\\n                self.speechSystemsChoice = self.speechSystemsChoices[i]\\n                valueSet = True\\n                break\\n            i += 1\\n\\n        if not valueSet:\\n            debug.println(debug.LEVEL_FINEST,\\n                          \"Could not find speech system match for %s\" \\\\\\n                          % systemName)\\n            self.get_widget(\"speechSystems\").set_active(0)\\n            self.speechSystemsChoice = self.speechSystemsChoices[0]\\n\\n        self._setupSpeechServers()', 'def _initSpeechState(self):\\n        \"\"\"Initialize the various speech components.\\n        \"\"\"\\n\\n        voices = self.prefsDict[\"voices\"]\\n        self.defaultVoice   = acss.ACSS(voices.get(settings.DEFAULT_VOICE))\\n        self.uppercaseVoice = acss.ACSS(voices.get(settings.UPPERCASE_VOICE))\\n        self.hyperlinkVoice = acss.ACSS(voices.get(settings.HYPERLINK_VOICE))\\n        self.systemVoice    = acss.ACSS(voices.get(settings.SYSTEM_VOICE))\\n\\n        # Just a note on general naming pattern:\\n        #\\n        # *        = The name of the combobox\\n        # *Model   = the name of the comobox model\\n        # *Choices = the Orca/speech python objects\\n        # *Choice  = a value from *Choices\\n        #\\n        # Where * = speechSystems, speechServers, speechLanguages, speechFamilies\\n        #\\n        factories = _settingsManager.getSpeechServerFactories()\\n        if len(factories) == 0 or not self.prefsDict.get(\\'enableSpeech\\', True):\\n            self.workingFactories = []\\n            self.speechSystemsChoice = None\\n            self.speechServersChoices = []\\n            self.speechServersChoice = None\\n            self.speechLanguagesChoices = []\\n            self.speechLanguagesChoice = None\\n            self.speechFamiliesChoices = []\\n            self.speechFamiliesChoice = None\\n            return\\n\\n        try:\\n            speech.init()\\n        except:\\n            self.workingFactories = []\\n            self.speechSystemsChoice = None\\n            self.speechServersChoices = []\\n            self.speechServersChoice = None\\n            self.speechLanguagesChoices = []\\n            self.speechLanguagesChoice = None\\n            self.speechFamiliesChoices = []\\n            self.speechFamiliesChoice = None\\n            return\\n\\n        # This cascades into systems->servers->voice_type->families...\\n        #\\n        self.initializingSpeech = True\\n        self._setupSpeechSystems(factories)\\n        self.initializingSpeech = False', 'def _setBrailledTextAttributes(self, view, setAttributes, state):\\n        \"\"\"Given a set of brailled text attributes, update the model used\\n        by the text attribute tree view.\\n\\n        Arguments:\\n        - view: the text attribute tree view.\\n        - setAttributes: the list of brailled text attributes to update.\\n        - state: the state (True or False) that they all should be set to.\\n        \"\"\"\\n\\n        model = view.get_model()\\n        view.set_model(None)\\n\\n        [attrList, attrDict] = \\\\\\n            self.script.utilities.stringToKeysAndDict(setAttributes)\\n        [allAttrList, allAttrDict] = self.script.utilities.stringToKeysAndDict(\\n                _settingsManager.getSetting(\\'allTextAttributes\\'))\\n\\n        for i in range(0, len(attrList)):\\n            for path in range(0, len(allAttrList)):\\n                localizedKey = text_attribute_names.getTextAttributeName(\\n                    attrList[i], self.script)\\n                if localizedKey == model[path][NAME]:\\n                    thisIter = model.get_iter(path)\\n                    model.set_value(thisIter, IS_BRAILLED, state)\\n                    break\\n\\n        view.set_model(model)', 'def _updateTextDictEntry(self):\\n        \"\"\"The user has updated the text attribute list in some way. Update\\n        the \"enabledSpokenTextAttributes\" and \"enabledBrailledTextAttributes\"\\n        preference strings to reflect the current state of the corresponding\\n        text attribute lists.\\n        \"\"\"\\n\\n        model = self.getTextAttributesView.get_model()\\n        spokenAttrStr = \"\"\\n        brailledAttrStr = \"\"\\n        noRows = model.iter_n_children(None)\\n        for path in range(0, noRows):\\n            localizedKey = model[path][NAME]\\n            key = text_attribute_names.getTextAttributeKey(localizedKey)\\n\\n            # Convert the normalized, Atk attribute name back into what\\n            # the app/toolkit uses.\\n            #\\n            key = self._getAppNameForAttribute(key)\\n\\n            localizedValue = model[path][VALUE]\\n            value = text_attribute_names.getTextAttributeKey(localizedValue)\\n\\n            if model[path][IS_SPOKEN]:\\n                spokenAttrStr += key + \":\" + value + \"; \"\\n            if model[path][IS_BRAILLED]:\\n                brailledAttrStr += key + \":\" + value + \"; \"\\n\\n        self.prefsDict[\"enabledSpokenTextAttributes\"] = spokenAttrStr\\n        self.prefsDict[\"enabledBrailledTextAttributes\"] = brailledAttrStr', 'def contractionTableComboChanged(self, combobox):\\n        model = combobox.get_model()\\n        myIter = combobox.get_active_iter()\\n        self.prefsDict[\"brailleContractionTable\"] = model[myIter][1]', 'def textAttributeSpokenToggled(self, cell, path, model):\\n        \"\"\"The user has toggled the state of one of the text attribute\\n        checkboxes to be spoken. Update our model to reflect this, then\\n        update the \"enabledSpokenTextAttributes\" preference string.\\n\\n        Arguments:\\n        - cell: the cell that changed.\\n        - path: the path of that cell.\\n        - model: the model that the cell is part of.\\n        \"\"\"\\n\\n        thisIter = model.get_iter(path)\\n        model.set(thisIter, IS_SPOKEN, not model[path][IS_SPOKEN])\\n        self._updateTextDictEntry()', 'def textAttrValueEdited(self, cell, path, new_text, model):\\n        \"\"\"The user has edited the value of one of the text attributes.\\n        Update our model to reflect this, then update the\\n        \"enabledSpokenTextAttributes\" and \"enabledBrailledTextAttributes\"\\n        preference strings.\\n\\n        Arguments:\\n        - cell: the cell that changed.\\n        - path: the path of that cell.\\n        - new_text: the new text attribute value string.\\n        - model: the model that the cell is part of.\\n        \"\"\"\\n\\n        thisIter = model.get_iter(path)\\n        model.set(thisIter, VALUE, new_text)\\n        self._updateTextDictEntry()', 'def _createTextAttributesTreeView(self):\\n        \"\"\"Create the text attributes tree view. The view is the\\n        textAttributesTreeView GtkTreeView widget. The view will consist\\n        of a list containing three columns:\\n          IS_SPOKEN - a checkbox whose state indicates whether this text\\n                      attribute will be spoken or not.\\n          NAME      - the text attribute name.\\n          VALUE     - if set, (and this attributes is enabled for speaking),\\n                      then this attribute will be spoken unless it equals\\n                      this value.\\n        \"\"\"\\n\\n        self.getTextAttributesView = self.get_widget(\"textAttributesTreeView\")\\n\\n        if self.getTextAttributesView.get_columns():\\n            for column in self.getTextAttributesView.get_columns():\\n                self.getTextAttributesView.remove_column(column)\\n\\n        model = Gtk.ListStore(GObject.TYPE_STRING,\\n                              GObject.TYPE_BOOLEAN,\\n                              GObject.TYPE_BOOLEAN,\\n                              GObject.TYPE_STRING)\\n\\n        # Initially setup the list store model based on the values of all\\n        # the known text attributes.\\n        #\\n        [allAttrList, allAttrDict] = self.script.utilities.stringToKeysAndDict(\\n            _settingsManager.getSetting(\\'allTextAttributes\\'))\\n        for i in range(0, len(allAttrList)):\\n            thisIter = model.append()\\n            localizedKey = text_attribute_names.getTextAttributeName(\\n                allAttrList[i], self.script)\\n            localizedValue = text_attribute_names.getTextAttributeName(\\n                allAttrDict[allAttrList[i]], self.script)\\n            model.set_value(thisIter, NAME, localizedKey)\\n            model.set_value(thisIter, IS_SPOKEN, False)\\n            model.set_value(thisIter, IS_BRAILLED, False)\\n            model.set_value(thisIter, VALUE, localizedValue)\\n\\n        self.getTextAttributesView.set_model(model)\\n\\n        # Attribute Name column (NAME).\\n        column = Gtk.TreeViewColumn(guilabels.TEXT_ATTRIBUTE_NAME)\\n        column.set_min_width(250)\\n        column.set_resizable(True)\\n        renderer = Gtk.CellRendererText()\\n        column.pack_end(renderer, True)\\n        column.add_attribute(renderer, \\'text\\', NAME)\\n        self.getTextAttributesView.insert_column(column, 0)\\n\\n        # Attribute Speak column (IS_SPOKEN).\\n        speakAttrColumnLabel = guilabels.PRESENTATION_SPEAK\\n        column = Gtk.TreeViewColumn(speakAttrColumnLabel)\\n        renderer = Gtk.CellRendererToggle()\\n        column.pack_start(renderer, False)\\n        column.add_attribute(renderer, \\'active\\', IS_SPOKEN)\\n        renderer.connect(\"toggled\",\\n                         self.textAttributeSpokenToggled,\\n                         model)\\n        self.getTextAttributesView.insert_column(column, 1)\\n        column.clicked()\\n\\n        # Attribute Mark in Braille column (IS_BRAILLED).\\n        markAttrColumnLabel = guilabels.PRESENTATION_MARK_IN_BRAILLE\\n        column = Gtk.TreeViewColumn(markAttrColumnLabel)\\n        renderer = Gtk.CellRendererToggle()\\n        column.pack_start(renderer, False)\\n        column.add_attribute(renderer, \\'active\\', IS_BRAILLED)\\n        renderer.connect(\"toggled\",\\n                         self.textAttributeBrailledToggled,\\n                         model)\\n        self.getTextAttributesView.insert_column(column, 2)\\n        column.clicked()\\n\\n        # Attribute Value column (VALUE)\\n        column = Gtk.TreeViewColumn(guilabels.PRESENTATION_PRESENT_UNLESS)\\n        renderer = Gtk.CellRendererText()\\n        renderer.set_property(\\'editable\\', True)\\n        column.pack_end(renderer, True)\\n        column.add_attribute(renderer, \\'text\\', VALUE)\\n        renderer.connect(\"edited\", self.textAttrValueEdited, model)\\n\\n        self.getTextAttributesView.insert_column(column, 4)\\n\\n        # Check all the enabled (spoken) text attributes.\\n        #\\n        self._setSpokenTextAttributes(\\n            self.getTextAttributesView,\\n            _settingsManager.getSetting(\\'enabledSpokenTextAttributes\\'),\\n            True, True)\\n\\n        # Check all the enabled (brailled) text attributes.\\n        #\\n        self._setBrailledTextAttributes(\\n            self.getTextAttributesView,\\n            _settingsManager.getSetting(\\'enabledBrailledTextAttributes\\'),\\n            True)\\n\\n        # Connect a handler for when the user changes columns within the\\n        # view, so that we can adjust the search column for item lookups.\\n        #\\n        self.getTextAttributesView.connect(\"cursor_changed\",\\n                                           self.textAttrCursorChanged)', 'def pronReplacementValueEdited(self, cell, path, new_text, model):\\n        \"\"\"The user has edited the value of one of the replacement strings\\n        in the pronunciation dictionary. Update our model to reflect this.\\n\\n        Arguments:\\n        - cell: the cell that changed.\\n        - path: the path of that cell.\\n        - new_text: the new pronunciation dictionary replacement string.\\n        - model: the model that the cell is part of.\\n        \"\"\"\\n\\n        thisIter = model.get_iter(path)\\n        model.set(thisIter, REPLACEMENT, new_text)', 'def pronunciationCursorChanged(self, widget):\\n        \"\"\"Set the search column in the pronunciation dictionary tree view\\n        depending upon which column the user currently has the cursor in.\\n        \"\"\"\\n\\n        [path, focusColumn] = self.pronunciationView.get_cursor()\\n        if focusColumn:\\n            noColumns = len(self.pronunciationView.get_columns())\\n            for i in range(0, noColumns):\\n                col = self.pronunciationView.get_column(i)\\n                if focusColumn == col:\\n                    self.pronunciationView.set_search_column(i)\\n                    break', 'def _initGUIState(self):\\n        \"\"\"Adjust the settings of the various components on the\\n        configuration GUI depending upon the users preferences.\\n        \"\"\"\\n\\n        prefs = self.prefsDict\\n\\n        # Speech pane.\\n        #\\n        enable = prefs[\"enableSpeech\"]\\n        self.get_widget(\"speechSupportCheckButton\").set_active(enable)\\n        self.get_widget(\"speechOptionsGrid\").set_sensitive(enable)\\n\\n        enable = prefs[\"onlySpeakDisplayedText\"]\\n        self.get_widget(\"onlySpeakDisplayedTextCheckButton\").set_active(enable)\\n        self.get_widget(\"contextOptionsGrid\").set_sensitive(not enable)\\n\\n        if prefs[\"verbalizePunctuationStyle\"] == \\\\\\n                               settings.PUNCTUATION_STYLE_NONE:\\n            self.get_widget(\"noneButton\").set_active(True)\\n        elif prefs[\"verbalizePunctuationStyle\"] == \\\\\\n                               settings.PUNCTUATION_STYLE_SOME:\\n            self.get_widget(\"someButton\").set_active(True)\\n        elif prefs[\"verbalizePunctuationStyle\"] == \\\\\\n                               settings.PUNCTUATION_STYLE_MOST:\\n            self.get_widget(\"mostButton\").set_active(True)\\n        else:\\n            self.get_widget(\"allButton\").set_active(True)\\n\\n        if prefs[\"speechVerbosityLevel\"] == settings.VERBOSITY_LEVEL_BRIEF:\\n            self.get_widget(\"speechBriefButton\").set_active(True)\\n        else:\\n            self.get_widget(\"speechVerboseButton\").set_active(True)\\n\\n        self.get_widget(\"onlySpeakDisplayedTextCheckButton\").set_active(\\n            prefs[\"onlySpeakDisplayedText\"])\\n\\n        self.get_widget(\"enableSpeechIndentationCheckButton\").set_active(\\\\\\n            prefs[\"enableSpeechIndentation\"])\\n\\n        self.get_widget(\"speakBlankLinesCheckButton\").set_active(\\\\\\n            prefs[\"speakBlankLines\"])\\n        self.get_widget(\"speakMultiCaseStringsAsWordsCheckButton\").set_active(\\\\\\n            prefs[\"speakMultiCaseStringsAsWords\"])\\n        self.get_widget(\"speakNumbersAsDigitsCheckButton\").set_active(\\n            prefs.get(\"speakNumbersAsDigits\", settings.speakNumbersAsDigits))\\n        self.get_widget(\"enableTutorialMessagesCheckButton\").set_active(\\\\\\n            prefs[\"enableTutorialMessages\"])\\n        self.get_widget(\"enablePauseBreaksCheckButton\").set_active(\\\\\\n            prefs[\"enablePauseBreaks\"])\\n        self.get_widget(\"enablePositionSpeakingCheckButton\").set_active(\\\\\\n            prefs[\"enablePositionSpeaking\"])\\n        self.get_widget(\"enableMnemonicSpeakingCheckButton\").set_active(\\\\\\n            prefs[\"enableMnemonicSpeaking\"])\\n        self.get_widget(\"speakMisspelledIndicatorCheckButton\").set_active(\\n            prefs.get(\"speakMisspelledIndicator\", settings.speakMisspelledIndicator))\\n        self.get_widget(\"speakDescriptionCheckButton\").set_active(\\n            prefs.get(\"speakDescription\", settings.speakDescription))\\n        self.get_widget(\"speakContextBlockquoteCheckButton\").set_active(\\n            prefs.get(\"speakContextBlockquote\", settings.speakContextList))\\n        self.get_widget(\"speakContextLandmarkCheckButton\").set_active(\\n            prefs.get(\"speakContextLandmark\", settings.speakContextLandmark))\\n        self.get_widget(\"speakContextNonLandmarkFormCheckButton\").set_active(\\n            prefs.get(\"speakContextNonLandmarkForm\", settings.speakContextNonLandmarkForm))\\n        self.get_widget(\"speakContextListCheckButton\").set_active(\\n            prefs.get(\"speakContextList\", settings.speakContextList))\\n        self.get_widget(\"speakContextPanelCheckButton\").set_active(\\n            prefs.get(\"speakContextPanel\", settings.speakContextPanel))\\n        self.get_widget(\"speakContextTableCheckButton\").set_active(\\n            prefs.get(\"speakContextTable\", settings.speakContextTable))\\n\\n        enable = prefs.get(\"messagesAreDetailed\", settings.messagesAreDetailed)\\n        self.get_widget(\"messagesAreDetailedCheckButton\").set_active(enable)\\n\\n        enable = prefs.get(\"useColorNames\", settings.useColorNames)\\n        self.get_widget(\"useColorNamesCheckButton\").set_active(enable)\\n\\n        enable = prefs.get(\"readFullRowInGUITable\", settings.readFullRowInGUITable)\\n        self.get_widget(\"readFullRowInGUITableCheckButton\").set_active(enable)\\n\\n        enable = prefs.get(\"readFullRowInDocumentTable\", settings.readFullRowInDocumentTable)\\n        self.get_widget(\"readFullRowInDocumentTableCheckButton\").set_active(enable)\\n\\n        enable = prefs.get(\"readFullRowInSpreadSheet\", settings.readFullRowInSpreadSheet)\\n        self.get_widget(\"readFullRowInSpreadSheetCheckButton\").set_active(enable)\\n\\n        style = prefs.get(\"capitalizationStyle\", settings.capitalizationStyle)\\n        combobox = self.get_widget(\"capitalizationStyle\")\\n        options = [guilabels.CAPITALIZATION_STYLE_NONE,\\n                   guilabels.CAPITALIZATION_STYLE_ICON,\\n                   guilabels.CAPITALIZATION_STYLE_SPELL]\\n        self.populateComboBox(combobox, options)\\n        if style == settings.CAPITALIZATION_STYLE_ICON:\\n            value = guilabels.CAPITALIZATION_STYLE_ICON\\n        elif style == settings.CAPITALIZATION_STYLE_SPELL:\\n            value = guilabels.CAPITALIZATION_STYLE_SPELL\\n        else:\\n            value = guilabels.CAPITALIZATION_STYLE_NONE\\n        combobox.set_active(options.index(value))\\n\\n        combobox2 = self.get_widget(\"dateFormatCombo\")\\n        sdtime = time.strftime\\n        ltime = time.localtime\\n        self.populateComboBox(combobox2,\\n          [sdtime(messages.DATE_FORMAT_LOCALE, ltime()),\\n           sdtime(messages.DATE_FORMAT_NUMBERS_DM, ltime()),\\n           sdtime(messages.DATE_FORMAT_NUMBERS_MD, ltime()),\\n           sdtime(messages.DATE_FORMAT_NUMBERS_DMY, ltime()),\\n           sdtime(messages.DATE_FORMAT_NUMBERS_MDY, ltime()),\\n           sdtime(messages.DATE_FORMAT_NUMBERS_YMD, ltime()),\\n           sdtime(messages.DATE_FORMAT_FULL_DM, ltime()),\\n           sdtime(messages.DATE_FORMAT_FULL_MD, ltime()),\\n           sdtime(messages.DATE_FORMAT_FULL_DMY, ltime()),\\n           sdtime(messages.DATE_FORMAT_FULL_MDY, ltime()),\\n           sdtime(messages.DATE_FORMAT_FULL_YMD, ltime()),\\n           sdtime(messages.DATE_FORMAT_ABBREVIATED_DM, ltime()),\\n           sdtime(messages.DATE_FORMAT_ABBREVIATED_MD, ltime()),\\n           sdtime(messages.DATE_FORMAT_ABBREVIATED_DMY, ltime()),\\n           sdtime(messages.DATE_FORMAT_ABBREVIATED_MDY, ltime()),\\n           sdtime(messages.DATE_FORMAT_ABBREVIATED_YMD, ltime())\\n          ])\\n\\n        indexdate = DATE_FORMAT_LOCALE\\n        dateFormat = self.prefsDict[\"presentDateFormat\"]\\n        if dateFormat == messages.DATE_FORMAT_LOCALE:\\n            indexdate = DATE_FORMAT_LOCALE\\n        elif dateFormat == messages.DATE_FORMAT_NUMBERS_DM:\\n            indexdate = DATE_FORMAT_NUMBERS_DM\\n        elif dateFormat == messages.DATE_FORMAT_NUMBERS_MD:\\n            indexdate = DATE_FORMAT_NUMBERS_MD\\n        elif dateFormat == messages.DATE_FORMAT_NUMBERS_DMY:\\n            indexdate = DATE_FORMAT_NUMBERS_DMY\\n        elif dateFormat == messages.DATE_FORMAT_NUMBERS_MDY:\\n            indexdate = DATE_FORMAT_NUMBERS_MDY\\n        elif dateFormat == messages.DATE_FORMAT_NUMBERS_YMD:\\n            indexdate = DATE_FORMAT_NUMBERS_YMD\\n        elif dateFormat == messages.DATE_FORMAT_FULL_DM:\\n            indexdate = DATE_FORMAT_FULL_DM\\n        elif dateFormat == messages.DATE_FORMAT_FULL_MD:\\n            indexdate = DATE_FORMAT_FULL_MD\\n        elif dateFormat == messages.DATE_FORMAT_FULL_DMY:\\n            indexdate = DATE_FORMAT_FULL_DMY\\n        elif dateFormat == messages.DATE_FORMAT_FULL_MDY:\\n            indexdate = DATE_FORMAT_FULL_MDY\\n        elif dateFormat == messages.DATE_FORMAT_FULL_YMD:\\n            indexdate = DATE_FORMAT_FULL_YMD\\n        elif dateFormat == messages.DATE_FORMAT_ABBREVIATED_DM:\\n            indexdate = DATE_FORMAT_ABBREVIATED_DM\\n        elif dateFormat == messages.DATE_FORMAT_ABBREVIATED_MD:\\n            indexdate = DATE_FORMAT_ABBREVIATED_MD\\n        elif dateFormat == messages.DATE_FORMAT_ABBREVIATED_DMY:\\n            indexdate = DATE_FORMAT_ABBREVIATED_DMY\\n        elif dateFormat == messages.DATE_FORMAT_ABBREVIATED_MDY:\\n            indexdate = DATE_FORMAT_ABBREVIATED_MDY\\n        elif dateFormat == messages.DATE_FORMAT_ABBREVIATED_YMD:\\n            indexdate = DATE_FORMAT_ABBREVIATED_YMD\\n        combobox2.set_active (indexdate)', 'def __initProfileCombo(self):\\n        \"\"\"Adding available profiles and setting active as the active one\"\"\"\\n\\n        availableProfiles = self.__getAvailableProfiles()\\n        self.profilesComboModel.clear()\\n\\n        if not len(availableProfiles):\\n            self.profilesComboModel.append(self._defaultProfile)\\n        else:\\n            for profile in availableProfiles:\\n                self.profilesComboModel.append(profile)\\n\\n        activeProfile = self.prefsDict.get(\\'activeProfile\\') or self._defaultProfile\\n        startingProfile = self.prefsDict.get(\\'startingProfile\\') or self._defaultProfile\\n\\n        activeProfileIter = self.getComboBoxIndex(self.profilesCombo,\\n                                                  activeProfile[0])\\n        startingProfileIter = self.getComboBoxIndex(self.startingProfileCombo,\\n                                                  startingProfile[0])\\n        self.profilesCombo.set_active(activeProfileIter)\\n        self.startingProfileCombo.set_active(startingProfileIter)', 'def _updateOrcaModifier(self):\\n        combobox = self.get_widget(\"orcaModifierComboBox\")\\n        keystring = \", \".join(self.prefsDict[\"orcaModifierKeys\"])\\n        combobox.set_active(self.getComboBoxIndex(combobox, keystring))', 'def getComboBoxIndex(self, combobox, searchStr, col=0):\\n        \"\"\" For each of the entries in the given combo box, look for searchStr.\\n            Return the index of the entry if searchStr is found.\\n\\n        Arguments:\\n        - combobox: the GtkComboBox to search.\\n        - searchStr: the string to search for.\\n\\n        Returns the index of the first entry in combobox with searchStr, or\\n        0 if not found.\\n        \"\"\"\\n\\n        model = combobox.get_model()\\n        myiter = model.get_iter_first()\\n        for i in range(0, len(model)):\\n            name = model.get_value(myiter, col)\\n            if name == searchStr:\\n                return i\\n            myiter = model.iter_next(myiter)\\n\\n        return 0', 'def getKeyBindingsModelDict(self, model, modifiedOnly=True):\\n        modelDict = {}\\n        node = model.get_iter_first()\\n        while node:\\n            child = model.iter_children(node)\\n            while child:\\n                key, modified = model.get(child, HANDLER, MODIF)\\n                if modified or not modifiedOnly:\\n                    value = []\\n                    value.append(list(model.get(\\n                            child, KEY1, MOD_MASK1, MOD_USED1, CLICK_COUNT1)))\\n                    modelDict[key] = value\\n                child = model.iter_next(child)\\n            node = model.iter_next(node)\\n\\n        return modelDict', 'def showGUI(self):\\n        \"\"\"Show the Orca configuration GUI window. This assumes that\\n        the GUI has already been created.\\n        \"\"\"\\n\\n        orcaSetupWindow = self.get_widget(\"orcaSetupWindow\")\\n\\n        accelGroup = Gtk.AccelGroup()\\n        orcaSetupWindow.add_accel_group(accelGroup)\\n        helpButton = self.get_widget(\"helpButton\")\\n        (keyVal, modifierMask) = Gtk.accelerator_parse(\"F1\")\\n        helpButton.add_accelerator(\"clicked\",\\n                                   accelGroup,\\n                                   keyVal,\\n                                   modifierMask,\\n                                   0)\\n\\n        try:\\n            ts = orca_state.lastInputEvent.timestamp\\n        except:\\n            ts = 0\\n        if ts == 0:\\n            ts = Gtk.get_current_event_time()\\n        orcaSetupWindow.present_with_time(ts)\\n\\n        # We always want to re-order the text attributes page so that enabled\\n        # items are consistently at the top.\\n        #\\n        self._setSpokenTextAttributes(\\n                self.getTextAttributesView,\\n                _settingsManager.getSetting(\\'enabledSpokenTextAttributes\\'),\\n                True, True)\\n\\n        if self.script.app:\\n            title = guilabels.PREFERENCES_APPLICATION_TITLE % self.script.app.name\\n            orcaSetupWindow.set_title(title)\\n\\n        orcaSetupWindow.show()', 'def _setKeyEchoItems(self):\\n        \"\"\"[In]sensitize the checkboxes for the various types of key echo,\\n        depending upon whether the value of the key echo check button is set.\\n        \"\"\"\\n\\n        enable = self.get_widget(\"keyEchoCheckButton\").get_active()\\n        self.get_widget(\"enableAlphabeticKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableNumericKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enablePunctuationKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableSpaceCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableModifierKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableFunctionKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableActionKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableNavigationKeysCheckButton\").set_sensitive(enable)\\n        self.get_widget(\"enableDiacriticalKeysCheckButton\").set_sensitive( \\\\\\n          enable)', 'def _createNode(self, appName):\\n        \"\"\"Create a new root node in the TreeStore model with the name of the\\n            application.\\n\\n        Arguments:\\n        - appName: the name of the TreeStore Node (the same of the application)\\n        \"\"\"\\n\\n        model = self.keyBindingsModel\\n\\n        myiter = model.append(None)\\n        model.set_value(myiter, DESCRIP, appName)\\n        model.set_value(myiter, MODIF, False)\\n\\n        return myiter', 'def _clickCountToString(self, clickCount):\\n        \"\"\"Given a numeric clickCount, returns a string for inclusion\\n        in the list of keybindings.\\n\\n        Argument:\\n        - clickCount: the number of clicks associated with the keybinding.\\n        \"\"\"\\n\\n        clickCountString = \"\"\\n        if clickCount == 2:\\n            clickCountString = \" (%s)\" % guilabels.CLICK_COUNT_DOUBLE\\n        elif clickCount == 3:\\n            clickCountString = \" (%s)\" % guilabels.CLICK_COUNT_TRIPLE\\n\\n        return clickCountString', 'def _insertRowBraille(self, handl, com, inputEvHand, \\n                          parent=None, modif=False):\\n        \"\"\"Appends a new row with the new braille binding data to the treeview\\n\\n        Arguments:\\n        - handl:       the name of the handler associated to the brailleBinding\\n        - com:         the BrlTTY command\\n        - inputEvHand: the inputEventHandler with the new brailleBinding\\n        - parent:      the parent node of the treeview, where to append the kb\\n        - modif:       whether to check the modified field or not.\\n\\n        Returns a Gtk.TreeIter pointing at the new row.\\n        \"\"\"\\n\\n        model = self.keyBindingsModel\\n\\n        if parent is None:\\n            parent = self._getIterOf(guilabels.KB_GROUP_BRAILLE)\\n\\n        if parent is not None:\\n            myiter = model.append(parent)\\n            model.set_value(myiter, HANDLER, handl)\\n            model.set_value(myiter, DESCRIP, inputEvHand.description)\\n            model.set_value(myiter, KEY1, str(com))\\n            model.set_value(myiter, TEXT1, braille.command_name[com])\\n            model.set_value(myiter, MODIF, modif)\\n            model.set_value(myiter, EDITABLE, False)\\n            return myiter\\n        else:\\n            return None', 'def _populateKeyBindings(self, clearModel=True):\\n        \"\"\"Fills the TreeView with the list of Orca keybindings\\n\\n        Arguments:\\n        - clearModel: if True, initially clear out the key bindings model.\\n        \"\"\"\\n\\n        self.keyBindView.set_model(None)\\n        self.keyBindView.set_headers_visible(False)\\n        self.keyBindView.hide()\\n        if clearModel:\\n            self.keyBindingsModel.clear()\\n            self.kbindings = None\\n\\n        try:\\n            appName = self.script.app.name\\n        except:\\n            appName = \"\"\\n\\n        iterApp = self._createNode(appName)\\n        iterOrca = self._createNode(guilabels.KB_GROUP_DEFAULT)\\n        iterUnbound = self._createNode(guilabels.KB_GROUP_UNBOUND)\\n\\n        if not self.kbindings:\\n            self.kbindings = keybindings.KeyBindings()\\n            self.script.setupInputEventHandlers()\\n            allKeyBindings = self.script.getKeyBindings()\\n            defKeyBindings = self.script.getDefaultKeyBindings()\\n            for kb in allKeyBindings.keyBindings:\\n                if not self.kbindings.hasKeyBinding(kb, \"strict\"):\\n                    handl = self.script.getInputEventHandlerKey(kb.handler)\\n                    if not defKeyBindings.hasKeyBinding(kb, \"description\"):\\n                        self._insertRow(handl, kb, iterApp)\\n                    elif kb.keysymstring:\\n                        self._insertRow(handl, kb, iterOrca)\\n                    else:\\n                        self._insertRow(handl, kb, iterUnbound)\\n                    self.kbindings.add(kb)\\n\\n        if not self.keyBindingsModel.iter_has_child(iterApp):\\n            self.keyBindingsModel.remove(iterApp)\\n\\n        if not self.keyBindingsModel.iter_has_child(iterUnbound):\\n            self.keyBindingsModel.remove(iterUnbound)\\n\\n        self._updateOrcaModifier()\\n        self._markModified()\\n        iterBB = self._createNode(guilabels.KB_GROUP_BRAILLE)\\n        self.bbindings = self.script.getBrailleBindings()\\n        for com, inputEvHand in self.bbindings.items():\\n            handl = self.script.getInputEventHandlerKey(inputEvHand)\\n            self._insertRowBraille(handl, com, inputEvHand, iterBB)\\n\\n        self.keyBindView.set_model(self.keyBindingsModel)\\n        self.keyBindView.set_headers_visible(True)\\n        self.keyBindView.expand_all()\\n        self.keyBindingsModel.set_sort_column_id(OLDTEXT1, Gtk.SortType.ASCENDING)\\n        self.keyBindView.show()\\n\\n        # Keep track of new/unbound keybindings that have yet to be applied.\\n        #\\n        self.pendingKeyBindings = {}', 'def speechSupportChecked(self, widget):\\n        \"\"\"Signal handler for the \"toggled\" signal for the\\n           speechSupportCheckButton GtkCheckButton widget. The user has\\n           [un]checked the \\'Enable Speech\\' checkbox. Set the \\'enableSpeech\\'\\n           preference to the new value. Set the rest of the speech pane items\\n           [in]sensensitive depending upon whether this checkbox is checked.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        enable = widget.get_active()\\n        self.prefsDict[\"enableSpeech\"] = enable\\n        self.get_widget(\"speechOptionsGrid\").set_sensitive(enable)', 'def speechSystemsChanged(self, widget):\\n        \"\"\"Signal handler for the \"changed\" signal for the speechSystems\\n           GtkComboBox widget. The user has selected a different speech\\n           system. Clear the existing list of speech servers, and setup\\n           a new list of speech servers based on the new choice. Setup a\\n           new list of voices for the first speech server in the list.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if self.initializingSpeech:\\n            return\\n\\n        selectedIndex = widget.get_active()\\n        self.speechSystemsChoice = self.speechSystemsChoices[selectedIndex]\\n        self._setupSpeechServers()', 'def speechLanguagesChanged(self, widget):\\n        \"\"\"Signal handler for the \"value_changed\" signal for the languages\\n           GtkComboBox widget. The user has selected a different voice\\n           language. Save the new voice language name based on the new choice.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if self.initializingSpeech:\\n            return\\n\\n        selectedIndex = widget.get_active()\\n        try:\\n            self.speechLanguagesChoice = self.speechLanguagesChoices[selectedIndex]\\n            if (self.speechServersChoice, self.speechLanguagesChoice) in \\\\\\n                    self.selectedFamilyChoices:\\n                i = self.selectedFamilyChoices[self.speechServersChoice, \\\\\\n                        self.speechLanguagesChoice]\\n                family = self.speechFamiliesChoices[i]\\n                name = family[speechserver.VoiceFamily.NAME]\\n                language = family[speechserver.VoiceFamily.LANG]\\n                dialect = family[speechserver.VoiceFamily.DIALECT]\\n                variant = family[speechserver.VoiceFamily.VARIANT]\\n                voiceType = self.get_widget(\"voiceTypesCombo\").get_active()\\n                self._setFamilyNameForVoiceType(voiceType, name, language, dialect, variant)\\n        except:\\n            debug.printException(debug.LEVEL_SEVERE)\\n\\n        # Remember the last family manually selected by the user for the\\n        # current speech server.\\n        #\\n        if not selectedIndex == -1:\\n            self.selectedLanguageChoices[self.speechServersChoice] = selectedIndex\\n\\n        self._setupFamilies()', 'def voiceTypesChanged(self, widget):\\n        \"\"\"Signal handler for the \"changed\" signal for the voiceTypes\\n           GtkComboBox widget. The user has selected a different voice\\n           type. Setup the new family, rate, pitch and volume component\\n           values based on the new choice.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if self.initializingSpeech:\\n            return\\n\\n        voiceType = widget.get_active()\\n        self._setVoiceSettingsForVoiceType(voiceType)', 'def pitchValueChanged(self, widget):\\n        \"\"\"Signal handler for the \"value_changed\" signal for the pitchScale\\n           GtkScale widget. The user has changed the current pitch value.\\n           Save the new pitch value based on the currently selected voice\\n           type.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        pitch = widget.get_value()\\n        voiceType = self.get_widget(\"voiceTypesCombo\").get_active()\\n        self._setPitchForVoiceType(voiceType, pitch)\\n        voices = _settingsManager.getSetting(\\'voices\\')\\n        voices[settings.DEFAULT_VOICE][acss.ACSS.AVERAGE_PITCH] = pitch\\n        _settingsManager.setSetting(\\'voices\\', voices)', 'def checkButtonToggled(self, widget):\\n        \"\"\"Signal handler for \"toggled\" signal for basic GtkCheckButton \\n           widgets. The user has altered the state of the checkbox.\\n           Set the preference to the new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        # To use this default handler please make sure:\\n        # The name of the setting that will be changed is: settingName\\n        # The id of the widget in the ui should be: settingNameCheckButton\\n        #\\n        settingName = Gtk.Buildable.get_name(widget)\\n        # strip \"CheckButton\" from the end.\\n        settingName = settingName[:-11] \\n        self.prefsDict[settingName] = widget.get_active()', 'def brailleSelectionChanged(self, widget):\\n        \"\"\"Signal handler for the \"toggled\" signal for the\\n           brailleSelectionNoneButton, brailleSelection7Button,\\n           brailleSelection8Button or brailleSelectionBothButton\\n           GtkRadioButton widgets. The user has toggled the braille\\n           selection indicator value. If this signal was generated\\n           as the result of a radio button getting selected (as\\n           opposed to a radio button losing the selection), set the\\n           \\'brailleSelectorIndicator\\' preference to the new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if widget.get_active():\\n            if widget.get_label() == guilabels.BRAILLE_DOT_7:\\n                self.prefsDict[\"brailleSelectorIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_7\\n            elif widget.get_label() == guilabels.BRAILLE_DOT_8:\\n                self.prefsDict[\"brailleSelectorIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_8\\n            elif widget.get_label() == guilabels.BRAILLE_DOT_7_8:\\n                self.prefsDict[\"brailleSelectorIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_BOTH\\n            else:\\n                self.prefsDict[\"brailleSelectorIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_NONE', 'def brailleIndicatorChanged(self, widget):\\n        \"\"\"Signal handler for the \"toggled\" signal for the\\n           textBrailleNoneButton, textBraille7Button, textBraille8Button\\n           or textBrailleBothButton GtkRadioButton widgets. The user has\\n           toggled the text attributes braille indicator value. If this signal\\n           was generated as the result of a radio button getting selected\\n           (as opposed to a radio button losing the selection), set the\\n           \\'textAttributesBrailleIndicator\\' preference to the new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if widget.get_active():\\n            if widget.get_label() == guilabels.BRAILLE_DOT_7:\\n                self.prefsDict[\"textAttributesBrailleIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_7\\n            elif widget.get_label() == guilabels.BRAILLE_DOT_8:\\n                self.prefsDict[\"textAttributesBrailleIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_8\\n            elif widget.get_label() == guilabels.BRAILLE_DOT_7_8:\\n                self.prefsDict[\"textAttributesBrailleIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_BOTH\\n            else:\\n                self.prefsDict[\"textAttributesBrailleIndicator\"] = \\\\\\n                    settings.BRAILLE_UNDERLINE_NONE', 'def orcaModifierChanged(self, widget):\\n        \"\"\"Signal handler for the changed signal for the orcaModifierComboBox\\n           Set the \\'orcaModifierKeys\\' preference to the new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        model = widget.get_model()\\n        myIter = widget.get_active_iter()\\n        orcaModifier = model[myIter][0]\\n        self.prefsDict[\"orcaModifierKeys\"] = orcaModifier.split(\\', \\')', 'def capitalizationStyleChanged(self, widget):\\n        model = widget.get_model()\\n        myIter = widget.get_active_iter()\\n        capitalizationStyle = model[myIter][0]\\n        if capitalizationStyle == guilabels.CAPITALIZATION_STYLE_ICON:\\n            self.prefsDict[\"capitalizationStyle\"] = settings.CAPITALIZATION_STYLE_ICON\\n        elif capitalizationStyle == guilabels.CAPITALIZATION_STYLE_SPELL:\\n            self.prefsDict[\"capitalizationStyle\"] = settings.CAPITALIZATION_STYLE_SPELL\\n        else:\\n            self.prefsDict[\"capitalizationStyle\"] = settings.CAPITALIZATION_STYLE_NONE\\n        speech.updateCapitalizationStyle()', 'def dateFormatChanged(self, widget):\\n        \"\"\"Signal handler for the \"changed\" signal for the dateFormat\\n           GtkComboBox widget. Set the \\'dateFormat\\' preference to the\\n           new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        dateFormatCombo = widget.get_active()\\n        if dateFormatCombo == DATE_FORMAT_LOCALE:\\n            newFormat = messages.DATE_FORMAT_LOCALE\\n        elif dateFormatCombo == DATE_FORMAT_NUMBERS_DM:\\n            newFormat = messages.DATE_FORMAT_NUMBERS_DM\\n        elif dateFormatCombo == DATE_FORMAT_NUMBERS_MD:\\n            newFormat = messages.DATE_FORMAT_NUMBERS_MD\\n        elif dateFormatCombo == DATE_FORMAT_NUMBERS_DMY:\\n            newFormat = messages.DATE_FORMAT_NUMBERS_DMY\\n        elif dateFormatCombo == DATE_FORMAT_NUMBERS_MDY:\\n            newFormat = messages.DATE_FORMAT_NUMBERS_MDY\\n        elif dateFormatCombo == DATE_FORMAT_NUMBERS_YMD:\\n            newFormat = messages.DATE_FORMAT_NUMBERS_YMD\\n        elif dateFormatCombo == DATE_FORMAT_FULL_DM:\\n            newFormat = messages.DATE_FORMAT_FULL_DM\\n        elif dateFormatCombo == DATE_FORMAT_FULL_MD:\\n            newFormat = messages.DATE_FORMAT_FULL_MD\\n        elif dateFormatCombo == DATE_FORMAT_FULL_DMY:\\n            newFormat = messages.DATE_FORMAT_FULL_DMY\\n        elif dateFormatCombo == DATE_FORMAT_FULL_MDY:\\n            newFormat = messages.DATE_FORMAT_FULL_MDY\\n        elif dateFormatCombo == DATE_FORMAT_FULL_YMD:\\n            newFormat = messages.DATE_FORMAT_FULL_YMD\\n        elif dateFormatCombo == DATE_FORMAT_ABBREVIATED_DM:\\n            newFormat = messages.DATE_FORMAT_ABBREVIATED_DM\\n        elif dateFormatCombo == DATE_FORMAT_ABBREVIATED_MD:\\n            newFormat = messages.DATE_FORMAT_ABBREVIATED_MD\\n        elif dateFormatCombo == DATE_FORMAT_ABBREVIATED_DMY:\\n            newFormat = messages.DATE_FORMAT_ABBREVIATED_DMY\\n        elif dateFormatCombo == DATE_FORMAT_ABBREVIATED_MDY:\\n            newFormat = messages.DATE_FORMAT_ABBREVIATED_MDY\\n        elif dateFormatCombo == DATE_FORMAT_ABBREVIATED_YMD:\\n            newFormat = messages.DATE_FORMAT_ABBREVIATED_YMD\\n        self.prefsDict[\"presentDateFormat\"] = newFormat', 'def timeFormatChanged(self, widget):\\n        \"\"\"Signal handler for the \"changed\" signal for the timeFormat\\n           GtkComboBox widget. Set the \\'timeFormat\\' preference to the\\n           new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        timeFormatCombo = widget.get_active()\\n        if timeFormatCombo == TIME_FORMAT_LOCALE:\\n            newFormat = messages.TIME_FORMAT_LOCALE\\n        elif timeFormatCombo == TIME_FORMAT_12_HM:\\n            newFormat = messages.TIME_FORMAT_12_HM\\n        elif timeFormatCombo == TIME_FORMAT_12_HMS:\\n            newFormat = messages.TIME_FORMAT_12_HMS\\n        elif timeFormatCombo == TIME_FORMAT_24_HMS:\\n            newFormat = messages.TIME_FORMAT_24_HMS\\n        elif timeFormatCombo == TIME_FORMAT_24_HMS_WITH_WORDS:\\n            newFormat = messages.TIME_FORMAT_24_HMS_WITH_WORDS\\n        elif timeFormatCombo == TIME_FORMAT_24_HM:\\n            newFormat = messages.TIME_FORMAT_24_HM\\n        elif timeFormatCombo == TIME_FORMAT_24_HM_WITH_WORDS:\\n            newFormat  = messages.TIME_FORMAT_24_HM_WITH_WORDS\\n        self.prefsDict[\"presentTimeFormat\"] =  newFormat', 'def progressBarUpdateIntervalValueChanged(self, widget):\\n        \"\"\"Signal handler for the \"value_changed\" signal for the\\n           progressBarUpdateIntervalSpinButton GtkSpinButton widget.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        self.prefsDict[\"progressBarUpdateInterval\"] = widget.get_value_as_int()', 'def abbrevRolenamesChecked(self, widget):\\n        \"\"\"Signal handler for the \"toggled\" signal for the abbrevRolenames\\n           GtkCheckButton widget. The user has [un]checked the \\'Abbreviated\\n           Rolenames\\' checkbox. Set the \\'brailleRolenameStyle\\' preference\\n           to the new value.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        if widget.get_active():\\n            self.prefsDict[\"brailleRolenameStyle\"] = \\\\\\n                settings.BRAILLE_ROLENAME_STYLE_SHORT\\n        else:\\n            self.prefsDict[\"brailleRolenameStyle\"] = \\\\\\n                settings.BRAILLE_ROLENAME_STYLE_LONG', 'def keyModifiedToggle(self, cell, path, model, col):\\n        \"\"\"When the user changes a checkbox field (boolean field)\"\"\"\\n\\n        model[path][col] = not model[path][col]\\n        return', 'def editingCanceledKey(self, editable):\\n        \"\"\"Stops user input of a Key for a selected key binding\"\"\"\\n\\n        orca_state.capturingKeys = False\\n        self._capturedKey = []\\n        return', 'def kbKeyPressed(self, editable, event):\\n        \"\"\"Special handler for the key_pressed events when editing the\\n        keybindings.  This lets us control what gets inserted into the\\n        entry.\\n        \"\"\"\\n\\n        keyProcessed = self._processKeyCaptured(event)\\n        if not keyProcessed:\\n            return True\\n\\n        if not self._capturedKey:\\n            return False\\n\\n        keyName, modifiers, clickCount = self._capturedKey\\n        if not keyName or keyName in [\"Return\", \"Escape\"]:\\n            return False\\n\\n        isOrcaModifier = modifiers & keybindings.ORCA_MODIFIER_MASK\\n        if keyName in [\"Delete\", \"BackSpace\"] and not isOrcaModifier:\\n            editable.set_text(\"\")\\n            self._presentMessage(messages.KB_DELETED)\\n            self._capturedKey = []\\n            self.newBinding = None\\n            return True\\n\\n        self.newBinding = keybindings.KeyBinding(keyName,\\n                                                 keybindings.defaultModifierMask,\\n                                                 modifiers,\\n                                                 None,\\n                                                 clickCount)\\n        modifierNames = keybindings.getModifierNames(modifiers)\\n        clickCountString = self._clickCountToString(clickCount)\\n        newString = modifierNames + keyName + clickCountString\\n        description = self.pendingKeyBindings.get(newString)\\n\\n        if description is None:\\n            match = lambda x: x.keysymstring == keyName \\\\\\n                          and x.modifiers == modifiers \\\\\\n                          and x.click_count == clickCount \\\\\\n                          and x.handler\\n            matches = list(filter(match, self.kbindings.keyBindings))\\n            if matches:\\n                description = matches[0].handler.description\\n\\n        if description:\\n            msg = messages.KB_ALREADY_BOUND % description\\n            delay = int(1000 * settings.doubleClickTimeout)\\n            GLib.timeout_add(delay, self._presentMessage, msg)\\n        else:\\n            msg = messages.KB_CAPTURED % newString\\n            editable.set_text(newString)\\n            self._presentMessage(msg)\\n\\n        return True', 'def presentToolTipsChecked(self, widget):\\n        \"\"\"Signal handler for the \"toggled\" signal for the\\n           presentToolTipsCheckButton GtkCheckButton widget.\\n           The user has [un]checked the \\'Present ToolTips\\'\\n           checkbox. Set the \\'presentToolTips\\'\\n           preference to the new value if the user can present tooltips.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        self.prefsDict[\"presentToolTips\"] = widget.get_active()', 'def pronunciationAddButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the\\n        pronunciationAddButton GtkButton widget. The user has clicked\\n        the Add button on the Pronunciation pane. A new row will be \\n        added to the end of the pronunciation dictionary list. Both the\\n        actual and replacement strings will initially be set to an empty\\n        string. Focus will be moved to that row.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        model = self.pronunciationView.get_model()\\n        thisIter = model.append()\\n        model.set(thisIter, ACTUAL, \"\", REPLACEMENT, \"\")        \\n        path = model.get_path(thisIter)\\n        col = self.pronunciationView.get_column(0)\\n        self.pronunciationView.grab_focus()\\n        self.pronunciationView.set_cursor(path, col, True)', 'def textSelectAllButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the\\n        textSelectAllButton GtkButton widget. The user has clicked\\n        the Speak all button.  Check all the text attributes and\\n        then update the \"enabledSpokenTextAttributes\" and\\n        \"enabledBrailledTextAttributes\" preference strings.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        attributes = _settingsManager.getSetting(\\'allTextAttributes\\')\\n        self._setSpokenTextAttributes(\\n            self.getTextAttributesView, attributes, True)\\n        self._setBrailledTextAttributes(\\n            self.getTextAttributesView, attributes, True)\\n        self._updateTextDictEntry()', 'def textResetButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the\\n        textResetButton GtkButton widget. The user has clicked\\n        the Reset button. Reset all the text attributes to their\\n        initial state and then update the \"enabledSpokenTextAttributes\"\\n        and \"enabledBrailledTextAttributes\" preference strings.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        attributes = _settingsManager.getSetting(\\'allTextAttributes\\')\\n        self._setSpokenTextAttributes(\\n            self.getTextAttributesView, attributes, False)\\n        self._setBrailledTextAttributes(\\n            self.getTextAttributesView, attributes, False)\\n\\n        attributes = _settingsManager.getSetting(\\'enabledSpokenTextAttributes\\')\\n        self._setSpokenTextAttributes(\\n            self.getTextAttributesView, attributes, True)\\n\\n        attributes = \\\\\\n            _settingsManager.getSetting(\\'enabledBrailledTextAttributes\\')\\n        self._setBrailledTextAttributes(\\n            self.getTextAttributesView, attributes, True)\\n\\n        self._updateTextDictEntry()', 'def textMoveUpOneButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the\\n        textMoveUpOneButton GtkButton widget. The user has clicked\\n        the Move up one button. Move the selected rows in the text\\n        attribute view up one row in the list and then update the\\n        \"enabledSpokenTextAttributes\" and \"enabledBrailledTextAttributes\"\\n        preference strings.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        textSelection = self.getTextAttributesView.get_selection()\\n        [model, paths] = textSelection.get_selected_rows()\\n        for path in paths:\\n            thisIter = model.get_iter(path)\\n            indices = path.get_indices()\\n            if indices[0]:\\n                otherIter = model.iter_nth_child(None, indices[0]-1)\\n                model.swap(thisIter, otherIter)\\n        self._updateTextDictEntry()', 'def textMoveToBottomButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the\\n        textMoveToBottomButton GtkButton widget. The user has clicked\\n        the Move to bottom button. Move the selected rows in the text\\n        attribute view to the bottom of the list and then update the\\n        \"enabledSpokenTextAttributes\" and \"enabledBrailledTextAttributes\"\\n        preference strings.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        textSelection = self.getTextAttributesView.get_selection()\\n        [model, paths] = textSelection.get_selected_rows()\\n        for path in paths:\\n            thisIter = model.get_iter(path)\\n            model.move_before(thisIter, None)\\n        self._updateTextDictEntry()', 'def restoreSettings(self):\\n        \"\"\"Restore the settings we saved away when opening the preferences\\n           dialog.\"\"\"\\n        # Restore the default rate/pitch/gain,\\n        # in case the user played with the sliders.\\n        #\\n        voices = _settingsManager.getSetting(\\'voices\\')\\n        defaultVoice = voices[settings.DEFAULT_VOICE]\\n        defaultVoice[acss.ACSS.GAIN] = self.savedGain\\n        defaultVoice[acss.ACSS.AVERAGE_PITCH] = self.savedPitch\\n        defaultVoice[acss.ACSS.RATE] =  self.savedRate', 'def applyButtonClicked(self, widget):\\n        \"\"\"Signal handler for the \"clicked\" signal for the applyButton\\n           GtkButton widget. The user has clicked the Apply button.\\n           Write out the users preferences. If GNOME accessibility hadn\\'t\\n           previously been enabled, warn the user that they will need to\\n           log out. Shut down any active speech servers that were started.\\n           Reload the users preferences to get the new speech, braille and\\n           key echo value to take effect. Do not dismiss the configuration\\n           window.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n        self.saveBasicSettings()\\n\\n        activeProfile = self.getComboBoxList(self.profilesCombo)\\n        startingProfile = self.getComboBoxList(self.startingProfileCombo)\\n\\n        self.prefsDict[\\'profile\\'] = activeProfile\\n        self.prefsDict[\\'activeProfile\\'] = activeProfile\\n        self.prefsDict[\\'startingProfile\\'] = startingProfile\\n        _settingsManager.setStartingProfile(startingProfile)\\n\\n        self.writeUserPreferences()\\n\\n        orca.loadUserSettings(self.script)\\n\\n        braille.checkBrailleSetting()\\n\\n        self._initSpeechState()\\n\\n        self._populateKeyBindings()\\n\\n        self.__initProfileCombo()', 'def okButtonClicked(self, widget=None):\\n        \"\"\"Signal handler for the \"clicked\" signal for the okButton\\n           GtkButton widget. The user has clicked the OK button.\\n           Write out the users preferences. If GNOME accessibility hadn\\'t\\n           previously been enabled, warn the user that they will need to\\n           log out. Shut down any active speech servers that were started.\\n           Reload the users preferences to get the new speech, braille and\\n           key echo value to take effect. Hide the configuration window.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        self.applyButtonClicked(widget)\\n        self._cleanupSpeechServers()\\n        self.get_widget(\"orcaSetupWindow\").destroy()', 'def windowDestroyed(self, widget):\\n        \"\"\"Signal handler for the \"destroyed\" signal for the orcaSetupWindow\\n           GtkWindow widget. Reset orca_state.orcaOS to None, so that the \\n           GUI can be rebuilt from the GtkBuilder file the next time the user\\n           wants to display the configuration GUI.\\n\\n        Arguments:\\n        - widget: the component that generated the signal.\\n        \"\"\"\\n\\n        self.keyBindView.set_model(None)\\n        self.getTextAttributesView.set_model(None)\\n        self.pronunciationView.set_model(None)\\n        self.keyBindView.set_headers_visible(False)\\n        self.getTextAttributesView.set_headers_visible(False)\\n        self.pronunciationView.set_headers_visible(False)\\n        self.keyBindView.hide()\\n        self.getTextAttributesView.hide()\\n        self.pronunciationView.hide()\\n        orca_state.orcaOS = None', 'def saveProfile(self, profileToSaveLabel):\\n        \"\"\"Creates a new profile based on the name profileToSaveLabel and\\n        updates the Preferences dialog combo boxes accordingly.\"\"\"\\n\\n        if not profileToSaveLabel:\\n            return\\n        profileToSave = profileToSaveLabel.replace(\\' \\', \\'_\\').lower()\\n        profile = [profileToSaveLabel, profileToSave]\\n\\n        def saveActiveProfile(newProfile = True):\\n            if newProfile:\\n                activeProfileIter = self.profilesComboModel.append(profile)\\n                self.profilesCombo.set_active_iter(activeProfileIter)\\n\\n            self.prefsDict[\\'profile\\'] = profile\\n            self.prefsDict[\\'activeProfile\\'] = profile\\n            self.saveBasicSettings()\\n            self.writeUserPreferences()\\n\\n        availableProfiles = [p[1] for p in self.__getAvailableProfiles()]\\n        if isinstance(profileToSave, str) \\\\\\n                and profileToSave != \\'\\' \\\\\\n                and not profileToSave in availableProfiles \\\\\\n                and profileToSave != self._defaultProfile[1]:\\n            saveActiveProfile()\\n        else:\\n            if profileToSave is not None:\\n                message = guilabels.PROFILE_CONFLICT_MESSAGE % \\\\\\n                    (\"<b>%s</b>\" % GLib.markup_escape_text(profileToSaveLabel))\\n                dialog = Gtk.MessageDialog(None,\\n                        Gtk.DialogFlags.MODAL,\\n                        type=Gtk.MessageType.INFO,\\n                        buttons=Gtk.ButtonsType.YES_NO)\\n                dialog.set_markup(\"<b>%s</b>\" % guilabels.PROFILE_CONFLICT_LABEL)\\n                dialog.format_secondary_markup(message)\\n                dialog.set_title(guilabels.PROFILE_CONFLICT_TITLE)\\n                response = dialog.run()\\n                if response == Gtk.ResponseType.YES:\\n                    dialog.destroy()\\n                    saveActiveProfile(False)\\n                else:\\n                    dialog.destroy()', 'def removeProfileButtonClicked(self, widget):\\n        \"\"\"Remove profile button clicked handler\\n\\n        If we removed the last profile, a default one will automatically get\\n        added back by the settings manager.\\n        \"\"\"\\n\\n        oldProfile = self.getComboBoxList(self.profilesCombo)\\n\\n        message = guilabels.PROFILE_REMOVE_MESSAGE % \\\\\\n            (\"<b>%s</b>\" % GLib.markup_escape_text(oldProfile[0]))\\n        dialog = Gtk.MessageDialog(self.window, Gtk.DialogFlags.MODAL,\\n                                   type=Gtk.MessageType.INFO,\\n                                   buttons=Gtk.ButtonsType.YES_NO)\\n        dialog.set_markup(\"<b>%s</b>\" % guilabels.PROFILE_REMOVE_LABEL)\\n        dialog.format_secondary_markup(message)\\n        if dialog.run() == Gtk.ResponseType.YES:\\n            # If we remove the currently used starting profile, fallback on\\n            # the first listed profile, or the default one if there\\'s\\n            # nothing better\\n            newStartingProfile = self.prefsDict.get(\\'startingProfile\\')\\n            if not newStartingProfile or newStartingProfile == oldProfile:\\n                newStartingProfile = self._defaultProfile\\n                for row in self.profilesComboModel:\\n                    rowProfile = row[:]\\n                    if rowProfile != oldProfile:\\n                        newStartingProfile = rowProfile\\n                        break\\n            # Update the current profile to the active profile unless we\\'re\\n            # removing that one, in which case we use the new starting\\n            # profile\\n            newProfile = self.prefsDict.get(\\'activeProfile\\')\\n            if not newProfile or newProfile == oldProfile:\\n                newProfile = newStartingProfile\\n\\n            _settingsManager.removeProfile(oldProfile[1])\\n            self.loadProfile(newProfile)\\n\\n            # Make sure nothing is referencing the removed profile anymore\\n            startingProfile = self.prefsDict.get(\\'startingProfile\\')\\n            if not startingProfile or startingProfile == oldProfile:\\n                self.prefsDict[\\'startingProfile\\'] = newStartingProfile\\n                _settingsManager.setStartingProfile(newStartingProfile)\\n                self.writeUserPreferences()\\n\\n        dialog.destroy()', 'def loadSelectedProfile(self):\\n        \"\"\"Load selected profile\"\"\"\\n\\n        activeProfile = self.getComboBoxList(self.profilesCombo)\\n        self.loadProfile(activeProfile)']}, {'features': [], 'snippets': ['def __init__(self, monitor, parent=None):\\n        \"\"\"\\n        Observe the given ``monitor`` (a :class:`~pyudev.Monitor`):\\n\\n        ``parent`` is the parent :class:`~PyQt4.QtCore.QObject` of this\\n        object.  It is passed unchanged to the inherited constructor of\\n        :class:`~PyQt4.QtCore.QObject`.\\n        \"\"\"\\n        QObject.__init__(self, parent)\\n        self._setup_notifier(monitor, QSocketNotifier)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        # Load available backends\\n        for entry in pkg_resources.iter_entry_points(\"gosa.object.backend\"):\\n            clazz = entry.load()\\n            ObjectBackendRegistry.backends[clazz.__name__] = clazz()', 'def uuid2dn(self, backend, uuid, from_db_only=False):\\n        dn = ObjectBackendRegistry.backends[backend].uuid2dn(uuid)\\n        if dn is None and from_db_only is True:\\n            # fallback to db\\n            if self.__index is None:\\n                self.__index = PluginRegistry.getInstance(\"ObjectIndex\")\\n            res = self.__index.search({\\'uuid\\': uuid}, {\\'dn\\': 1})\\n            if len(res) == 1:\\n                dn = res[0][\\'dn\\']\\n        return dn', 'def getInstance():\\n        if not ObjectBackendRegistry.instance:\\n            ObjectBackendRegistry.instance = ObjectBackendRegistry()\\n\\n        return ObjectBackendRegistry.instance']}, {'features': [], 'snippets': ['def linear_reaction_coefficients(model, reactions=None):\\n    \"\"\"Coefficient for the reactions in a linear objective.\\n\\n    Parameters\\n    ----------\\n    model : cobra model\\n        the model object that defined the objective\\n    reactions : list\\n        an optional list for the reactions to get the coefficients for. All\\n        reactions if left missing.\\n\\n    Returns\\n    -------\\n    dict\\n        A dictionary where the key is the reaction object and the value is\\n        the corresponding coefficient. Empty dictionary if there are no\\n        linear terms in the objective.\\n    \"\"\"\\n    linear_coefficients = {}\\n    reactions = model.reactions if not reactions else reactions\\n    try:\\n        objective_expression = model.solver.objective.expression\\n        coefficients = objective_expression.as_coefficients_dict()\\n    except AttributeError:\\n        return linear_coefficients\\n    for rxn in reactions:\\n        forward_coefficient = coefficients.get(rxn.forward_variable, 0)\\n        reverse_coefficient = coefficients.get(rxn.reverse_variable, 0)\\n        if forward_coefficient != 0:\\n            if forward_coefficient == -reverse_coefficient:\\n                linear_coefficients[rxn] = float(forward_coefficient)\\n    return linear_coefficients', 'def set_objective(model, value, additive=False):\\n    \"\"\"Set the model objective.\\n\\n    Parameters\\n    ----------\\n    model : cobra model\\n       The model to set the objective for\\n    value : model.problem.Objective,\\n            e.g. optlang.glpk_interface.Objective, sympy.Basic or dict\\n\\n        If the model objective is linear, the value can be a new Objective\\n        object or a dictionary with linear coefficients where each key is a\\n        reaction and the element the new coefficient (float).\\n\\n        If the objective is not linear and `additive` is true, only values\\n        of class Objective.\\n\\n    additive : bool\\n        If true, add the terms to the current objective, otherwise start with\\n        an empty objective.\\n    \"\"\"\\n    interface = model.problem\\n    reverse_value = model.solver.objective.expression\\n    reverse_value = interface.Objective(\\n        reverse_value, direction=model.solver.objective.direction,\\n        sloppy=True)\\n\\n    if isinstance(value, dict):\\n        if not model.objective.is_Linear:\\n            raise ValueError(\\'can only update non-linear objectives \\'\\n                             \\'additively using object of class \\'\\n                             \\'model.problem.Objective, not %s\\' %\\n                             type(value))\\n\\n        if not additive:\\n            model.solver.objective = interface.Objective(\\n                Zero, direction=model.solver.objective.direction)\\n        for reaction, coef in value.items():\\n            model.solver.objective.set_linear_coefficients(\\n                {reaction.forward_variable: coef,\\n                 reaction.reverse_variable: -coef})\\n\\n    elif isinstance(value, (Basic, optlang.interface.Objective)):\\n        if isinstance(value, Basic):\\n            value = interface.Objective(\\n                value, direction=model.solver.objective.direction,\\n                sloppy=False)\\n        # Check whether expression only uses variables from current model\\n        # clone the objective if not, faster than cloning without checking\\n        if not _valid_atoms(model, value.expression):\\n            value = interface.Objective.clone(value, model=model.solver)\\n\\n        if not additive:\\n            model.solver.objective = value\\n        else:\\n            model.solver.objective += value.expression\\n    else:\\n        raise TypeError(\\n            \\'%r is not a valid objective for %r.\\' % (value, model.solver))\\n\\n    context = get_context(model)\\n    if context:\\n        def reset():\\n            model.solver.objective = reverse_value\\n            model.solver.objective.direction = reverse_value.direction\\n\\n        context(reset)', 'def get_solver_name(mip=False, qp=False):\\n    \"\"\"Select a solver for a given optimization problem.\\n\\n    Parameters\\n    ----------\\n    mip : bool\\n        Does the solver require mixed integer linear programming capabilities?\\n    qp : bool\\n        Does the solver require quadratic programming capabilities?\\n\\n    Returns\\n    -------\\n    string\\n        The name of feasible solver.\\n\\n    Raises\\n    ------\\n    SolverNotFound\\n        If no suitable solver could be found.\\n    \"\"\"\\n    if len(solvers) == 0:\\n        raise SolverNotFound(\"no solvers installed\")\\n    # Those lists need to be updated as optlang implements more solvers\\n    mip_order = [\"gurobi\", \"cplex\", \"glpk\"]\\n    lp_order = [\"glpk\", \"cplex\", \"gurobi\"]\\n    qp_order = [\"cplex\"]\\n\\n    if mip is False and qp is False:\\n        for solver_name in lp_order:\\n            if solver_name in solvers:\\n                return solver_name\\n        # none of them are in the list order - so return the first one\\n        return list(solvers)[0]\\n    elif qp:  # mip does not yet matter for this determination\\n        for solver_name in qp_order:\\n            if solver_name in solvers:\\n                return solver_name\\n        raise SolverNotFound(\"no qp-capable solver found\")\\n    else:\\n        for solver_name in mip_order:\\n            if solver_name in solvers:\\n                return solver_name\\n    raise SolverNotFound(\"no mip-capable solver found\")', 'def add_cons_vars_to_problem(model, what, **kwargs):\\n    \"\"\"Add variables and constraints to a Model\\'s solver object.\\n\\n    Useful for variables and constraints that can not be expressed with\\n    reactions and lower/upper bounds. Will integrate with the Model\\'s context\\n    manager in order to revert changes upon leaving the context.\\n\\n    Parameters\\n    ----------\\n    model : a cobra model\\n       The model to which to add the variables and constraints.\\n    what : list or tuple of optlang variables or constraints.\\n       The variables or constraints to add to the model. Must be of class\\n       `model.problem.Variable` or\\n       `model.problem.Constraint`.\\n    **kwargs : keyword arguments\\n        passed to solver.add()\\n    \"\"\"\\n    context = get_context(model)\\n\\n    model.solver.add(what, **kwargs)\\n    if context:\\n        context(partial(model.solver.remove, what))', 'def add_absolute_expression(model, expression, name=\"abs_var\", ub=None,\\n                            difference=0, add=True):\\n    \"\"\"Add the absolute value of an expression to the model.\\n\\n    Also defines a variable for the absolute value that can be used in other\\n    objectives or constraints.\\n\\n    Parameters\\n    ----------\\n    model : a cobra model\\n       The model to which to add the absolute expression.\\n    expression : A sympy expression\\n       Must be a valid expression within the Model\\'s solver object. The\\n       absolute value is applied automatically on the expression.\\n    name : string\\n       The name of the newly created variable.\\n    ub : positive float\\n       The upper bound for the variable.\\n    difference : positive float\\n        The difference between the expression and the variable.\\n    add : bool\\n        Whether to add the variable to the model at once.\\n\\n    Returns\\n    -------\\n    namedtuple\\n        A named tuple with variable and two constraints (upper_constraint,\\n        lower_constraint) describing the new variable and the constraints\\n        that assign the absolute value of the expression to it.\\n    \"\"\"\\n    Components = namedtuple(\\'Components\\', [\\'variable\\', \\'upper_constraint\\',\\n                                           \\'lower_constraint\\'])\\n    variable = model.problem.Variable(name, lb=0, ub=ub)\\n    # The following constraints enforce variable > expression and\\n    # variable > -expression\\n    upper_constraint = model.problem.Constraint(expression - variable,\\n                                                ub=difference,\\n                                                name=\"abs_pos_\" + name),\\n    lower_constraint = model.problem.Constraint(expression + variable,\\n                                                lb=difference,\\n                                                name=\"abs_neg_\" + name)\\n    to_add = Components(variable, upper_constraint, lower_constraint)\\n    if add:\\n        add_cons_vars_to_problem(model, to_add)\\n    return to_add', 'def check_solver_status(status, raise_error=False):\\n    \"\"\"Perform standard checks on a solver\\'s status.\"\"\"\\n    if status == optlang.interface.OPTIMAL:\\n        return\\n    elif status == optlang.interface.INFEASIBLE and not raise_error:\\n        warn(\"solver status is \\'{}\\'\".format(status), UserWarning)\\n    elif status is None:\\n        raise RuntimeError(\\n            \"model was not optimized yet or solver context switched\")\\n    else:\\n        raise OptimizationError(\"solver status is \\'{}\\'\".format(status))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def main():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'-i\\', \\'--input\\', type=str, action=\\'store\\', dest=\\'input\\', default=None, help=\"Input file\")\\n    args = parser.parse_args()\\n    stats = dict()\\n    if args.input is None:\\n        print \"Error: No input file\"\\n    with open(args.input) as in_file:\\n        for line in in_file.readlines():\\n            time = int(line.split()[0])\\n            tx_bytes = int(line.split()[1])\\n            stats[time] = tx_bytes\\n    stats = sorted(stats.items())\\n    start_time = stats[0][0]\\n    prev_tx = stats[0][1]\\n    no_traffic_flag = True\\n    for time, tx_bytes in stats:\\n        if no_traffic_flag:\\n            if tx_bytes > (prev_tx+100000):\\n                no_traffic_flag = False\\n                start_time, prev_tx = time, tx_bytes\\n        else:\\n            print (time-start_time), (tx_bytes-prev_tx)\\n            prev_tx = tx_bytes']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_short_name(self):\\n        return self.name']}, {'features': [], 'snippets': ['def get_message(self):\\n        xpath = \\'//div[@class=\"bienvenueMdp\"]/following-sibling::div\\'\\n        return \\'%s%s\\' % (CleanText(xpath + \\'/strong\\')(self.doc), CleanText(xpath, children=False)(self.doc))', 'def logged(self):\\n        if len(self.doc.xpath(\\'//b[text()=\"Session interrompue\"]\\')) > 0:\\n            return False\\n        return True', 'def filter(self, label):\\n                    for pattern, actype in AccountsPage.TYPES.items():\\n                        if label.startswith(pattern):\\n                            return actype\\n                    return Account.TYPE_UNKNOWN', \"def validate(self, obj):\\n                if obj.id is None:\\n                    obj.id = obj.label.replace(' ', '')\\n                return True\", 'def has_error(self):\\n        return CleanText(\\'//span[@id=\"id_error_msg\"]\\')(self.doc)', 'def obj_id(self):\\n                area_id = Regexp(CleanText(\\'(./preceding-sibling::tr[@class=\"LnMnTiers\"][1])//span[@class=\"CelMnTiersT1\"]\\'),\\n                            r\\'\\\\((\\\\d+)\\\\)\\', default=\\'\\')(self)\\n                acc_id = Regexp(CleanText(\\'./td[1]\\'), r\\'(\\\\d+)\\\\s*(\\\\d+)\\', r\\'\\\\1\\\\2\\')(self)\\n                if area_id:\\n                    return \\'%s.%s\\' % (area_id, acc_id)\\n                return acc_id', \"def go_account(self, form, idx, idroot):\\n        form = self.get_form(name=form)\\n        form['indiceCompte'] = idx\\n        form['idRacine'] = idroot\\n        form.submit()\", 'def obj_code(self):\\n                if Field(\\'label\\')(self) == \"LIQUIDITES\":\\n                    return \\'XX-liquidity\\'\\n                code = CleanText(TableCell(\\'code\\'))(self)\\n                return code if is_isin_valid(code) else NotAvailable', \"def condition(self):\\n        return len(self.el) >= 5 and not self.el.get('id', '').startswith('libelleLong')\", 'def get_date_range_list(self):\\n        return [d for d in self.doc.xpath(\\'//select[@name=\"date\"]/option/@value\\') if d]', 'def next_page(self):\\n            pager = self.page.doc.xpath(\\'//div[@class=\"pager\"]\\')\\n            if pager:  # more than one page if only enough transactions\\n                assert len(pager) == 1\\n\\n                next_links = pager[0].xpath(\\'./span/following-sibling::a[@class=\"page\"]\\')\\n                if next_links:\\n                    url_next_page = Link(\\'.\\')(next_links[0])\\n                    url_next_page = urljoin(self.page.url, url_next_page)\\n                    return self.page.browser.build_request(url_next_page)', \"def date(selector):\\n                return DateGuesser(Regexp(CleanText(selector), r'\\\\w+ (\\\\d{2}/\\\\d{2})'), Env('date_guesser')) | Transaction.Date(selector)\", \"def on_load(self):\\n        if 'Authentication' in self.response.headers:\\n            self.browser.token = self.response.headers['Authentication'].split(' ')[-1]\", \"def get_sso_url(self):\\n        return self.doc['urlSSO']\"]}, {'features': [], 'snippets': ['def __init__(self, *args, **kwargs):\\n        Signature.__init__(self, *args, **kwargs)']}, {'features': [], 'snippets': ['def __init__(cls, name, bases, dct):\\n        super(MetaTest, cls).__init__(name, bases, dct)\\n\\n        def gen_codes():\\n            \"\"\"Generate the 702 possible input codes\"\"\"\\n            # First, the 1-character codes\\n            for c in string.ascii_lowercase:\\n                yield c\\n\\n            # Next, the 2-characters-with-wildcard codes\\n            for t in itertools.product(string.ascii_lowercase, repeat=2):\\n                yield \\'*\\'.join(t)\\n\\n        def tester(code):\\n            def func(cls):\\n                return cls.run_test(code)\\n            return func\\n\\n        # Generate the test_* methods\\n        for code in gen_codes():\\n            setattr(cls, \"test_%s\" % code.replace(\"*\", \"\"), tester(code))', 'def __init__(self, name):\\n        super().__init__(name)\\n\\n        self.cli_cmd = [\"/usr/bin/libcangjie_cli\"] + self.cli_options\\n\\n        self.language = (cangjie.filters.BIG5 | cangjie.filters.HKSCS |\\n                         cangjie.filters.PUNCTUATION |\\n                         cangjie.filters.CHINESE |\\n                         cangjie.filters.ZHUYIN | cangjie.filters.KANJI |\\n                         cangjie.filters.KATAKANA |\\n                         cangjie.filters.HIRAGANA |\\n                         cangjie.filters.SYMBOLS)', 'def tearDown(self):\\n        del self.cj']}, {'features': [], 'snippets': ['def save_utf8_file(fn, lines):\\r\\n    \"\"\"Save string lines into an UTF8 text files.\\r\\n    \"\"\"\\r\\n    with open(fn, \"w\") as out_file:\\r\\n        out_file.write(\"\\\\n\".join(lines).encode(\"utf-8\"))', 'def main_basename(path):\\r\\n    r\"\"\"Return a main name of a basename of a given file path.', 'def is_numeric(str):\\r\\n    try:\\r\\n        _offset = int(eval(str))\\r\\n    except:\\r\\n        return False\\r\\n    return True', 'def replace_chars(text, replaced_pairs=\\'\\', deleted_chars=\\'\\'):\\r\\n    \"\"\"Return a char replaced text.', 'def camel_case(string):\\r\\n    \"\"\"Return camel case string from a space-separated string.', 'def replace_punctuations(text):\\r\\n    \"\"\"Replace punctuation characters with abbreviations for a string.\\r\\n    \"\"\"\\r\\n    punctuations = [\\r\\n        (\\'?\\', \\'Q\\'),   # Q:  question mark\\r\\n        (\\'.\\', \\'P\\'),   # P:  period; full stop\\r\\n        (\\'!\\', \\'E\\'),   # E:  exclamation mark\\r\\n        (\"\\'\", \\'SQ\\'),  # SQ: single quotation mark; single quote\\r\\n        (\\'\"\\', \\'DQ\\'),  # DQ: double quotation mark; double quotes\\r\\n        (\\'(\\', \\'LP\\'),  # LP: left parenthese\\r\\n        (\\')\\', \\'RP\\'),  # RP: right parenthese\\r\\n        (\\':\\', \\'Cn\\'),  # Cn: colon\\r\\n        (\\',\\', \\'Ca\\'),  # Ca: comma\\r\\n        (\\';\\', \\'S\\'),   # S:  semicolon\\r\\n    ]\\r\\n    deleted = \\'+-*/^=%$#@|\\\\\\\\<>{}[]\\'\\r\\n    return replace_chars(text, punctuations, deleted)', 'def remain_alnum(text):\\r\\n    \"\"\"Remain digits and English letters of a string.\\r\\n    \"\"\"\\r\\n    return \\'\\'.join(c for c in text if c.isalnum()\\r\\n                                   and ord(\\' \\') <= ord(c) <= ord(\\'z\\'))', 'def c_identifier(text):\\r\\n    \"\"\"Convert input text into an legal identifier in C.', 'def wrap_header_guard(lines, h_fn):\\r\\n    \"\"\"Wrap a C header guard for a given line list.\\r\\n    \"\"\"\\r\\n    def underscore(txt):\\r\\n        \"\"\"Return an under_scores text from a CamelCase text.', 'def prefix_info(lines, software, version, author, comment_mark=\\'//\\'):\\r\\n    \"\"\"Prefix information to the given lines with given comment-mark.\\r\\n    \"\"\"\\r\\n    prefix = [\\'%s Generated by the %s v%s\\' % (comment_mark,\\r\\n              software, version)]\\r\\n    prefix += [\\'%s    !author: %s\\' % (comment_mark, author)]\\r\\n    prefix += [\\'%s    !trail: %s %s\\' % (comment_mark,\\r\\n               os.path.basename(sys.argv[0]), \\' \\'.join(sys.argv[1:]))]\\r\\n    return prefix + lines']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__( self, parent, log ):\\n        wx.Panel.__init__(self, parent, wx.ID_ANY)\\n        self.log = log\\n        self.callback = None\\n\\n        self.panel = panel = wx.Panel(self, wx.ID_ANY)\\n        topsizer = wx.BoxSizer(wx.VERTICAL)\\n\\n        # Difference between using PropertyGridManager vs PropertyGrid is that\\n        # the manager supports multiple pages and a description box.\\n        self.pg = pg = wxpg.PropertyGrid(panel,\\n                        style=wxpg.PG_SPLITTER_AUTO_CENTER |\\n                              wxpg.PG_AUTO_SORT |\\n                              wxpg.PG_TOOLBAR)\\n\\n        # Show help as tooltips\\n        pg.SetExtraStyle(wxpg.PG_EX_HELP_AS_TOOLTIPS)\\n\\n        pg.Bind( wxpg.EVT_PG_CHANGED, self.OnPropGridChange )\\n        pg.Bind( wxpg.EVT_PG_PAGE_CHANGED, self.OnPropGridPageChange )\\n        pg.Bind( wxpg.EVT_PG_SELECTED, self.OnPropGridSelect )\\n        pg.Bind( wxpg.EVT_PG_RIGHT_CLICK, self.OnPropGridRightClick )\\n\\n        ##pg.AddPage( \"Page 1 - Testing All\" )\\n        # store the property grid for future reference\\n        self.pg = pg', 'def load_object(self, obj, callback=None):\\n        pg = self.pg                    # get the property grid reference\\n        self.callback = callback        # store the update method', 'def edit(self, name=\"\"):\\n        \"Programatically select a (default) property to start editing it\"\\n        # for more info see DoSelectAndEdit in propgrid.cpp\\n        for name in (name, \"label\", \"value\", \"text\", \"title\", \"filename\", \\n                           \"name\"):\\n            prop = self.pg.GetPropertyByName(name)\\n            if prop is not None:\\n                break\\n        self.Parent.SetFocus()\\n        self.Parent.Raise()\\n        self.pg.SetFocus()\\n        # give time to the ui to show the prop grid and set focus:\\n        wx.CallLater(250, self.select, prop.GetName())', 'def OnPropGridChange(self, event):\\n        p = event.GetProperty()\\n        if DEBUG: print \"change!\", p\\n        if p:\\n            name = p.GetName()\\n            spec = p.GetPyClientData()\\n            if spec and \\'enum\\' in spec.type:\\n                value = p.GetValueAsString()\\n            else:\\n                value = p.GetValue()\\n            #self.log.write(u\\'%s changed to \"%s\"\\\\n\\' % (p,p.GetValueAsString()))\\n            # if it a property child (parent.child), extract its name\\n            if \".\" in name:\\n                name = name[name.rindex(\".\") + 1:]\\n            if spec and not name in self.groups:                \\n                if name == \\'font\\':  # TODO: detect property type\\n                    # create a gui font from the wx.Font\\n                    font = Font()\\n                    font.set_wx_font(value)\\n                    value = font\\n                # expressions must be evaluated to store the python object\\n                if spec.type == \"expr\":\\n                    value = eval(value)\\n                # re-create the wx_object with the new property value\\n                # (this is required at least to apply new styles and init specs)\\n                if DEBUG: print \"changed\", self.obj.name\\n                kwargs = {str(name): value}\\n                wx.CallAfter(self.obj.rebuild,  **kwargs)\\n                if name == \\'name\\':\\n                    wx.CallAfter(self.callback, **dict(name=self.obj.name))', 'def OnDeleteProperty(self, event):\\n        p = self.pg.GetSelectedProperty()\\n        if p:\\n            self.pg.DeleteProperty(p)\\n        else:\\n            wx.MessageBox(\"First select a property to delete\")', \"def OnPropGridRightClick(self, event):\\n        p = event.GetProperty()\\n        if p:\\n            self.log.write(u'%s right clicked\\\\n' % (event.GetProperty().GetName()))\\n        else:\\n            self.log.write(u'Nothing right clicked\\\\n')\\n        #self.obj.get_parent().Refresh()\"]}, {'features': [], 'snippets': ['def main():\\n\\tif len(sys.argv) < 2:\\n\\t\\tprint(\"Error: Must specify an argument of either \\'tokens\\' or \\'emitters\\'!\", file=sys.stderr)\\n\\t\\treturn 1\\n\\n\\tlist_scopes = set()\\n\\n\\twith list_scope_path.open(\\'r\\') as f:\\n\\t\\tfor line in f:\\n\\t\\t\\tline = line.strip()\\n\\t\\t\\tif line.startswith(\\'#\\') or len(line) == 0:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tlist_scopes.add(line)\\n\\n\\tmax_kw_len = max( len(kw) for kw in list_scopes )\\n\\n\\tif sys.argv[1] == \\'tokens\\':\\n\\t\\tt_id = (1 << (keyword_bit - 1)) | (1 << (list_scope_bit-1))\\n\\n\\t\\tfor t in sorted(list_scopes):\\n\\t\\t\\tprint(\\'  {:<{width}} = 0x{:4X};\\'.format(t.upper(), t_id, width=max_kw_len))\\n\\t\\t\\tt_id += 1\\n\\n\\telif sys.argv[1] == \\'emitters\\':\\n\\t\\tfor t in sorted(list_scopes):\\n\\t\\t\\tprint(\\'  {:<{width}} => T_{}(Lexeme);\\'.format(\\'\"\\' + t + \\'\"\\', t.upper(), width = max_kw_len + 2))\\n\\telse:\\n\\t\\tprint(\"Error: Must specify an argument of either \\'tokens\\' or \\'emitters\\'!\", file=sys.stderr)\\n\\t\\treturn 1\\n\\n\\treturn 0']}, {'features': [], 'snippets': ['def is_quantile(stat):\\n    return re.match(\"q[0-9][0-9]?$\", stat)', 'def get_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"data\", \\n                        type=argparse.FileType(\"r\"),\\n                        help=\"data file to be summarized.\"\\n                             \"Should have columns seed, \"\\\\\\n                             \"set, and metrics columns.\")\\n    parser.add_argument(\"parameterizations\",\\n                        type=argparse.FileType(\"r\"),\\n                        help=\"file containing parameter\"\\\\\\n                             \"izations.  Number of param\"\\\\\\n                             \"eterizations should be the \"\\\\\\n                             \"same as number of rows per \"\\\\\\n                             \"seed in the data file.\"\\n                       )\\n    parser.add_argument(\"parameters\",\\n                        type=argparse.FileType(\"r\"),\\n                        help=\"file describing parameters. \"\\\\\\n                             \"Should have as many rows as \"\\\\\\n                             \"parameterizations file has \"\\\\\\n                             \"columns.\"\\n                       )\\n    stats = [\"mean\", \"variance\", \"q10\", \"q50\", \"q90\"]\\n    parser.add_argument(\"-s\", \"--stats\", nargs=\"+\",\\n                        default = stats, type = is_stat,\\n                        help=\"statistics to compute\")\\n    parser.add_argument(\"-g\", \"--group\", nargs=\"+\",\\n                        help=\"parameters by which to \"\\\\\\n                             \"group.  Names should be \"\\\\\\n                             \"found in the parameters \"\\\\\\n                             \"file.  \"\\n                       )\\n    parser.add_argument(\"-d\", \"--deltas\",\\n                        help=\"If group is specified, \"\\\\\\n                             \"deltas may be used to impose \"\\\\\\n                             \"grid boxes on the summary \"\\\\\\n                             \"rather than using point \"\\\\\\n                             \"values.\",\\n                         nargs=\"+\", type = float\\n                       )\\n    parser.add_argument(\"-o\", \"--output-directory\",\\n                        default=\"/gpfs/scratch/mjw5407/\"\\n                                \"task1/stats/\"\\n                       )\\n    return parser.parse_args()', 'def analyze(data, stats, group=None, deltas=None):\\n    results = []\\n    if group is None:\\n        group = [\"Set\"]\\n    togroupby = copy.copy(group)\\n    ii = 0\\n    if deltas is None:\\n        togroupby = group\\n    else:\\n        while ii < len(group) and ii < len(deltas):\\n            colname = \"grid_{0}\".format(group[ii])\\n            gridnumbers = numpy.floor(data[group[ii]].apply(\\n                            lambda val: val / deltas[ii]))\\n            data[colname] = gridnumbers.apply(\\n                            lambda val: val * deltas[ii])\\n            togroupby[ii] = colname\\n            ii += 1', 'def write_result(infn, result, outputdir):\\n    fn = \"_\".join([result[0], os.path.basename(infn)])\\n    fn = re.sub(\"\\\\.hv$\", \"\", fn)\\n    fn = os.path.join(outputdir, fn)\\n    print \"writing {0}\".format(fn)\\n    result[1].to_csv(fn, sep=\" \", index=True)']}, {'features': [], 'snippets': [\"def setUp(self):\\n        # Create coordinate cf variables and pyke engine.\\n        points = np.arange(6).reshape(2, 3)\\n        self.cf_coord_var = mock.Mock(\\n            dimensions=('foo', 'bar'),\\n            cf_name='wibble',\\n            standard_name=None,\\n            long_name='wibble',\\n            units='m',\\n            shape=points.shape,\\n            dtype=points.dtype,\\n            __getitem__=lambda self, key: points[key])\\n\\n        self.engine = mock.Mock(\\n            cube=mock.Mock(),\\n            cf_var=mock.Mock(dimensions=('foo', 'bar')),\\n            filename='DUMMY',\\n            provides=dict(coordinates=[]))\\n\\n        # Create patch for deferred loading that prevents attempted\\n        # file access. This assumes that self.cf_bounds_var is\\n        # defined in the test case.\\n        def patched__getitem__(proxy_self, keys):\\n            variable = None\\n            for var in (self.cf_coord_var, self.cf_bounds_var):\\n                if proxy_self.variable_name == var.cf_name:\\n                    return var[keys]\\n            raise RuntimeError()\\n\\n        self.deferred_load_patch = mock.patch(\\n            'iris.fileformats.netcdf.NetCDFDataProxy.__getitem__',\\n            new=patched__getitem__)\", \"def test_fastest_varying_vertex_dim(self):\\n        bounds = np.arange(24).reshape(2, 3, 4)\\n        self.cf_bounds_var = mock.Mock(\\n            dimensions=('foo', 'bar', 'nv'),\\n            cf_name='wibble_bnds',\\n            shape=bounds.shape,\\n            dtype=bounds.dtype,\\n            __getitem__=lambda self, key: bounds[key])\\n\\n        expected_coord = AuxCoord(\\n            self.cf_coord_var[:],\\n            long_name=self.cf_coord_var.long_name,\\n            var_name=self.cf_coord_var.cf_name,\\n            units=self.cf_coord_var.units,\\n            bounds=bounds)\\n\\n        get_cf_bounds_var_patch = mock.patch(\\n            'iris.fileformats._pyke_rules.compiled_krb.'\\n            'fc_rules_cf_fc.get_cf_bounds_var',\\n            return_value=self.cf_bounds_var)\\n\\n        # Asserts must lie within context manager because of deferred loading.\\n        with self.deferred_load_patch, get_cf_bounds_var_patch:\\n            build_auxiliary_coordinate(self.engine, self.cf_coord_var)\\n\\n            # Test that expected coord is built and added to cube.\\n            self.engine.cube.add_aux_coord.assert_called_with(\\n                expected_coord, [0, 1])\\n\\n            # Test that engine.provides container is correctly populated.\\n            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]\\n            self.assertEqual(self.engine.provides['coordinates'],\\n                             expected_list)\"]}, {'features': [], 'snippets': ['def copy_clfftdll_to_package():\\n    import shutil\\n    shutil.copy(\\n        os.path.join(CLFFT_DIR, \\'bin\\', \\'clFFT.dll\\'),\\n        \\'gpyfft\\')\\n\\n    shutil.copy(\\n        os.path.join(CLFFT_DIR, \\'bin\\', \\'StatTimer.dll\\'),\\n        \\'gpyfft\\')\\n    print(\"copied clFFT.dll, StatTimer.dll\")', \"def get_version():\\n    main_ns = {}\\n    version_path = convert_path('gpyfft/version.py')\\n    with open(version_path) as version_file:\\n        exec(version_file.read(), main_ns)\\n    version = main_ns['__version__']\\n    return version\"]}, {'features': [], 'snippets': ['def setup(self):\\n        IfcStore.purge()\\n        bpy.ops.wm.read_homefile(app_template=\"\")\\n        if bpy.data.objects:\\n            bpy.data.batch_remove(bpy.data.objects)\\n            bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)\\n        blenderbim.bim.handler.setDefaultProperties(None)', 'def setup(self):\\n        IfcStore.purge()\\n        bpy.ops.wm.read_homefile(app_template=\"\")\\n        bpy.data.batch_remove(bpy.data.objects)\\n        bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)\\n        blenderbim.bim.handler.setDefaultProperties(None)\\n        bpy.ops.bim.create_project()', 'def subfunction(self):\\n        run(function(self))', 'def scenario_debug(function):\\n    def subfunction(self):\\n        run_debug(function(self))\\n\\n    return subfunction', 'def i_add_a_cube():\\n    bpy.ops.mesh.primitive_cube_add()', 'def the_object_name_is_selected(name):\\n    i_deselect_all_objects()\\n    additionally_the_object_name_is_selected(name)', 'def i_deselect_all_objects():\\n    bpy.context.view_layer.objects.active = None\\n    bpy.ops.object.select_all(action=\"DESELECT\")', 'def i_set_prop_to_value(prop, value):\\n    try:\\n        eval(f\"bpy.context.{prop}\")\\n    except:\\n        assert False, \"Property does not exist\"\\n    try:\\n        exec(f\\'bpy.context.{prop} = \"{value}\"\\')\\n    except:\\n        exec(f\"bpy.context.{prop} = {value}\")', 'def i_enable_prop(prop):\\n    exec(f\"bpy.context.{prop} = True\")', 'def i_rename_the_object_name1_to_name2(name1, name2):\\n    the_object_name_exists(name1).name = name2', 'def an_ifc_file_exists():\\n    ifc = IfcStore.get_file()\\n    if not ifc:\\n        assert False, \"No IFC file is available\"\\n    return ifc', 'def the_object_name_does_not_exist(name):\\n    assert bpy.data.objects.get(name) is None, \"Object exists\"', 'def the_object_name_is_not_an_ifc_element(name):\\n    id = the_object_name_exists(name).BIMObjectProperties.ifc_definition_id\\n    assert id == 0, f\"The ID is {id}\"', 'def the_object_name_is_not_in_the_collection_collection(name, collection):\\n    assert collection not in [c.name for c in the_object_name_exists(name).users_collection]', 'def the_collection_name1_is_in_the_collection_name2(name1, name2):\\n    assert bpy.data.collections.get(name2).children.get(name1)', 'def the_object_name_is_placed_in_the_collection_collection(name, collection):\\n    obj = the_object_name_exists(name)\\n    [c.objects.unlink(obj) for c in obj.users_collection]\\n    bpy.data.collections.get(collection).objects.link(obj)', 'def the_object_name_is_contained_in_container_name(name, container_name):\\n    ifc = an_ifc_file_exists()\\n    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)\\n    container = ifcopenshell.util.element.get_container(element)\\n    if not container:\\n        assert False, f\\'Object \"{name}\" is not in any container\\'\\n    assert container.Name == container_name, f\\'Object \"{name}\" is in {container}\\'', 'def i_delete_the_selected_objects():\\n    bpy.ops.object.delete()\\n    blenderbim.bim.handler.active_object_callback()', 'def the_file_name_should_contain_value(name, value):\\n    with open(name, \"r\") as f:\\n        assert value in f.read()', 'def the_object_name1_has_no_boolean_difference_by_name2(name1, name2):\\n    obj = the_object_name_exists(name1)\\n    for modifier in obj.modifiers:\\n        if modifier.type == \"BOOLEAN\" and modifier.object and modifier.object.name == name2:\\n            assert False, \"A boolean was found\"', 'def the_object_name_is_not_voided_by_void(name, void):\\n    ifc = IfcStore.get_file()\\n    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)\\n    for rel in element.HasOpenings:\\n        if rel.RelatedOpeningElement.Name == void:\\n            assert False, \"A void was found\"', 'def the_object_name_is_not_a_void(name):\\n    ifc = IfcStore.get_file()\\n    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)\\n    if any(element.VoidsElements):\\n        assert False, \"A void was found\"', 'def the_void_name_is_not_filled_by_filling(name, filling):\\n    ifc = IfcStore.get_file()\\n    element = ifc.by_id(the_object_name_exists(name).BIMObjectProperties.ifc_definition_id)\\n    if any(rel.RelatedBuildingElement.Name == filling for rel in element.HasFillings):\\n        assert False, \"A filling was found\"', 'def the_object_name_should_display_as_mode(name, mode):\\n    assert the_object_name_exists(name).display_type == mode', 'def the_object_name_is_at_location(name, location):\\n    obj_location = the_object_name_exists(name).location\\n    assert (\\n        obj_location - Vector([float(co) for co in location.split(\",\")])\\n    ).length < 0.1, f\"Object is at {obj_location}\"', 'def run(scenario):\\n    keywords = [\"Given\", \"When\", \"Then\", \"And\", \"But\"]\\n    for line in scenario.split(\"\\\\n\"):\\n        for key, value in variables.items():\\n            line = line.replace(\"{\" + key + \"}\", str(value))\\n        for keyword in keywords:\\n            line = line.replace(keyword, \"\")\\n        line = line.strip()\\n        if not line:\\n            continue\\n        match = None\\n        for definition, callback in definitions.items():\\n            match = re.search(\"^\" + definition + \"$\", line)\\n            if match:\\n                try:\\n                    callback(*match.groups())\\n                except AssertionError as e:\\n                    assert False, f\"Failed: {line}, with error: {e}\"\\n                break\\n        if not match:\\n            assert False, f\"Definition not implemented: {line}\"\\n    return True']}, {'features': [], 'snippets': ['def deposit(amount):\\n    global balance\\n    balance += amount\\n    return balance', \"def make_account():\\n    return {'balance': 0}\", \"def withdraw(account, amount):\\n    account['balance'] -= amount\\n    return account['balance']\", 'def __init__(self, balance=0):\\n        self.balance = balance', 'def deposit(self, amount):\\n        self.balance += amount\\n        return self.balance', 'def __init__(self, minimum_balance):\\n        BankAccount.__init__(self)\\n        self.minimum_balance = minimum_balance', \"def generate_id(n=16):\\n    alphabet = string.ascii_letters + string.digits\\n    return ''.join(random.choice(alphabet) for _ in range(n))\", 'def __init__(self, amount):\\n        super().__init__()\\n        self.amount = amount', 'def __init__(self):\\n        self._balance = 0\\n        self.__id = generate_id()', 'def deposit(self, amount):\\n        self._balance += amount\\n        return self._balance', \"def tricky():\\n        try:\\n            print('Tricky called')\\n            return 1\\n        finally:\\n            print('Tricky finally called')\\n            return 42\\n        return 0\", 'def area(self):\\n        raise NotImplementedError', 'def __init__(self, radius):\\n        self.radius = radius', 'def __init__(self, side):\\n        self.side = side']}, {'features': [], 'snippets': ['def __init__(self, perm):\\n        self.perm = perm', 'def __init__(self, view_or_error=None):\\n        self.wrapped = callable(view_or_error)\\n        error_view = None', 'def __call__(self, view_or_request, *args, **kwargs):\\n        if not self.wrapped:\\n            self.view = view_or_request', \"def dec(*args, **kwargs):\\n            try:\\n                return self.view(*args, **kwargs)\\n            except PermissionRequired as e:\\n                kwargs['_perm'] = e.perm\\n                kwargs['_view'] = self.view\\n                return self.error_view(*args, **kwargs)\", 'def __init__(self, perm, error_view=None):\\n        self.perm = perm\\n        self.error_view = error_view']}, {'features': [], 'snippets': ['def getSample(pointA, pointB, numberOfPoints):\\n    pointList = list(zip(np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)))\\n    sample = np.array([(i[0], i[1], isLeft(pointA, pointB, i)) for i in pointList])\\n    y = sample[:,2]\\n    breakpoint = False\\n    while not breakpoint:\\n        if(len(y[y==-1]) == 0 or len(y[y==1]) == 0):\\n            pointList = list(zip(np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)))\\n            sample = np.array([(i[0], i[1], isLeft(pointA, pointB, i)) for i in pointList])\\n            y = sample[:,2]\\n        else: \\n            breakpoint = True\\n    return sample', 'def getPoints(numberOfPoints):\\n    pointList = list(zip(np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)))\\n    return pointList', 'def sign(x):\\n    return 1 if x > 0 else -1', 'def doMonteCarloQP(pointa, pointb, clf, nopoint):\\n    #print \"weights \", weight\\n    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]\\n    #print points\\n    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])\\n    #print dataset_Monte\\n    return getMisMatchesQP(dataset_Monte, clf)', 'def getMisMatches(data, weights):\\n    #print data\\n    list1 = np.empty(len(data))\\n    list1.fill(weights[0])\\n    results = list1+ weights[1]*data[:,0]+weights[2]*data[:,1]\\n    results = -1 * results\\n    return float(len(data) - np.sum(np.sign(results) == np.sign(data[:,2])))/len(data)', 'def doMonteCarloNP(pointa, pointb, weights, nopoint):\\n    #print \"weights \", weight\\n    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]\\n    #print points\\n    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])\\n    #print dataset_Monte\\n    return getMisMatches(dataset_Monte, weights)']}, {'features': [], 'snippets': ['def __init__(self, first_name, last_name):\\n        self.first_name = first_name\\n        self.last_name = last_name', 'def __init__(self, name):\\n        self.name = name']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def readlineCR(uart):\\n\\tline = b''\\n\\twhile True:\\n\\t\\tbyte = uart.read()\\n\\t\\tline += byte\\n\\t\\tif byte == b'\\\\r':\\n\\t\\t\\treturn line\"]}, {'features': [], 'snippets': ['def __init__(self, evals=None):\\n\\n        self.evals   = evals\\n        self.courses = None\\n        self.location  = None\\n\\n        self.daily_weights  = {\"M\": {}, \"T\": {}, \"W\": {}, \"R\": {}, \"F\": {} ,\"S\": {}}\\n        self.daily_totals   = {\"M\": {}, \"T\": {}, \"W\": {}, \"R\": {}, \"F\": {} ,\"S\": {}}\\n        self.final_weighted = 0\\n        self.weight_rank    = 0 # 0 = worst, 1 = best\\n        if evals != None:\\n            self.courses = self.evals.get_records()\\n            self.location    = self.find_location()\\n            self.final_weighted = self.calculate_final_weighted_score()', 'def get_daily_weight(self,day_of_week):\\n        return self.daily_weights[day_of_week]', 'def calculate_final_weighted_score(self):\\n        score_sum   = 0.00\\n        score_total = 0.00\\n\\n        #reset daily stuff\\n        self.reset_daily_weights()\\n\\n        for course, score in self.courses:\\n            days = course.rec[\"DAYS_OF_WEEK\"]\\n            #score_sum   += score.get_weighted_score(course)\\n            score_total += 1.00\\n            for day in [\"M\", \"T\", \"W\", \"R\", \"F\", \"S\"]:\\n                if day in days:\\n                    self.daily_weights[day] += score.get_weighted_score(course)\\n                    self.daily_totals[day]  += 1\\n\\n        for day in [\"M\", \"T\", \"W\", \"R\", \"F\", \"S\"]:\\n            if self.daily_totals[day] > 0:\\n                self.daily_weights[day] /= self.daily_totals[day]\\n                self.daily_weights[day] = self.adjust_utilization(self.daily_weights[day],self.daily_totals[day])\\n                score_sum += self.daily_weights[day]\\n            else:\\n                self.daily_weights[day] = 0\\n        return score_sum / score_total', 'def get_location(self):\\n        return self.location', 'def get_final_weighted_score(self):\\n        return self.final_weighted']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, *args, **kwargs):\\n        super(self.__class__, self).__init__(*args, **kwargs)\\n        self.host = 'localhost'\\n        self.port = 8000\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _testCpuMatmul(self, x, y, transpose_x=False, transpose_y=False):\\n    x_mat = np.matrix(x).T if transpose_x else np.matrix(x)\\n    y_mat = np.matrix(y).T if transpose_y else np.matrix(y)\\n    np_ans = x_mat * y_mat\\n    with self.test_session(use_gpu=False):\\n      tf_ans = tf.matmul(x, y, transpose_x, transpose_y).eval()\\n    self.assertAllClose(np_ans, tf_ans)\\n    self.assertAllEqual(np_ans.shape, tf_ans.shape)', 'def _randMatrix(self, rows, cols, dtype):\\n    if dtype is np.complex64:\\n      real = self._randMatrix(rows, cols, np.float32)\\n      imag = self._randMatrix(rows, cols, np.float32)\\n      return real + np.complex(0, 1) * imag\\n    else:\\n      return np.random.uniform(low=1.0, high=100.0, size=rows * cols).reshape(\\n          [rows, cols]).astype(dtype)', 'def testFloatBasic(self):\\n    x = np.arange(1., 5.).reshape([4, 1]).astype(np.float32)\\n    y = np.arange(1., 3.).reshape([1, 2]).astype(np.float32)\\n    self._testCpuMatmul(x, y)\\n    self._testGpuMatmul(x, y)', 'def testInt32Basic(self):\\n    x = np.arange(1., 5.).reshape([4, 1]).astype(np.int32)\\n    y = np.arange(1., 3.).reshape([1, 2]).astype(np.int32)\\n    self._testCpuMatmul(x, y)', 'def testFloatRandom(self):\\n    for _ in range(10):\\n      n, k, m = np.random.randint(1, 100, size=3)\\n      x = self._randMatrix(n, k, np.float32)\\n      y = self._randMatrix(k, m, np.float32)\\n      self._testCpuMatmul(x, y)\\n      self._testGpuMatmul(x, y)', 'def testInt32Random(self):\\n    for _ in range(10):\\n      n, k, m = np.random.randint(1, 100, size=3)\\n      x = self._randMatrix(n, k, np.int32)\\n      y = self._randMatrix(k, m, np.int32)\\n      self._testCpuMatmul(x, y)', 'def testFloatRandomTransposeBoth(self):\\n    for _ in range(10):\\n      n, k, m = np.random.randint(1, 100, size=3)\\n      x = self._randMatrix(k, n, np.float32)\\n      y = self._randMatrix(m, k, np.float32)\\n      self._testCpuMatmul(x, y, True, True)\\n      self._testGpuMatmul(x, y, True, True)', 'def testMatMul_OutEmpty_A(self):\\n    n, k, m = 0, 8, 3\\n    x = self._randMatrix(n, k, np.float32)\\n    y = self._randMatrix(k, m, np.float32)\\n    self._testCpuMatmul(x, y)\\n    self._testGpuMatmul(x, y)', 'def testMatMul_Inputs_Empty(self):\\n    n, k, m = 3, 0, 4\\n    x = self._randMatrix(n, k, np.float32)\\n    y = self._randMatrix(k, m, np.float32)\\n    self._testCpuMatmul(x, y)\\n    self._testGpuMatmul(x, y)', 'def testGradientInput0(self):\\n    with self.test_session(use_gpu=False):\\n      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2],\\n                   dtype=tf.float64, name=\"x\")\\n      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\\n                   shape=[2, 4], dtype=tf.float64, name=\"y\")\\n      m = tf.matmul(x, y, name=\"matmul\")\\n      err = gc.ComputeGradientError(x, [3, 2], m, [3, 4])\\n    print(\"matmul input0 gradient err = \", err)\\n    self.assertLess(err, 1e-10)', 'def _VerifyInput0(self, transpose_a, transpose_b):\\n    shape_x = [3, 2]\\n    shape_y = [2, 4]\\n    if transpose_a:\\n      shape_x = list(reversed(shape_x))\\n    if transpose_b:\\n      shape_y = list(reversed(shape_y))\\n    with self.test_session(use_gpu=False):\\n      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=shape_x,\\n                   dtype=tf.float64, name=\"x\")\\n      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\\n                   shape=shape_y, dtype=tf.float64, name=\"y\")\\n      m = tf.matmul(x, y, transpose_a, transpose_b, name=\"matmul\")\\n      err = gc.ComputeGradientError(x, shape_x, m, [3, 4])\\n    print(\"matmul input0 gradient err = \", err)\\n    self.assertLess(err, 1e-10)', 'def _VerifyInput1(self, transpose_a, transpose_b):\\n    shape_x = [3, 2]\\n    shape_y = [2, 4]\\n    if transpose_a:\\n      shape_x = list(reversed(shape_x))\\n    if transpose_b:\\n      shape_y = list(reversed(shape_y))\\n    with self.test_session(use_gpu=False):\\n      x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=shape_x,\\n                   dtype=tf.float64, name=\"x\")\\n      y = tf.constant([1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\\n                   shape=shape_y, dtype=tf.float64, name=\"y\")\\n      m = tf.matmul(x, y, transpose_a, transpose_b, name=\"matmul\")\\n      err = gc.ComputeGradientError(y, shape_y, m, [3, 4])\\n    print(\"matmul input1 gradient err = \", err)\\n    self.assertLess(err, 1e-10)']}, {'features': [], 'snippets': ['def __init__(self, driver):\\n        \"\"\"Creates a new ActionChains.\\n        Args:\\n            driver: The WebDriver instance which performs user actions.\\n        \"\"\"\\n        self._driver = driver\\n        self._actions = []', 'def click(self, on_element=None):\\n        \"\"\"Clicks an element.\\n        Args:\\n            on_element: The element to click.\\n                        If None, clicks on current mouse position.\\n        \"\"\"\\n        if on_element: self.move_to_element(on_element)\\n        self._actions.append(lambda:\\n            self._driver.execute(Command.CLICK, {\\'button\\': 0}))\\n        return self', 'def context_click(self, on_element):\\n        \"\"\"Performs a context-click (right click) on an element.\\n        Args:\\n            on_element: The element to context-click.\\n                        If None, clicks on current mouse position.\\n        \"\"\"\\n        if on_element: self.move_to_element(on_element)\\n        self._actions.append(lambda:\\n            self._driver.execute(Command.CLICK, {\\'button\\': 2}))\\n        return self', 'def drag_and_drop(self, source, target):\\n        \"\"\"Holds down the left mouse button on the source element,\\n           then moves to the target element and releases the mouse button.\\n        Args:\\n            source: The element to mouse down.\\n            target: The element to mouse up.\\n        \"\"\"\\n        self.click_and_hold(source)\\n        self.release(target)\\n        return self', 'def key_down(self, key, element=None):\\n        \"\"\"Sends a key press only, without releasing it.\\n        Should only be used with modifier keys (Control, Alt and Shift).\\n        Args:\\n            key: The modifier key to send. Values are defined in Keys class.\\n            target: The element to send keys.\\n                    If None, sends a key to current focused element.\\n        \"\"\"\\n        if element: self.click(element)\\n        self._actions.append(lambda:\\n            self._driver.execute(Command.SEND_MODIFIER_KEY_TO_ACTIVE_ELEMENT, {\\n                \"value\": key,\\n                \"isdown\": True}))\\n        return self', 'def move_by_offset(self, xoffset, yoffset):\\n        \"\"\"Moving the mouse to an offset from current mouse position.\\n        Args:\\n            xoffset: X offset to move to.\\n            yoffset: Y offset to move to.\\n        \"\"\"\\n        self._actions.append(lambda:\\n            self._driver.execute(Command.MOVE_TO, {\\n                \\'xoffset\\': xoffset,\\n                \\'yoffset\\': yoffset}))\\n        return self', 'def move_to_element_with_offset(self, to_element, xoffset, yoffset):\\n        \"\"\"Move the mouse by an offset of the specificed element.\\n        Offsets are relative to the top-left corner of the element.\\n        Args:\\n            to_element: The element to move to.\\n            xoffset: X offset to move to.\\n            yoffset: Y offset to move to.\\n        \"\"\"\\n        self._actions.append(lambda:\\n            self._driver.execute(Command.MOVE_TO, {\\n                \\'element\\': to_element.id,\\n                \\'xoffset\\': xoffset,\\n                \\'yoffset\\': yoffset}))\\n        return self', 'def send_keys(self, *keys_to_send):\\n        \"\"\"Sends keys to current focused element.\\n        Args:\\n            keys_to_send: The keys to send.\\n        \"\"\"\\n        self._actions.append(lambda:\\n            self._driver.switch_to_active_element().send_keys(*keys_to_send))\\n        return self']}, {'features': [], 'snippets': [\"def build_pot(srcdir, project_id, sources):\\n    # Must be relative paths\\n    sources = [os.path.join('C', source) for source in sources]\\n    outfile = os.path.join(srcdir, project_id + '.pot')\\n    subprocess.call(['itstool', '-o', outfile] + sources)\", \"def build_translations(srcdir, blddir, langs):\\n    for lang in langs:\\n        outdir = os.path.join(blddir, lang)\\n        os.makedirs(outdir, exist_ok=True)\\n        subprocess.call([\\n            'msgfmt', os.path.join(srcdir, lang, lang + '.po'),\\n            '-o', os.path.join(outdir, lang + '.gmo')\\n        ])\", 'def install_help(srcdir, blddir, sources, media, langs, install_dir, destdir, project_id, symlinks):\\n    c_install_dir = os.path.join(install_dir, \\'C\\', project_id)\\n    for lang in langs + [\\'C\\']:\\n        indir = destdir_join(destdir, os.path.join(install_dir, lang, project_id))\\n        os.makedirs(indir, exist_ok=True)\\n        for source in sources:\\n            infile = os.path.join(srcdir if lang == \\'C\\' else blddir, lang, source)\\n            outfile = os.path.join(indir, source)\\n            mlog.log(\\'Installing %s to %s\\' % (infile, outfile))\\n            shutil.copyfile(infile, outfile)\\n            shutil.copystat(infile, outfile)\\n        for m in media:\\n            infile = os.path.join(srcdir, lang, m)\\n            outfile = os.path.join(indir, m)\\n            c_infile = os.path.join(srcdir, \\'C\\', m)\\n            if not os.path.exists(infile):\\n                if not os.path.exists(c_infile):\\n                    mlog.warning(\\'Media file \"%s\" did not exist in C directory\\' % m)\\n                    continue\\n                elif symlinks:\\n                    srcfile = os.path.join(c_install_dir, m)\\n                    mlog.log(\\'Symlinking %s to %s.\\' % (outfile, srcfile))\\n                    if has_path_sep(m):\\n                        os.makedirs(os.path.dirname(outfile), exist_ok=True)\\n                    try:\\n                        try:\\n                            os.symlink(srcfile, outfile)\\n                        except FileExistsError:\\n                            os.remove(outfile)\\n                            os.symlink(srcfile, outfile)\\n                        continue\\n                    except (NotImplementedError, OSError):\\n                        mlog.warning(\\'Symlinking not supported, falling back to copying\\')\\n                        infile = c_infile\\n                else:\\n                    # Lang doesn\\'t have media file so copy it over \\'C\\' one\\n                    infile = c_infile\\n            mlog.log(\\'Installing %s to %s\\' % (infile, outfile))\\n            if has_path_sep(m):\\n                os.makedirs(os.path.dirname(outfile), exist_ok=True)\\n            shutil.copyfile(infile, outfile)\\n            shutil.copystat(infile, outfile)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_endpoint(self, template, *args):\\n        return template % ((self.service_port,) + tuple(args))', 'def tearDown(self):\\n        super(BaseResourceTestCase, self).tearDown()\\n        self._service.stop()\\n        self.session.execute(\"DROP TABLE IF EXISTS vms;\", None)', 'def test_get_versions_list(self):\\n\\n        response = requests.get(self.get_endpoint(\\n            TEMPL_ROOT_COLLECTION_ENDPOINT))\\n\\n        self.assertEqual(response.status_code, 200)\\n        self.assertEqual(response.json(), [\"v1\"])', 'def test_get_resources_list(self):\\n\\n        response = requests.get(\\n            self.get_endpoint(TEMPL_V1_COLLECTION_ENDPOINT))\\n\\n        self.assertEqual(response.status_code, 200)\\n        self.assertEqual(response.json(), [\"vms\"])', 'def _insert_vm_to_db(self, uuid, name, state):\\n        vm = models.VM(uuid=uuid, name=name, state=state)\\n        vm.save()', 'def test_create_vm_resource_successful(self, uuid4_mock):\\n        RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\")\\n        uuid4_mock.return_value = RESOURCE_ID\\n        vm_request_body = {\\n            \"name\": \"test\"\\n        }\\n        vm_response_body = {\\n            \"uuid\": str(RESOURCE_ID),\\n            \"name\": \"test\",\\n            \"state\": \"off\"\\n        }\\n        LOCATION = self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT, RESOURCE_ID)\\n\\n        response = requests.post(self.get_endpoint(\\n            TEMPL_VMS_COLLECTION_ENDPOINT), json=vm_request_body)\\n\\n        self.assertEqual(response.status_code, 201)\\n        self.assertEqual(response.headers[\\'location\\'], LOCATION)\\n        self.assertEqual(response.json(), vm_response_body)', 'def test_update_vm_resource_successful(self):\\n        RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\")\\n        self._insert_vm_to_db(uuid=RESOURCE_ID, name=\"old\", state=\"off\")\\n        vm_request_body = {\\n            \"name\": \"new\"\\n        }\\n        vm_response_body = {\\n            \"uuid\": str(RESOURCE_ID),\\n            \"name\": \"new\",\\n            \"state\": \"off\"\\n        }\\n        VM_RES_ENDPOINT = self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,\\n                                            RESOURCE_ID)\\n\\n        response = requests.put(VM_RES_ENDPOINT, json=vm_request_body)\\n\\n        self.assertEqual(response.status_code, 200)\\n        self.assertEqual(response.json(), vm_response_body)', 'def test_process_vm_action_successful(self):\\n        RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\")\\n        self._insert_vm_to_db(uuid=RESOURCE_ID, name=\"test\", state=\"off\")\\n        vm_response_body = {\\n            \"uuid\": str(RESOURCE_ID),\\n            \"name\": \"test\",\\n            \"state\": \"on\"\\n        }\\n        POWERON_ACT_ENDPOINT = self.get_endpoint(TEMPL_POWERON_ACTION_ENDPOINT,\\n                                                 RESOURCE_ID)\\n\\n        response = requests.post(POWERON_ACT_ENDPOINT)\\n\\n        self.assertEqual(response.status_code, 200)\\n        self.assertEqual(response.json(), vm_response_body)', 'def setUp(self):\\n        super(TestNestedResourceTestCase, self).setUp()\\n        self.session.execute(\"\"\"CREATE TABLE IF NOT EXISTS ports (\\n            uuid CHAR(36) NOT NULL,\\n            mac CHAR(17) NOT NULL,\\n            vm CHAR(36) NOT NULL,\\n            PRIMARY KEY (uuid),\\n            CONSTRAINT FOREIGN KEY ix_vms_uuid (vm) REFERENCES vms (uuid)\\n        ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\"\"\", None)\\n        self.vm1 = models.VM(\\n            uuid=pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\"),\\n            name=\"vm1\",\\n            state=\"on\")\\n        self.vm1.save(session=self.session)\\n        self.vm2 = models.VM(\\n            uuid=pyuuid.UUID(\"00000000-0000-0000-0000-000000000002\"),\\n            name=\"vm2\",\\n            state=\"off\")\\n        self.vm2.save(session=self.session)\\n        self.session.commit()', 'def test_create_nested_resource_successful(self, uuid4_mock):\\n        VM_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\")\\n        PORT_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000003\")\\n        uuid4_mock.return_value = PORT_RESOURCE_ID\\n        port_request_body = {\\n            \"mac\": \"00:00:00:00:00:03\"\\n        }\\n        port_response_body = {\\n            \"uuid\": str(PORT_RESOURCE_ID),\\n            \"mac\": \"00:00:00:00:00:03\",\\n            \"vm\": parse.urlparse(\\n                self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,\\n                                  VM_RESOURCE_ID)).path\\n        }\\n        LOCATION = self.get_endpoint(TEMPL_PORT_RESOURCE_ENDPOINT,\\n                                     VM_RESOURCE_ID,\\n                                     PORT_RESOURCE_ID)\\n\\n        response = requests.post(\\n            self.get_endpoint(TEMPL_PORTS_COLLECTION_ENDPOINT, VM_RESOURCE_ID),\\n            json=port_request_body)\\n\\n        self.assertEqual(response.status_code, 201)\\n        self.assertEqual(response.headers[\\'location\\'], LOCATION)\\n        self.assertEqual(response.json(), port_response_body)', 'def test_get_ports_collection_successful(self):\\n        VM_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000001\")\\n        PORT1_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000003\")\\n        PORT2_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000004\")\\n        PORT3_RESOURCE_ID = pyuuid.UUID(\"00000000-0000-0000-0000-000000000005\")\\n        port1 = models.Port(uuid=PORT1_RESOURCE_ID,\\n                            mac=\"00:00:00:00:00:03\",\\n                            vm=self.vm1)\\n        port1.save(session=self.session)\\n        port2 = models.Port(uuid=PORT2_RESOURCE_ID,\\n                            mac=\"00:00:00:00:00:04\",\\n                            vm=self.vm1)\\n        port2.save(session=self.session)\\n        port3 = models.Port(uuid=PORT3_RESOURCE_ID,\\n                            mac=\"00:00:00:00:00:05\",\\n                            vm=self.vm2)\\n        port3.save(session=self.session)\\n        ports_response_body = [{\\n            \"uuid\": str(PORT1_RESOURCE_ID),\\n            \"mac\": \"00:00:00:00:00:03\",\\n            \"vm\": parse.urlparse(\\n                self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,\\n                                  VM_RESOURCE_ID)).path\\n        }, {\\n            \"uuid\": str(PORT2_RESOURCE_ID),\\n            \"mac\": \"00:00:00:00:00:04\",\\n            \"vm\": parse.urlparse(\\n                self.get_endpoint(TEMPL_VM_RESOURCE_ENDPOINT,\\n                                  VM_RESOURCE_ID)).path\\n        }]\\n        self.session.commit()\\n\\n        response = requests.get(\\n            self.get_endpoint(TEMPL_PORTS_COLLECTION_ENDPOINT, VM_RESOURCE_ID))\\n\\n        self.assertEqual(response.status_code, 200)\\n        self.assertEqual(response.json(), ports_response_body)']}, {'features': [], 'snippets': ['def prefetch_rule(context: models.Context):\\n  # Make sure that we have the IAM policy in cache.\\n  project_ids = {c.project_id for c in gke.get_clusters(context).values()}\\n  for pid in project_ids:\\n    iam.get_project_policy(pid)']}, {'features': [], 'snippets': [\"def get_credentials(env=None) -> tuple:\\n    path = None\\n    if env is None:\\n        env = os.environ.copy()\\n    if 'GCE_CREDENTIALS' in env:\\n        json_credentials = env['GCE_CREDENTIALS']\\n    elif 'GOOGLE_APPLICATION_CREDENTIALS' in env:\\n        path = env['GOOGLE_APPLICATION_CREDENTIALS']\\n        json_credentials = util.read_file(path)\\n    else:\\n        raise util.LauncherError(\\n            'MissingParameter', 'Either GCE_CREDENTIALS or GOOGLE_APPLICATION_CREDENTIALS must be set in env')\\n\\n    return json_credentials, path\", 'def __init__(self, config: dict, env=None):\\n        creds_string, _ = get_credentials(env)\\n        self.gcp_wrapper = gcp.GcpWrapper(json.loads(creds_string))\\n        self.config = config', 'def deployment(self):\\n        \"\"\" Builds a BareClusterDeployment instance with self.config, but only returns it successfully if the\\n        corresponding real deployment (active machines) exists and doesn\\'t contain any errors.\\n        \"\"\"\\n        try:\\n            deployment = gcp.BareClusterDeployment(self.gcp_wrapper, self.config[\\'deployment_name\\'],\\n                                                   self.config[\\'gce_zone\\'])\\n            info = deployment.get_info()\\n            errors = info[\\'operation\\'].get(\\'error\\')\\n            if errors:\\n                raise util.LauncherError(\\'DeploymentContainsErrors\\', str(errors))\\n            return deployment\\n        except HttpError as e:\\n            if e.resp.status == 404:\\n                raise util.LauncherError(\\'DeploymentNotFound\\',\\n                                         \"The deployment you are trying to access doesn\\'t exist\") from e\\n            raise e', 'def key_helper(self):\\n        \"\"\" Generates a public key and a private key and stores them in the config. The public key will be applied to\\n        all the instances in the deployment later on when wait() is called.\\n        \"\"\"\\n        if self.config[\\'key_helper\\']:\\n            private_key, public_key = util.generate_rsa_keypair()\\n            self.config[\\'ssh_private_key\\'] = private_key.decode()\\n            self.config[\\'ssh_public_key\\'] = public_key.decode()', 'def get_bootstrap_host(self) -> Host:\\n        return list(self.deployment.hosts)[0]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_describe_filters(self):\\n        return {\\n            \"Filters\": [\\n                {\"Name\": \"tag:Name\", \"Values\": [self.resource.name]},\\n                {\\n                    \"Name\": \"instance-state-name\",\\n                    \"Values\": [\\n                        \"pending\",\\n                        \"running\",\\n                        \"shutting-down\",\\n                        \" stopping\",\\n                        \"stopped\",\\n                    ],\\n                },\\n            ]\\n        }', 'def get_create_serializer(self):\\n        return serializers.Resource(MaxCount=1, MinCount=1)', 'def get_destroy_serializer(self):\\n        return serializers.Dict(\\n            InstanceIds=serializers.ListOfOne(serializers.Property(\"InstanceId\"))\\n        )', 'def get_network_id(self, runner):\\n        # FIXME: We can save on some steps if we only do this once\\n        obj = runner.get_plan(self.adapts).describe_object()\\n        return obj.get(\"VpcId\", None)']}, {'features': [], 'snippets': ['def _is_printer_printing(printer: OctoprintPrinterInfo) -> bool:\\n    return (\\n        printer\\n        and printer.state\\n        and printer.state.flags\\n        and printer.state.flags.printing\\n    )', 'def __init__(\\n        self,\\n        coordinator: OctoprintDataUpdateCoordinator,\\n        sensor_type: str,\\n        device_id: str,', 'def device_info(self):\\n        \"\"\"Device info.\"\"\"\\n        return self.coordinator.device_info', 'def __init__(\\n        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str', 'def native_value(self):\\n        \"\"\"Return sensor state.\"\"\"\\n        printer: OctoprintPrinterInfo = self.coordinator.data[\"printer\"]\\n        if not printer:\\n            return None\\n\\n        return printer.state.text', 'def available(self) -> bool:\\n        \"\"\"Return if entity is available.\"\"\"\\n        return self.coordinator.last_update_success and self.coordinator.data[\"printer\"]', 'def __init__(\\n        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str', 'def native_value(self):\\n        \"\"\"Return sensor state.\"\"\"\\n        job: OctoprintJobInfo = self.coordinator.data[\"job\"]\\n        if not job:\\n            return None\\n\\n        if not (state := job.progress.completion):\\n            return 0\\n\\n        return round(state, 2)', 'def __init__(\\n        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str', 'def native_value(self) -> datetime | None:\\n        \"\"\"Return sensor state.\"\"\"\\n        job: OctoprintJobInfo = self.coordinator.data[\"job\"]\\n        if (\\n            not job\\n            or not job.progress.print_time_left\\n            or not _is_printer_printing(self.coordinator.data[\"printer\"])\\n        ):\\n            return None\\n\\n        read_time = self.coordinator.data[\"last_read_time\"]\\n\\n        return read_time + timedelta(seconds=job.progress.print_time_left)', 'def __init__(\\n        self, coordinator: OctoprintDataUpdateCoordinator, device_id: str', 'def native_value(self) -> datetime | None:\\n        \"\"\"Return sensor state.\"\"\"\\n        job: OctoprintJobInfo = self.coordinator.data[\"job\"]\\n\\n        if (\\n            not job\\n            or not job.progress.print_time\\n            or not _is_printer_printing(self.coordinator.data[\"printer\"])\\n        ):\\n            return None\\n\\n        read_time = self.coordinator.data[\"last_read_time\"]\\n\\n        return read_time - timedelta(seconds=job.progress.print_time)', 'def __init__(\\n        self,\\n        coordinator: OctoprintDataUpdateCoordinator,\\n        tool: str,\\n        temp_type: str,\\n        device_id: str,', 'def native_value(self):\\n        \"\"\"Return sensor state.\"\"\"\\n        printer: OctoprintPrinterInfo = self.coordinator.data[\"printer\"]\\n        if not printer:\\n            return None\\n\\n        for temp in printer.temperatures:\\n            if temp.name == self._api_tool:\\n                val = (\\n                    temp.actual_temp\\n                    if self._temp_type == \"actual\"\\n                    else temp.target_temp\\n                )\\n                if val is None:\\n                    return None\\n\\n                return round(val, 2)\\n\\n        return None']}, {'features': [], 'snippets': ['def __init__(self):\\n        self._saved_msg = []', 'def get_msg(self):\\n        return self._saved_msg', 'def test_to_obj():\\n    msg = \\'{\"aa\": 1, \"bb\": [\"hoge\", \"hogi\"], \"cc\": {\"cc1\" : 50}}\\'\\n    converted = jps.utils.to_obj(msg)\\n    assert converted.aa == 1\\n    assert converted.bb[0] == \\'hoge\\'\\n    assert converted.bb[1] == \\'hogi\\'\\n    assert len(converted.bb) == 2\\n    assert converted.cc.cc1 == 50\\n    # todo: do\\n    #   json = converted.to_json()\\n    #   assert json == msg', 'def test_to_obj_list():\\n    msg = \\'[\"hoge\", \"hogi\", {\"atr1\": \"val2\", \"atr2\": 1.0}]\\'\\n    bb = jps.utils.to_obj(msg)\\n    assert len(bb) == 2\\n    assert bb[0] == \\'hoge\\'\\n    assert bb[1] == \\'hogi\\'\\n    assert bb[2].atr1 == \\'val2\\'\\n    assert bb[2].atr2 == 1.0', 'def test_to_obj_list():\\n    msg = \\'[{\"hoge\": 1}, {\"hogi\": 2}]\\'\\n    bb = jps.utils.to_obj(msg)\\n    assert len(bb) == 2\\n    assert bb[0].hoge == 1\\n    assert bb[1].hogi == 2']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_transpose_parameters_into_template(self):\\n        self.maxDiff = None\\n        template = \"kinto.tpl\"\\n        dest = tempfile.mktemp()\\n        config.render_template(template, dest,\\n                               secret=\\'secret\\',\\n                               storage_backend=\\'storage_backend\\',\\n                               cache_backend=\\'cache_backend\\',\\n                               permission_backend=\\'permission_backend\\',\\n                               storage_url=\\'storage_url\\',\\n                               cache_url=\\'cache_url\\',\\n                               permission_url=\\'permission_url\\',\\n                               kinto_version=\\'kinto_version\\',\\n                               config_file_timestamp=\\'config_file_timestamp\\')\\n\\n        with codecs.open(dest, \\'r\\', encoding=\\'utf-8\\') as d:\\n            destination_temp = d.read()\\n\\n        sample_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\\n                                   \"test_configuration/test.ini\")\\n        with codecs.open(sample_path, \\'r\\', encoding=\\'utf-8\\') as c:\\n            sample = c.read()\\n\\n        self.assertEqual(destination_temp, sample)', \"def test_hmac_secret_is_text(self, mocked_render_template):\\n        config.init('kinto.ini', 'postgresql')\\n        args, kwargs = list(mocked_render_template.call_args)\\n        self.assertEquals(type(kwargs['secret']), six.text_type)\", 'def test_init_postgresql_values(self, mocked_render_template):\\n        config.init(\\'kinto.ini\\', \\'postgresql\\')\\n\\n        args, kwargs = list(mocked_render_template.call_args)\\n        self.assertEquals(args, (\\'kinto.tpl\\', \\'kinto.ini\\'))\\n\\n        postgresql_url = \"postgres://postgres:postgres@localhost/postgres\"\\n        self.assertDictEqual(kwargs, {\\n            \\'secret\\': kwargs[\\'secret\\'],\\n            \\'storage_backend\\': \\'kinto.core.storage.postgresql\\',\\n            \\'cache_backend\\': \\'kinto.core.cache.postgresql\\',\\n            \\'permission_backend\\': \\'kinto.core.permission.postgresql\\',\\n            \\'storage_url\\': postgresql_url,\\n            \\'cache_url\\':  postgresql_url,\\n            \\'permission_url\\': postgresql_url,\\n            \\'kinto_version\\': __version__,\\n            \\'config_file_timestamp\\': strftime(\\'%a, %d %b %Y %H:%M:%S %z\\')\\n        })', 'def test_init_redis_values(self, mocked_render_template):\\n        config.init(\\'kinto.ini\\', \\'redis\\')\\n\\n        args, kwargs = list(mocked_render_template.call_args)\\n        self.assertEquals(args, (\\'kinto.tpl\\', \\'kinto.ini\\'))\\n\\n        redis_url = \"redis://localhost:6379\"\\n\\n        self.maxDiff = None  # See the full diff in case of error\\n        self.assertDictEqual(kwargs, {\\n            \\'secret\\': kwargs[\\'secret\\'],\\n            \\'storage_backend\\': \\'kinto_redis.storage\\',\\n            \\'cache_backend\\': \\'kinto_redis.cache\\',\\n            \\'permission_backend\\': \\'kinto_redis.permission\\',\\n            \\'storage_url\\': redis_url + \\'/1\\',\\n            \\'cache_url\\':  redis_url + \\'/2\\',\\n            \\'permission_url\\': redis_url + \\'/3\\',\\n            \\'kinto_version\\': __version__,\\n            \\'config_file_timestamp\\': strftime(\\'%a, %d %b %Y %H:%M:%S %z\\')\\n        })', \"def test_init_memory_values(self, mocked_render_template):\\n        config.init('kinto.ini', 'memory')\\n\\n        args, kwargs = list(mocked_render_template.call_args)\\n        self.assertEquals(args, ('kinto.tpl', 'kinto.ini'))\\n\\n        self.assertDictEqual(kwargs, {\\n            'secret': kwargs['secret'],\\n            'storage_backend': 'kinto.core.storage.memory',\\n            'cache_backend': 'kinto.core.cache.memory',\\n            'permission_backend': 'kinto.core.permission.memory',\\n            'storage_url': '',\\n            'cache_url':  '',\\n            'permission_url': '',\\n            'kinto_version': __version__,\\n            'config_file_timestamp': strftime('%a, %d %b %Y %H:%M:%S %z')\\n        })\"]}, {'features': [], 'snippets': ['def containsNearbyAlmostDuplicate(self, nums, k, t):\\n        \"\"\"\\n        :type nums: List[int]\\n        :type k: int\\n        :type t: int\\n        :rtype: bool\\n        \"\"\"\\n        if k < 1 or t < 0:\\n            return False\\n        dic = {}\\n        t += 1\\n        for i in range(len(nums)):\\n            if i > k:\\n                del dic[nums[i - k - 1] // t]\\n            m = nums[i] // t\\n            if m in dic:\\n                return True\\n            if m - 1 in dic and abs(nums[i] - dic[m - 1]) < t:\\n                return True\\n            if m + 1 in dic and abs(nums[i] - dic[m + 1]) < t:\\n                return True\\n            dic[m] = nums[i]\\n        return False']}, {'features': [], 'snippets': ['def test_error_on_wrong_value_for_consumed_capacity():\\n    resource = boto3.resource(\"dynamodb\", region_name=\"ap-northeast-3\")\\n    client = boto3.client(\"dynamodb\", region_name=\"ap-northeast-3\")\\n    client.create_table(\\n        TableName=\"jobs\",\\n        KeySchema=[{\"AttributeName\": \"job_id\", \"KeyType\": \"HASH\"}],\\n        AttributeDefinitions=[{\"AttributeName\": \"job_id\", \"AttributeType\": \"S\"}],\\n        ProvisionedThroughput={\"ReadCapacityUnits\": 5, \"WriteCapacityUnits\": 5},\\n    )\\n\\n    table = resource.Table(\"jobs\")\\n    item = {\"job_id\": \"asdasdasd\", \"expires_at\": \"1\"}\\n\\n    # PUT_ITEM\\n    with pytest.raises(ClientError) as ex:\\n        table.put_item(Item=item, ReturnConsumedCapacity=\"Garbage\")\\n    err = ex.value.response[\"Error\"]\\n    err[\"Code\"].should.equal(\"ValidationException\")\\n    err[\"Message\"].should.equal(\\n        \"1 validation error detected: Value \\'Garbage\\' at \\'returnConsumedCapacity\\' failed to satisfy constraint: Member must satisfy enum value set: [INDEXES, TOTAL, NONE]\"\\n    )', 'def test_consumed_capacity_get_unknown_item():\\n    conn = boto3.client(\"dynamodb\", region_name=\"us-east-1\")\\n    conn.create_table(\\n        TableName=\"test_table\",\\n        KeySchema=[{\"AttributeName\": \"u\", \"KeyType\": \"HASH\"}],\\n        AttributeDefinitions=[{\"AttributeName\": \"u\", \"AttributeType\": \"S\"}],\\n        BillingMode=\"PAY_PER_REQUEST\",\\n    )\\n    response = conn.get_item(\\n        TableName=\"test_table\",\\n        Key={\"u\": {\"S\": \"does_not_exist\"}},\\n        ReturnConsumedCapacity=\"TOTAL\",\\n    )\\n\\n    # Should still return ConsumedCapacity, even if it does not return an item\\n    response.should.have.key(\"ConsumedCapacity\")\\n    response[\"ConsumedCapacity\"].should.equal(\\n        {\"TableName\": \"test_table\", \"CapacityUnits\": 0.5}\\n    )', 'def test_only_return_consumed_capacity_when_required(\\n    capacity, should_have_capacity, should_have_table', 'def validate_response(\\n    response, should_have_capacity, should_have_table, is_index=False, value=1.0']}, {'features': [], 'snippets': [\"def index(self, request):\\n        queryset = Condition.objects.select_related('source', 'target_option')\\n        serializer = ConditionIndexSerializer(queryset, many=True)\\n        return Response(serializer.data)\", \"def export(self, request):\\n        serializer = ConditionExportSerializer(self.get_queryset(), many=True)\\n        xml = ConditionRenderer().render(serializer.data)\\n        return XMLResponse(xml, name='conditions')\", 'def detail_export(self, request, pk=None):\\n        serializer = ConditionExportSerializer(self.get_object())\\n        xml = ConditionRenderer().render([serializer.data])\\n        return XMLResponse(xml, name=self.get_object().key)']}, {'features': [], 'snippets': [\"def fbcode_builder_spec(builder):\\n    return {\\n        'depends_on': [fbthrift],\\n    }\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def fail(returncode, e):\\n    sys.stderr.write(\"ERROR: %s\\\\n\" % e)\\n    sys.exit(returncode)']}, {'features': [], 'snippets': [\"def main():\\n\\n    # Definition of routers\\n    rtr1 = {\\n        'device_type': 'cisco_ios',\\n        'ip':   '50.76.53.27',\\n        'username': 'pyclass',\\n        'password': '88newclass',\\n    }\\n\\n    rtr2 = {\\n        'device_type': 'cisco_ios',\\n        'ip':   '50.76.53.27',\\n        'username': 'pyclass',\\n        'password': '88newclass',\\n        'port': 8022,\\n    }\\n\\n    srx = {\\n        'device_type': 'juniper',\\n        'ip':   '50.76.53.27',\\n        'username': 'pyclass',\\n        'password': '88newclass',\\n        'port': 9822,\\n    }\"]}, {'features': [], 'snippets': ['def _write(self):\\n        \"\"\" Write a file (overrided)\\n        \"\"\"\\n        with open(self.filename, \\'w\\') as f:\\n            for nml in self.data :\\n                f.write(\\'&\\'+nml+\\'\\\\n\\')\\n\\n                # Sorting dictionary data (in the same order as it was created, thanks to id)\\n                SortedList = sorted(self.data[nml].items(), key=lambda(k, v): v[\\'id\\'])', 'def _read(self):\\n        \"\"\" Read the file (overrided)\\n        \"\"\"\\n        with open(self.filename, \\'r\\') as f:\\n            data = f.read()\\n\\n        varname   = r\\'\\\\b[a-zA-Z][a-zA-Z0-9_]*\\\\b\\'\\n        valueInt  = re.compile(r\\'[+-]?[0-9]+\\')\\n        valueReal = re.compile(r\\'[+-]?([0-9]+\\\\.[0-9]*|[0-9]*\\\\.[0-9]+)\\')\\n        valueNumber = re.compile(r\\'\\\\b(([\\\\+\\\\-]?[0-9]+)?\\\\.)?[0-9]*([eE][-+]?[0-9]+)?\\')\\n        valueBool = re.compile(r\"(\\\\.(true|false|t|f)\\\\.)\",re.I)\\n        valueTrue = re.compile(r\"(\\\\.(true|t)\\\\.)\",re.I)\\n        spaces = r\\'[\\\\s\\\\t]*\\'\\n        quote = re.compile(r\"[\\\\s\\\\t]*[\\\\\\'\\\\\"]\")\\n\\n        namelistname = re.compile(r\"^[\\\\s\\\\t]*&(\" + varname + r\")[\\\\s\\\\t]*$\")\\n        paramname = re.compile(r\"[\\\\s\\\\t]*(\" + varname+r\\')[\\\\s\\\\t]*=[\\\\s\\\\t]*\\')\\n        namlistend = re.compile(r\"^\" + spaces + r\"/\" + spaces + r\"$\")\\n\\n        #split sections/namelists\\n        mynmlfile    = {}\\n        mynmlfileRaw = {}\\n        mynmlname  = \\'\\'\\n        for item in FortranNamelistIO.clean(data.split(\"\\\\n\"),cleancomma=1):\\n            if re.match(namelistname,item):\\n                mynmlname = re.sub(namelistname,r\"\\\\1\",item)\\n                mynmlfile[mynmlname] = {}\\n                mynmlfileRaw[mynmlname] = []\\n            elif re.match(namlistend,item):\\n                mynmlname = \\'\\'\\n            else:\\n                if mynmlname:\\n                    mynmlfileRaw[mynmlname].append(item)\\n\\n        #parse param in each section/namelist\\n        for mynmlname in mynmlfile.keys():\\n            #split strings\\n            bb = []\\n            for item in mynmlfileRaw[mynmlname]:\\n                if item[0]!=\\'!\\':\\n                    # discarding lines that starts with a comment\\n                    bb.extend(FortranNamelistIO.splitstring(item))\\n            #split comma and =\\n            aa = []\\n            for item in bb:\\n                if not re.match(quote,item):\\n                    aa.extend(re.sub(r\"[\\\\s\\\\t]*=\",r\" =\\\\n\",re.sub(r\",+\",r\"\\\\n\",item)).split(\"\\\\n\"))', 'def __getitem__(self, key):\\n        \"\"\" Transform the class instance into a dictionary.\"\"\"\\n        return self.data[key]', 'def clean(mystringlist,commentexpr=r\"^[\\\\s\\\\t]*\\\\#.*$\",spacemerge=0,cleancomma=0):\\n        \"\"\"\\n        Remove leading and trailing blanks, comments/empty lines from a list of strings\\n        mystringlist = foo.clean(mystringlist,spacemerge=0,commentline=r\"^[\\\\s\\\\t]*\\\\#\",cleancharlist=\"\")\\n            commentline: definition of commentline\\n            spacemerge: if <>0, merge/collapse multi space\\n            cleancomma: Remove leading and trailing commas\\n        \"\"\"\\n        aa = mystringlist\\n        if cleancomma:\\n            aa = [re.sub(\"(^([\\\\s\\\\t]*\\\\,)+)|((\\\\,[\\\\s\\\\t]*)+$)\",\"\",item).strip() for item in aa]\\n        if commentexpr:\\n            aa = [re.sub(commentexpr,\"\",item).strip() for item in aa]\\n        if spacemerge:\\n            aa = [re.sub(\"[\\\\s\\\\t]+\",\" \",item).strip() for item in aa if len(item.strip()) <> 0]\\n        else:\\n            aa = [item.strip() for item in aa if len(item.strip()) <> 0]\\n        return aa', 'def splitstring(mystr):\\n        \"\"\"\\n        Split a string in a list of strings at quote boundaries\\n            Input: String\\n            Output: list of strings\\n        \"\"\"\\n        dquote=r\\'(^[^\\\\\"\\\\\\']*)(\\\\\"[^\"]*\\\\\")(.*)$\\'\\n        squote=r\"(^[^\\\\\"\\\\\\']*)(\\\\\\'[^\\']*\\\\\\')(.*$)\"\\n        mystrarr = re.sub(dquote,r\"\\\\1\\\\n\\\\2\\\\n\\\\3\",re.sub(squote,r\"\\\\1\\\\n\\\\2\\\\n\\\\3\",mystr)).split(\"\\\\n\")\\n        #remove zerolenght items\\n        mystrarr = [item for item in mystrarr if len(item) <> 0]\\n        if len(mystrarr) > 1:\\n            mystrarr2 = []\\n            for item in mystrarr:\\n                mystrarr2.extend(FortranNamelistIO.splitstring(item))\\n            mystrarr = mystrarr2\\n        return mystrarr', \"def test_output_identical(self):\\n        InputFile=FortranNamelistIO(self.test_file)\\n        test_fileout=tempfile.mkstemp()[1]\\n        InputFile.write(test_fileout)\\n\\n        with open(self.test_file, 'r') as f:\\n            data_expected = f.read()\\n\\n        with open(test_fileout, 'r') as f:\\n            data_read = f.read()\\n        try:\\n            self.assertMultiLineEqual(data_read, data_expected)\\n        finally:\\n            os.remove(test_fileout)\"]}, {'features': [], 'snippets': ['def _check_option_support(options):\\n    \"\"\"Checks if the specific ipmitool options are supported on host.\\n\\n    This method updates the module-level variables indicating whether\\n    an option is supported so that it is accessible by any driver\\n    interface class in this module. It is intended to be called from\\n    the __init__ method of such classes only.\\n\\n    :param options: list of ipmitool options to be checked\\n    :raises: OSError\\n    \"\"\"\\n    for opt in options:\\n        if _is_option_supported(opt) is None:\\n            try:\\n                cmd = ipmitool_command_options[opt]\\n                # NOTE(cinerama): use subprocess.check_call to\\n                # check options & suppress ipmitool output to\\n                # avoid alarming people\\n                with open(os.devnull, \\'wb\\') as nullfile:\\n                    subprocess.check_call(cmd, stdout=nullfile,\\n                                          stderr=nullfile)\\n            except subprocess.CalledProcessError:\\n                LOG.info(_LI(\"Option %(opt)s is not supported by ipmitool\"),\\n                         {\\'opt\\': opt})\\n                _is_option_supported(opt, False)\\n            else:\\n                LOG.info(_LI(\"Option %(opt)s is supported by ipmitool\"),\\n                         {\\'opt\\': opt})\\n                _is_option_supported(opt, True)', 'def _console_pwfile_path(uuid):\\n    \"\"\"Return the file path for storing the ipmi password for a console.\"\"\"\\n    file_name = \"%(uuid)s.pw\" % {\\'uuid\\': uuid}\\n    return os.path.join(CONF.tempdir, file_name)', 'def _make_password_file(password):\\n    \"\"\"Makes a temporary file that contains the password.\\n\\n    :param password: the password\\n    :returns: the absolute pathname of the temporary file\\n    :raises: PasswordFileFailedToCreate from creating or writing to the\\n             temporary file\\n    \"\"\"\\n    f = None\\n    try:\\n        f = tempfile.NamedTemporaryFile(mode=\\'w\\', dir=CONF.tempdir)\\n        f.write(str(password))\\n        f.flush()\\n    except (IOError, OSError) as exc:\\n        if f is not None:\\n            f.close()\\n        raise exception.PasswordFileFailedToCreate(error=exc)\\n    except Exception:\\n        with excutils.save_and_reraise_exception():\\n            if f is not None:\\n                f.close()\\n\\n    try:\\n        # NOTE(jlvillal): This yield can not be in the try/except block above\\n        # because an exception by the caller of this function would then get\\n        # changed to a PasswordFileFailedToCreate exception which would mislead\\n        # about the problem and its cause.\\n        yield f.name\\n    finally:\\n        if f is not None:\\n            f.close()', 'def _exec_ipmitool(driver_info, command):\\n    \"\"\"Execute the ipmitool command.\\n\\n    :param driver_info: the ipmitool parameters for accessing a node.\\n    :param command: the ipmitool command to be executed.\\n    :returns: (stdout, stderr) from executing the command.\\n    :raises: PasswordFileFailedToCreate from creating or writing to the\\n             temporary file.\\n    :raises: processutils.ProcessExecutionError from executing the command.\\n\\n    \"\"\"\\n    ipmi_version = (\\'lanplus\\'\\n                    if driver_info[\\'protocol_version\\'] == \\'2.0\\'\\n                    else \\'lan\\')\\n    args = [\\'ipmitool\\',\\n            \\'-I\\',\\n            ipmi_version,\\n            \\'-H\\',\\n            driver_info[\\'address\\'],\\n            \\'-L\\', driver_info[\\'priv_level\\']\\n            ]\\n    if driver_info[\\'username\\']:\\n        args.append(\\'-U\\')\\n        args.append(driver_info[\\'username\\'])\\n\\n    for name, option in BRIDGING_OPTIONS:\\n        if driver_info[name] is not None:\\n            args.append(option)\\n            args.append(driver_info[name])\\n\\n    # specify retry timing more precisely, if supported\\n    num_tries = max(\\n        (CONF.ipmi.retry_timeout // CONF.ipmi.min_command_interval), 1)\\n\\n    if _is_option_supported(\\'timing\\'):\\n        args.append(\\'-R\\')\\n        args.append(str(num_tries))\\n\\n        args.append(\\'-N\\')\\n        args.append(str(CONF.ipmi.min_command_interval))\\n\\n    end_time = (time.time() + CONF.ipmi.retry_timeout)\\n\\n    while True:\\n        num_tries = num_tries - 1\\n        # NOTE(deva): ensure that no communications are sent to a BMC more\\n        #             often than once every min_command_interval seconds.\\n        time_till_next_poll = CONF.ipmi.min_command_interval - (\\n            time.time() - LAST_CMD_TIME.get(driver_info[\\'address\\'], 0))\\n        if time_till_next_poll > 0:\\n            time.sleep(time_till_next_poll)\\n        # Resetting the list that will be utilized so the password arguments\\n        # from any previous execution are preserved.\\n        cmd_args = args[:]\\n        # \\'ipmitool\\' command will prompt password if there is no \\'-f\\'\\n        # option, we set it to \\'\\\\0\\' to write a password file to support\\n        # empty password\\n        with _make_password_file(driver_info[\\'password\\'] or \\'\\\\0\\') as pw_file:\\n            cmd_args.append(\\'-f\\')\\n            cmd_args.append(pw_file)\\n            cmd_args.extend(command.split(\" \"))\\n            try:\\n                out, err = utils.execute(*cmd_args)\\n                return out, err\\n            except processutils.ProcessExecutionError as e:\\n                with excutils.save_and_reraise_exception() as ctxt:\\n                    err_list = [x for x in IPMITOOL_RETRYABLE_FAILURES\\n                                if x in e.args[0]]\\n                    if ((time.time() > end_time) or\\n                        (num_tries == 0) or\\n                        not err_list):\\n                        LOG.error(_LE(\\'IPMI Error while attempting \"%(cmd)s\"\\'\\n                                      \\'for node %(node)s. Error: %(error)s\\'), {\\n                                  \\'node\\': driver_info[\\'uuid\\'],\\n                                  \\'cmd\\': e.cmd, \\'error\\': e\\n                                  })\\n                    else:\\n                        ctxt.reraise = False\\n                        LOG.warning(_LW(\\'IPMI Error encountered, retrying \\'\\n                                        \\'\"%(cmd)s\" for node %(node)s. \\'\\n                                        \\'Error: %(error)s\\'), {\\n                                    \\'node\\': driver_info[\\'uuid\\'],\\n                                    \\'cmd\\': e.cmd, \\'error\\': e\\n                                    })\\n            finally:\\n                LAST_CMD_TIME[driver_info[\\'address\\']] = time.time()', 'def _set_and_wait(target_state, driver_info):\\n    \"\"\"Helper function for DynamicLoopingCall.\\n\\n    This method changes the power state and polls the BMCuntil the desired\\n    power state is reached, or CONF.ipmi.retry_timeout would be exceeded by the\\n    next iteration.\\n\\n    This method assumes the caller knows the current power state and does not\\n    check it prior to changing the power state. Most BMCs should be fine, but\\n    if a driver is concerned, the state should be checked prior to calling this\\n    method.\\n\\n    :param target_state: desired power state\\n    :param driver_info: the ipmitool parameters for accessing a node.\\n    :returns: one of ironic.common.states\\n\\n    \"\"\"\\n    if target_state == states.POWER_ON:\\n        state_name = \"on\"\\n    elif target_state == states.POWER_OFF:\\n        state_name = \"off\"\\n\\n    def _wait(mutable):\\n        try:\\n            # Only issue power change command once\\n            if mutable[\\'iter\\'] < 0:\\n                _exec_ipmitool(driver_info, \"power %s\" % state_name)\\n            else:\\n                mutable[\\'power\\'] = _power_status(driver_info)\\n        except (exception.PasswordFileFailedToCreate,\\n                processutils.ProcessExecutionError,\\n                exception.IPMIFailure):\\n            # Log failures but keep trying\\n            LOG.warning(_LW(\"IPMI power %(state)s failed for node %(node)s.\"),\\n                        {\\'state\\': state_name, \\'node\\': driver_info[\\'uuid\\']})\\n        finally:\\n            mutable[\\'iter\\'] += 1\\n\\n        if mutable[\\'power\\'] == target_state:\\n            raise loopingcall.LoopingCallDone()\\n\\n        sleep_time = _sleep_time(mutable[\\'iter\\'])\\n        if (sleep_time + mutable[\\'total_time\\']) > CONF.ipmi.retry_timeout:\\n            # Stop if the next loop would exceed maximum retry_timeout\\n            LOG.error(_LE(\\'IPMI power %(state)s timed out after \\'\\n                          \\'%(tries)s retries on node %(node_id)s.\\'),\\n                      {\\'state\\': state_name, \\'tries\\': mutable[\\'iter\\'],\\n                       \\'node_id\\': driver_info[\\'uuid\\']})\\n            mutable[\\'power\\'] = states.ERROR\\n            raise loopingcall.LoopingCallDone()\\n        else:\\n            mutable[\\'total_time\\'] += sleep_time\\n            return sleep_time\\n\\n    # Use mutable objects so the looped method can change them.\\n    # Start \\'iter\\' from -1 so that the first two checks are one second apart.\\n    status = {\\'power\\': None, \\'iter\\': -1, \\'total_time\\': 0}\\n\\n    timer = loopingcall.DynamicLoopingCall(_wait, status)\\n    timer.start().wait()\\n    return status[\\'power\\']', 'def _power_off(driver_info):\\n    \"\"\"Turn the power OFF for this node.\\n\\n    :param driver_info: the ipmitool parameters for accessing a node.\\n    :returns: one of ironic.common.states POWER_OFF or ERROR.\\n    :raises: IPMIFailure on an error from ipmitool (from _power_status call).\\n\\n    \"\"\"\\n    return _set_and_wait(states.POWER_OFF, driver_info)', \"def _process_sensor(sensor_data):\\n    sensor_data_fields = sensor_data.split('\\\\n')\\n    sensor_data_dict = {}\\n    for field in sensor_data_fields:\\n        if not field:\\n            continue\\n        kv_value = field.split(':')\\n        if len(kv_value) != 2:\\n            continue\\n        sensor_data_dict[kv_value[0].strip()] = kv_value[1].strip()\\n\\n    return sensor_data_dict\", 'def _parse_ipmi_sensors_data(node, sensors_data):\\n    \"\"\"Parse the IPMI sensors data and format to the dict grouping by type.\\n\\n    We run \\'ipmitool\\' command with \\'sdr -v\\' options, which can return sensor\\n    details in human-readable format, we need to format them to JSON string\\n    dict-based data for Ceilometer Collector which can be sent it as payload\\n    out via notification bus and consumed by Ceilometer Collector.\\n\\n    :param sensors_data: the sensor data returned by ipmitool command.\\n    :returns: the sensor data with JSON format, grouped by sensor type.\\n    :raises: FailedToParseSensorData when error encountered during parsing.\\n\\n    \"\"\"\\n    sensors_data_dict = {}\\n    if not sensors_data:\\n        return sensors_data_dict\\n\\n    sensors_data_array = sensors_data.split(\\'\\\\n\\\\n\\')\\n    for sensor_data in sensors_data_array:\\n        sensor_data_dict = _process_sensor(sensor_data)\\n        if not sensor_data_dict:\\n            continue\\n\\n        sensor_type = _get_sensor_type(node, sensor_data_dict)\\n\\n        # ignore the sensors which has no current \\'Sensor Reading\\' data\\n        if \\'Sensor Reading\\' in sensor_data_dict:\\n            sensors_data_dict.setdefault(\\n                sensor_type,\\n                {})[sensor_data_dict[\\'Sensor ID\\']] = sensor_data_dict\\n\\n    # get nothing, no valid sensor data\\n    if not sensors_data_dict:\\n        raise exception.FailedToParseSensorData(\\n            node=node.uuid,\\n            error=(_(\"parse ipmi sensor data failed, get nothing with input\"\\n                     \" data: %(sensors_data)s\")\\n                   % {\\'sensors_data\\': sensors_data}))\\n    return sensors_data_dict', 'def send_raw(task, raw_bytes):\\n    \"\"\"Send raw bytes to the BMC. Bytes should be a string of bytes.\\n\\n    :param task: a TaskManager instance.\\n    :param raw_bytes: a string of raw bytes to send, e.g. \\'0x00 0x01\\'\\n    :raises: IPMIFailure on an error from ipmitool.\\n    :raises: MissingParameterValue if a required parameter is missing.\\n    :raises:  InvalidParameterValue when an invalid value is specified.\\n\\n    \"\"\"\\n    node_uuid = task.node.uuid\\n    LOG.debug(\\'Sending node %(node)s raw bytes %(bytes)s\\',\\n              {\\'bytes\\': raw_bytes, \\'node\\': node_uuid})\\n    driver_info = _parse_driver_info(task.node)\\n    cmd = \\'raw %s\\' % raw_bytes\\n\\n    try:\\n        out, err = _exec_ipmitool(driver_info, cmd)\\n        LOG.debug(\\'send raw bytes returned stdout: %(stdout)s, stderr:\\'\\n                  \\' %(stderr)s\\', {\\'stdout\\': out, \\'stderr\\': err})\\n    except (exception.PasswordFileFailedToCreate,\\n            processutils.ProcessExecutionError) as e:\\n        LOG.exception(_LE(\\'IPMI \"raw bytes\" failed for node %(node_id)s \\'\\n                      \\'with error: %(error)s.\\'),\\n                      {\\'node_id\\': node_uuid, \\'error\\': e})\\n        raise exception.IPMIFailure(cmd=cmd)', 'def __init__(self):\\n        try:\\n            _check_option_support([\\'timing\\', \\'single_bridge\\', \\'dual_bridge\\'])\\n        except OSError:\\n            raise exception.DriverLoadError(\\n                driver=self.__class__.__name__,\\n                reason=_(\"Unable to locate usable ipmitool command in \"\\n                         \"the system path when checking ipmitool version\"))\\n        _check_temp_dir()', 'def validate(self, task):\\n        \"\"\"Validate driver_info for ipmitool driver.\\n\\n        Check that node[\\'driver_info\\'] contains IPMI credentials.\\n\\n        :param task: a TaskManager instance containing the node to act on.\\n        :raises: InvalidParameterValue if required ipmi parameters are missing.\\n        :raises: MissingParameterValue if a required parameter is missing.\\n\\n        \"\"\"\\n        _parse_driver_info(task.node)\\n        # NOTE(deva): don\\'t actually touch the BMC in validate because it is\\n        #             called too often, and BMCs are too fragile.\\n        #             This is a temporary measure to mitigate problems while\\n        #             1314954 and 1314961 are resolved.', 'def set_power_state(self, task, pstate):\\n        \"\"\"Turn the power on or off.\\n\\n        :param task: a TaskManager instance containing the node to act on.\\n        :param pstate: The desired power state, one of ironic.common.states\\n            POWER_ON, POWER_OFF.\\n        :raises: InvalidParameterValue if an invalid power state was specified.\\n        :raises: MissingParameterValue if required ipmi parameters are missing\\n        :raises: PowerStateFailure if the power couldn\\'t be set to pstate.\\n\\n        \"\"\"\\n        driver_info = _parse_driver_info(task.node)\\n\\n        if pstate == states.POWER_ON:\\n            state = _power_on(driver_info)\\n        elif pstate == states.POWER_OFF:\\n            state = _power_off(driver_info)\\n        else:\\n            raise exception.InvalidParameterValue(\\n                _(\"set_power_state called \"\\n                  \"with invalid power state %s.\") % pstate)\\n\\n        if state != pstate:\\n            raise exception.PowerStateFailure(pstate=pstate)', 'def reboot(self, task):\\n        \"\"\"Cycles the power to the task\\'s node.\\n\\n        :param task: a TaskManager instance containing the node to act on.\\n        :raises: MissingParameterValue if required ipmi parameters are missing.\\n        :raises: InvalidParameterValue if an invalid power state was specified.\\n        :raises: PowerStateFailure if the final state of the node is not\\n            POWER_ON.\\n\\n        \"\"\"\\n        driver_info = _parse_driver_info(task.node)\\n        _power_off(driver_info)\\n        state = _power_on(driver_info)\\n\\n        if state != states.POWER_ON:\\n            raise exception.PowerStateFailure(pstate=states.POWER_ON)', 'def get_properties(self):\\n        return COMMON_PROPERTIES', 'def validate(self, task):\\n        \"\"\"Check that \\'driver_info\\' contains IPMI credentials.\\n\\n        Validates whether the \\'driver_info\\' property of the supplied\\n        task\\'s node contains the required credentials information.\\n\\n        :param task: a task from TaskManager.\\n        :raises: InvalidParameterValue if required IPMI parameters\\n            are missing.\\n        :raises: MissingParameterValue if a required parameter is missing.\\n\\n        \"\"\"\\n        _parse_driver_info(task.node)', 'def set_boot_device(self, task, device, persistent=False):\\n        \"\"\"Set the boot device for the task\\'s node.\\n\\n        Set the boot device to use on next reboot of the node.\\n\\n        :param task: a task from TaskManager.\\n        :param device: the boot device, one of\\n                       :mod:`ironic.common.boot_devices`.\\n        :param persistent: Boolean value. True if the boot device will\\n                           persist to all future boots, False if not.\\n                           Default: False.\\n        :raises: InvalidParameterValue if an invalid boot device is specified\\n        :raises: MissingParameterValue if required ipmi parameters are missing.\\n        :raises: IPMIFailure on an error from ipmitool.\\n\\n        \"\"\"\\n        if device not in self.get_supported_boot_devices(task):\\n            raise exception.InvalidParameterValue(_(\\n                \"Invalid boot device %s specified.\") % device)\\n\\n        # note(JayF): IPMI spec indicates unless you send these raw bytes the\\n        # boot device setting times out after 60s. Since it\\'s possible it\\n        # could be >60s before a node is rebooted, we should always send them.\\n        # This mimics pyghmi\\'s current behavior, and the \"option=timeout\"\\n        # setting on newer ipmitool binaries.\\n        timeout_disable = \"0x00 0x08 0x03 0x08\"\\n        send_raw(task, timeout_disable)\\n\\n        cmd = \"chassis bootdev %s\" % device\\n        if persistent:\\n            cmd = cmd + \" options=persistent\"\\n        driver_info = _parse_driver_info(task.node)\\n        try:\\n            out, err = _exec_ipmitool(driver_info, cmd)\\n        except (exception.PasswordFileFailedToCreate,\\n                processutils.ProcessExecutionError) as e:\\n            LOG.warning(_LW(\\'IPMI set boot device failed for node %(node)s \\'\\n                            \\'when executing \"ipmitool %(cmd)s\". \\'\\n                            \\'Error: %(error)s\\'),\\n                        {\\'node\\': driver_info[\\'uuid\\'], \\'cmd\\': cmd, \\'error\\': e})\\n            raise exception.IPMIFailure(cmd=cmd)', 'def get_sensors_data(self, task):\\n        \"\"\"Get sensors data.\\n\\n        :param task: a TaskManager instance.\\n        :raises: FailedToGetSensorData when getting the sensor data fails.\\n        :raises: FailedToParseSensorData when parsing sensor data fails.\\n        :raises: InvalidParameterValue if required ipmi parameters are missing\\n        :raises: MissingParameterValue if a required parameter is missing.\\n        :returns: returns a dict of sensor data group by sensor type.\\n\\n        \"\"\"\\n        driver_info = _parse_driver_info(task.node)\\n        # with \\'-v\\' option, we can get the entire sensor data including the\\n        # extended sensor informations\\n        cmd = \"sdr -v\"\\n        try:\\n            out, err = _exec_ipmitool(driver_info, cmd)\\n        except (exception.PasswordFileFailedToCreate,\\n                processutils.ProcessExecutionError) as e:\\n            raise exception.FailedToGetSensorData(node=task.node.uuid,\\n                                                  error=e)\\n\\n        return _parse_ipmi_sensors_data(task.node, out)', 'def __init__(self):\\n        try:\\n            _check_option_support([\\'single_bridge\\', \\'dual_bridge\\'])\\n        except OSError:\\n            raise exception.DriverLoadError(\\n                driver=self.__class__.__name__,\\n                reason=_(\"Unable to locate usable ipmitool command in \"\\n                         \"the system path when checking ipmitool version\"))\\n        _check_temp_dir()', 'def send_raw(self, task, http_method, raw_bytes):\\n        \"\"\"Send raw bytes to the BMC. Bytes should be a string of bytes.\\n\\n        :param task: a TaskManager instance.\\n        :param http_method: the HTTP method used on the request.\\n        :param raw_bytes: a string of raw bytes to send, e.g. \\'0x00 0x01\\'\\n        :raises: IPMIFailure on an error from ipmitool.\\n        :raises: MissingParameterValue if a required parameter is missing.\\n        :raises:  InvalidParameterValue when an invalid value is specified.\\n\\n        \"\"\"\\n        send_raw(task, raw_bytes)', 'def bmc_reset(self, task, http_method, warm=True):\\n        \"\"\"Reset BMC with IPMI command \\'bmc reset (warm|cold)\\'.\\n\\n        :param task: a TaskManager instance.\\n        :param http_method: the HTTP method used on the request.\\n        :param warm: boolean parameter to decide on warm or cold reset.\\n        :raises: IPMIFailure on an error from ipmitool.\\n        :raises: MissingParameterValue if a required parameter is missing.\\n        :raises: InvalidParameterValue when an invalid value is specified\\n\\n        \"\"\"\\n        node_uuid = task.node.uuid\\n\\n        if warm:\\n            warm_param = \\'warm\\'\\n        else:\\n            warm_param = \\'cold\\'\\n\\n        LOG.debug(\\'Doing %(warm)s BMC reset on node %(node)s\\',\\n                  {\\'warm\\': warm_param, \\'node\\': node_uuid})\\n        driver_info = _parse_driver_info(task.node)\\n        cmd = \\'bmc reset %s\\' % warm_param\\n\\n        try:\\n            out, err = _exec_ipmitool(driver_info, cmd)\\n            LOG.debug(\\'bmc reset returned stdout: %(stdout)s, stderr:\\'\\n                      \\' %(stderr)s\\', {\\'stdout\\': out, \\'stderr\\': err})\\n        except (exception.PasswordFileFailedToCreate,\\n                processutils.ProcessExecutionError) as e:\\n            LOG.exception(_LE(\\'IPMI \"bmc reset\" failed for node %(node_id)s \\'\\n                          \\'with error: %(error)s.\\'),\\n                          {\\'node_id\\': node_uuid, \\'error\\': e})\\n            raise exception.IPMIFailure(cmd=cmd)', 'def validate(self, task, method, **kwargs):\\n        \"\"\"Validate vendor-specific actions.\\n\\n        If invalid, raises an exception; otherwise returns None.\\n\\n        Valid methods:\\n          * send_raw\\n          * bmc_reset\\n\\n        :param task: a task from TaskManager.\\n        :param method: method to be validated\\n        :param kwargs: info for action.\\n        :raises: InvalidParameterValue when an invalid parameter value is\\n                 specified.\\n        :raises: MissingParameterValue if a required parameter is missing.\\n\\n        \"\"\"\\n        if method == \\'send_raw\\':\\n            if not kwargs.get(\\'raw_bytes\\'):\\n                raise exception.MissingParameterValue(_(\\n                    \\'Parameter raw_bytes (string of bytes) was not \\'\\n                    \\'specified.\\'))\\n\\n        _parse_driver_info(task.node)', 'def __init__(self):\\n        try:\\n            _check_option_support([\\'timing\\', \\'single_bridge\\', \\'dual_bridge\\'])\\n        except OSError:\\n            raise exception.DriverLoadError(\\n                driver=self.__class__.__name__,\\n                reason=_(\"Unable to locate usable ipmitool command in \"\\n                         \"the system path when checking ipmitool version\"))\\n        _check_temp_dir()', 'def validate(self, task):\\n        \"\"\"Validate the Node console info.\\n\\n        :param task: a task from TaskManager.\\n        :raises: InvalidParameterValue\\n        :raises: MissingParameterValue when a required parameter is missing\\n\\n        \"\"\"\\n        driver_info = _parse_driver_info(task.node)\\n        if not driver_info[\\'port\\']:\\n            raise exception.MissingParameterValue(_(\\n                \"Missing \\'ipmi_terminal_port\\' parameter in node\\'s\"\\n                \" driver_info.\"))\\n\\n        if driver_info[\\'protocol_version\\'] != \\'2.0\\':\\n            raise exception.InvalidParameterValue(_(\\n                \"Serial over lan only works with IPMI protocol version 2.0. \"\\n                \"Check the \\'ipmi_protocol_version\\' parameter in \"\\n                \"node\\'s driver_info\"))', 'def stop_console(self, task):\\n        \"\"\"Stop the remote console session for the node.\\n\\n        :param task: a task from TaskManager\\n        :raises: InvalidParameterValue if required ipmi parameters are missing\\n        :raises: ConsoleError if unable to stop the console\\n        \"\"\"\\n        driver_info = _parse_driver_info(task.node)\\n        try:\\n            console_utils.stop_shellinabox_console(driver_info[\\'uuid\\'])\\n        finally:\\n            utils.unlink_without_raise(\\n                _console_pwfile_path(driver_info[\\'uuid\\']))']}, {'features': [], 'snippets': ['def roll_die(size):\\n    first_die = choice(range(1, size + 1))\\n    second_die = choice(range(1, size + 1))\\n\\n    return (first_die + second_die, (first_die == second_die))', 'def next_specific(square, next_type):\\n    if next_type not in [\"R\", \"U\"]:\\n        raise Exception(\"next_specific only intended for R and U\")\\n\\n    # R1=5, R2=15, R3=25, R4=35\\n    index = SQUARES.index(square)\\n    if next_type == \"R\":\\n        if 0 <= index < 5 or 35 < index:\\n            return \"R1\"\\n        elif 5 < index < 15:\\n            return \"R2\"\\n        elif 15 < index < 25:\\n            return \"R3\"\\n        elif 25 < index < 35:\\n            return \"R4\"\\n        else:\\n            raise Exception(\"Case should not occur\")\\n    # U1=12, U2=28\\n    elif next_type == \"U\":\\n        if 0 <= index < 12 or index > 28:\\n            return \"U1\"\\n        elif 12 < index < 28:\\n            return \"U2\"\\n        else:\\n            return Exception(\"Case should not occur\")\\n    else:\\n        raise Exception(\"Case should not occur\")', 'def main(verbose=False):\\n    GAME_PLAY = 10 ** 6\\n    dice_size = 4\\n    visited = {\"GO\": 1}\\n    current = \"GO\"\\n    chance_card = 0\\n    chest_card = 0\\n    doubles = 0\\n    for place in xrange(GAME_PLAY):\\n        total, double = roll_die(dice_size)\\n        if double:\\n            doubles += 1\\n        else:\\n            doubles = 0\\n\\n        if doubles == 3:\\n            doubles = 0\\n            current = \"JAIL\"\\n        else:\\n            index = SQUARES.index(current)\\n            landing_square = SQUARES[(index + total) % len(SQUARES)]\\n            (current, chance_card,\\n             chest_card) = next_square(landing_square, chance_card, chest_card)\\n\\n        # if current is not in visited, sets to 1\\n        # (default 0 returned by get)\\n        visited[current] = visited.get(current, 0) + 1\\n\\n    top_visited = sorted(visited.items(),\\n                         key=lambda pair: pair[1],\\n                         reverse=True)\\n    top_visited = [SQUARES.index(square[0]) for square in top_visited[:3]]\\n\\n    return \\'\\'.join(str(index).zfill(2) for index in top_visited)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def matplotlib_pyplot():\\n  import matplotlib  # pylint: disable=g-import-not-at-top\\n  matplotlib.use(\"agg\")\\n  import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\\n  return plt', 'def convert_predictions_to_image_summaries(hook_args):\\n  \"\"\"Optionally converts images from hooks_args to image summaries.\\n\\n  Args:\\n    hook_args: DecodeHookArgs namedtuple\\n  Returns:\\n    summaries: list of tf.Summary values if hook_args.decode_hpara\\n  \"\"\"\\n  decode_hparams = hook_args.decode_hparams\\n  if not decode_hparams.display_decoded_images:\\n    return []\\n  predictions = hook_args.predictions[0]\\n\\n  # Display ten random inputs and outputs so that tensorboard does not hang.\\n  all_summaries = []\\n  rand_predictions = np.random.choice(predictions, size=10)\\n  for ind, prediction in enumerate(rand_predictions):\\n    output_summary = image_to_tf_summary_value(\\n        prediction[\"outputs\"], tag=\"%d_output\" % ind)\\n    input_summary = image_to_tf_summary_value(\\n        prediction[\"inputs\"], tag=\"%d_input\" % ind)\\n    all_summaries.append(input_summary)\\n    all_summaries.append(output_summary)\\n  return all_summaries', 'def make_multiscale(image, resolutions,\\n                    resize_method=tf.image.ResizeMethod.BICUBIC,\\n                    num_channels=3):\\n  \"\"\"Returns list of scaled images, one for each resolution.\\n\\n  Args:\\n    image: Tensor of shape [height, height, num_channels].\\n    resolutions: List of heights that image\\'s height is resized to.\\n    resize_method: tf.image.ResizeMethod.\\n    num_channels: Number of channels in image.\\n\\n  Returns:\\n    List of Tensors, one for each resolution with shape given by\\n    [resolutions[i], resolutions[i], num_channels].\\n  \"\"\"\\n  scaled_images = []\\n  for height in resolutions:\\n    scaled_image = tf.image.resize_images(\\n        image,\\n        size=[height, height],  # assuming that height = width\\n        method=resize_method)\\n    scaled_image = tf.to_int64(scaled_image)\\n    scaled_image.set_shape([height, height, num_channels])\\n    scaled_images.append(scaled_image)\\n\\n  return scaled_images', 'def num_channels(self):\\n    \"\"\"Number of color channels.\"\"\"\\n    return 3', 'def vocab_size(self):\\n    \"\"\"Number of pixel values.\"\"\"\\n    return 256', 'def preprocess_example(self, example, mode, hparams):\\n    if not self._was_reversed:\\n      example[\"inputs\"] = tf.image.per_image_standardization(example[\"inputs\"])\\n    return example', 'def decode_hooks(self):\\n    return [convert_predictions_to_image_summaries]', 'def is_small(self):\\n    raise NotImplementedError()', 'def num_classes(self):\\n    raise NotImplementedError()', 'def train_shards(self):\\n    raise NotImplementedError()', 'def dev_shards(self):\\n    return 1', 'def class_labels(self):\\n    return [\"ID_%d\" % i for i in range(self.num_classes)]', 'def generator(self, data_dir, tmp_dir, is_training):\\n    raise NotImplementedError()', 'def hparams(self, defaults, unused_model_hparams):\\n    p = defaults\\n    p.modality = {\"inputs\": modalities.ModalityType.IMAGE,\\n                  \"targets\": modalities.ModalityType.CLASS_LABEL}\\n    p.vocab_size = {\"inputs\": 256,\\n                    \"targets\": self.num_classes}\\n    p.batch_size_multiplier = 4 if self.is_small else 256\\n    p.loss_multiplier = 3.0 if self.is_small else 1.0\\n    if self._was_reversed:\\n      p.loss_multiplier = 1.0\\n    p.input_space_id = problem.SpaceID.IMAGE\\n    p.target_space_id = problem.SpaceID.IMAGE_LABEL', 'def encode_images_as_png(images):\\n  \"\"\"Yield images encoded as pngs.\"\"\"\\n  if tf.executing_eagerly():\\n    for image in images:\\n      yield tf.image.encode_png(image).numpy()\\n  else:\\n    (height, width, channels) = images[0].shape\\n    with tf.Graph().as_default():\\n      image_t = tf.placeholder(dtype=tf.uint8, shape=(height, width, channels))\\n      encoded_image_t = tf.image.encode_png(image_t)\\n      with tf.Session() as sess:\\n        for image in images:\\n          enc_string = sess.run(encoded_image_t, feed_dict={image_t: image})\\n          yield enc_string', 'def is_character_level(self):\\n    raise NotImplementedError()', 'def vocab_problem(self):\\n    raise NotImplementedError()  # Not needed if self.is_character_level.', 'def target_space_id(self):\\n    raise NotImplementedError()', 'def train_shards(self):\\n    raise NotImplementedError()', 'def dev_shards(self):\\n    raise NotImplementedError()', 'def example_reading_spec(self):\\n    label_key = \"image/class/label\"\\n    data_fields, data_items_to_decoders = (\\n        super(Image2TextProblem, self).example_reading_spec())\\n    data_fields[label_key] = tf.VarLenFeature(tf.int64)\\n    data_items_to_decoders[\"targets\"] = contrib.slim().tfexample_decoder.Tensor(\\n        label_key)\\n    return data_fields, data_items_to_decoders', 'def hparams(self, defaults, unused_model_hparams):\\n    p = defaults\\n    p.modality = {\"inputs\": modalities.ModalityType.IMAGE,\\n                  \"targets\": modalities.ModalityType.SYMBOL}\\n    p.vocab_size = {\"inputs\": 256,\\n                    \"targets\": self._encoders[\"targets\"].vocab_size}\\n    p.batch_size_multiplier = 256\\n    p.loss_multiplier = 1.0\\n    p.input_space_id = problem.SpaceID.IMAGE\\n    p.target_space_id = self.target_space_id', 'def image_augmentation(images, do_colors=False, crop_size=None):\\n  \"\"\"Image augmentation: cropping, flipping, and color transforms.\"\"\"\\n  if crop_size is None:\\n    crop_size = [299, 299]\\n  images = tf.random_crop(images, crop_size + [3])\\n  images = tf.image.random_flip_left_right(images)\\n  if do_colors:  # More augmentation, but might be slow.\\n    images = tf.image.random_brightness(images, max_delta=32. / 255.)\\n    images = tf.image.random_saturation(images, lower=0.5, upper=1.5)\\n    images = tf.image.random_hue(images, max_delta=0.2)\\n    images = tf.image.random_contrast(images, lower=0.5, upper=1.5)\\n  return images']}, {'features': [], 'snippets': ['def generate_dataset(I,J,K,lambdaU,lambdaV,tau):\\n    # Generate U, V\\n    U = numpy.zeros((I,K))\\n    for i,k in itertools.product(xrange(0,I),xrange(0,K)):\\n        U[i,k] = exponential_draw(lambdaU[i,k])\\n    V = numpy.zeros((J,K))\\n    for j,k in itertools.product(xrange(0,J),xrange(0,K)):\\n        V[j,k] = exponential_draw(lambdaV[j,k])', 'def add_noise(true_R,tau):\\n    if numpy.isinf(tau):\\n        return numpy.copy(true_R)', 'def try_generate_M(I,J,fraction_unknown,attempts):\\n    for attempt in range(1,attempts+1):\\n        try:\\n            M = generate_M(I,J,fraction_unknown)\\n            sums_columns = M.sum(axis=0)\\n            sums_rows = M.sum(axis=1)\\n            for i,c in enumerate(sums_rows):\\n                assert c != 0, \"Fully unobserved row in M, row %s. Fraction %s.\" % (i,fraction_unknown)\\n            for j,c in enumerate(sums_columns):\\n                assert c != 0, \"Fully unobserved column in M, column %s. Fraction %s.\" % (j,fraction_unknown)\\n            print \"Took %s attempts to generate M.\" % attempt\\n            return M\\n        except AssertionError:\\n            pass\\n    raise Exception(\"Tried to generate M %s times, with I=%s, J=%s, fraction=%s, but failed.\" % (attempts,I,J,fraction_unknown))']}, {'features': [], 'snippets': ['def set_time(value):\\n    global test_time\\n    test_time = value\\n    log.debug(\"Time now set to : %d\" % test_time)', 'def get_mac():\\n    \"\"\"\\n    Gets a random mac address.\\n    \"\"\"\\n    mac = (\"%02x:%02x:%02x:%02x:%02x:%02x\" %\\n                    (random.randint(0x00, 0xff),\\n                     random.randint(0x00, 0xff),\\n                     random.randint(0x00, 0xff),\\n                     random.randint(0x00, 0xff),\\n                     random.randint(0x00, 0xff),\\n                     random.randint(0x00, 0xff)))\\n    return mac']}, {'features': [], 'snippets': ['def create_channel(\\n        cls,\\n        host: str = \"analyticsdata.googleapis.com\",\\n        credentials: ga_credentials.Credentials = None,\\n        credentials_file: Optional[str] = None,\\n        scopes: Optional[Sequence[str]] = None,\\n        quota_project_id: Optional[str] = None,\\n        **kwargs,', 'def __init__(\\n        self,\\n        *,\\n        host: str = \"analyticsdata.googleapis.com\",\\n        credentials: ga_credentials.Credentials = None,\\n        credentials_file: Optional[str] = None,\\n        scopes: Optional[Sequence[str]] = None,\\n        channel: aio.Channel = None,\\n        api_mtls_endpoint: str = None,\\n        client_cert_source: Callable[[], Tuple[bytes, bytes]] = None,\\n        ssl_channel_credentials: grpc.ChannelCredentials = None,\\n        client_cert_source_for_mtls: Callable[[], Tuple[bytes, bytes]] = None,\\n        quota_project_id=None,\\n        client_info: gapic_v1.client_info.ClientInfo = DEFAULT_CLIENT_INFO,\\n        always_use_jwt_access: Optional[bool] = False,', 'def grpc_channel(self) -> aio.Channel:\\n        \"\"\"Create the channel designed to connect to this service.\\n\\n        This property caches on the instance; repeated calls return\\n        the same channel.\\n        \"\"\"\\n        # Return the channel from cache.\\n        return self._grpc_channel', 'def run_report(\\n        self,', 'def run_pivot_report(\\n        self,', 'def batch_run_reports(\\n        self,', 'def batch_run_pivot_reports(\\n        self,', 'def get_metadata(\\n        self,', 'def run_realtime_report(\\n        self,']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, **kwds):']}, {'features': [], 'snippets': ['def __init__(self, subtype, msg=None):\\n        if msg is None:\\n            msg = \"An error occured for subtype {}\".format(subtype)\\n        super(PhylotyperError, self).__init__(msg)\\n        self.subtype = subtype', 'def __init__(self, subtype, msg=None):\\n        super(PhylotyperError, self).__init__(\\n            subtype, msg=\"Unrecognized subtype {}\".format(subtype))']}, {'features': [], 'snippets': ['def setUp(self):\\n        crypto._fernet = None', 'def test_variable_no_encryption(self):\\n        \"\"\"\\n        Test variables without encryption\\n        \"\"\"\\n        Variable.set(\\'key\\', \\'value\\')\\n        session = settings.Session()\\n        test_var = session.query(Variable).filter(Variable.key == \\'key\\').one()\\n        self.assertFalse(test_var.is_encrypted)\\n        self.assertEqual(test_var.val, \\'value\\')', 'def test_variable_with_encryption(self):\\n        \"\"\"\\n        Test variables with encryption\\n        \"\"\"\\n        Variable.set(\\'key\\', \\'value\\')\\n        session = settings.Session()\\n        test_var = session.query(Variable).filter(Variable.key == \\'key\\').one()\\n        self.assertTrue(test_var.is_encrypted)\\n        self.assertEqual(test_var.val, \\'value\\')']}, {'features': [], 'snippets': ['def setUp(self):\\n        self._mock_multiplexer = mock.create_autospec(\\n            plugin_event_multiplexer.EventMultiplexer\\n        )\\n        self._mock_tb_context = base_plugin.TBContext(\\n            multiplexer=self._mock_multiplexer\\n        )', 'def test_csv(self):\\n        body, mime_type = self._run_handler(\\n            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.CSV\\n        )\\n        self.assertEqual(\"text/csv\", mime_type)\\n        self.assertEqual(EXPECTED_CSV, body)', 'def test_json(self):\\n        body, mime_type = self._run_handler(\\n            EXPERIMENT, SESSION_GROUPS, download_data.OutputFormat.JSON\\n        )\\n        self.assertEqual(\"application/json\", mime_type)\\n        expected_result = {\\n            \"header\": [\\n                \"initial_temp\",\\n                \"final_temp\",\\n                \"string_hparam\",\\n                \"bool_hparam\",\\n                \"optional_string_hparam\",\\n                \"current_temp\",\\n                \"delta_temp\",\\n                \"optional_metric\",\\n            ],\\n            \"rows\": [\\n                [270.0, 150.0, \"a string\", True, \"\", 10.0, 15.0, 33.0],\\n                [280.0, 100.0, \"AAAAA\", False, \"\", 51.0, 44.5, None],\\n                [\\n                    300.0,\\n                    1.2e-05,\\n                    \"a string_3\",\\n                    True,\\n                    \"BB\",\\n                    101.0,\\n                    -15100000.0,\\n                    None,\\n                ],\\n            ],\\n        }\\n        self.assertEqual(expected_result, body)']}, {'features': [], 'snippets': ['def __init__(self, *args, **kwargs):\\n        global scheduler\\n        scheduler = \"Current scheduler is PyCOMPSs\"\\n        self.task_instance = task(*args, **kwargs)', 'def barrier():  # Wait\\n    compss_barrier()', 'def delete_object(obj):  # Release\\n    compss_delete_object(obj)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, number: int):\\n        self.name = 'игрок{}'.format(number)\\n        tactic = randint(0, FIG_LEN-1)\\n        self.main_figure = FIGURES[tactic]\\n        self.__figures = [FIGURES[(tactic+i) % FIG_LEN] for i in range(FIG_LEN)]\"]}, {'features': [], 'snippets': ['def get_arguments():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'--toolchain\\', metavar=\\'FILE\\',\\n                        help=\\'Add toolchain file\\')\\n    parser.add_argument(\\'-q\\', \\'--quiet\\', action=\\'store_true\\',\\n                        help=\\'Only print out failing tests\\')\\n    parser.add_argument(\\'--buildoptions\\', metavar=\\'LIST\\',\\n                        help=\\'Add a comma separated list of extra build options to each test\\')\\n    parser.add_argument(\\'--skip-list\\', metavar=\\'LIST\\',\\n                        help=\\'Add a comma separated list of patterns of the excluded JS-tests\\')\\n    parser.add_argument(\\'--outdir\\', metavar=\\'DIR\\', default=OUTPUT_DIR,\\n                        help=\\'Specify output directory (default: %(default)s)\\')\\n    parser.add_argument(\\'--check-signed-off\\', metavar=\\'TYPE\\', nargs=\\'?\\',\\n                        choices=[\\'strict\\', \\'tolerant\\', \\'travis\\'], const=\\'strict\\',\\n                        help=\\'Run signed-off check (%(choices)s; default type if not given: %(const)s)\\')\\n    parser.add_argument(\\'--check-cppcheck\\', action=\\'store_true\\',\\n                        help=\\'Run cppcheck\\')\\n    parser.add_argument(\\'--check-doxygen\\', action=\\'store_true\\',\\n                        help=\\'Run doxygen\\')\\n    parser.add_argument(\\'--check-pylint\\', action=\\'store_true\\',\\n                        help=\\'Run pylint\\')\\n    parser.add_argument(\\'--check-vera\\', action=\\'store_true\\',\\n                        help=\\'Run vera check\\')\\n    parser.add_argument(\\'--check-license\\', action=\\'store_true\\',\\n                        help=\\'Run license check\\')\\n    parser.add_argument(\\'--check-magic-strings\\', action=\\'store_true\\',\\n                        help=\\'Run \"magic string source code generator should be executed\" check\\')\\n    parser.add_argument(\\'--jerry-debugger\\', action=\\'store_true\\',\\n                        help=\\'Run jerry-debugger tests\\')\\n    parser.add_argument(\\'--jerry-tests\\', action=\\'store_true\\',\\n                        help=\\'Run jerry-tests\\')\\n    parser.add_argument(\\'--jerry-test-suite\\', action=\\'store_true\\',\\n                        help=\\'Run jerry-test-suite\\')\\n    parser.add_argument(\\'--test262\\', action=\\'store_true\\',\\n                        help=\\'Run test262\\')\\n    parser.add_argument(\\'--unittests\\', action=\\'store_true\\',\\n                        help=\\'Run unittests (including doctests)\\')\\n    parser.add_argument(\\'--buildoption-test\\', action=\\'store_true\\',\\n                        help=\\'Run buildoption-test\\')\\n    parser.add_argument(\\'--all\\', \\'--precommit\\', action=\\'store_true\\',\\n                        help=\\'Run all tests\\')\\n\\n    if len(sys.argv) == 1:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    script_args = parser.parse_args()\\n\\n    return script_args', \"def create_binary(job, options):\\n    build_args = job.build_args[:]\\n    if options.buildoptions:\\n        for option in options.buildoptions.split(','):\\n            if option not in build_args:\\n                build_args.append(option)\\n\\n    build_cmd = [settings.BUILD_SCRIPT] + build_args\\n\\n    build_dir_path = os.path.join(options.outdir, job.name)\\n    build_cmd.append('--builddir=%s' % build_dir_path)\\n\\n    install_dir_path = os.path.join(build_dir_path, 'local')\\n    build_cmd.append('--install=%s' % install_dir_path)\\n\\n    if options.toolchain:\\n        build_cmd.append('--toolchain=%s' % options.toolchain)\\n\\n    sys.stderr.write('Build command: %s\\\\n' % ' '.join(build_cmd))\\n\\n    binary_key = tuple(sorted(build_args))\\n    if binary_key in BINARY_CACHE:\\n        ret, build_dir_path = BINARY_CACHE[binary_key]\\n        sys.stderr.write('(skipping: already built at %s with returncode %d)\\\\n' % (build_dir_path, ret))\\n        return ret, build_dir_path\\n\\n    try:\\n        subprocess.check_output(build_cmd)\\n        ret = 0\\n    except subprocess.CalledProcessError as err:\\n        ret = err.returncode\\n\\n    BINARY_CACHE[binary_key] = (ret, build_dir_path)\\n    return ret, build_dir_path\", \"def hash_binary(bin_path):\\n    blocksize = 65536\\n    hasher = hashlib.sha1()\\n    with open(bin_path, 'rb') as bin_file:\\n        buf = bin_file.read(blocksize)\\n        while len(buf) > 0:\\n            hasher.update(buf)\\n            buf = bin_file.read(blocksize)\\n    return hasher.hexdigest()\", \"def run_check(runnable):\\n    sys.stderr.write('Test command: %s\\\\n' % ' '.join(runnable))\\n\\n    try:\\n        ret = subprocess.check_call(runnable)\\n    except subprocess.CalledProcessError as err:\\n        return err.returncode\\n\\n    return ret\", 'def run_jerry_tests(options):\\n    ret_build = ret_test = 0\\n    for job, ret_build, test_cmd in iterate_test_runner_jobs(JERRY_TESTS_OPTIONS, options):\\n        if ret_build:\\n            break\\n\\n        test_cmd.append(settings.JERRY_TESTS_DIR)\\n\\n        if options.quiet:\\n            test_cmd.append(\"-q\")\\n\\n        skip_list = []\\n\\n        if \\'--profile=es2015-subset\\' in job.build_args:\\n            skip_list.append(r\"es5.1\\\\/\")\\n        else:\\n            skip_list.append(r\"es2015\\\\/\")\\n\\n        if options.skip_list:\\n            skip_list.append(options.skip_list)\\n\\n        if skip_list:\\n            test_cmd.append(\"--skip-list=\" + \",\".join(skip_list))\\n\\n        if job.test_args:\\n            test_cmd.extend(job.test_args)\\n\\n        ret_test |= run_check(test_cmd)\\n\\n    return ret_build | ret_test', 'def run_test262_test_suite(options):\\n    ret_build = ret_test = 0\\n    for job in TEST262_TEST_SUITE_OPTIONS:\\n        ret_build, build_dir_path = create_binary(job, options)\\n        if ret_build:\\n            break\\n\\n        test_cmd = [\\n            settings.TEST262_RUNNER_SCRIPT,\\n            get_binary_path(build_dir_path),\\n            settings.TEST262_TEST_SUITE_DIR\\n        ]\\n\\n        if job.test_args:\\n            test_cmd.extend(job.test_args)\\n\\n        ret_test |= run_check(test_cmd)\\n\\n    return ret_build | ret_test', 'def run_buildoption_test(options):\\n    for job in JERRY_BUILDOPTIONS:\\n        ret, _ = create_binary(job, options)\\n        if ret:\\n            break\\n\\n    return ret', \"def main(options):\\n    checks = [\\n        Check(options.check_signed_off, run_check, [settings.SIGNED_OFF_SCRIPT]\\n              + {'tolerant': ['--tolerant'], 'travis': ['--travis']}.get(options.check_signed_off, [])),\\n        Check(options.check_cppcheck, run_check, [settings.CPPCHECK_SCRIPT]),\\n        Check(options.check_doxygen, run_check, [settings.DOXYGEN_SCRIPT]),\\n        Check(options.check_pylint, run_check, [settings.PYLINT_SCRIPT]),\\n        Check(options.check_vera, run_check, [settings.VERA_SCRIPT]),\\n        Check(options.check_license, run_check, [settings.LICENSE_SCRIPT]),\\n        Check(options.check_magic_strings, run_check, [settings.MAGIC_STRINGS_SCRIPT]),\\n        Check(options.jerry_debugger, run_jerry_debugger_tests, options),\\n        Check(options.jerry_tests, run_jerry_tests, options),\\n        Check(options.jerry_test_suite, run_jerry_test_suite, options),\\n        Check(options.test262, run_test262_test_suite, options),\\n        Check(options.unittests, run_unittests, options),\\n        Check(options.buildoption_test, run_buildoption_test, options),\\n    ]\\n\\n    for check in checks:\\n        if check.enabled or options.all:\\n            ret = check.runner(check.arg)\\n            if ret:\\n                sys.exit(ret)\"]}, {'features': [], 'snippets': ['def isPrime(num):\\n    if num < 2:\\n        return False  # 0, 1不是质数\\n\\n    # num为100时, 它是不可能有因子是大于50的. 比如说60 * ? = 100, 这是不可能的, 所以这里只要比较sqrt(), 平方根\\n    boundary = int(math.sqrt(num)) + 1\\n    for i in range(2, boundary):\\n        if num % i == 0:\\n            return False\\n\\n    return True']}, {'features': [], 'snippets': ['def _parsed_query2dict(parsed_query):\\n    result = None\\n    while parsed_query:\\n        part = parsed_query.pop()\\n        if part in binary_operator:\\n            result = {part: {parsed_query.pop(): result}}\\n\\n        elif part in multiple_operators:\\n            if result.get(part):\\n                result[part].append(\\n                    _parsed_query2dict(parsed_query.pop()))\\n            else:\\n                result = {part: [result]}\\n\\n        elif part in uninary_operators:\\n            result = {part: result}\\n        elif isinstance(part, pp.ParseResults):\\n            kind = part.getName()\\n            if kind == \"list\":\\n                res = part.asList()\\n            else:\\n                res = _parsed_query2dict(part)\\n            if result is None:\\n                result = res\\n            elif isinstance(result, dict):\\n                list(result.values())[0].append(res)\\n        else:\\n            result = part\\n    return result', 'def list2cols(cols, objs):\\n    return cols, [tuple([o[k] for k in cols])\\n                  for o in objs]', 'def format_dict_list(objs, field):\\n    objs[field] = \"\\\\n\".join(\\n        \"- \" + \", \".join(\"%s: %s\" % (k, v)\\n                         for k, v in elem.items())\\n        for elem in objs[field])', 'def format_archive_policy(ap):\\n    format_dict_list(ap, \"definition\")\\n    format_string_list(ap, \"aggregation_methods\")']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_get_profiler_not_inited(self):\\n        profiler.clean()\\n        self.assertIsNone(profiler.get())', 'def test_start_not_inited(self):\\n        profiler.clean()\\n        profiler.start(\"name\")', 'def test_stop_not_inited(self):\\n        profiler.clean()\\n        profiler.stop()', 'def test_profiler_get_shorten_id(self):\\n        uuid_id = \"4e3e0ec6-2938-40b1-8504-09eb1d4b0dee\"\\n        prof = profiler._Profiler(\"secret\", base_id=\"1\", parent_id=\"2\")\\n        result = prof.get_shorten_id(uuid_id)\\n        expected = \"850409eb1d4b0dee\"\\n        self.assertEqual(expected, result)', 'def test_profiler_get_base_id(self):\\n        prof = profiler._Profiler(\"secret\", base_id=\"1\", parent_id=\"2\")\\n        self.assertEqual(prof.get_base_id(), \"1\")', 'def test_profiler_get_parent_id(self, mock_generate_uuid):\\n        mock_generate_uuid.return_value = \"42\"\\n        prof = profiler._Profiler(\"secret\", base_id=\"1\", parent_id=\"2\")\\n        prof.start(\"test\")\\n        self.assertEqual(prof.get_parent_id(), \"2\")', 'def test_profiler_get_base_id_unset_case(self, mock_generate_uuid):\\n        mock_generate_uuid.return_value = \"42\"\\n        prof = profiler._Profiler(\"secret\")\\n        self.assertEqual(prof.get_base_id(), \"42\")\\n        self.assertEqual(prof.get_parent_id(), \"42\")', 'def test_profiler_get_id(self, mock_generate_uuid):\\n        mock_generate_uuid.return_value = \"43\"\\n        prof = profiler._Profiler(\"secret\")\\n        prof.start(\"test\")\\n        self.assertEqual(prof.get_id(), \"43\")', 'def test_profiler_start(self, mock_notify, mock_generate_uuid,\\n                            mock_datetime):\\n        mock_generate_uuid.return_value = \"44\"\\n        now = datetime.datetime.utcnow()\\n        mock_datetime.datetime.utcnow.return_value = now\\n\\n        info = {\"some\": \"info\"}\\n        payload = {\\n            \"name\": \"test-start\",\\n            \"base_id\": \"1\",\\n            \"parent_id\": \"2\",\\n            \"trace_id\": \"44\",\\n            \"info\": info,\\n            \"timestamp\": now.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\\n        }\\n\\n        prof = profiler._Profiler(\"secret\", base_id=\"1\", parent_id=\"2\")\\n        prof.start(\"test\", info=info)\\n\\n        mock_notify.assert_called_once_with(payload)', 'def test_profiler_stop(self, mock_notify, mock_datetime):\\n        now = datetime.datetime.utcnow()\\n        mock_datetime.datetime.utcnow.return_value = now\\n        prof = profiler._Profiler(\"secret\", base_id=\"1\", parent_id=\"2\")\\n        prof._trace_stack.append(\"44\")\\n        prof._name.append(\"abc\")\\n\\n        info = {\"some\": \"info\"}\\n        prof.stop(info=info)\\n\\n        payload = {\\n            \"name\": \"abc-stop\",\\n            \"base_id\": \"1\",\\n            \"parent_id\": \"2\",\\n            \"trace_id\": \"44\",\\n            \"info\": info,\\n            \"timestamp\": now.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\\n        }\\n\\n        mock_notify.assert_called_once_with(payload)\\n        self.assertEqual(len(prof._name), 0)\\n        self.assertEqual(prof._trace_stack, collections.deque([\"1\", \"2\"]))', 'def test_with_trace(self, mock_start, mock_stop):\\n\\n        with profiler.Trace(\"a\", info=\"a1\"):\\n            mock_start.assert_called_once_with(\"a\", info=\"a1\")\\n            mock_start.reset_mock()\\n            with profiler.Trace(\"b\", info=\"b1\"):\\n                mock_start.assert_called_once_with(\"b\", info=\"b1\")\\n            mock_stop.assert_called_once_with()\\n            mock_stop.reset_mock()\\n        mock_stop.assert_called_once_with()', 'def test_with_trace_etype(self, mock_start, mock_stop):\\n\\n        def foo():\\n            with profiler.Trace(\"foo\"):\\n                raise ValueError(\"bar\")\\n\\n        self.assertRaises(ValueError, foo)\\n        mock_start.assert_called_once_with(\"foo\", info=None)\\n        mock_stop.assert_called_once_with(info={\\n            \"etype\": \"ValueError\",\\n            \"message\": \"bar\"\\n        })', 'def traced_func(i):\\n    return i', 'def trace_hide_args_func(a, i=10):\\n    return (a, i)', 'def test_fn_exc():\\n    raise ValueError()', 'def trace_with_result_func(a, i=10):\\n    return (a, i)', 'def test_duplicate_trace_disallow(self, mock_start, mock_stop):\\n\\n        @profiler.trace(\"test\")\\n        def trace_me():\\n            pass\\n\\n        self.assertRaises(\\n            ValueError,\\n            profiler.trace(\"test-again\", allow_multiple_trace=False),\\n            trace_me)', 'def test_with_args(self, mock_start, mock_stop):\\n        self.assertEqual(1, traced_func(1))\\n        expected_info = {\\n            \"info\": \"some_info\",\\n            \"function\": {\\n                \"name\": \"osprofiler.tests.unit.test_profiler.traced_func\",\\n                \"args\": str((1,)),\\n                \"kwargs\": str({})\\n            }\\n        }\\n        mock_start.assert_called_once_with(\"function\", info=expected_info)\\n        mock_stop.assert_called_once_with()', 'def test_without_args(self, mock_start, mock_stop):\\n        self.assertEqual((1, 2), trace_hide_args_func(1, i=2))\\n        expected_info = {\\n            \"function\": {\\n                \"name\": \"osprofiler.tests.unit.test_profiler\"\\n                        \".trace_hide_args_func\"\\n            }\\n        }\\n        mock_start.assert_called_once_with(\"hide_args\", info=expected_info)\\n        mock_stop.assert_called_once_with()', 'def test_with_exception(self, mock_start, mock_stop):\\n\\n        self.assertRaises(ValueError, test_fn_exc)\\n        expected_info = {\\n            \"function\": {\\n                \"name\": \"osprofiler.tests.unit.test_profiler.test_fn_exc\"\\n            }\\n        }\\n        expected_stop_info = {\"etype\": \"ValueError\", \"message\": \"\"}\\n        mock_start.assert_called_once_with(\"foo\", info=expected_info)\\n        mock_stop.assert_called_once_with(info=expected_stop_info)', 'def test_with_result(self, mock_start, mock_stop):\\n        self.assertEqual((1, 2), trace_with_result_func(1, i=2))\\n        start_info = {\\n            \"function\": {\\n                \"name\": \"osprofiler.tests.unit.test_profiler\"\\n                        \".trace_with_result_func\",\\n                \"args\": str((1,)),\\n                \"kwargs\": str({\"i\": 2})\\n            }\\n        }\\n\\n        stop_info = {\\n            \"function\": {\\n                \"result\": str((1, 2))\\n            }\\n        }\\n        mock_start.assert_called_once_with(\"hide_result\", info=start_info)\\n        mock_stop.assert_called_once_with(info=stop_info)', 'def method1(self, a, b, c=10):\\n        return a + b + c', 'def method3(self, g=10, h=20):\\n        return g * h', 'def static_method(arg):\\n        return arg', 'def class_method(cls, arg):\\n        return arg', 'def py3_info(info):\\n    # NOTE(boris-42): py33 I hate you.\\n    info_py3 = copy.deepcopy(info)\\n    new_name = re.sub(\"FakeTrace[^.]*\", \"FakeTracedCls\",\\n                      info_py3[\"function\"][\"name\"])\\n    info_py3[\"function\"][\"name\"] = new_name\\n    return info_py3', 'def test_args(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceClassWithInfo()\\n        self.assertEqual(30, fake_cls.method1(5, 15))\\n        expected_info = {\\n            \"a\": 10,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceClassWithInfo.method1\"),\\n                \"args\": str((fake_cls, 5, 15)),\\n                \"kwargs\": str({})\\n            }\\n        }\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_kwargs(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceClassWithInfo()\\n        self.assertEqual(50, fake_cls.method3(g=5, h=10))\\n        expected_info = {\\n            \"a\": 10,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceClassWithInfo.method3\"),\\n                \"args\": str((fake_cls,)),\\n                \"kwargs\": str({\"g\": 5, \"h\": 10})\\n            }\\n        }\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_without_private(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceClassHideArgs()\\n        self.assertEqual(10, fake_cls._method(10))\\n        self.assertFalse(mock_start.called)\\n        self.assertFalse(mock_stop.called)', 'def test_without_args(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceClassHideArgs()\\n        self.assertEqual(40, fake_cls.method1(5, 15, c=20))\\n        expected_info = {\\n            \"b\": 20,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceClassHideArgs.method1\"),\\n            }\\n        }\\n\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"a\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_private_methods(self, mock_start, mock_stop):\\n        fake_cls = FakeTracePrivate()\\n        self.assertEqual(5, fake_cls._method(5))\\n\\n        expected_info = {\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTracePrivate._method\"),\\n                \"args\": str((fake_cls, 5)),\\n                \"kwargs\": str({})\\n            }\\n        }\\n\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_static(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceStaticMethod()\\n\\n        self.assertEqual(25, fake_cls.static_method(25))\\n\\n        expected_info = {\\n            \"function\": {\\n                # fixme(boris-42): Static methods are treated differently in\\n                #                  Python 2.x and Python 3.x. So in PY2 we\\n                #                  expect to see method4 because method is\\n                #                  static and doesn\\'t have reference to class\\n                #                  - and FakeTraceStatic.method4 in PY3\\n                \"name\":\\n                    \"osprofiler.tests.unit.test_profiler\"\\n                    \".method4\" if six.PY2 else\\n                    \"osprofiler.tests.unit.test_profiler.FakeTraceStatic\"\\n                    \".method4\",\\n                \"args\": str((25,)),\\n                \"kwargs\": str({})\\n            }\\n        }\\n\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_static_method_skip(self, mock_start, mock_stop):\\n        self.assertEqual(25, FakeTraceStaticMethodSkip.static_method(25))\\n        self.assertFalse(mock_start.called)\\n        self.assertFalse(mock_stop.called)', 'def test_class_method_skip(self, mock_start, mock_stop):\\n        self.assertEqual(\"foo\", FakeTraceClassMethodSkip.class_method(\"foo\"))\\n        self.assertFalse(mock_start.called)\\n        self.assertFalse(mock_stop.called)', 'def method1(self, a, b, c=10):\\n        return a + b + c', 'def method3(self, g=10, h=20):\\n        return g * h', 'def method4(self, j):\\n        return j', 'def method5(self, k, l):\\n        return k + l', 'def _new_private_method(self, m):\\n        return 2 * m', 'def test_no_name_exception(self):\\n        def define_class_with_no_name():\\n            @six.add_metaclass(profiler.TracedMeta)\\n            class FakeTraceWithMetaclassNoName(FakeTracedCls):\\n                pass\\n        self.assertRaises(TypeError, define_class_with_no_name, 1)', 'def test_args(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceWithMetaclassBase()\\n        self.assertEqual(30, fake_cls.method1(5, 15))\\n        expected_info = {\\n            \"a\": 10,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceWithMetaclassBase.method1\"),\\n                \"args\": str((fake_cls, 5, 15)),\\n                \"kwargs\": str({})\\n            }\\n        }\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_kwargs(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceWithMetaclassBase()\\n        self.assertEqual(50, fake_cls.method3(g=5, h=10))\\n        expected_info = {\\n            \"a\": 10,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceWithMetaclassBase.method3\"),\\n                \"args\": str((fake_cls,)),\\n                \"kwargs\": str({\"g\": 5, \"h\": 10})\\n            }\\n        }\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"rpc\", expected_info))\\n        mock_stop.assert_called_once_with()', 'def test_without_private(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceWithMetaclassHideArgs()\\n        self.assertEqual(10, fake_cls._method(10))\\n        self.assertFalse(mock_start.called)\\n        self.assertFalse(mock_stop.called)', 'def test_without_args(self, mock_start, mock_stop):\\n        fake_cls = FakeTraceWithMetaclassHideArgs()\\n        self.assertEqual(20, fake_cls.method5(5, 15))\\n        expected_info = {\\n            \"b\": 20,\\n            \"function\": {\\n                \"name\": (\"osprofiler.tests.unit.test_profiler\"\\n                         \".FakeTraceWithMetaclassHideArgs.method5\")\\n            }\\n        }\\n\\n        self.assertEqual(1, len(mock_start.call_args_list))\\n        self.assertIn(mock_start.call_args_list[0],\\n                      possible_mock_calls(\"a\", expected_info))\\n        mock_stop.assert_called_once_with()']}, {'features': [], 'snippets': ['def testInitialization(self):\\n    \"\"\"Tests the initialization.\"\"\"\\n    event_formatter = chrome_cache.ChromeCacheEntryEventFormatter()\\n    self.assertIsNotNone(event_formatter)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        self.client = ListsClient(**helper.get_options())', 'def test_get_list(self):\\n        # create a list to get\\n        dummy_data = json.dumps(dict(\\n            name=\\'try_and_get_me\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was created\\n        self.asserTrue(len(created_list[\\'lists\\']))\\n\\n        # the id number of the list the test is trying to get\\n        id_to_get = created_list[\\'listID\\']\\n\\n        # try and get it\\n        recieved_lists = self.client.get_list(id_to_get)\\n\\n        # see if the test got the right list\\n        self.assertEqual(recieved_lists[\\'lists\\'][0][\\'listId\\'], created_list[\\'listId\\'])\\n\\n        print \"Got this list: %s\" % json.dumps(recieved_list[\\'lists\\'][0])\\n\\n        # clean up\\n        self.client.delete_list(id_to_get)', \"def test_get_batch_lists(self):\\n        # holds the ids of the lists being retrieved\\n        list_ids = []\\n\\n        # make a list to get\\n        dummy_data = json.dumps(dict(\\n            name='first_test_list',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(created_list['listID'])\\n\\n        # put the id of the newly made list in list_ids\\n        list_ids[0] = created_list['listId']\\n\\n        #change the data a little and make another list\\n        dummy_data['name'] = 'second_test_list'\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure itwas actually made\\n        self.assertTrue(created_list['listID'])\\n\\n        # put the id number in list_ids\\n        list_ids[1] = created_list['listId']\\n\\n        # try and get them\\n        batch_lists = self.client.get_batch_lists(list_ids)\\n\\n        # make sure you got as many lists as you were searching for\\n        self.assertEqual(len(list_ids), len(batch_lists['lists']))\\n\\n        # clean up\\n        self.client.delete_list(list_ids[0])\\n        self.client.delete_list(list_ids[1])\", 'def test_get_lists(self):\\n        # try and get lists\\n        recieved_lists = self.client.get_lists()\\n\\n        # see if the test got at least one\\n        if len(recieved_lists[\\'lists\\']) == 0:\\n            self.fail(\"Unable to retrieve any lists\")\\n        else:\\n            print \"Got these lists %s\" % json.dumps(recieved_lists)', 'def test_get_static_lists(self):\\n        # create a static list to get\\n        dummy_data = json.dumps(dict(\\n            name=\\'static_test_list\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(created_list[\\'listID\\'])\\n\\n        # this call will return 20 lists if not given another value\\n        static_lists = self.client.get_static_lists()\\n\\n        if len(static_lists[\\'lists\\']) == 0:\\n            self.fail(\"Unable to retrieve any static lists\")\\n        else:\\n            print \"Found these static lists: %s\" % json.dumps(static_lists)\\n\\n            # clean up\\n            self.client.delete_list(created_list[\\'listId\\'])', 'def test_get_dynamic_lists(self):\\n        # make a dynamic list to get\\n        dummy_data = json.dumps(dict(\\n            name=\\'test_dynamic_list\\',\\n            dynamic=True,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure the dynamic list was made\\n        self.assertTrue(created_list[\\'listId\\'])\\n\\n        dynamic_lists = self.client.get_dynamic_lists()\\n\\n        if len(dynamic_lists[\\'lists\\']) == 0:\\n            self.fail(\"Unable to retrieve any dynamic lists\")\\n        else:\\n            print \"Found these dynamic lists: %s\" % json.dumps(dynamic_lists)\\n\\n            # clean up\\n            self.client.delete_list(created_list[\\'listId\\'])', 'def test_get_list_contacts(self):\\n        # the id number of the list you want the contacts of\\n        # which_list =\\n\\n        # try and get the contacts\\n        contacts = self.client.get_list_contacts(which_list)\\n\\n        # make sure you get at least one\\n        self.assertTrue(len(contacts[\\'contacts\\'])\\n\\n        print \"Got these contacts: %s from this list: %s\" % json.dumps(contacts), which_list)', 'def test_get_list_contacts_recent(self):\\n        # the id number of the list you want the recent contacts of\\n        which_list =\\n\\n        recent_contacts = self.client.get_list_contacts_recent(which_list)\\n\\n        if len(recent_contacts[\\'lists\\']) == 0:\\n            self.fail(\"Did not find any recent contacts\")\\n        else:\\n            print \"Found these recent contacts: %s\" % json.dumps(recent_conacts)', 'def test_create_list(self):\\n        # the data for the list the test is making\\n        dummy_data = json.dumps(dict(\\n            list_name=\\'test_list\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        # try and make the list\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was created\\n        if len(created_lists[\\'lists\\']) == 0:\\n            self.fail(\"Did not create the list\")\\n        else:\\n            print \"Created this list: %s\" % json.dumps(created_lists)\\n\\n            # clean up\\n            self.client.delete_list(created_lists[\\'lists\\'][0][\\'listId\\'])', 'def test_update_list(self):\\n        # make a list to update\\n        dummy_data = json.dumps(dict(\\n            name=\\'delete_me\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(len(created_list[\\'listId\\']))\\n\\n        # get the id number of the list\\n        update_list_id = created_list[\\'listId\\']\\n\\n        # this is the data updating the list\\n        update_data = json.dumps(dict(\\n            list_name=\\'really_delete_me\\',\\n        ))\\n\\n        # try and do the update\\n        http_response = self.client.update_list(update_list_id, update_data)\\n\\n        if http_response >= 400:\\n            self.fail(\"Unable to update list!\")\\n        else:\\n            print(\"Updated a list!\")\\n\\n        # clean up\\n        self.client.delete_list(update_list_id)', \"def test_add_contacts_to_list_from_emails(self):\\n        # make a list to add contacts to\\n        dummy_data = json.dumps(dict(\\n            name='give_me_contact_emails',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(len(created_list['lists']))\\n\\n        # the id number of the list being added to\\n        which_list = created_list['listId']\\n\\n        # the emails of the contacts being added\\n        emails = json.dumps(dict(\\n            emails\\n        ))\\n\\n        # try and add the contacts\\n        self.client.add_contacts_to_list_from_emails(which_list, emails)\", 'def test_add_contact_to_list(self):\\n        # make a list to add a contact to\\n        dummy_data = json.dumps(dict(\\n            name=\\'add_a_contact\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(created_list[\\'listId\\'])\\n\\n        # the id number of the list the contact is being added to\\n        which_list = created_list[\\'listId\\']\\n\\n        # the id number of the contact being added to the list\\n        which_contact =\\n\\n        added = self.client.add_contact_to_list(which_list, which_contact)\\n\\n        if added[\\'updated\\'] == which_contact:\\n            print \"Succesfully added contact: %s to list: %s\" % which_contact, which_list\\n\\n            # if it worked, clean up\\n            self.client.delete_list(which_list)\\n\\n        else:\\n            self.fail(\"Did not add contact: %s to list: %a\" % which_contact, which_list)', 'def test_remove_contact_from_list(self):\\n        # make a list to remove a contact from\\n        fake_data = json.dumps(dict(\\n            name=\\'remove_this_contact\\'\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(fake_data)\\n\\n        # make sure it was actually made\\n        self.assertTrue(created_list[\\'listId\\'])\\n\\n        # the id number of the list the contact is being deleted from\\n        which_list = created_list[\\'listId\\']\\n\\n        # the id number of the contact being deleted\\n        which_contact =\\n\\n        # put the contact in the list so it can be removed\\n        added = self.client.add_contact_to_list(which_list, which_contact)\\n\\n        # make sure it was added\\n        self.assertTrue(added[\\'updated\\'])\\n\\n        # try and remove it\\n        removed = self.client.remove_contact_from_list(which_list, which_contact)\\n\\n        # check if it was actually removed\\n        if removed[\\'updated\\'] == which_contact:\\n            print \"Succesfully removed contact: %s from list: %s\" % which_contact, which_list\\n\\n            # clean up\\n            self.client.delete_list(created_list[\\'listId\\'])\\n        else:\\n            self.fail(\"Did not remove contact %s from list: %s\" % which_contact, which_list)', 'def test_delete_list(self):\\n        # make a list to delete\\n        dummy_data = json.dumps(dict(\\n            name=\\'should_be_deleted\\',\\n            dynamic=False,\\n            portalId=PORTAL_ID\\n\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # check if it was actually made\\n        self.assertTrue(created_list[\\'listId\\'])\\n\\n        # the id number of the list being deleted\\n        id_to_delete = created_list[\\'listId\\']\\n\\n        # try deleting it\\n        self.client.delete_list(id_to_delete)\\n\\n        # try and get the list that should have been deleted\\n        check = self.client.get_list(id_to_delete)\\n\\n        # check should not have any lists\\n        self.assertEqual(len(check[\\'lists\\']), 0)\\n\\n        print \"Sucessfully deleted a test list\"', 'def test_refresh_list(self):\\n        # make a dynamic list to refresh\\n        dummy_data = json.dumps(dict(\\n            name=\\'refresh_this_list\\',\\n            dynamic=True,\\n            portalId=PORTAL_ID\\n        ))\\n\\n        created_list = self.client.create_list(dummy_data)\\n\\n        # make sure it actually made the list\\n        self.assertTrue(created_list[\\'listId\\'])\\n\\n        # do the refresh\\n        refresh_response = self.client.refresh_list(created_list[\\'listId\\'])\\n\\n        # check if it worked\\n        if refresh_response >= 400:\\n            self.fail(\"Failed to refresh list: %s\" % json.dumps(created_list))\\n        else:\\n            print \"Succesfully refreshed list: %s\" % json.dumps(created_list)\\n\\n            # clean up\\n            self.client.delete_list(created_list[\\'listId\\'])']}, {'features': [], 'snippets': ['def create_image(self, server_id, name, meta=None):\\n        \"\"\"Creates an image of the original server.\"\"\"\\n\\n        post_body = {\\n            \\'createImage\\': {\\n                \\'name\\': name,\\n            }\\n        }\\n\\n        if meta is not None:\\n            post_body[\\'createImage\\'][\\'metadata\\'] = meta\\n\\n        post_body = json.dumps(post_body)\\n        resp, body = self.post(\\'servers/%s/action\\' % str(server_id),\\n                               post_body)\\n        self.validate_response(schema.create_image, resp, body)\\n        return service_client.ResponseBody(resp, body)', 'def list_images_with_detail(self, params=None):\\n        \"\"\"Returns a detailed list of images filtered by any parameters.\"\"\"\\n        url = \\'images/detail\\'\\n        if params:\\n            url += \\'?%s\\' % urllib.urlencode(params)\\n\\n        resp, body = self.get(url)\\n        body = json.loads(body)\\n        self.validate_response(schema.list_images_details, resp, body)\\n        return service_client.ResponseBodyList(resp, body[\\'images\\'])', 'def delete_image(self, image_id):\\n        \"\"\"Deletes the provided image.\"\"\"\\n        resp, body = self.delete(\"images/%s\" % str(image_id))\\n        self.validate_response(schema.delete, resp, body)\\n        return service_client.ResponseBody(resp, body)', 'def list_image_metadata(self, image_id):\\n        \"\"\"Lists all metadata items for an image.\"\"\"\\n        resp, body = self.get(\"images/%s/metadata\" % str(image_id))\\n        body = json.loads(body)\\n        self.validate_response(schema.image_metadata, resp, body)\\n        return service_client.ResponseBody(resp, body[\\'metadata\\'])', 'def update_image_metadata(self, image_id, meta):\\n        \"\"\"Updates the metadata for an image.\"\"\"\\n        post_body = json.dumps({\\'metadata\\': meta})\\n        resp, body = self.post(\\'images/%s/metadata\\' % str(image_id), post_body)\\n        body = json.loads(body)\\n        self.validate_response(schema.image_metadata, resp, body)\\n        return service_client.ResponseBody(resp, body[\\'metadata\\'])', 'def set_image_metadata_item(self, image_id, key, meta):\\n        \"\"\"Sets the value for a specific image metadata key.\"\"\"\\n        post_body = json.dumps({\\'meta\\': meta})\\n        resp, body = self.put(\\'images/%s/metadata/%s\\' % (str(image_id), key),\\n                              post_body)\\n        body = json.loads(body)\\n        self.validate_response(schema.image_meta_item, resp, body)\\n        return service_client.ResponseBody(resp, body[\\'meta\\'])', 'def is_resource_deleted(self, id):\\n        try:\\n            self.show_image(id)\\n        except lib_exc.NotFound:\\n            return True\\n        return False']}, {'features': [], 'snippets': [\"def __init__(self, message, error):\\n        super(ImejiError, self).__init__(message)\\n        self.error = error.get('error') if isinstance(error, dict) else error\", 'def __init__(self, api, name):\\n        \"\"\"Initialize a handler.\\n\\n        :param api: An Imeji API instance.\\n        :param name: Name specifying the kind of object(s) to retrieve. We check whether\\\\\\n        this name has a plural \"s\" to determine if a list is to be retrieved.\\n        \"\"\"\\n        self._list = name.endswith(\\'s\\')\\n        self.rsc = getattr(resource, (name[:-1] if self._list else name).capitalize())\\n        self.api = api\\n        self.name = name\\n        self.path = name\\n        if not self._list:\\n            self.path += \\'s\\'', \"def __init__(self, cfg=None, service_url=None):\\n        self.cfg = cfg or Config()\\n        self.service_url = service_url or self.cfg.get('service', 'url')\\n        user = self.cfg.get('service', 'user', default=None)\\n        password = self.cfg.get('service', 'password', default=None)\\n        self.session = requests.Session()\\n        if user and password:\\n            self.session.auth = (user, password)\", 'def __getattr__(self, name):\\n        \"\"\"Names of resource classes are accepted and resolved as dynamic attribute names.\\n\\n        This allows convenient retrieval of resources as api.<resource-class>(id=<id>),\\n        or api.<resource-class>s(q=\\'x\\').\\n        \"\"\"\\n        return GET(self, name)', 'def delete(self, rsc):\\n        return rsc.delete()']}, {'features': [], 'snippets': ['def first_message(database, write):\\n    messages = [\\n            firestore_pb2.WriteRequest(database = database, writes = [])\\n    ]\\n    for msg in messages:\\n            yield msg', \"def main():\\n\\n\\n   fl = os.path.dirname(os.path.abspath(__file__))\\n   fn = os.path.join(fl, 'grpc.json')\\n\\n   with open(fn) as grpc_file:\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def mnist_many_cols_gbm_large():\\n  train = h2o.import_file(path=pyunit_utils.locate(\"bigdata/laptop/mnist/train.csv.gz\"))\\n  train.tail()\\n\\n\\n  gbm_mnist = H2OGradientBoostingEstimator(ntrees=1,\\n                                           max_depth=1,\\n                                           min_rows=10,\\n                                           learn_rate=0.01)\\n  gbm_mnist.train(x=range(784), y=784, training_frame=train)\\n  gbm_mnist.show()']}, {'features': [], 'snippets': ['def __init__(self, master, im):\\n        if isinstance(im, list):\\n            # list of images\\n            self.im = im[1:]\\n            im = self.im[0]\\n        else:\\n            # sequence\\n            self.im = im\\n\\n        if im.mode == \"1\":\\n            self.image = ImageTk.BitmapImage(im, foreground=\"white\")\\n        else:\\n            self.image = ImageTk.PhotoImage(im)\\n\\n        tkinter.Label.__init__(self, master, image=self.image, bg=\"black\", bd=0)\\n\\n        self.update()\\n\\n        duration = im.info.get(\"duration\", 100)\\n        self.after(duration, self.next)']}, {'features': [], 'snippets': [\"def create(self, context):\\n        # To ensure the creating type is PF\\n        if self.type != 'pf':\\n            raise exception.InvalidDeployType()\\n        super(PhysicalFunction, self).create(context)\", 'def add_vf(self, vf):\\n        \"\"\"add a vf object to the virtual_function_list.\\n        If the vf already exists, it will ignore,\\n        otherwise, the vf will be appended to the list\\n        \"\"\"\\n        if not isinstance(vf, VirtualFunction) or vf.type != \\'vf\\':\\n            raise exception.InvalidDeployType()\\n        for exist_vf in self.virtual_function_list:\\n            if base.obj_equal_prims(vf, exist_vf):\\n                LOG.warning(\"The vf already exists\")\\n                return None\\n        vf.parent_uuid = self.uuid\\n        vf.root_uuid = self.root_uuid\\n        vf_copy = copy.deepcopy(vf)\\n        self.virtual_function_list.append(vf_copy)', 'def destroy(self, context):\\n        \"\"\"Delete a the pf from the DB.\"\"\"\\n        del self.virtual_function_list[:]\\n        super(PhysicalFunction, self).destroy(context)', 'def get(cls, context, uuid):\\n        \"\"\"Find a DB Physical Function and return an Obj Physical Function.\\n        In addition, it will also finds all the Virtual Functions associated\\n        with this Physical Function and place them in virtual_function_list\\n        \"\"\"\\n        db_pf = cls.dbapi.deployable_get(context, uuid)\\n        obj_pf = cls._from_db_object(cls(context), db_pf)\\n        pf_uuid = obj_pf.uuid\\n\\n        query = {\"parent_uuid\": pf_uuid, \"type\": \"vf\"}\\n        db_vf_list = cls.dbapi.deployable_get_by_filters(context, query)\\n\\n        for db_vf in db_vf_list:\\n            obj_vf = VirtualFunction.get(context, db_vf.uuid)\\n            obj_pf.virtual_function_list.append(obj_vf)\\n        return obj_pf', 'def get_by_filter(cls, context,\\n                      filters, sort_key=\\'created_at\\',\\n                      sort_dir=\\'desc\\', limit=None,\\n                      marker=None, join=None):\\n        obj_dpl_list = []\\n        filters[\\'type\\'] = \\'pf\\'\\n        db_dpl_list = cls.dbapi.deployable_get_by_filters(context, filters,\\n                                                          sort_key=sort_key,\\n                                                          sort_dir=sort_dir,\\n                                                          limit=limit,\\n                                                          marker=marker,\\n                                                          join_columns=join)\\n        for db_dpl in db_dpl_list:\\n            obj_dpl = cls._from_db_object(cls(context), db_dpl)\\n            query = {\"parent_uuid\": obj_dpl.uuid}\\n            vf_get_list = VirtualFunction.get_by_filter(context,\\n                                                        query)\\n            obj_dpl.virtual_function_list = vf_get_list\\n            obj_dpl_list.append(obj_dpl)\\n        return obj_dpl_list']}, {'features': [], 'snippets': ['def isValidSerialization(self, preorder):\\n        \"\"\"\\n        :type preorder: str\\n        :rtype: bool\\n        \"\"\"\\n        arr_pre_order = preorder.split(\\',\\')']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def friendly_time(msecs):\\n    secs, msecs = divmod(msecs, 1000)\\n    mins, secs = divmod(secs, 60)\\n    hours, mins = divmod(mins, 60)\\n    if hours:\\n        return '%dh%dm%ds' % (hours, mins, secs)\\n    elif mins:\\n        return '%dm%ds' % (mins, secs)\\n    elif secs:\\n        return '%ds%dms' % (secs, msecs)\\n    else:\\n        return '%.2fms' % msecs\"]}, {'features': [], 'snippets': ['def setUp(self):\\n        pass']}, {'features': [], 'snippets': ['def forwards(self, orm):', 'def backwards(self, orm):']}, {'features': [], 'snippets': [\"def dem_threshold(domain, b):\\n    '''Just use a height threshold on the DEM!'''\\n\\n    heightLevel = float(domain.algorithm_params['dem_threshold'])\\n    dem         = domain.get_dem().image\\n    return dem.lt(heightLevel).select(['elevation'], ['b1'])\", \"def evi(domain, b):\\n    '''Simple EVI based classifier'''\\n    #no_clouds = b['b3'].lte(2100).select(['sur_refl_b03'], ['b1'])\\n    criteria1 = b['EVI'].lte(0.3).And(b['LSWI'].subtract(b['EVI']).gte(0.05)).select(['sur_refl_b02'], ['b1'])\\n    criteria2 = b['EVI'].lte(0.05).And(b['LSWI'].lte(0.0)).select(['sur_refl_b02'], ['b1'])\\n    #return no_clouds.And(criteria1.Or(criteria2))\\n    return criteria1.Or(criteria2)\", \"def get_diff(b):\\n    '''Just the internals of the difference method'''\\n    return b['b2'].subtract(b['b1']).select(['sur_refl_b02'], ['b1'])\", \"def modis_diff(domain, b, threshold=None):\\n    '''Compute (b2-b1) < threshold, a simple water detection index.\", \"def get_dartmouth(b):\\n    A = 500\\n    B = 2500\\n    return b['b2'].add(A).divide(b['b1'].add(B)).select(['sur_refl_b02'], ['b1'])\", \"def dartmouth(domain, b, threshold=None):\\n    '''A flood detection method from the Dartmouth Flood Observatory.\", \"def get_mod_ndwi(b):\\n    return b['b6'].subtract(b['b4']).divide(b['b4'].add(b['b6'])).select(['sur_refl_b06'], ['b1'])\", \"def mod_ndwi(domain, b, threshold=None):\\n    if threshold == None:\\n        threshold = float(domain.algorithm_params['mod_ndwi_threshold'])\\n    return get_mod_ndwi(b).lte(threshold)\", \"def get_fai(b):\\n    '''Just the internals of the FAI method'''\\n    return b['b2'].subtract(b['b1'].add(b['b5'].subtract(b['b1']).multiply((859.0 - 645) / (1240 - 645)))).select(['sur_refl_b02'], ['b1'])\"]}, {'features': [], 'snippets': ['def __init__(self, public_ip_api, api_ip, domain, user, password,\\n                 cascading_domain, cascading_api_ip, cascaded_domain,\\n                 cascaded_api_ip, cascaded_api_subnet_gateway):\\n        self.public_ip_api = public_ip_api\\n        self.api_ip = api_ip\\n        self.domain = domain\\n        self.user = user\\n        self.password = password\\n        self.cascading_domain = cascading_domain\\n        self.cascading_api_ip = cascading_api_ip\\n        self.cascaded_domain = cascaded_domain\\n        self.cascaded_ip = cascaded_api_ip\\n        self.gateway = cascaded_api_subnet_gateway']}, {'features': [], 'snippets': ['def conv2d(data,\\n           weight,\\n           strides=(1, 1),\\n           padding=(0, 0),\\n           dilation=(1, 1),\\n           groups=1,\\n           channels=None,\\n           kernel_size=None,\\n           data_layout=\"NCHW\",\\n           weight_layout=\"OIHW\",\\n           out_layout=\"\",\\n           out_dtype=\"\"):\\n    r\"\"\"2D convolution.\\n\\n    This operator takes the weight as the convolution kernel\\n    and convolves it with data to produce an output.\\n\\n\\n    In the default case, where the data_layout is `NCHW`\\n    and weight_layout is `OIHW`, conv2d takes in\\n    a data Tensor with shape `(batch_size, in_channels, height, width)`,\\n    and a weight Tensor with shape `(channels, in_channels, kernel_size[0], kernel_size[1])`\\n    to produce an output Tensor with the following rule:\\n\\n    .. math::\\n\\n        \\\\mbox{out}[b, c, y, x] = \\\\sum_{dy, dx, k}\\n           \\\\mbox{data}[b, k, \\\\mbox{strides}[0] * y  + dy, \\\\mbox{strides}[1] * x + dx] *\\n           \\\\mbox{weight}[c, k, dy, dx]\\n\\n    Padding and dilation are applied to data and weight respectively before the computation.\\n    This operator accepts data layout specification.\\n    Semantically, the operator will convert the layout to the canonical layout\\n    (`NCHW` for data and `OIHW` for weight), perform the computation,\\n    then convert to the out_layout.\\n\\n\\n    Parameters\\n    ----------\\n    data : relay.Expr\\n        The input data to the operator.\\n\\n    weight : relay.Expr\\n        The weight expressions.\\n\\n    strides : tuple of int, optional\\n        The strides of convoltution.\\n\\n    padding : tuple of int, optional\\n        The padding of convolution on both sides of inputs before convolution.\\n\\n    dilation : tuple of int, optional\\n        Specifies the dilation rate to be used for dilated convolution.\\n\\n    groups : int, optional\\n        Number of groups for grouped convolution.\\n\\n    channels : int, optional\\n        Number of output channels of this convolution.\\n\\n    kernel_size : tuple of int, optional\\n        The spatial of the convolution kernel.\\n\\n    data_layout : str, optional\\n        Layout of the input.\\n\\n    weight_layout : str, optional\\n        Layout of the weight.\\n\\n    out_layout : str, optional\\n        Layout of the output, by default, out_layout is the same as data_layout\\n\\n    out_dtype : str, optional\\n        Specifies the output data type for mixed precision conv2d.\\n\\n    Returns\\n    -------\\n    result : relay.Expr\\n        The computed result.\\n    \"\"\"\\n    return _make.conv2d(data, weight, strides, padding, dilation,\\n                        groups, channels, kernel_size, data_layout,\\n                        weight_layout, out_layout, out_dtype)', 'def log_softmax(data, axis):\\n    r\"\"\"Computes log softmax.\\n\\n    .. math::\\n\\n        \\\\text{log_softmax}(x)_i = \\\\log \\\\frac{exp(x_i)}{\\\\sum_j exp(x_j)}\\n\\n    .. note::\\n        This operator can be optimized away for inference.\\n\\n    Parameters\\n    ----------\\n    data: relay.Expr\\n        The input data to the operator.\\n\\n    axis: int\\n        The axis to sum over when computing softmax\\n    \"\"\"\\n\\n    return _make.log_softmax(data, axis)', 'def avg_pool2d(data,\\n               pool_size=(1, 1),\\n               strides=(1, 1),\\n               padding=(0, 0),\\n               layout=\"NCHW\",\\n               ceil_mode=False,\\n               count_include_pad=False):\\n    r\"\"\"2D average pooling operator.\\n\\n    This operator takes data as input and does 2D average value calculation\\n    with in pool_size sized window by striding defined by stride\\n\\n\\n    In the default case, where the data_layout is `NCHW`\\n    a data Tensor with shape `(batch_size, in_channels, height, width)`,\\n    to produce an output Tensor with the following rule:\\n\\n    with data of shape (b, c, h, w), pool_size (kh, kw)\\n\\n    .. math::\\n\\n        \\\\mbox{out}(b, c, y, x)  = \\\\frac{1}{kh * kw} \\\\sum_{m=0}^{kh-1} \\\\sum_{n=0}^{kw-1}\\n             \\\\mbox{data}(b, c, \\\\mbox{stride}[0] * y + m, \\\\mbox{stride}[1] * x + n)\\n\\n    Padding is applied to data before the computation.\\n    ceil_mode is used to take ceil or floor while computing out shape.\\n    count_include_pad indicates including or excluding padded input values in computation.\\n    This operator accepts data layout specification.\\n\\n    Parameters\\n    ----------\\n    data : relay.Expr\\n        The input data to the operator.\\n\\n    strides : tuple of int, optional\\n        The strides of pooling.\\n\\n    padding : tuple of int, optional\\n        The padding for pooling.\\n\\n    layout : str, optional\\n        Layout of the input.\\n\\n    ceil_mode : bool, optional\\n        To enable or disable ceil while pooling.\\n\\n    count_include_pad : bool, optional\\n        To include padding to compute the average.\\n\\n    Returns\\n    -------\\n    result : relay.Expr\\n        The computed result.\\n    \"\"\"\\n    return _make.avg_pool2d(data, pool_size, strides, padding,\\n                            layout, ceil_mode, count_include_pad)', 'def global_avg_pool2d(data,\\n                      layout=\"NCHW\"):\\n    r\"\"\"2D global average pooling operator.\\n\\n    This operator takes data as input and does 2D average value calculation\\n    across each window represented by WxH.\\n\\n\\n    In the default case, where the data_layout is `NCHW`\\n    a data Tensor with shape `(batch_size, in_channels, height, width)`,\\n    to produce an output Tensor with the following rule:\\n\\n    with data of shape (b, c, h, w)\\n\\n    .. math::\\n\\n        \\\\mbox{out}(b, c, 1, 1)  = \\\\frac{1}{h * w} \\\\sum_{m=0}^{h-1} \\\\sum_{n=0}^{w-1}\\n             \\\\mbox{data}(b, c, m, n)\\n\\n    Parameters\\n    ----------\\n    data : relay.Expr\\n        The input data to the operator.\\n\\n    layout : str, optional\\n        Layout of the input.\\n\\n    Returns\\n    -------\\n    result : relay.Expr\\n        The computed result.\\n    \"\"\"\\n    return _make.global_avg_pool2d(data, layout)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_spec(field, limit=10, query=\\'\\', query_dsl=\\'\\'):\\n    \"\"\"Returns aggregation specs for a term of filtered events.\\n\\n    The aggregation spec will summarize values of an attribute\\n    whose events fall under a filter.\\n\\n    Args:\\n        field (str): this denotes the event attribute that is used\\n            for aggregation.\\n        limit (int): How many buckets to return, defaults to 10.\\n        query (str): the query field to run on all documents prior to\\n            aggregating the results.\\n        query_dsl (str): the query DSL field to run on all documents prior\\n            to aggregating the results (optional). Either a query string\\n            or a query DSL has to be present.\\n\\n    Raises:\\n        ValueError: if neither query_string or query_dsl is provided.\\n\\n    Returns:\\n        a dict value that can be used as an aggregation spec.\\n    \"\"\"\\n    if query:\\n        query_filter = {\\n            \\'bool\\': {\\n                \\'must\\': [\\n                    {\\n                        \\'query_string\\': {\\n                            \\'query\\': query\\n                        }\\n                    }\\n                ]\\n            }\\n        }\\n    elif query_dsl:\\n        query_filter = query_dsl\\n    else:\\n        raise ValueError(\\'Neither query nor query DSL provided.\\')\\n\\n\\n    return {\\n        \\'query\\': query_filter,\\n        \\'aggs\\': {\\n            \\'aggregation\\': {\\n                \\'terms\\': {\\n                    \\'field\\': field,\\n                    \\'size\\': limit\\n                }\\n            }\\n        }\\n    }', 'def chart_title(self):\\n        \"\"\"Returns a title for the chart.\"\"\"\\n        if self.field:\\n            return \\'Top filtered results for \"{0:s}\"\\'.format(self.field)\\n        return \\'Top results for an unknown field after filtering\\'', 'def run(\\n            self, field, query_string=\\'\\', query_dsl=\\'\\',\\n            supported_charts=\\'table\\', start_time=\\'\\', end_time=\\'\\', limit=10):\\n        \"\"\"Run the aggregation.\\n\\n        Args:\\n            field (str): this denotes the event attribute that is used\\n                for aggregation.\\n            query_string (str): the query field to run on all documents prior to\\n                aggregating the results.\\n            query_dsl (str): the query DSL field to run on all documents prior\\n                to aggregating the results. Either a query string or a query\\n                DSL has to be present.\\n            supported_charts: Chart type to render. Defaults to table.\\n            start_time: Optional ISO formatted date string that limits the time\\n                range for the aggregation.\\n            end_time: Optional ISO formatted date string that limits the time\\n                range for the aggregation.\\n            limit (int): How many buckets to return, defaults to 10.\\n\\n        Returns:\\n            Instance of interface.AggregationResult with aggregation result.\\n\\n        Raises:\\n            ValueError: if neither query_string or query_dsl is provided.\\n        \"\"\"\\n        if not (query_string or query_dsl):\\n            raise ValueError(\\'Both query_string and query_dsl are missing\\')\\n\\n        self.field = field\\n        formatted_field_name = self.format_field_by_type(field)\\n\\n        aggregation_spec = get_spec(\\n            field=formatted_field_name, limit=limit, query=query_string,\\n            query_dsl=query_dsl)\\n\\n        aggregation_spec = self._add_query_to_aggregation_spec(\\n            aggregation_spec, start_time=start_time, end_time=end_time)\\n\\n        # Encoding information for Vega-Lite.\\n        encoding = {\\n            \\'x\\': {\\n                \\'field\\': field,\\n                \\'type\\': \\'nominal\\',\\n                \\'sort\\': {\\n                    \\'op\\': \\'sum\\',\\n                    \\'field\\': \\'count\\',\\n                    \\'order\\': \\'descending\\'\\n                }\\n            },\\n            \\'y\\': {\\'field\\': \\'count\\', \\'type\\': \\'quantitative\\'},\\n            \\'tooltip\\': [\\n                {\\'field\\': field, \\'type\\': \\'nominal\\'},\\n                {\\'field\\': \\'count\\', \\'type\\': \\'quantitative\\'}],\\n        }\\n\\n        response = self.opensearch_aggregation(aggregation_spec)\\n        aggregations = response.get(\\'aggregations\\', {})\\n        aggregation = aggregations.get(\\'aggregation\\', {})\\n\\n        buckets = aggregation.get(\\'buckets\\', [])\\n        values = []\\n        for bucket in buckets:\\n            d = {\\n                field: bucket.get(\\'key\\', \\'N/A\\'),\\n                \\'count\\': bucket.get(\\'doc_count\\', 0)\\n            }\\n            values.append(d)\\n\\n        if query_string:\\n            extra_query_url = \\'AND {0:s}\\'.format(query_string)\\n        else:\\n            extra_query_url = \\'\\'\\n\\n        return interface.AggregationResult(\\n            encoding=encoding, values=values, chart_type=supported_charts,\\n            sketch_url=self._sketch_url, field=field,\\n            extra_query_url=extra_query_url)']}, {'features': [], 'snippets': ['def __call__(self, env, start_response):\\n        return env', \"def read(self, path):\\n        raise Exception('read called with %r' % path)\", 'def read(self, path):\\n        return False', \"def items(self, section_name):\\n            if section_name != section:\\n                raise NoSectionError(section_name)\\n            return {\\n                'memcache_servers': memcache_servers,\\n                'memcache_serialization_support':\\n                memcache_serialization_support,\\n                'memcache_max_connections': memcache_max_connections,\\n            }\", \"def get(self, section, option):\\n            if _section == section:\\n                if option == 'memcache_servers':\\n                    if _srvs == 'error':\\n                        raise NoOptionError(option, section)\\n                    return _srvs\\n                elif option == 'memcache_serialization_support':\\n                    if _sers == 'error':\\n                        raise NoOptionError(option, section)\\n                    return _sers\\n                elif option in ('memcache_max_connections',\\n                                'max_connections'):\\n                    if _maxc == 'error':\\n                        raise NoOptionError(option, section)\\n                    return _maxc\\n                else:\\n                    raise NoOptionError(option, section)\\n            else:\\n                raise NoSectionError(option)\", 'def start_response(*args):\\n    pass', 'def setUp(self):\\n        self.app = memcache.MemcacheMiddleware(FakeApp(), {})', 'def test_conf_default_read(self):\\n        with mock.patch.object(memcache, \\'ConfigParser\\', ExcConfigParser):\\n            for d in ({},\\n                      {\\'memcache_servers\\': \\'6.7.8.9:10\\'},\\n                      {\\'memcache_serialization_support\\': \\'0\\'},\\n                      {\\'memcache_max_connections\\': \\'30\\'},\\n                      {\\'memcache_servers\\': \\'6.7.8.9:10\\',\\n                       \\'memcache_serialization_support\\': \\'0\\'},\\n                      {\\'memcache_servers\\': \\'6.7.8.9:10\\',\\n                       \\'memcache_max_connections\\': \\'30\\'},\\n                      {\\'memcache_serialization_support\\': \\'0\\',\\n                       \\'memcache_max_connections\\': \\'30\\'}\\n                      ):\\n                with self.assertRaises(Exception) as catcher:\\n                    memcache.MemcacheMiddleware(FakeApp(), d)\\n                self.assertEqual(\\n                    str(catcher.exception),\\n                    \"read called with \\'/etc/swift/memcache.conf\\'\")', \"def test_conf_default(self):\\n        with mock.patch.object(memcache, 'ConfigParser', EmptyConfigParser):\\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\\n        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')\\n        self.assertEqual(app.memcache._allow_pickle, False)\\n        self.assertEqual(app.memcache._allow_unpickle, False)\\n        self.assertEqual(\\n            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)\", \"def test_conf_inline_ratelimiting(self):\\n        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):\\n            app = memcache.MemcacheMiddleware(\\n                FakeApp(),\\n                {'error_suppression_limit': '5',\\n                 'error_suppression_interval': '2.5'})\\n        self.assertEqual(app.memcache._error_limit_count, 5)\\n        self.assertEqual(app.memcache._error_limit_time, 2.5)\\n        self.assertEqual(app.memcache._error_limit_duration, 2.5)\", \"def test_conf_extra_no_section(self):\\n        with mock.patch.object(memcache, 'ConfigParser',\\n                               get_config_parser(section='foobar')):\\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\\n        self.assertEqual(app.memcache_servers, '127.0.0.1:11211')\\n        self.assertEqual(app.memcache._allow_pickle, False)\\n        self.assertEqual(app.memcache._allow_unpickle, False)\\n        self.assertEqual(\\n            app.memcache._client_cache['127.0.0.1:11211'].max_size, 2)\", \"def test_conf_inline_other_max_conn(self):\\n        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):\\n            app = memcache.MemcacheMiddleware(\\n                FakeApp(),\\n                {'memcache_servers': '6.7.8.9:10',\\n                 'memcache_serialization_support': '0',\\n                 'max_connections': '5'})\\n        self.assertEqual(app.memcache_servers, '6.7.8.9:10')\\n        self.assertEqual(app.memcache._allow_pickle, True)\\n        self.assertEqual(app.memcache._allow_unpickle, True)\\n        self.assertEqual(\\n            app.memcache._client_cache['6.7.8.9:10'].max_size, 5)\", \"def test_conf_from_extra_conf(self):\\n        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):\\n            app = memcache.MemcacheMiddleware(FakeApp(), {})\\n        self.assertEqual(app.memcache_servers, '1.2.3.4:5')\\n        self.assertEqual(app.memcache._allow_pickle, False)\\n        self.assertEqual(app.memcache._allow_unpickle, True)\\n        self.assertEqual(\\n            app.memcache._client_cache['1.2.3.4:5'].max_size, 4)\", \"def test_conf_from_inline_and_maxc_from_extra_conf(self):\\n        with mock.patch.object(memcache, 'ConfigParser', get_config_parser()):\\n            app = memcache.MemcacheMiddleware(\\n                FakeApp(),\\n                {'memcache_servers': '6.7.8.9:10',\\n                 'memcache_serialization_support': '0'})\\n        self.assertEqual(app.memcache_servers, '6.7.8.9:10')\\n        self.assertEqual(app.memcache._allow_pickle, True)\\n        self.assertEqual(app.memcache._allow_unpickle, True)\\n        self.assertEqual(\\n            app.memcache._client_cache['6.7.8.9:10'].max_size, 4)\", \"def test_filter_factory(self):\\n        factory = memcache.filter_factory({'max_connections': '3'},\\n                                          memcache_servers='10.10.10.10:10',\\n                                          memcache_serialization_support='1')\\n        thefilter = factory('myapp')\\n        self.assertEqual(thefilter.app, 'myapp')\\n        self.assertEqual(thefilter.memcache_servers, '10.10.10.10:10')\\n        self.assertEqual(thefilter.memcache._allow_pickle, False)\\n        self.assertEqual(thefilter.memcache._allow_unpickle, True)\\n        self.assertEqual(\\n            thefilter.memcache._client_cache['10.10.10.10:10'].max_size, 3)\", 'def _loadapp(self, proxy_config_path):\\n        \"\"\"\\n        Load a proxy from an app.conf to get the memcache_ring\\n\\n        :returns: the memcache_ring of the memcache middleware filter\\n        \"\"\"\\n        with mock.patch(\\'swift.proxy.server.Ring\\'):\\n            app = loadapp(proxy_config_path)\\n        memcache_ring = None\\n        while True:\\n            memcache_ring = getattr(app, \\'memcache\\', None)\\n            if memcache_ring:\\n                break\\n            app = app.app\\n        return memcache_ring', 'def test_real_config(self, tempdir):\\n        config = \"\"\"\\n        [pipeline:main]\\n        pipeline = cache proxy-server\\n\\n        [app:proxy-server]\\n        use = egg:swift#proxy\\n\\n        [filter:cache]\\n        use = egg:swift#memcache\\n        \"\"\"\\n        config_path = os.path.join(tempdir, \\'test.conf\\')\\n        with open(config_path, \\'w\\') as f:\\n            f.write(dedent(config))\\n        memcache_ring = self._loadapp(config_path)\\n        # only one server by default\\n        self.assertEqual(list(memcache_ring._client_cache.keys()),\\n                         [\\'127.0.0.1:11211\\'])\\n        # extra options\\n        self.assertEqual(memcache_ring._connect_timeout, 0.3)\\n        self.assertEqual(memcache_ring._pool_timeout, 1.0)\\n        # tries is limited to server count\\n        self.assertEqual(memcache_ring._tries, 1)\\n        self.assertEqual(memcache_ring._io_timeout, 2.0)', 'def test_real_config_with_options(self, tempdir):\\n        config = \"\"\"\\n        [pipeline:main]\\n        pipeline = cache proxy-server\\n\\n        [app:proxy-server]\\n        use = egg:swift#proxy\\n\\n        [filter:cache]\\n        use = egg:swift#memcache\\n        memcache_servers = 10.0.0.1:11211,10.0.0.2:11211,10.0.0.3:11211,\\n            10.0.0.4:11211\\n        connect_timeout = 1.0\\n        pool_timeout = 0.5\\n        tries = 4\\n        io_timeout = 1.0\\n        tls_enabled = true\\n        \"\"\"\\n        config_path = os.path.join(tempdir, \\'test.conf\\')\\n        with open(config_path, \\'w\\') as f:\\n            f.write(dedent(config))\\n        memcache_ring = self._loadapp(config_path)\\n        self.assertEqual(sorted(memcache_ring._client_cache.keys()),\\n                         [\\'10.0.0.%d:11211\\' % i for i in range(1, 5)])\\n        # extra options\\n        self.assertEqual(memcache_ring._connect_timeout, 1.0)\\n        self.assertEqual(memcache_ring._pool_timeout, 0.5)\\n        # tries is limited to server count\\n        self.assertEqual(memcache_ring._tries, 4)\\n        self.assertEqual(memcache_ring._io_timeout, 1.0)\\n        self.assertEqual(memcache_ring._error_limit_count, 10)\\n        self.assertEqual(memcache_ring._error_limit_time, 60)\\n        self.assertEqual(memcache_ring._error_limit_duration, 60)\\n        self.assertIsInstance(\\n            list(memcache_ring._client_cache.values())[0]._tls_context,\\n            ssl.SSLContext)', 'def test_real_memcache_config(self, tempdir):\\n        proxy_config = \"\"\"\\n        [DEFAULT]\\n        swift_dir = %s\\n\\n        [pipeline:main]\\n        pipeline = cache proxy-server\\n\\n        [app:proxy-server]\\n        use = egg:swift#proxy\\n\\n        [filter:cache]\\n        use = egg:swift#memcache\\n        connect_timeout = 1.0\\n        \"\"\" % tempdir\\n        proxy_config_path = os.path.join(tempdir, \\'test.conf\\')\\n        with open(proxy_config_path, \\'w\\') as f:\\n            f.write(dedent(proxy_config))\\n\\n        memcache_config = \"\"\"\\n        [memcache]\\n        memcache_servers = 10.0.0.1:11211,10.0.0.2:11211,10.0.0.3:11211,\\n            10.0.0.4:11211\\n        connect_timeout = 0.5\\n        io_timeout = 1.0\\n        error_suppression_limit = 0\\n        error_suppression_interval = 1.5\\n        \"\"\"\\n        memcache_config_path = os.path.join(tempdir, \\'memcache.conf\\')\\n        with open(memcache_config_path, \\'w\\') as f:\\n            f.write(dedent(memcache_config))\\n        memcache_ring = self._loadapp(proxy_config_path)\\n        self.assertEqual(sorted(memcache_ring._client_cache.keys()),\\n                         [\\'10.0.0.%d:11211\\' % i for i in range(1, 5)])\\n        # proxy option takes precedence\\n        self.assertEqual(memcache_ring._connect_timeout, 1.0)\\n        # default tries are not limited by servers\\n        self.assertEqual(memcache_ring._tries, 3)\\n        # memcache conf options are defaults\\n        self.assertEqual(memcache_ring._io_timeout, 1.0)\\n        self.assertEqual(memcache_ring._error_limit_count, 0)\\n        self.assertEqual(memcache_ring._error_limit_time, 1.5)\\n        self.assertEqual(memcache_ring._error_limit_duration, 1.5)']}, {'features': [], 'snippets': [\"def generate_type(type_id, is_public):\\n    return {\\n        'id': type_id,\\n        'name': u'test',\\n        'deleted': False,\\n        'created_at': datetime.datetime(2012, 1, 1, 1, 1, 1, 1),\\n        'updated_at': None,\\n        'deleted_at': None,\\n        'is_public': bool(is_public)\\n    }\", \"def fake_volume_type_get(context, id, inactive=False, expected_fields=None):\\n    vol = VOLUME_TYPES[id]\\n    if expected_fields and 'projects' in expected_fields:\\n        vol['projects'] = [a['project_id']\\n                           for a in ACCESS_LIST if a['volume_type_id'] == id]\\n    return vol\", \"def fake_volume_type_get_all(context, inactive=False, filters=None,\\n                             marker=None, limit=None, sort_keys=None,\\n                             sort_dirs=None, offset=None, list_result=False):\\n    if filters is None or filters['is_public'] is None:\\n        if list_result:\\n            return list(VOLUME_TYPES.values())\\n        return VOLUME_TYPES\\n    res = {}\\n    for k, v in VOLUME_TYPES.items():\\n        if filters['is_public'] and _has_type_access(k, context.project_id):\\n            res.update({k: v})\\n            continue\\n        if v['is_public'] == filters['is_public']:\\n            res.update({k: v})\\n    if list_result:\\n        return list(res.values())\\n    return res\", 'def attach(self, **kwargs):\\n        pass', 'def cached_resource_by_id(self, resource_id, name=None):\\n        return VOLUME_TYPES[resource_id]', \"def setUp(self):\\n        super(VolumeTypeAccessTest, self).setUp()\\n        self.type_controller_v2 = types_api_v2.VolumeTypesController()\\n        self.type_access_controller = type_access.VolumeTypeAccessController()\\n        self.type_action_controller = type_access.VolumeTypeActionController()\\n        self.req = FakeRequest()\\n        self.context = self.req.environ['cinder.context']\\n        self.stubs.Set(db, 'volume_type_get',\\n                       fake_volume_type_get)\\n        self.stubs.Set(db, 'volume_type_get_all',\\n                       fake_volume_type_get_all)\", 'def test_list_type_access_public(self):\\n        \"\"\"Querying os-volume-type-access on public type should return 404.\"\"\"\\n        req = fakes.HTTPRequest.blank(\\'/v2/%s/types/os-volume-type-access\\' %\\n                                      fake.PROJECT_ID,\\n                                      use_admin_context=True)\\n        self.assertRaises(webob.exc.HTTPNotFound,\\n                          self.type_access_controller.index,\\n                          req, fake.VOLUME_TYPE2_ID)', \"def test_list_with_no_context(self):\\n        req = fakes.HTTPRequest.blank('/v2/flavors/%s/flavors' %\\n                                      fake.PROJECT_ID)\\n\\n        def fake_authorize(context, target=None, action=None):\\n            raise exception.PolicyNotAuthorized(action='index')\\n        self.stubs.Set(type_access, 'authorize', fake_authorize)\\n\\n        self.assertRaises(exception.PolicyNotAuthorized,\\n                          self.type_access_controller.index,\\n                          req, fake.PROJECT_ID)\", \"def test_list_type_with_admin_default_proj2(self):\\n        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},\\n                                     {'id': fake.VOLUME_TYPE2_ID},\\n                                     {'id': fake.VOLUME_TYPE3_ID}]}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types' % PROJ2_UUID,\\n                                      use_admin_context=True)\\n        req.environ['cinder.context'].project_id = PROJ2_UUID\\n        result = self.type_controller_v2.index(req)\\n        self.assertVolumeTypeListEqual(expected['volume_types'],\\n                                       result['volume_types'])\", \"def test_list_type_with_admin_ispublic_false(self):\\n        expected = {'volume_types': [{'id': fake.VOLUME_TYPE3_ID},\\n                                     {'id': fake.VOLUME_TYPE4_ID}]}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types?is_public=false' %\\n                                      fake.PROJECT_ID,\\n                                      use_admin_context=True)\\n        result = self.type_controller_v2.index(req)\\n        self.assertVolumeTypeListEqual(expected['volume_types'],\\n                                       result['volume_types'])\", \"def test_list_type_with_admin_ispublic_none(self):\\n        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},\\n                                     {'id': fake.VOLUME_TYPE2_ID},\\n                                     {'id': fake.VOLUME_TYPE3_ID},\\n                                     {'id': fake.VOLUME_TYPE4_ID}]}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types?is_public=none' %\\n                                      fake.PROJECT_ID,\\n                                      use_admin_context=True)\\n        result = self.type_controller_v2.index(req)\\n        self.assertVolumeTypeListEqual(expected['volume_types'],\\n                                       result['volume_types'])\", \"def test_list_type_with_no_admin_ispublic_true(self):\\n        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},\\n                                     {'id': fake.VOLUME_TYPE2_ID}]}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types?is_public=true' %\\n                                      fake.PROJECT_ID,\\n                                      use_admin_context=False)\\n        result = self.type_controller_v2.index(req)\\n        self.assertVolumeTypeListEqual(expected['volume_types'],\\n                                       result['volume_types'])\", \"def test_list_type_with_no_admin_ispublic_none(self):\\n        expected = {'volume_types': [{'id': fake.VOLUME_TYPE_ID},\\n                                     {'id': fake.VOLUME_TYPE2_ID}]}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types?is_public=none' %\\n                                      fake.PROJECT_ID,\\n                                      use_admin_context=False)\\n        result = self.type_controller_v2.index(req)\\n        self.assertVolumeTypeListEqual(expected['volume_types'],\\n                                       result['volume_types'])\", \"def test_detail(self):\\n        resp = FakeResponse()\\n        self.type_action_controller.detail(self.req, resp)\\n        self.assertEqual(\\n            [{'id': fake.VOLUME_TYPE_ID,\\n              'os-volume-type-access:is_public': True},\\n             {'id': fake.VOLUME_TYPE3_ID,\\n              'os-volume-type-access:is_public': False}],\\n            resp.obj['volume_types'])\", 'def test_add_project_access(self):\\n        def stub_add_volume_type_access(context, type_id, project_id):\\n            self.assertEqual(fake.VOLUME_TYPE4_ID, type_id, \"type_id\")\\n            self.assertEqual(PROJ2_UUID, project_id, \"project_id\")\\n        self.stubs.Set(db, \\'volume_type_access_add\\',\\n                       stub_add_volume_type_access)\\n        body = {\\'addProjectAccess\\': {\\'project\\': PROJ2_UUID}}\\n        req = fakes.HTTPRequest.blank(\\'/v2/%s/types/%s/action\\' % (\\n            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID),\\n            use_admin_context=True)\\n        result = self.type_action_controller._addProjectAccess(\\n            req, fake.VOLUME_TYPE4_ID, body)\\n        self.assertEqual(202, result.status_code)', \"def test_add_project_access_with_already_added_access(self):\\n        def stub_add_volume_type_access(context, type_id, project_id):\\n            raise exception.VolumeTypeAccessExists(volume_type_id=type_id,\\n                                                   project_id=project_id)\\n        self.stubs.Set(db, 'volume_type_access_add',\\n                       stub_add_volume_type_access)\\n        body = {'addProjectAccess': {'project': PROJ2_UUID}}\\n        req = fakes.HTTPRequest.blank('/v2/%s/types/%s/action' % (\\n            fake.PROJECT_ID, fake.VOLUME_TYPE3_ID), use_admin_context=True)\\n        self.assertRaises(webob.exc.HTTPConflict,\\n                          self.type_action_controller._addProjectAccess,\\n                          req, fake.VOLUME_TYPE3_ID, body)\", 'def stub_remove_volume_type_access(context, type_id, project_id):\\n            raise exception.VolumeTypeAccessNotFound(volume_type_id=type_id,\\n                                                     project_id=project_id)']}, {'features': [], 'snippets': ['def create_colors_list():\\n    colors_list = []\\n    for color in plt.cm.tab10(np.linspace(0, 1, 10))[:-1]:\\n        colors_list.append(tuple(color))\\n    colors_list.append(\"black\")\\n    for color in plt.cm.Set2(np.linspace(0, 1, 8)):\\n        colors_list.append(tuple(color))\\n    for color in plt.cm.Set3(np.linspace(0, 1, 12)):\\n        colors_list.append(tuple(color))\\n    return colors_list', \"def plot_precision_vs_bin_size(pd_bins, output_dir):\\n    pd_plot = pd_bins[pd_bins[utils_labels.TOOL] != utils_labels.GS]\\n\\n    for tool_label, pd_tool in pd_plot.groupby(utils_labels.TOOL):\\n        fig, axs = plt.subplots(figsize=(5, 4.5))\\n        axs.scatter(np.log(pd_tool['total_length']), pd_tool['precision_bp'], marker='o')\\n\\n        axs.set_xlim([None, np.log(pd_tool['total_length'].max())])\\n        axs.set_ylim([0.0, 1.0])\\n        axs.set_title(tool_label, fontsize=12)\\n\\n        plt.ylabel('Purity per bin (%)', fontsize=12)\\n        plt.xlabel('Bin size [log(# bp)]', fontsize=12)\\n\\n        fig.savefig(os.path.join(output_dir, 'genome', tool_label, 'purity_vs_bin_size.png'), dpi=200, format='png', bbox_inches='tight')\\n        plt.close(fig)\", \"def get_pd_genomes_recall(sample_id_to_queries_list):\\n    pd_genomes_recall = pd.DataFrame()\\n    for sample_id in sample_id_to_queries_list:\\n        for query in sample_id_to_queries_list[sample_id]:\\n            if not isinstance(query, binning_classes.GenomeQuery):\\n                continue\\n            recall_df = query.recall_df_cami1[['genome_id', 'recall_bp']].copy()\\n            recall_df[utils_labels.TOOL] = query.label\\n            recall_df['sample_id'] = sample_id\\n            recall_df = recall_df.reset_index().set_index(['sample_id', utils_labels.TOOL])\\n            pd_genomes_recall = pd.concat([pd_genomes_recall, recall_df])\\n    return pd_genomes_recall\", 'def plot_heatmap(df_confusion, sample_id, output_dir, label, separate_bar=False, log_scale=False):\\n    if log_scale:\\n        df_confusion = df_confusion.apply(np.log10, inplace=True).replace(-np.inf, 0)\\n    fig, axs = plt.subplots(figsize=(10, 8))\\n    fontsize = 20\\n\\n    # replace columns and rows labels by numbers\\n    d = {value: key for (key, value) in enumerate(df_confusion.columns.tolist(), 1)}\\n    df_confusion = df_confusion.rename(index=str, columns=d)\\n    df_confusion.index = range(1, len(df_confusion) + 1)\\n\\n    xticklabels = int(round(df_confusion.shape[1] / 10, -1))\\n    yticklabels = int(round(df_confusion.shape[0] / 10, -1))\\n    sns_plot = sns.heatmap(df_confusion, ax=axs, annot=False, cmap=\"YlGnBu_r\", xticklabels=xticklabels, yticklabels=yticklabels, cbar=False, rasterized=True)\\n\\n    # sns_plot = sns.heatmap(df_confusion, ax=axs, annot=False, cmap=\"YlGnBu_r\", xticklabels=False, yticklabels=False, cbar=True, rasterized=True)\\n    sns_plot.set_xlabel(\"Genomes\", fontsize=fontsize)\\n    sns_plot.set_ylabel(\"Predicted bins\", fontsize=fontsize)\\n    plt.yticks(fontsize=12, rotation=0)\\n    plt.xticks(fontsize=12)\\n\\n    mappable = sns_plot.get_children()[0]\\n\\n    cbar_ax = fig.add_axes([.915, .11, .017, .77])\\n    cbar = plt.colorbar(mappable, ax=axs, cax=cbar_ax, orientation=\\'vertical\\')\\n    if log_scale:\\n        cbar.set_label(fontsize=fontsize, label=\\'log$_{10}$(# bp)\\')\\n    else:\\n        fmt = lambda x, pos: \\'{:.0f}\\'.format(x / 1000000)\\n        cbar = plt.colorbar(mappable, ax=axs, cax=cbar_ax, orientation=\\'vertical\\', format=ticker.FuncFormatter(fmt))\\n        cbar.set_label(fontsize=fontsize, label=\\'Millions of base pairs\\')\\n\\n    cbar.ax.tick_params(labelsize=fontsize)\\n    cbar.outline.set_edgecolor(None)\\n\\n    axs.set_title(label, fontsize=fontsize, pad=10)\\n\\n    axs.set_ylim([len(df_confusion), 0])\\n\\n    # plt.yticks(fontsize=14, rotation=0)\\n    # plt.xticks(fontsize=14)\\n\\n    output_dir = os.path.join(output_dir, \\'genome\\', label)\\n\\n    fig.savefig(os.path.join(output_dir, \\'heatmap_\\' + sample_id + \\'.pdf\\'), dpi=100, format=\\'pdf\\', bbox_inches=\\'tight\\')\\n    fig.savefig(os.path.join(output_dir, \\'heatmap_\\' + sample_id + \\'.png\\'), dpi=200, format=\\'png\\', bbox_inches=\\'tight\\')\\n    plt.close(fig)\\n\\n    if not separate_bar:\\n        return\\n\\n    # create separate figure for bar\\n    fig = plt.figure(figsize=(6, 6))\\n    mappable = sns_plot.get_children()[0]\\n    fmt = lambda x, pos: \\'{:.0f}\\'.format(x / 1000000)\\n\\n    cbar = plt.colorbar(mappable, orientation=\\'vertical\\', label=\\'[millions of base pairs]\\', format=ticker.FuncFormatter(fmt))\\n\\n    text = cbar.ax.yaxis.label\\n    font = matplotlib.font_manager.FontProperties(size=16)\\n    text.set_font_properties(font)\\n\\n    cbar.outline.set_visible(False)\\n    cbar.ax.tick_params(labelsize=14)\\n\\n    # store separate bar figure\\n    plt.gca().set_visible(False)\\n    fig.savefig(os.path.join(output_dir, \\'heatmap_bar.pdf\\'), dpi=100, format=\\'pdf\\', bbox_inches=\\'tight\\')\\n\\n    plt.close(fig)', 'def plot_summary(color_indices, df_results, labels, output_dir, rank, plot_type, file_name, xlabel, ylabel):\\n    available_tools = df_results[utils_labels.TOOL].unique()\\n    tools = [tool for tool in labels if tool in available_tools]\\n\\n    colors_list = create_colors_list()\\n    if color_indices:\\n        colors_list = [colors_list[i] for i in color_indices]\\n    df_mean = df_results.groupby(utils_labels.TOOL).mean().reindex(tools)\\n\\n    binning_type = df_results[utils_labels.BINNING_TYPE].iloc[0]\\n\\n    if len(df_mean) > len(colors_list):\\n        raise RuntimeError(\"Plot only supports 29 colors\")\\n\\n    fig, axs = plt.subplots(figsize=(5, 4.5))\\n\\n    # force axes to be from 0 to 100%\\n    axs.set_xlim([0.0, 1.0])\\n    axs.set_ylim([0.0, 1.0])\\n\\n    if plot_type == \\'e\\':\\n        for i, (tool, df_row) in enumerate(df_mean.iterrows()):\\n            axs.errorbar(df_row[utils_labels.AVG_PRECISION_BP], df_row[utils_labels.AVG_RECALL_BP], xerr=df_row[\\'avg_precision_bp_var\\'], yerr=df_row[\\'avg_recall_bp_var\\'],\\n                         fmt=\\'o\\',\\n                         ecolor=colors_list[i],\\n                         mec=colors_list[i],\\n                         mfc=colors_list[i],\\n                         capsize=3,\\n                         markersize=8)\\n    if plot_type == \\'f\\':\\n        for i, (tool, df_row) in enumerate(df_mean.iterrows()):\\n            axs.errorbar(df_row[utils_labels.AVG_PRECISION_SEQ], df_row[utils_labels.AVG_RECALL_SEQ], xerr=df_row[utils_labels.AVG_PRECISION_SEQ_SEM], yerr=df_row[utils_labels.AVG_RECALL_SEQ_SEM],\\n                         fmt=\\'o\\',\\n                         ecolor=colors_list[i],\\n                         mec=colors_list[i],\\n                         mfc=colors_list[i],\\n                         capsize=3,\\n                         markersize=8)\\n    if plot_type == \\'w\\':\\n        for i, (tool, df_row) in enumerate(df_mean.iterrows()):\\n            axs.plot(df_row[utils_labels.PRECISION_PER_BP], df_row[utils_labels.RECALL_PER_BP], marker=\\'o\\', color=colors_list[i], markersize=10)\\n    if plot_type == \\'x\\':\\n        for i, (tool, df_row) in enumerate(df_mean.iterrows()):\\n            axs.plot(df_row[utils_labels.PRECISION_PER_SEQ], df_row[utils_labels.RECALL_PER_SEQ], marker=\\'o\\', color=colors_list[i], markersize=10)\\n    elif plot_type == \\'p\\':\\n        for i, (tool, df_row) in enumerate(df_mean.iterrows()):\\n            axs.plot(df_row[utils_labels.ARI_BY_BP], df_row[utils_labels.PERCENTAGE_ASSIGNED_BPS], marker=\\'o\\', color=colors_list[i], markersize=10)\\n\\n    # turn on grid\\n    # axs.minorticks_on()\\n    axs.grid(which=\\'major\\', linestyle=\\':\\', linewidth=\\'0.5\\')\\n    # axs.grid(which=\\'minor\\', linestyle=\\':\\', linewidth=\\'0.5\\')\\n\\n    # transform plot_labels to percentages\\n    if plot_type != \\'p\\':\\n        vals = axs.get_xticks()\\n        axs.set_xticklabels([\\'{:3.0f}\\'.format(x * 100) for x in vals], fontsize=11)\\n    else:\\n        axs.tick_params(axis=\\'x\\', labelsize=12)\\n    vals = axs.get_yticks()\\n    axs.set_yticklabels([\\'{:3.0f}\\'.format(x * 100) for x in vals], fontsize=11)\\n\\n    if rank:\\n        file_name = rank + \\'_\\' + file_name\\n        plt.title(rank)\\n        ylabel = ylabel.replace(\\'genome\\', \\'taxon\\')\\n\\n    plt.xlabel(xlabel, fontsize=13)\\n    plt.ylabel(ylabel, fontsize=13)\\n    plt.tight_layout()\\n    fig.savefig(os.path.join(output_dir, binning_type, file_name + \\'.eps\\'), dpi=100, format=\\'eps\\', bbox_inches=\\'tight\\')\\n\\n    colors_iter = iter(colors_list)\\n    circles = []\\n    for x in range(len(df_mean)):\\n        circles.append(Line2D([], [], markeredgewidth=0.0, linestyle=\"None\", marker=\"o\", markersize=11, markerfacecolor=next(colors_iter)))\\n    lgd = plt.legend(circles, tools, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., handlelength=0, frameon=False, fontsize=12)\\n\\n    fig.savefig(os.path.join(output_dir, binning_type, file_name + \\'.pdf\\'), dpi=100, format=\\'pdf\\', bbox_extra_artists=(lgd,), bbox_inches=\\'tight\\')\\n    fig.savefig(os.path.join(output_dir, binning_type, file_name + \\'.png\\'), dpi=200, format=\\'png\\', bbox_extra_artists=(lgd,), bbox_inches=\\'tight\\')\\n    plt.close(fig)', \"def plot_precision_recall(colors, summary_per_query, labels, output_dir, rank=None):\\n    plot_summary(colors,\\n                 summary_per_query,\\n                 labels,\\n                 output_dir,\\n                 rank,\\n                 'w',\\n                 'purity_recall_bp',\\n                 'Purity for sample (%)',\\n                 'Completeness for sample (%)')\\n    plot_summary(colors,\\n                 summary_per_query,\\n                 labels,\\n                 output_dir,\\n                 rank,\\n                 'x',\\n                 'purity_completeness_seq',\\n                 'Purity for sample (%)',\\n                 'Completeness for sample (%)')\", 'def plot_taxonomic_results(df_summary_t, metrics_list, errors_list, file_name, output_dir):\\n    colors_list = [\"#006cba\", \"#008000\", \"#ba9e00\", \"red\"]\\n\\n    for tool, pd_results in df_summary_t.groupby(utils_labels.TOOL):\\n        dict_metric_list = []\\n        for metric in metrics_list:\\n            rank_to_metric = OrderedDict([(k, .0) for k in load_ncbi_taxinfo.RANKS])\\n            dict_metric_list.append(rank_to_metric)\\n        dict_error_list = []\\n        for error in errors_list:\\n            rank_to_metric_error = OrderedDict([(k, .0) for k in load_ncbi_taxinfo.RANKS])\\n            dict_error_list.append(rank_to_metric_error)\\n\\n        for index, row in pd_results.iterrows():\\n            for rank_to_metric, metric in zip(dict_metric_list, metrics_list):\\n                rank_to_metric[row[utils_labels.RANK]] = .0 if np.isnan(row[metric]) else row[metric]\\n            for rank_to_metric_error, error in zip(dict_error_list, errors_list):\\n                rank_to_metric_error[row[utils_labels.RANK]] = .0 if np.isnan(row[error]) else row[error]\\n\\n        fig, axs = plt.subplots(figsize=(6, 5))\\n\\n        # force axes to be from 0 to 100%\\n        axs.set_xlim([0, 7])\\n        axs.set_ylim([0.0, 1.0])\\n        x_values = range(len(load_ncbi_taxinfo.RANKS))\\n\\n        y_values_list = []\\n        for rank_to_metric, color in zip(dict_metric_list, colors_list):\\n            y_values = list(rank_to_metric.values())\\n            axs.plot(x_values, y_values, color=color)\\n            y_values_list.append(y_values)\\n\\n        for rank_to_metric_error, y_values, color in zip(dict_error_list, y_values_list, colors_list):\\n            sem = list(rank_to_metric_error.values())\\n            plt.fill_between(x_values, np.subtract(y_values, sem).tolist(), np.add(y_values, sem).tolist(), color=color, alpha=0.5)\\n\\n        plt.xticks(x_values, load_ncbi_taxinfo.RANKS, rotation=\\'vertical\\')\\n\\n        vals = axs.get_yticks()\\n        axs.set_yticklabels([\\'{:3.0f}%\\'.format(x * 100) for x in vals])\\n\\n        lgd = plt.legend(metrics_list, loc=1, borderaxespad=0., handlelength=2, frameon=False)\\n\\n        plt.tight_layout()\\n        fig.savefig(os.path.join(output_dir, \\'taxonomic\\', tool, file_name + \\'.png\\'), dpi=100, format=\\'png\\', bbox_extra_artists=(lgd,), bbox_inches=\\'tight\\')\\n        fig.savefig(os.path.join(output_dir, \\'taxonomic\\', tool, file_name + \\'.pdf\\'), dpi=100, format=\\'pdf\\', bbox_extra_artists=(lgd,), bbox_inches=\\'tight\\')\\n        plt.close(fig)', \"def create_completeness_minus_contamination_column(pd_tool_bins):\\n    pd_tool_bins['newcolumn'] = pd_tool_bins['recall_bp'] + pd_tool_bins['precision_bp'] - 1\", \"def get_number_of_hq_bins(tools, pd_bins):\\n    pd_counts = pd.DataFrame()\\n    pd_bins_copy = pd_bins[[utils_labels.TOOL, 'precision_bp', 'recall_bp']].copy().dropna(subset=['precision_bp'])\\n    for tool in tools:\\n        pd_tool_bins = pd_bins_copy[pd_bins_copy[utils_labels.TOOL] == tool]\\n        x50 = pd_tool_bins[(pd_tool_bins['recall_bp'] > .5) & (pd_tool_bins['precision_bp'] > .9)].shape[0]\\n        x70 = pd_tool_bins[(pd_tool_bins['recall_bp'] > .7) & (pd_tool_bins['precision_bp'] > .9)].shape[0]\\n        x90 = pd_tool_bins[(pd_tool_bins['recall_bp'] > .9) & (pd_tool_bins['precision_bp'] > .9)].shape[0]\\n        pd_tool_counts = pd.DataFrame([[x90, x70, x50]], columns=['>90%', '>70%', '>50%'], index=[tool])\\n        pd_counts = pd_counts.append(pd_tool_counts)\\n    return pd_counts\"]}, {'features': [], 'snippets': [\"def __init__(self, **kwargs):\\n        env = Environment(loader=PackageLoader('hotzenplotz.worker','templates'))\\n        self.template =  env.get_template('cron')\\n        self.dir_path = None\", \"def do_config(self, request):\\n        try:\\n            self._validate_request(request)\\n        except exception.BadRequest as e:\\n            LOG.warn('Bad request: %s' % e)\\n            raise exception.CronConfigureError(explanation=str(e))\\n\\n        cmd = request['method']\\n        msg = request['cron_resource']\\n\\n        if cmd == 'create_cron':\\n            try:\\n                self._create_cron(msg)\\n            except exception.CronCreateError as e:\\n                raise exception.CronConfigureError(explanation=str(e))\\n\\n        elif cmd == 'delete_cron':\\n            try:\\n                self._delete_cron(msg)\\n            except exception.HaproxyDeleteError as e:\\n                raise exception.CronConfigureError(explanation=str(e))\\n\\n        elif cmd == 'update_cron':\\n            try:\\n                self._update_cron(msg)\\n            except exception.CronUpdateError as e:\\n                raise exception.CronConfigureError(explanation=str(e))\", 'def _delete_cron(self, msg):\\n        LOG.debug(\"Deleting cron  for NAME:%s USER: %s PROJECT:%s\" %\\n                  (msg[\\'id\\'], msg[\\'user_id\\'], msg[\\'project_id\\']))\\n        try:\\n            new_cfg_path = self._create_lb_deleted_haproxy_cfg(msg)\\n        except exception.HaproxyLBNotExists as e:\\n            LOG.warn(\\'%s\\', e)\\n            return\\n            ##raise exception.HaproxyDeleteError(explanation=str(e))\\n\\n        try:\\n            self._test_haproxy_config(new_cfg_path)\\n        except exception.ProcessExecutionError as e:\\n            raise exception.HaproxyDeleteError(explanation=str(e))\\n\\n        rc, backup_path = self._backup_original_cfg()\\n        if rc != 0:\\n            raise exception.HaproxyDeleteError(explanation=backup_path)\\n\\n        rc, strerror = self._replace_original_cfg_with_new(new_cfg_path)\\n        if rc != 0:\\n            raise exception.HaproxyDeleteError(explanation=strerror)\\n\\n        if self._reload_haproxy_cfg(backup_path) != 0:\\n            e = \\'Failed to reload haproxy\\'\\n            raise exception.HaproxyDeleteError(explanation=str(e))\\n\\n        LOG.debug(\"Deleted the new load balancer successfully\")', 'def _validate_request(self, request):\\n        validate.check_tcp_request(request)', 'def _is_lb_in_use(self, lb_name,\\n                      base_cfg_path=\\'/etc/haproxy/haproxy.cfg\\'):\\n        with open(base_cfg_path) as cfg:\\n            lines = cfg.readlines()\\n\\n        try:\\n            in_use_lb_name = [line.split()[1] for line in lines\\n                              if line.startswith(\\'listen\\')]\\n        except IndexError:\\n            LOG.error(\"No item was found after listen directive,\"\\n                      \"is the haproxy configuraion file valid?\")\\n            raise\\n\\n        return lb_name in in_use_lb_name']}, {'features': [], 'snippets': ['def setup_class(cls):\\n        cls.test_patterns = MustHavePatterns(Successor)']}, {'features': [], 'snippets': [\"def __init__(self):\\n        self.data = {}\\n        # Data format: {'XXCiro|BNC': {'id': 123456, 'nick': 'XXCiro', 'level': 49, 'strength': 532.5, 'rank_points': 1233354, 'citizenship': 'Argentina'}}\", 'def data_loader(self):\\n        self.load_data()\\n\\n        self.data_saver_th = threading.Thread(target=self.data_saver)\\n        self.data_saver_th.daemon = True\\n        self.data_saver_th.start()\\n\\n        self.data_updater_th = threading.Thread(target=self.data_updater)\\n        self.data_updater_th.daemon = True\\n        self.data_updater_th.start()', 'def data_saver(self):\\n        while self.run:\\n            self.save_data()\\n\\n            time.sleep(60)', 'def data_updater(self):\\n        while self.run:\\n            for irc_nick in self.data:\\n                self.update_data(irc_nick)\\n                time.sleep(30)\\n\\n            time.sleep(600)', \"def load_data(self):\\n        try:\\n            f = open('data/er_nick-data.csv', 'rt')\\n            reader = csv.reader(f)\\n            for nick_irc,id,nick_er,level,strength,rank_points,citizenship in reader:\\n                self.data[nick_irc] = {'id': int(id), 'nick': nick_er, 'level': int(level), 'strength': float(strength), 'rank_points': int(rank_points), 'citizenship': citizenship}\\n            f.close()\\n        except:\\n            pass\", \"def save_data(self):\\n        try:\\n            f = open('data/er_nick-data.csv', 'wt')\\n            writer = csv.writer(f)\\n            for u in self.data:\\n                writer.writerow([u, self.data[u]['id'], self.data[u]['nick'], self.data[u]['level'], self.data[u]['strength'], self.data[u]['rank_points'], self.data[u]['citizenship']])\\n            f.close()\\n        except:\\n            pass\", 'def update_data(self, irc_nick):\\n        try:\\n            id = self.data[irc_nick][\\'id\\']\\n\\n            c = urlopen(\\'http://www.erepublik.com/es/citizen/profile/%d\\' % id)\\n            page = c.read()\\n            c.close()\\n\\n            self.data[irc_nick][\\'nick\\'] = re.search(\\'<meta name=\"title\" content=\"(.+?) - Ciudadano del Nuevo Mundo\" \\\\/>\\', page.decode(\\'utf-8\\')).group(1)\\n            self.data[irc_nick][\\'level\\'] = int(re.search(\\'<strong class=\"citizen_level\">(.+?)<\\\\/strong>\\', page.decode(\\'utf-8\\'), re.DOTALL).group(1))\\n            self.data[irc_nick][\\'strength\\'] = float(re.search(\\'<span class=\"military_box_info mb_bottom\">(.+?)</span>\\', page.decode(\\'utf-8\\'), re.DOTALL).group(1).strip(\\'\\\\r\\\\n\\\\t \\').replace(\\',\\',\\'\\'))\\n            self.data[irc_nick][\\'rank_points\\'] = int(re.search(\\'<span class=\"rank_numbers\">(.+?) \\\\/\\', page.decode(\\'utf-8\\'), re.DOTALL).group(1).replace(\\',\\',\\'\\'))\\n            self.data[irc_nick][\\'citizenship\\'] = re.search(\\'<a href=\"http\\\\:\\\\/\\\\/www.erepublik.com\\\\/es\\\\/country\\\\/society\\\\/([^ \\\\t\\\\n\\\\x0B\\\\f\\\\r]+?)\">\\', page.decode(\\'utf-8\\')).group(1)\\n        except:\\n            pass', \"def reg_nick_write(self, nick, id):\\n        if(nick.lower() in self.data.keys()):\\n            self.data[nick.lower()]['id'] = int(id)\\n        else:\\n            self.data[nick.lower()] = {'id': int(id), 'nick': nick, 'level': 1, 'strength': 0, 'rank_points': 0, 'citizenship': ''}\\n\\n        self.update_data(nick.lower())\", \"def get_id(self, nick):\\n        return self.data[nick.lower()]['id']\", \"def get_level(self, nick):\\n        return self.data[nick.lower()]['level']\", \"def get_strength(self, nick):\\n        return self.data[nick.lower()]['strength']\", \"def get_rank_points(self, nick):\\n        return self.data[nick.lower()]['rank_points']\", \"def get_citizenship(self, nick):\\n        return self.data[nick.lower()]['citizenship']\", \"def get_nick(self, nick):\\n        return self.data[nick.lower()]['nick']\", 'def calculate_rank_name(self, rank_points):\\n        index = 0\\n\\n        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:\\n            if(self.rank_to_pos.index(k) > index):\\n                index = self.rank_to_pos.index(k)\\n\\n        return self.rank_to_pos[index]']}, {'features': [], 'snippets': [\"def create_configuration():\\n    configuration = mox.MockObject(conf.Configuration)\\n    configuration.append_config_values(mox.IgnoreArg())\\n    configuration.nfs_mount_point_base = '/mnt/test'\\n    configuration.nfs_mount_options = None\\n    return configuration\", 'def __init__(self, size=0):\\n        self.size = size\\n        self.id = hash(self)\\n        self.name = None', 'def __setitem__(self, key, val):\\n        self.__dict__[key] = val', 'def __init__(self, volume_size=0):\\n        self.volume_name = None\\n        self.name = None\\n        self.volume_id = None\\n        self.volume_size = volume_size\\n        self.user_id = None\\n        self.status = None', 'def __init__(self, status):\\n        \"\"\"Initialize FakeResponse.\\n\\n        :param status: Either \\'failed\\' or \\'passed\\'\\n        \"\"\"\\n        self.Status = status\\n\\n        if status == \\'failed\\':\\n            self.Reason = \\'Sample error\\'', 'def setUp(self):\\n        super(NetappDirectCmodeNfsDriverTestCase, self).setUp()\\n        self._custom_setup()', 'def test_create_volume_from_snapshot(self):\\n        \"\"\"Tests volume creation from snapshot.\"\"\"\\n        drv = self._driver\\n        mox = self.mox\\n        volume = FakeVolume(1)\\n        snapshot = FakeSnapshot(1)\\n\\n        location = \\'127.0.0.1:/nfs\\'\\n        expected_result = {\\'provider_location\\': location}\\n        mox.StubOutWithMock(drv, \\'_clone_volume\\')\\n        mox.StubOutWithMock(drv, \\'_get_volume_location\\')\\n        mox.StubOutWithMock(drv, \\'local_path\\')\\n        mox.StubOutWithMock(drv, \\'_discover_file_till_timeout\\')\\n        mox.StubOutWithMock(drv, \\'_set_rw_permissions_for_all\\')\\n        drv._clone_volume(IgnoreArg(), IgnoreArg(), IgnoreArg())\\n        drv._get_volume_location(IgnoreArg()).AndReturn(location)\\n        drv.local_path(IgnoreArg()).AndReturn(\\'/mnt\\')\\n        drv._discover_file_till_timeout(IgnoreArg()).AndReturn(True)\\n        drv._set_rw_permissions_for_all(IgnoreArg())\\n\\n        mox.ReplayAll()\\n\\n        loc = drv.create_volume_from_snapshot(volume, snapshot)\\n\\n        self.assertEqual(loc, expected_result)\\n\\n        mox.VerifyAll()', 'def test_delete_existing_snapshot(self):\\n        drv = self._driver\\n        mox = self._prepare_delete_snapshot_mock(True)\\n\\n        drv.delete_snapshot(FakeSnapshot())\\n\\n        mox.VerifyAll()', \"def _custom_setup(self):\\n        kwargs = {}\\n        kwargs['netapp_mode'] = 'proxy'\\n        kwargs['configuration'] = create_configuration()\\n        self._driver = netapp_nfs.NetAppDirectCmodeNfsDriver(**kwargs)\", \"def test_do_setup(self):\\n        mox = self.mox\\n        drv = self._driver\\n\\n        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')\\n        mox.StubOutWithMock(drv, '_get_client')\\n        mox.StubOutWithMock(drv, '_do_custom_setup')\\n\\n        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())\\n        drv._get_client()\\n        drv._do_custom_setup(IgnoreArg())\\n\\n        mox.ReplayAll()\\n\\n        drv.do_setup(IsA(context.RequestContext))\\n\\n        mox.VerifyAll()\", 'def _prepare_info_by_ip_response(self):\\n        res = \"\"\"<attributes-list>\\n        <net-interface-info>\\n        <address>127.0.0.1</address>\\n        <administrative-status>up</administrative-status>\\n        <current-node>fas3170rre-cmode-01</current-node>\\n        <current-port>e1b-1165</current-port>\\n        <data-protocols>\\n          <data-protocol>nfs</data-protocol>\\n        </data-protocols>\\n        <dns-domain-name>none</dns-domain-name>\\n        <failover-group/>\\n        <failover-policy>disabled</failover-policy>\\n        <firewall-policy>data</firewall-policy>\\n        <home-node>fas3170rre-cmode-01</home-node>\\n        <home-port>e1b-1165</home-port>\\n        <interface-name>nfs_data1</interface-name>\\n        <is-auto-revert>false</is-auto-revert>\\n        <is-home>true</is-home>\\n        <netmask>255.255.255.0</netmask>\\n        <netmask-length>24</netmask-length>\\n        <operational-status>up</operational-status>\\n        <role>data</role>\\n        <routing-group-name>c10.63.165.0/24</routing-group-name>\\n        <use-failover-group>disabled</use-failover-group>\\n        <vserver>openstack</vserver>\\n      </net-interface-info></attributes-list>\"\"\"\\n        response_el = etree.XML(res)\\n        return api.NaElement(response_el).get_children()', \"def test_register_img_in_cache_noshare(self):\\n        volume = {'id': '1', 'name': 'testvol'}\\n        volume['provider_location'] = '10.61.170.1:/share/path'\\n        drv = self._driver\\n        mox = self.mox\\n        mox.StubOutWithMock(drv, '_do_clone_rel_img_cache')\\n\\n        drv._do_clone_rel_img_cache('testvol', 'img-cache-12345',\\n                                    '10.61.170.1:/share/path',\\n                                    'img-cache-12345')\\n\\n        mox.ReplayAll()\\n        drv._register_image_in_cache(volume, '12345')\\n        mox.VerifyAll()\", \"def test_find_image_in_cache_no_shares(self):\\n        drv = self._driver\\n        drv._mounted_shares = []\\n        result = drv._find_image_in_cache('image_id')\\n        if not result:\\n            pass\\n        else:\\n            self.fail('Return result is unexpected')\", \"def test_find_old_cache_files_notexists(self):\\n        drv = self._driver\\n        mox = self.mox\\n        cmd = ['find', '/mnt', '-maxdepth', '1', '-name',\\n               'img-cache*', '-amin', '+720']\\n        setattr(drv.configuration, 'expiry_thres_minutes', 720)\\n        mox.StubOutWithMock(drv, '_get_mount_point_for_share')\\n        mox.StubOutWithMock(drv, '_execute')\\n\\n        drv._get_mount_point_for_share(IgnoreArg()).AndReturn('/mnt')\\n        drv._execute(*cmd, run_as_root=True).AndReturn((None, ''))\\n        mox.ReplayAll()\\n        res = drv._find_old_cache_files('share')\\n        mox.VerifyAll()\\n        if len(res) == 0:\\n            pass\\n        else:\\n            self.fail('No files expected but got return values.')\", \"def test_delete_files_till_bytes_free_success(self):\\n        drv = self._driver\\n        mox = self.mox\\n        files = [('img-cache-1', 230), ('img-cache-2', 380)]\\n        mox.StubOutWithMock(drv, '_get_mount_point_for_share')\\n        mox.StubOutWithMock(drv, '_delete_file')\\n\\n        drv._get_mount_point_for_share(IgnoreArg()).AndReturn('/mnt')\\n        drv._delete_file('/mnt/img-cache-2').AndReturn(True)\\n        drv._delete_file('/mnt/img-cache-1').AndReturn(True)\\n        mox.ReplayAll()\\n        drv._delete_files_till_bytes_free(files, 'share', bytes_to_free=1024)\\n        mox.VerifyAll()\", \"def test_clean_image_cache_noexec(self):\\n        drv = self._driver\\n        mox = self.mox\\n        drv.configuration.thres_avl_size_perc_start = 20\\n        drv.configuration.thres_avl_size_perc_stop = 50\\n        drv._mounted_shares = ['testshare']\\n\\n        mox.StubOutWithMock(drv, '_get_capacity_info')\\n\\n        drv._get_capacity_info('testshare').AndReturn((100, 30, 70))\\n        mox.ReplayAll()\\n        drv._clean_image_cache()\\n        mox.VerifyAll()\\n        drv._mounted_shares.remove('testshare')\\n        if not drv.cleaning:\\n            pass\\n        else:\\n            self.fail('Clean image cache failed.')\", 'def get_img_info(self, format):\\n        class img_info(object):\\n            def __init__(self, fmt):\\n                self.file_format = fmt\\n\\n        return img_info(format)', \"def test_clone_image_cloneableshare_raw(self):\\n        drv = self._driver\\n        mox = self.mox\\n        volume = {'name': 'vol', 'size': '20'}\\n        mox.StubOutWithMock(drv, '_find_image_in_cache')\\n        mox.StubOutWithMock(drv, '_is_cloneable_share')\\n        mox.StubOutWithMock(drv, '_get_mount_point_for_share')\\n        mox.StubOutWithMock(image_utils, 'qemu_img_info')\\n        mox.StubOutWithMock(drv, '_clone_volume')\\n        mox.StubOutWithMock(drv, '_discover_file_till_timeout')\\n        mox.StubOutWithMock(drv, '_set_rw_permissions_for_all')\\n        mox.StubOutWithMock(drv, '_resize_image_file')\\n        mox.StubOutWithMock(drv, '_is_share_vol_compatible')\\n\\n        drv._find_image_in_cache(IgnoreArg()).AndReturn([])\\n        drv._is_cloneable_share(IgnoreArg()).AndReturn('127.0.0.1:/share')\\n        drv._is_share_vol_compatible(IgnoreArg(), IgnoreArg()).AndReturn(True)\\n        drv._get_mount_point_for_share(IgnoreArg()).AndReturn('/mnt')\\n        image_utils.qemu_img_info('/mnt/img-id').AndReturn(\\n            self.get_img_info('raw'))\\n        drv._clone_volume(\\n            'img-id', 'vol', share='127.0.0.1:/share', volume_id=None)\\n        drv._get_mount_point_for_share(IgnoreArg()).AndReturn('/mnt')\\n        drv._discover_file_till_timeout(IgnoreArg()).AndReturn(True)\\n        drv._set_rw_permissions_for_all('/mnt/vol')\\n        drv._resize_image_file({'name': 'vol'}, IgnoreArg())\\n\\n        mox.ReplayAll()\\n        drv. clone_image(\\n            volume, ('nfs://127.0.0.1:/share/img-id', None), 'image_id', {})\\n        mox.VerifyAll()\", \"def test_clone_image_file_not_discovered(self):\\n        drv = self._driver\\n        mox = self.mox\\n        volume = {'name': 'vol', 'size': '20'}\\n        mox.StubOutWithMock(drv, '_find_image_in_cache')\\n        mox.StubOutWithMock(drv, '_is_cloneable_share')\\n        mox.StubOutWithMock(drv, '_get_mount_point_for_share')\\n        mox.StubOutWithMock(image_utils, 'qemu_img_info')\\n        mox.StubOutWithMock(drv, '_clone_volume')\\n        mox.StubOutWithMock(drv, '_discover_file_till_timeout')\\n        mox.StubOutWithMock(image_utils, 'convert_image')\\n        mox.StubOutWithMock(drv, '_register_image_in_cache')\\n        mox.StubOutWithMock(drv, '_is_share_vol_compatible')\\n        mox.StubOutWithMock(drv, 'local_path')\\n        mox.StubOutWithMock(os.path, 'exists')\\n        mox.StubOutWithMock(drv, '_delete_file')\\n\\n        drv._find_image_in_cache(IgnoreArg()).AndReturn([])\\n        drv._is_cloneable_share('nfs://127.0.0.1/share/img-id').AndReturn(\\n            '127.0.0.1:/share')\\n        drv._is_share_vol_compatible(IgnoreArg(), IgnoreArg()).AndReturn(True)\\n        drv._get_mount_point_for_share('127.0.0.1:/share').AndReturn('/mnt')\\n        image_utils.qemu_img_info('/mnt/img-id').AndReturn(\\n            self.get_img_info('notraw'))\\n        image_utils.convert_image(IgnoreArg(), IgnoreArg(), 'raw')\\n        image_utils.qemu_img_info('/mnt/vol').AndReturn(\\n            self.get_img_info('raw'))\\n        drv._register_image_in_cache(IgnoreArg(), IgnoreArg())\\n        drv.local_path(IgnoreArg()).AndReturn('/mnt/vol')\\n        drv._discover_file_till_timeout(IgnoreArg()).AndReturn(False)\\n        drv.local_path(IgnoreArg()).AndReturn('/mnt/vol')\\n        os.path.exists('/mnt/vol').AndReturn(True)\\n        drv._delete_file('/mnt/vol')\\n\\n        mox.ReplayAll()\\n        vol_dict, result = drv. clone_image(\\n            volume, ('nfs://127.0.0.1/share/img-id', None), 'image_id', {})\\n        mox.VerifyAll()\\n        self.assertFalse(result)\\n        self.assertFalse(vol_dict['bootable'])\\n        self.assertIsNone(vol_dict['provider_location'])\", \"def test_is_cloneable_share_badformats(self):\\n        drv = self._driver\\n        strgs = ['10.61.666.22:/share/img',\\n                 'nfs://10.61.666.22:/share/img',\\n                 'nfs://10.61.666.22//share/img',\\n                 'nfs://com.netapp.com:/share/img',\\n                 'nfs://com.netapp.com//share/img',\\n                 'com.netapp.com://share/im\\\\g',\\n                 'http://com.netapp.com://share/img',\\n                 'nfs://com.netapp.com:/share/img',\\n                 'nfs://com.netapp.com:8080//share/img'\\n                 'nfs://com.netapp.com//img',\\n                 'nfs://[ae::sr::ty::po]/img']\\n        for strg in strgs:\\n            res = drv._is_cloneable_share(strg)\\n            if res:\\n                msg = 'Invalid format matched for url %s.' % strg\\n                self.fail(msg)\", \"def test_is_cloneable_share_goodformat2(self):\\n        drv = self._driver\\n        mox = self.mox\\n        strg = 'nfs://10.61.222.333:8080/share/img'\\n        mox.StubOutWithMock(drv, '_check_share_in_use')\\n        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')\\n        mox.ReplayAll()\\n        drv._is_cloneable_share(strg)\\n        mox.VerifyAll()\", \"def test_is_cloneable_share_goodformat4(self):\\n        drv = self._driver\\n        mox = self.mox\\n        strg = 'nfs://netapp.com/share/img'\\n        mox.StubOutWithMock(drv, '_check_share_in_use')\\n        drv._check_share_in_use(IgnoreArg(), IgnoreArg()).AndReturn('share')\\n        mox.ReplayAll()\\n        drv._is_cloneable_share(strg)\\n        mox.VerifyAll()\", \"def test_check_share_in_use_no_conn(self):\\n        drv = self._driver\\n        share = drv._check_share_in_use(None, '/dir')\\n        if share:\\n            self.fail('Unexpected share detected.')\", \"def test_check_share_in_use_incorrect_host(self):\\n        drv = self._driver\\n        mox = self.mox\\n        mox.StubOutWithMock(utils, 'resolve_hostname')\\n        utils.resolve_hostname(IgnoreArg()).AndRaise(Exception())\\n        mox.ReplayAll()\\n        share = drv._check_share_in_use('incorrect:8989', '/dir')\\n        mox.VerifyAll()\\n        if share:\\n            self.fail('Unexpected share detected.')\", 'def test_construct_image_url_loc(self):\\n        drv = self._driver\\n        img_loc = (None,\\n                   [{\\'metadata\\':\\n                     {\\'share_location\\': \\'nfs://host/path\\',\\n                      \\'mount_point\\': \\'/opt/stack/data/glance\\',\\n                      \\'type\\': \\'nfs\\'},\\n                     \\'url\\': \\'file:///opt/stack/data/glance/image-id\\'}])\\n        location = drv._construct_image_nfs_url(img_loc)\\n        if location != \"nfs://host/path/image-id\":\\n            self.fail(\"Unexpected direct url.\")', 'def setUp(self):\\n        super(NetappDirectCmodeNfsDriverOnlyTestCase, self).setUp()\\n        self._custom_setup()', \"def test_create_volume(self, mock_volume_extra_specs):\\n        drv = self._driver\\n        drv.ssc_enabled = False\\n        extra_specs = {}\\n        mock_volume_extra_specs.return_value = extra_specs\\n        fake_share = 'localhost:myshare'\\n        with mock.patch.object(drv, '_ensure_shares_mounted'):\\n            with mock.patch.object(drv, '_find_shares',\\n                                   return_value=['localhost:myshare']):\\n                with mock.patch.object(drv, '_do_create_volume'):\\n                    volume_info = self._driver.create_volume(FakeVolume(1))\\n                    self.assertEqual(volume_info.get('provider_location'),\\n                                     fake_share)\", \"def test_create_volume_with_qos_policy(self, mock_volume_extra_specs):\\n        drv = self._driver\\n        drv.ssc_enabled = False\\n        extra_specs = {'netapp:qos_policy_group': 'qos_policy_1'}\\n        fake_volume = FakeVolume(1)\\n        fake_share = 'localhost:myshare'\\n        fake_qos_policy = 'qos_policy_1'\\n        mock_volume_extra_specs.return_value = extra_specs\\n\\n        with mock.patch.object(drv, '_ensure_shares_mounted'):\\n            with mock.patch.object(drv, '_find_shares',\\n                                   return_value=['localhost:myshare']):\\n                with mock.patch.object(drv, '_do_create_volume'):\\n                    with mock.patch.object(drv,\\n                                           '_set_qos_policy_group_on_volume'\\n                                           ) as mock_set_qos:\\n                        volume_info = self._driver.create_volume(fake_volume)\\n                        self.assertEqual(volume_info.get('provider_location'),\\n                                         'localhost:myshare')\\n                        mock_set_qos.assert_called_once_with(fake_volume,\\n                                                             fake_share,\\n                                                             fake_qos_policy)\", \"def test_copy_img_to_vol_copyoffload_failure(self):\\n        drv = self._driver\\n        context = object()\\n        volume = {'id': 'vol_id', 'name': 'name'}\\n        image_service = object()\\n        image_id = 'image_id'\\n        drv._client = mock.Mock()\\n        drv._client.get_api_version = mock.Mock(return_value=(1, 20))\\n        drv._try_copyoffload = mock.Mock(side_effect=Exception())\\n        netapp_nfs.NetAppNFSDriver.copy_image_to_volume = mock.Mock()\\n        drv._get_provider_location = mock.Mock(return_value='share')\\n        drv._get_vol_for_share = mock.Mock(return_value='vol')\\n        drv._update_stale_vols = mock.Mock()\\n\\n        drv.copy_image_to_volume(context, volume, image_service, image_id)\\n        drv._try_copyoffload.assert_called_once_with(context, volume,\\n                                                     image_service,\\n                                                     image_id)\\n        netapp_nfs.NetAppNFSDriver.copy_image_to_volume.\\\\\\n            assert_called_once_with(context, volume, image_service, image_id)\\n        drv._update_stale_vols.assert_called_once_with('vol')\", \"def test_copyoffload_frm_cache_success(self):\\n        drv = self._driver\\n        context = object()\\n        volume = {'id': 'vol_id', 'name': 'name'}\\n        image_service = object()\\n        image_id = 'image_id'\\n        drv._find_image_in_cache = mock.Mock(return_value=[('share', 'img')])\\n        drv._copy_from_cache = mock.Mock(return_value=True)\\n\\n        drv._try_copyoffload(context, volume, image_service, image_id)\\n        drv._copy_from_cache.assert_called_once_with(volume,\\n                                                     image_id,\\n                                                     [('share', 'img')])\", \"def test_cache_copyoffload_workflow_success(self):\\n        drv = self._driver\\n        volume = {'id': 'vol_id', 'name': 'name', 'size': 1}\\n        image_id = 'image_id'\\n        cache_result = [('ip1:/openstack', 'img-cache-imgid')]\\n        drv._get_ip_verify_on_cluster = mock.Mock(return_value='ip1')\\n        drv._get_host_ip = mock.Mock(return_value='ip2')\\n        drv._get_export_path = mock.Mock(return_value='/exp_path')\\n        drv._execute = mock.Mock()\\n        drv._register_image_in_cache = mock.Mock()\\n        drv._get_provider_location = mock.Mock(return_value='/share')\\n        drv._post_clone_image = mock.Mock()\\n\\n        copied = drv._copy_from_cache(volume, image_id, cache_result)\\n        self.assertTrue(copied)\\n        drv._get_ip_verify_on_cluster.assert_any_call('ip1')\\n        drv._get_export_path.assert_called_with('vol_id')\\n        drv._execute.assert_called_once_with('cof_path', 'ip1', 'ip1',\\n                                             '/openstack/img-cache-imgid',\\n                                             '/exp_path/name',\\n                                             run_as_root=False,\\n                                             check_exit_code=0)\\n        drv._post_clone_image.assert_called_with(volume)\\n        drv._get_provider_location.assert_called_with('vol_id')\", \"def test_img_service_raw_copyoffload_workflow_success(self,\\n                                                          mock_qemu_img_info):\\n        drv = self._driver\\n        volume = {'id': 'vol_id', 'name': 'name', 'size': 1}\\n        image_id = 'image_id'\\n        context = object()\\n        image_service = mock.Mock()\\n        image_service.get_location.return_value = ('nfs://ip1/openstack/img',\\n                                                   None)\\n        image_service.show.return_value = {'size': 1,\\n                                           'disk_format': 'raw'}\\n\\n        drv._check_get_nfs_path_segs = mock.Mock(return_value=\\n                                                 ('ip1', '/openstack'))\\n        drv._get_ip_verify_on_cluster = mock.Mock(return_value='ip1')\\n        drv._get_host_ip = mock.Mock(return_value='ip2')\\n        drv._get_export_path = mock.Mock(return_value='/exp_path')\\n        drv._get_provider_location = mock.Mock(return_value='share')\\n        drv._execute = mock.Mock()\\n        drv._get_mount_point_for_share = mock.Mock(return_value='mnt_point')\\n        drv._discover_file_till_timeout = mock.Mock(return_value=True)\\n        img_inf = mock.Mock()\\n        img_inf.file_format = 'raw'\\n        mock_qemu_img_info.return_value = img_inf\\n        drv._check_share_can_hold_size = mock.Mock()\\n        drv._move_nfs_file = mock.Mock(return_value=True)\\n        drv._delete_file = mock.Mock()\\n        drv._clone_file_dst_exists = mock.Mock()\\n        drv._post_clone_image = mock.Mock()\\n\\n        drv._copy_from_img_service(context, volume, image_service, image_id)\\n        drv._get_ip_verify_on_cluster.assert_any_call('ip1')\\n        drv._get_export_path.assert_called_with('vol_id')\\n        drv._check_share_can_hold_size.assert_called_with('share', 1)\\n\\n        assert drv._execute.call_count == 1\\n        drv._post_clone_image.assert_called_with(volume)\", \"def test_img_service_qcow2_copyoffload_workflow_success(self, mock_exists,\\n                                                            mock_qemu_img_info,\\n                                                            mock_cvrt_image):\\n        drv = self._driver\\n        volume = {'id': 'vol_id', 'name': 'name', 'size': 1}\\n        image_id = 'image_id'\\n        context = object()\\n        image_service = mock.Mock()\\n        image_service.get_location.return_value = ('nfs://ip1/openstack/img',\\n                                                   None)\\n        image_service.show.return_value = {'size': 1,\\n                                           'disk_format': 'qcow2'}\\n        drv._check_get_nfs_path_segs = mock.Mock(return_value=\\n                                                 ('ip1', '/openstack'))\\n\\n        drv._get_ip_verify_on_cluster = mock.Mock(return_value='ip1')\\n        drv._get_host_ip = mock.Mock(return_value='ip2')\\n        drv._get_export_path = mock.Mock(return_value='/exp_path')\\n        drv._get_provider_location = mock.Mock(return_value='share')\\n        drv._execute = mock.Mock()\\n        drv._get_mount_point_for_share = mock.Mock(return_value='mnt_point')\\n        img_inf = mock.Mock()\\n        img_inf.file_format = 'raw'\\n        mock_qemu_img_info.return_value = img_inf\\n        drv._check_share_can_hold_size = mock.Mock()\\n\\n        drv._move_nfs_file = mock.Mock(return_value=True)\\n        drv._delete_file = mock.Mock()\\n        drv._clone_file_dst_exists = mock.Mock()\\n        drv._post_clone_image = mock.Mock()\\n\\n        drv._copy_from_img_service(context, volume, image_service, image_id)\\n        drv._get_ip_verify_on_cluster.assert_any_call('ip1')\\n        drv._get_export_path.assert_called_with('vol_id')\\n        drv._check_share_can_hold_size.assert_called_with('share', 1)\\n        assert mock_cvrt_image.call_count == 1\\n        assert drv._execute.call_count == 1\\n        assert drv._delete_file.call_count == 2\\n        drv._clone_file_dst_exists.call_count == 1\\n        drv._post_clone_image.assert_called_with(volume)\", 'def _custom_setup(self):\\n        self._driver = netapp_nfs.NetAppDirect7modeNfsDriver(\\n            configuration=create_configuration())', 'def test_check_for_setup_error_version(self):\\n        drv = self._driver\\n        drv._client = api.NaServer(\"127.0.0.1\")\\n\\n        # check exception raises when version not found\\n        self.assertRaises(exception.VolumeBackendAPIException,\\n                          drv.check_for_setup_error)\\n\\n        drv._client.set_api_version(1, 8)\\n\\n        # check exception raises when not supported version\\n        self.assertRaises(exception.VolumeBackendAPIException,\\n                          drv.check_for_setup_error)', \"def test_do_setup(self):\\n        mox = self.mox\\n        drv = self._driver\\n        mox.StubOutWithMock(netapp_nfs.NetAppNFSDriver, 'do_setup')\\n        mox.StubOutWithMock(drv, '_get_client')\\n        mox.StubOutWithMock(drv, '_do_custom_setup')\\n        netapp_nfs.NetAppNFSDriver.do_setup(IgnoreArg())\\n        drv._get_client()\\n        drv._do_custom_setup(IgnoreArg())\\n\\n        mox.ReplayAll()\\n\\n        drv.do_setup(IsA(context.RequestContext))\\n\\n        mox.VerifyAll()\"]}, {'features': [], 'snippets': ['def setUp(self):\\n        super(JsonToYamlTest, self).setUp()\\n        self.expected_test_count = 2\\n        self.longMessage = True\\n        self.maxDiff = None', \"def compare_json_vs_yaml(self, json_str, yml_str, file_name):\\n        yml = template_format.parse(yml_str)\\n\\n        self.assertEqual(u'2012-12-12', yml[u'HeatTemplateFormatVersion'],\\n                         file_name)\\n        self.assertFalse(u'AWSTemplateFormatVersion' in yml, file_name)\\n        del(yml[u'HeatTemplateFormatVersion'])\\n\\n        jsn = template_format.parse(json_str)\\n\\n        if u'AWSTemplateFormatVersion' in jsn:\\n            del(jsn[u'AWSTemplateFormatVersion'])\\n\\n        self.assertEqual(yml, jsn, file_name)\", 'def _parse_template(self, tmpl_str, msg_str):\\n        parse_ex = self.assertRaises(ValueError,\\n                                     template_format.parse,\\n                                     tmpl_str)\\n        self.assertIn(msg_str, six.text_type(parse_ex))', \"def test_parse_no_version_format(self):\\n        yaml = ''\\n        self._parse_template(yaml, 'Template format version not found')\\n        yaml2 = '''Parameters: {}\", \"def test_parse_string_template(self):\\n        tmpl_str = 'just string'\\n        msg = 'The template is not a JSON object or YAML mapping.'\\n        self._parse_template(tmpl_str, msg)\", 'def test_parse_json_document(self):\\n        tmpl_str = \\'[\"foo\" , \"bar\"]\\'\\n        msg = \\'The template is not a JSON object or YAML mapping.\\'\\n        self._parse_template(tmpl_str, msg)', \"def test_parse_yaml_template(self):\\n        tmpl_str = 'heat_template_version: 2013-05-23'\\n        expected = {'heat_template_version': '2013-05-23'}\\n        self.assertEqual(expected, template_format.parse(tmpl_str))\", \"def test_parse_to_value_exception(self):\\n        text = 'not important'\\n\\n        with mock.patch.object(yaml, 'load') as yaml_loader:\\n            yaml_loader.side_effect = self.raised_exception\\n\\n            self.assertRaises(ValueError,\\n                              template_format.parse, text)\", 'def setUp(self):\\n        super(JsonYamlResolvedCompareTest, self).setUp()\\n        self.longMessage = True\\n        self.maxDiff = None', \"def compare_stacks(self, json_file, yaml_file, parameters):\\n        t1 = self.load_template(json_file)\\n        t2 = self.load_template(yaml_file)\\n        del(t1[u'AWSTemplateFormatVersion'])\\n        t1[u'HeatTemplateFormatVersion'] = t2[u'HeatTemplateFormatVersion']\\n        stack1 = utils.parse_stack(t1, parameters)\\n        stack2 = utils.parse_stack(t2, parameters)\\n\\n        # compare resources separately so that resolved static data\\n        # is compared\\n        t1nr = dict(stack1.t.t)\\n        del(t1nr['Resources'])\\n\\n        t2nr = dict(stack2.t.t)\\n        del(t2nr['Resources'])\\n        self.assertEqual(t1nr, t2nr)\\n\\n        self.assertEqual(set(stack1.keys()), set(stack2.keys()))\\n        for key in stack1:\\n            self.assertEqual(stack1[key].t, stack2[key].t)\"]}, {'features': [], 'snippets': [\"def test_action_defaults_from_env(self):\\n        wf_service.create_workflows(WORKFLOW1)\\n\\n        wf_ex = self.engine.start_workflow('wf1', env=ENV)\\n\\n        self.await_workflow_success(wf_ex.id)\\n\\n        with db_api.transaction():\\n            wf_ex = db_api.get_workflow_execution(wf_ex.id)\\n\\n            self.assertEqual(states.SUCCESS, wf_ex.state)\\n            self._assert_single_item(wf_ex.task_executions, name='task1')\\n\\n        requests.request.assert_called_with(\\n            'GET', 'https://api.library.org/books',\\n            params=None, data=None, headers=None, cookies=None,\\n            allow_redirects=None, proxies=None, verify=None,\\n            auth=EXPECTED_ENV_AUTH,\\n            timeout=ENV['__actions']['std.http']['timeout'])\", \"def test_action_defaults_from_env_not_applied(self):\\n        wf_service.create_workflows(WORKFLOW2)\\n\\n        wf_ex = self.engine.start_workflow('wf2', env=ENV)\\n\\n        self.await_workflow_success(wf_ex.id)\\n\\n        with db_api.transaction():\\n            wf_ex = db_api.get_workflow_execution(wf_ex.id)\\n\\n            self.assertEqual(states.SUCCESS, wf_ex.state)\\n            self._assert_single_item(wf_ex.task_executions, name='task1')\\n\\n        requests.request.assert_called_with(\\n            'GET', 'https://api.library.org/books',\\n            params=None, data=None, headers=None, cookies=None,\\n            allow_redirects=None, proxies=None, verify=None,\\n            auth=EXPECTED_ENV_AUTH,\\n            timeout=60\\n        )\", \"def test_with_items_action_defaults_from_env(self):\\n        wf_service.create_workflows(WORKFLOW1_WITH_ITEMS)\\n\\n        wf_input = {\\n            'links': [\\n                'https://api.library.org/books',\\n                'https://api.library.org/authors'\\n            ]\\n        }\\n\\n        wf_ex = self.engine.start_workflow(\\n            'wf1_with_items',\\n            wf_input=wf_input,\\n            env=ENV\\n        )\\n\\n        self.await_workflow_success(wf_ex.id)\\n\\n        with db_api.transaction():\\n            wf_ex = db_api.get_workflow_execution(wf_ex.id)\\n\\n            self.assertEqual(states.SUCCESS, wf_ex.state)\\n            self._assert_single_item(wf_ex.task_executions, name='task1')\\n\\n            calls = [mock.call('GET', url, params=None, data=None,\\n                               headers=None, cookies=None,\\n                               allow_redirects=None, proxies=None,\\n                               auth=EXPECTED_ENV_AUTH, verify=None,\\n                               timeout=ENV['__actions']['std.http']['timeout'])\\n                     for url in wf_input['links']]\\n\\n        requests.request.assert_has_calls(calls, any_order=True)\"]}, {'features': [], 'snippets': ['def check_full_name(full_name_raw: str) -> str:\\n    full_name = full_name_raw.strip()\\n    if len(full_name) > UserProfile.MAX_NAME_LENGTH:\\n        raise JsonableError(_(\"Name too long!\"))\\n    if len(full_name) < UserProfile.MIN_NAME_LENGTH:\\n        raise JsonableError(_(\"Name too short!\"))\\n    for character in full_name:\\n        if unicodedata.category(character)[0] == \"C\" or character in UserProfile.NAME_INVALID_CHARS:\\n            raise JsonableError(_(\"Invalid characters in name!\"))\\n    # Names ending with e.g. `|15` could be ambiguous for\\n    # sloppily-written parsers of our Markdown syntax for mentioning\\n    # users with ambiguous names, and likely have no real use, so we\\n    # ban them.\\n    if re.search(r\"\\\\|\\\\d+$\", full_name_raw):\\n        raise JsonableError(_(\"Invalid format!\"))\\n    return full_name', 'def check_bot_name_available(realm_id: int, full_name: str) -> None:\\n    dup_exists = UserProfile.objects.filter(\\n        realm_id=realm_id,\\n        full_name=full_name.strip(),\\n        is_active=True,\\n    ).exists()\\n\\n    if dup_exists:\\n        raise JsonableError(_(\"Name is already in use!\"))', 'def check_valid_bot_config(bot_type: int, service_name: str, config_data: Dict[str, str]) -> None:\\n    if bot_type == UserProfile.INCOMING_WEBHOOK_BOT:\\n        from zerver.lib.integrations import WEBHOOK_INTEGRATIONS\\n\\n        config_options = None\\n        for integration in WEBHOOK_INTEGRATIONS:\\n            if integration.name == service_name:\\n                # key: validator\\n                config_options = {c[1]: c[2] for c in integration.config_options}\\n                break\\n        if not config_options:\\n            raise JsonableError(_(\"Invalid integration \\'{}\\'.\").format(service_name))\\n\\n        missing_keys = set(config_options.keys()) - set(config_data.keys())\\n        if missing_keys:\\n            raise JsonableError(\\n                _(\"Missing configuration parameters: {}\").format(\\n                    missing_keys,\\n                )\\n            )\\n\\n        for key, validator in config_options.items():\\n            value = config_data[key]\\n            error = validator(key, value)\\n            if error:\\n                raise JsonableError(_(\"Invalid {} value {} ({})\").format(key, value, error))\\n\\n    elif bot_type == UserProfile.EMBEDDED_BOT:\\n        try:\\n            from zerver.lib.bot_lib import get_bot_handler\\n\\n            bot_handler = get_bot_handler(service_name)\\n            if hasattr(bot_handler, \"validate_config\"):\\n                bot_handler.validate_config(config_data)\\n        except ConfigValidationError:\\n            # The exception provides a specific error message, but that\\n            # message is not tagged translatable, because it is\\n            # triggered in the external zulip_bots package.\\n            # TODO: Think of some clever way to provide a more specific\\n            # error message.\\n            raise JsonableError(_(\"Invalid configuration data!\"))', 'def add_service(\\n    name: str,\\n    user_profile: UserProfile,\\n    base_url: Optional[str] = None,\\n    interface: Optional[int] = None,\\n    token: Optional[str] = None,', 'def check_bot_creation_policy(user_profile: UserProfile, bot_type: int) -> None:\\n    # Realm administrators can always add bot\\n    if user_profile.is_realm_admin:\\n        return\\n\\n    if user_profile.realm.bot_creation_policy == Realm.BOT_CREATION_EVERYONE:\\n        return\\n    if user_profile.realm.bot_creation_policy == Realm.BOT_CREATION_ADMINS_ONLY:\\n        raise OrganizationAdministratorRequired()\\n    if (\\n        user_profile.realm.bot_creation_policy == Realm.BOT_CREATION_LIMIT_GENERIC_BOTS\\n        and bot_type == UserProfile.DEFAULT_BOT\\n    ):\\n        raise OrganizationAdministratorRequired()', 'def check_valid_interface_type(interface_type: Optional[int]) -> None:\\n    if interface_type not in Service.ALLOWED_INTERFACE_TYPES:\\n        raise JsonableError(_(\"Invalid interface type\"))', 'def bulk_get_users(\\n    emails: List[str], realm: Optional[Realm], base_query: \"QuerySet[UserProfile]\" = None', 'def fetch_users_by_email(emails: List[str]) -> List[UserProfile]:\\n        # This should be just\\n        #\\n        # UserProfile.objects.select_related(\"realm\").filter(email__iexact__in=emails,\\n        #                                                    realm=realm)\\n        #\\n        # But chaining __in and __iexact doesn\\'t work with Django\\'s\\n        # ORM, so we have the following hack to construct the relevant where clause\\n        where_clause = \"upper(zerver_userprofile.email::text) IN (SELECT upper(email) FROM unnest(%s) AS email)\"\\n        return query.select_related(\"realm\").extra(where=[where_clause], params=(emails,))', 'def get_user_id(user: UserProfile) -> int:\\n    return user.id', 'def fetch_users_by_id(user_ids: List[int]) -> List[UserProfile]:\\n        return list(UserProfile.objects.filter(id__in=user_ids).select_related())', 'def access_bot_by_id(user_profile: UserProfile, user_id: int) -> UserProfile:\\n    try:\\n        target = get_user_profile_by_id_in_realm(user_id, user_profile.realm)\\n    except UserProfile.DoesNotExist:\\n        raise JsonableError(_(\"No such bot\"))\\n    if not target.is_bot:\\n        raise JsonableError(_(\"No such bot\"))\\n    if not user_profile.can_admin_user(target):\\n        raise JsonableError(_(\"Insufficient permission\"))\\n    return target', 'def get_accounts_for_email(email: str) -> List[Accounts]:\\n    profiles = (\\n        UserProfile.objects.select_related(\"realm\")\\n        .filter(\\n            delivery_email__iexact=email.strip(),\\n            is_active=True,\\n            realm__deactivated=False,\\n            is_bot=False,\\n        )\\n        .order_by(\"date_joined\")\\n    )\\n    accounts: List[Accounts] = []\\n    for profile in profiles:\\n        accounts.append(\\n            dict(\\n                realm_name=profile.realm.name,\\n                realm_id=profile.realm.id,\\n                full_name=profile.full_name,\\n                avatar=avatar_url(profile),\\n            )\\n        )\\n    return accounts', 'def get_all_api_keys(user_profile: UserProfile) -> List[str]:\\n    # Users can only have one API key for now\\n    return [user_profile.api_key]', 'def validate_user_custom_profile_data(\\n    realm_id: int, profile_data: List[Dict[str, Union[int, str, List[int]]]]', 'def can_access_delivery_email(user_profile: UserProfile) -> bool:\\n    realm = user_profile.realm\\n    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_ADMINS:\\n        return user_profile.is_realm_admin\\n\\n    if realm.email_address_visibility == Realm.EMAIL_ADDRESS_VISIBILITY_MODERATORS:\\n        return user_profile.is_realm_admin or user_profile.is_moderator\\n\\n    return False', 'def user_profile_to_user_row(user_profile: UserProfile) -> Dict[str, Any]:\\n    # What we\\'re trying to do is simulate the user_profile having been\\n    # fetched from a QuerySet using `.values(*realm_user_dict_fields)`\\n    # even though we fetched UserProfile objects.  This is messier\\n    # than it seems.\\n    #\\n    # What we\\'d like to do is just call model_to_dict(user,\\n    # fields=realm_user_dict_fields).  The problem with this is\\n    # that model_to_dict has a different convention than\\n    # `.values()` in its handling of foreign keys, naming them as\\n    # e.g. `bot_owner`, not `bot_owner_id`; we work around that\\n    # here.\\n    #\\n    # This could be potentially simplified in the future by\\n    # changing realm_user_dict_fields to name the bot owner with\\n    # the less readable `bot_owner` (instead of `bot_owner_id`).\\n    user_row = model_to_dict(user_profile, fields=[*realm_user_dict_fields, \"bot_owner\"])\\n    user_row[\"bot_owner_id\"] = user_row[\"bot_owner\"]\\n    del user_row[\"bot_owner\"]\\n    return user_row', 'def get_custom_profile_field_values(\\n    custom_profile_field_values: List[CustomProfileFieldValue],', 'def get_raw_user_data(\\n    realm: Realm,\\n    acting_user: Optional[UserProfile],\\n    *,\\n    target_user: Optional[UserProfile] = None,\\n    client_gravatar: bool,\\n    user_avatar_url_field_optional: bool,\\n    include_custom_profile_fields: bool = True,']}, {'features': [], 'snippets': ['def __init__(self, models, config):\\n        self.models = models\\n        self.config = config\\n        self.es = Elasticsearch(\\n            hosts=self.config.hosts,\\n            **self.config.get(\"client_options\", {})\\n        )\\n\\n        self.types = AttributeDict()', 'def reindex(self, index=None, alias=True, keep_old=False):\\n        # Generate an Index Name for Warehouse\\n        index = \"\".join([\\n            index if index is not None else self._index,\\n            hashlib.md5(os.urandom(16)).hexdigest()[:8],\\n        ])\\n\\n        # Create this index\\n        self.es.indices.create(index, {\\n            \"mappings\": {\\n                doc_type._type: doc_type.get_mapping()\\n                for doc_type in self.types.values()\\n            },\\n        })\\n\\n        # Index everything into the new index\\n        for doc_type in self.types.values():\\n            doc_type.index_all(index=index)\\n\\n        # Update the alias unless we\\'ve been told not to\\n        if alias:\\n            self.update_alias(self._index, index, keep_old=keep_old)', 'def __init__(self, index):\\n        self.index = index', 'def get_indexable(self):\\n        raise NotImplementedError', 'def extract_document(self, item):\\n        raise NotImplementedError']}, {'features': [], 'snippets': ['def __init__(self, remote, letter, number, name, colour, *args, **kwargs):\\n        super(TeamPanel, self).__init__(*args, **kwargs) \\n        self.remote = remote\\n        self.InitUI(letter, number, name, colour)', 'def do_get_name(self, event):\\n        self.name = configurator.get_team_name(self.number)', 'def name(self):\\n        return self.name_ctrl.GetValue()', 'def name(self, val):\\n        self.name_ctrl.SetValue(val)', 'def number(self):\\n        try:\\n            return int(self.num_ctrl.GetValue())\\n        except ValueError:\\n            return 0', 'def number(self, val):\\n        self.num_ctrl.SetValue(str(val))', 'def __init__(self, remote, *args, **kwargs):\\n        super(MatchControl, self).__init__(*args, **kwargs) \\n        self.remote = remote\\n        self.InitUI()', 'def do_go(self, e):\\n        self.remote.do_go()', 'def do_save(self, e):\\n        self.remote.do_save(self.get_match())', 'def _set_match_panel(self, match, team_idx, panel_idx):\\n        match.team_numbers[team_idx] = self.team_panels[panel_idx].number\\n        match.team_names[team_idx] = self.team_panels[panel_idx].name', 'def get_match(self):\\n        match = Forseti.Match()\\n        self._set_match_panel(match, 0, 0)\\n        self._set_match_panel(match, 1, 2)\\n        self._set_match_panel(match, 2, 1)\\n        self._set_match_panel(match, 3, 3)\\n        try:\\n            match.match_number = int(self.match_num_ctrl.GetValue())\\n        except ValueError:\\n            match.match_number = random.getrandbits(31)\\n        return match', 'def set_time(self, match):\\n        self.time_text.SetLabel(format_time(match.game_time_so_far))\\n        self.stage_text.SetLabel(match.stage_name)', 'def __init__(self, remote, match_control, *args, **kwargs):\\n        self.remote = remote\\n        super(ScheduleControl, self).__init__(*args, **kwargs)\\n        self.InitUI()\\n        self.remote.match_list_box = self.match_list\\n        self.match_control = match_control', 'def do_load(self, e):\\n        self.remote.do_load(self.clear_first.GetValue())', 'def __init__(self, remote, *args, **kwargs):\\n        super(MainWindow, self).__init__(*args, **kwargs) \\n        self.remote = remote\\n        self.InitUI()', 'def OnQuit(self, e):\\n        self.Close()', \"def __init__(self):\\n        self.lc = lcm.LCM('udpm://239.255.76.67:7667?ttl=1')\\n        self.lc.subscribe('Schedule/Schedule', self.handle_schedule)\\n        self.lc.subscribe('Timer/Time', self.handle_time)\\n        self.match_list_box = None\\n        self.match_control = None\\n        self.thread = threading.Thread(target=self._loop)\\n        self.thread.daemon = True\", \"def _loop(self):\\n        while True:\\n            try:\\n                self.lc.handle()\\n            except Exception as ex:\\n                print('Got exception while handling lcm message', ex)\", 'def handle_time(self, channel, data):\\n        msg = Forseti.Time.decode(data)\\n        #wx.CallAfter(self.time_text.SetLabel, format_time(msg.game_time_so_far))\\n        wx.CallAfter(self.match_control.set_time, msg)', \"def do_save(self, match):\\n        self.lc.publish('Match/Save', match.encode())\", \"def do_time_ctrl(self, command):\\n        msg = Forseti.TimeControl()\\n        msg.command_name = command\\n        self.lc.publish('Timer/Control', msg.encode())\", \"def do_pause(self):\\n        self.do_time_ctrl('pause')\", 'def main():']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def print_banner(bootstrap, no_shell_file):\\n    \"\"\"Print the Pigweed or project-specific banner\"\"\"\\n    enable_colors()\\n\\n    print(Color.green(\\'\\\\n  WELCOME TO...\\'))\\n    print(Color.magenta(_PIGWEED_BANNER))\\n\\n    if bootstrap:\\n        print(\\n            Color.green(\\'\\\\n  BOOTSTRAP! Bootstrap may take a few minutes; \\'\\n                        \\'please be patient\\'))\\n        print(\\n            Color.green(\\n                \\'  On Windows, this stage is extremely slow (~10 minutes).\\\\n\\'))\\n    else:\\n        print(\\n            Color.green(\\n                \\'\\\\n  ACTIVATOR! This sets your console environment variables.\\\\n\\'\\n            ))\\n\\n        if no_shell_file:\\n            print(Color.bold_red(\\'Error!\\\\n\\'))\\n            print(\\n                Color.red(\\'  Your Pigweed environment does not seem to be\\'\\n                          \\' configured.\\'))\\n            print(Color.red(\\'  Run bootstrap.bat to perform initial setup.\\'))\\n\\n    return 0', 'def main():\\n    \"\"\"Script entry point.\"\"\"\\n    if os.name != \\'nt\\':\\n        return 1\\n    return print_banner(**vars(parse()))']}, {'features': [], 'snippets': [\"def __init__(self,\\n                 filename=None,\\n                 content=None,\\n                 content_type='yaml',\\n                 separator='.',\\n                 backup=False):\\n        self.content = content\\n        self._separator = separator\\n        self.filename = filename\\n        self.__yaml_dict = content\\n        self.content_type = content_type\\n        self.backup = backup\\n        self.load(content_type=self.content_type)\\n        if self.__yaml_dict is None:\\n            self.__yaml_dict = {}\", \"def separator(self):\\n        ''' getter method for separator '''\\n        return self._separator\", \"def separator(self, inc_sep):\\n        ''' setter method for separator '''\\n        self._separator = inc_sep\", \"def yaml_dict(self):\\n        ''' getter method for yaml_dict '''\\n        return self.__yaml_dict\", \"def yaml_dict(self, value):\\n        ''' setter method for yaml_dict '''\\n        self.__yaml_dict = value\", \"def parse_key(key, sep='.'):\\n        '''parse the key allowing the appropriate separator'''\\n        common_separators = list(Yedit.com_sep - set([sep]))\\n        return re.findall(Yedit.re_key.format(''.join(common_separators)), key)\", \"def valid_key(key, sep='.'):\\n        '''validate the incoming key'''\\n        common_separators = list(Yedit.com_sep - set([sep]))\\n        if not re.match(Yedit.re_valid_key.format(''.join(common_separators)), key):\\n            return False\\n\\n        return True\", \"def remove_entry(data, key, sep='.'):\\n        ''' remove data at location key '''\\n        if key == '' and isinstance(data, dict):\\n            data.clear()\\n            return True\\n        elif key == '' and isinstance(data, list):\\n            del data[:]\\n            return True\\n\\n        if not (key and Yedit.valid_key(key, sep)) and \\\\\\n           isinstance(data, (list, dict)):\\n            return None\\n\\n        key_indexes = Yedit.parse_key(key, sep)\\n        for arr_ind, dict_key in key_indexes[:-1]:\\n            if dict_key and isinstance(data, dict):\\n                data = data.get(dict_key)\\n            elif (arr_ind and isinstance(data, list) and\\n                  int(arr_ind) <= len(data) - 1):\\n                data = data[int(arr_ind)]\\n            else:\\n                return None\\n\\n        # process last index for remove\\n        # expected list entry\\n        if key_indexes[-1][0]:\\n            if isinstance(data, list) and int(key_indexes[-1][0]) <= len(data) - 1:  # noqa: E501\\n                del data[int(key_indexes[-1][0])]\\n                return True\\n\\n        # expected dict entry\\n        elif key_indexes[-1][1]:\\n            if isinstance(data, dict):\\n                del data[key_indexes[-1][1]]\\n                return True\", 'def add_entry(data, key, item=None, sep=\\'.\\'):\\n        \\'\\'\\' Get an item from a dictionary with key notation a.b.c\\n            d = {\\'a\\': {\\'b\\': \\'c\\'}}}\\n            key = a#b\\n            return c\\n        \\'\\'\\'\\n        if key == \\'\\':\\n            pass\\n        elif (not (key and Yedit.valid_key(key, sep)) and\\n              isinstance(data, (list, dict))):\\n            return None\\n\\n        key_indexes = Yedit.parse_key(key, sep)\\n        for arr_ind, dict_key in key_indexes[:-1]:\\n            if dict_key:\\n                if isinstance(data, dict) and dict_key in data and data[dict_key]:  # noqa: E501\\n                    data = data[dict_key]\\n                    continue\\n\\n                elif data and not isinstance(data, dict):\\n                    raise YeditException(\"Unexpected item type found while going through key \" +\\n                                         \"path: {} (at key: {})\".format(key, dict_key))\\n\\n                data[dict_key] = {}\\n                data = data[dict_key]\\n\\n            elif (arr_ind and isinstance(data, list) and\\n                  int(arr_ind) <= len(data) - 1):\\n                data = data[int(arr_ind)]\\n            else:\\n                raise YeditException(\"Unexpected item type found while going through key path: {}\".format(key))\\n\\n        if key == \\'\\':\\n            data = item\\n\\n        # process last index for add\\n        # expected list entry\\n        elif key_indexes[-1][0] and isinstance(data, list) and int(key_indexes[-1][0]) <= len(data) - 1:  # noqa: E501\\n            data[int(key_indexes[-1][0])] = item\\n\\n        # expected dict entry\\n        elif key_indexes[-1][1] and isinstance(data, dict):\\n            data[key_indexes[-1][1]] = item\\n\\n        # didn\\'t add/update to an existing list, nor add/update key to a dict\\n        # so we must have been provided some syntax like a.b.c[<int>] = \"data\" for a\\n        # non-existent array\\n        else:\\n            raise YeditException(\"Error adding to object at path: {}\".format(key))\\n\\n        return data', \"def get_entry(data, key, sep='.'):\\n        ''' Get an item from a dictionary with key notation a.b.c\\n            d = {'a': {'b': 'c'}}}\\n            key = a.b\\n            return c\\n        '''\\n        if key == '':\\n            pass\\n        elif (not (key and Yedit.valid_key(key, sep)) and\\n              isinstance(data, (list, dict))):\\n            return None\\n\\n        key_indexes = Yedit.parse_key(key, sep)\\n        for arr_ind, dict_key in key_indexes:\\n            if dict_key and isinstance(data, dict):\\n                data = data.get(dict_key)\\n            elif (arr_ind and isinstance(data, list) and\\n                  int(arr_ind) <= len(data) - 1):\\n                data = data[int(arr_ind)]\\n            else:\\n                return None\\n\\n        return data\", \"def _write(filename, contents):\\n        ''' Actually write the file contents to disk. This helps with mocking. '''\\n\\n        tmp_filename = filename + '.yedit'\\n\\n        with open(tmp_filename, 'w') as yfd:\\n            yfd.write(contents)\\n\\n        os.rename(tmp_filename, filename)\", \"def read(self):\\n        ''' read from file '''\\n        # check if it exists\\n        if self.filename is None or not self.file_exists():\\n            return None\\n\\n        contents = None\\n        with open(self.filename) as yfd:\\n            contents = yfd.read()\\n\\n        return contents\", \"def load(self, content_type='yaml'):\\n        ''' return yaml file '''\\n        contents = self.read()\\n\\n        if not contents and not self.content:\\n            return None\\n\\n        if self.content:\\n            if isinstance(self.content, dict):\\n                self.yaml_dict = self.content\\n                return self.yaml_dict\\n            elif isinstance(self.content, str):\\n                contents = self.content\\n\\n        # check if it is yaml\\n        try:\\n            if content_type == 'yaml' and contents:\\n                # Try to set format attributes if supported\\n                try:\\n                    self.yaml_dict.fa.set_block_style()\\n                except AttributeError:\\n                    pass\\n\\n                # Try to use RoundTripLoader if supported.\\n                try:\\n                    self.yaml_dict = yaml.safe_load(contents, yaml.RoundTripLoader)\\n                except AttributeError:\\n                    self.yaml_dict = yaml.safe_load(contents)\\n\\n                # Try to set format attributes if supported\\n                try:\\n                    self.yaml_dict.fa.set_block_style()\\n                except AttributeError:\\n                    pass\\n\\n            elif content_type == 'json' and contents:\\n                self.yaml_dict = json.loads(contents)\\n        except yaml.YAMLError as err:\\n            # Error loading yaml or json\\n            raise YeditException('Problem with loading yaml file. {}'.format(err))\\n\\n        return self.yaml_dict\", \"def pop(self, path, key_or_item):\\n        ''' remove a key, value pair from a dict or an item for a list'''\\n        try:\\n            entry = Yedit.get_entry(self.yaml_dict, path, self.separator)\\n        except KeyError:\\n            entry = None\\n\\n        if entry is None:\\n            return (False, self.yaml_dict)\\n\\n        if isinstance(entry, dict):\\n            # AUDIT:maybe-no-member makes sense due to fuzzy types\\n            # pylint: disable=maybe-no-member\\n            if key_or_item in entry:\\n                entry.pop(key_or_item)\\n                return (True, self.yaml_dict)\\n            return (False, self.yaml_dict)\\n\\n        elif isinstance(entry, list):\\n            # AUDIT:maybe-no-member makes sense due to fuzzy types\\n            # pylint: disable=maybe-no-member\\n            ind = None\\n            try:\\n                ind = entry.index(key_or_item)\\n            except ValueError:\\n                return (False, self.yaml_dict)\\n\\n            entry.pop(ind)\\n            return (True, self.yaml_dict)\\n\\n        return (False, self.yaml_dict)\", \"def exists(self, path, value):\\n        ''' check if value exists at path'''\\n        try:\\n            entry = Yedit.get_entry(self.yaml_dict, path, self.separator)\\n        except KeyError:\\n            entry = None\\n\\n        if isinstance(entry, list):\\n            if value in entry:\\n                return True\\n            return False\\n\\n        elif isinstance(entry, dict):\\n            if isinstance(value, dict):\\n                rval = False\\n                for key, val in value.items():\\n                    if entry[key] != val:\\n                        rval = False\\n                        break\\n                else:\\n                    rval = True\\n                return rval\\n\\n            return value in entry\\n\\n        return entry == value\", \"def update(self, path, value, index=None, curr_value=None):\\n        ''' put path, value into a dict '''\\n        try:\\n            entry = Yedit.get_entry(self.yaml_dict, path, self.separator)\\n        except KeyError:\\n            entry = None\\n\\n        if isinstance(entry, dict):\\n            # AUDIT:maybe-no-member makes sense due to fuzzy types\\n            # pylint: disable=maybe-no-member\\n            if not isinstance(value, dict):\\n                raise YeditException('Cannot replace key, value entry in dict with non-dict type. ' +\\n                                     'value=[{}] type=[{}]'.format(value, type(value)))\\n\\n            entry.update(value)\\n            return (True, self.yaml_dict)\\n\\n        elif isinstance(entry, list):\\n            # AUDIT:maybe-no-member makes sense due to fuzzy types\\n            # pylint: disable=maybe-no-member\\n            ind = None\\n            if curr_value:\\n                try:\\n                    ind = entry.index(curr_value)\\n                except ValueError:\\n                    return (False, self.yaml_dict)\\n\\n            elif index is not None:\\n                ind = index\\n\\n            if ind is not None and entry[ind] != value:\\n                entry[ind] = value\\n                return (True, self.yaml_dict)\\n\\n            # see if it exists in the list\\n            try:\\n                ind = entry.index(value)\\n            except ValueError:\\n                # doesn't exist, append it\\n                entry.append(value)\\n                return (True, self.yaml_dict)\\n\\n            # already exists, return\\n            if ind is not None:\\n                return (False, self.yaml_dict)\\n        return (False, self.yaml_dict)\", \"def create(self, path, value):\\n        ''' create a yaml file '''\\n        if not self.file_exists():\\n            # deepcopy didn't work\\n            # Try to use ruamel.yaml and fallback to pyyaml\\n            try:\\n                tmp_copy = yaml.load(yaml.round_trip_dump(self.yaml_dict,\\n                                                          default_flow_style=False),\\n                                     yaml.RoundTripLoader)\\n            except AttributeError:\\n                tmp_copy = copy.deepcopy(self.yaml_dict)\\n\\n            # set the format attributes if available\\n            try:\\n                tmp_copy.fa.set_block_style()\\n            except AttributeError:\\n                pass\\n\\n            result = Yedit.add_entry(tmp_copy, path, value, self.separator)\\n            if result is not None:\\n                self.yaml_dict = tmp_copy\\n                return (True, self.yaml_dict)\\n\\n        return (False, self.yaml_dict)\", \"def get_curr_value(invalue, val_type):\\n        '''return the current value'''\\n        if invalue is None:\\n            return None\\n\\n        curr_value = invalue\\n        if val_type == 'yaml':\\n            curr_value = yaml.load(invalue)\\n        elif val_type == 'json':\\n            curr_value = json.loads(invalue)\\n\\n        return curr_value\", \"def parse_value(inc_value, vtype=''):\\n        '''determine value type passed'''\\n        true_bools = ['y', 'Y', 'yes', 'Yes', 'YES', 'true', 'True', 'TRUE',\\n                      'on', 'On', 'ON', ]\\n        false_bools = ['n', 'N', 'no', 'No', 'NO', 'false', 'False', 'FALSE',\\n                       'off', 'Off', 'OFF']\\n\\n        # It came in as a string but you didn't specify value_type as string\\n        # we will convert to bool if it matches any of the above cases\\n        if isinstance(inc_value, str) and 'bool' in vtype:\\n            if inc_value not in true_bools and inc_value not in false_bools:\\n                raise YeditException('Not a boolean type. str=[{}] vtype=[{}]'.format(inc_value, vtype))\\n        elif isinstance(inc_value, bool) and 'str' in vtype:\\n            inc_value = str(inc_value)\\n\\n        # There is a special case where '' will turn into None after yaml loading it so skip\\n        if isinstance(inc_value, str) and inc_value == '':\\n            pass\\n        # If vtype is not str then go ahead and attempt to yaml load it.\\n        elif isinstance(inc_value, str) and 'str' not in vtype:\\n            try:\\n                inc_value = yaml.safe_load(inc_value)\\n            except Exception:\\n                raise YeditException('Could not determine type of incoming value. ' +\\n                                     'value=[{}] vtype=[{}]'.format(type(inc_value), vtype))\\n\\n        return inc_value\", \"def process_edits(edits, yamlfile):\\n        '''run through a list of edits and process them one-by-one'''\\n        results = []\\n        for edit in edits:\\n            value = Yedit.parse_value(edit['value'], edit.get('value_type', ''))\\n            if edit.get('action') == 'update':\\n                # pylint: disable=line-too-long\\n                curr_value = Yedit.get_curr_value(\\n                    Yedit.parse_value(edit.get('curr_value')),\\n                    edit.get('curr_value_format'))\\n\\n                rval = yamlfile.update(edit['key'],\\n                                       value,\\n                                       edit.get('index'),\\n                                       curr_value)\\n\\n            elif edit.get('action') == 'append':\\n                rval = yamlfile.append(edit['key'], value)\\n\\n            else:\\n                rval = yamlfile.put(edit['key'], value)\\n\\n            if rval[0]:\\n                results.append({'key': edit['key'], 'edit': rval[1]})\\n\\n        return {'changed': len(results) > 0, 'results': results}\", \"def run_ansible(params):\\n        '''perform the idempotent crud operations'''\\n        yamlfile = Yedit(filename=params['src'],\\n                         backup=params['backup'],\\n                         separator=params['separator'])\\n\\n        state = params['state']\\n\\n        if params['src']:\\n            rval = yamlfile.load()\\n\\n            if yamlfile.yaml_dict is None and state != 'present':\\n                return {'failed': True,\\n                        'msg': 'Error opening file [{}].  Verify that the '.format(params['src']) +\\n                               'file exists, that it is has correct permissions, and is valid yaml.'}\\n\\n        if state == 'list':\\n            if params['content']:\\n                content = Yedit.parse_value(params['content'], params['content_type'])\\n                yamlfile.yaml_dict = content\\n\\n            if params['key']:\\n                rval = yamlfile.get(params['key']) or {}\\n\\n            return {'changed': False, 'result': rval, 'state': state}\\n\\n        elif state == 'absent':\\n            if params['content']:\\n                content = Yedit.parse_value(params['content'], params['content_type'])\\n                yamlfile.yaml_dict = content\\n\\n            if params['update']:\\n                rval = yamlfile.pop(params['key'], params['value'])\\n            else:\\n                rval = yamlfile.delete(params['key'])\\n\\n            if rval[0] and params['src']:\\n                yamlfile.write()\\n\\n            return {'changed': rval[0], 'result': rval[1], 'state': state}\\n\\n        elif state == 'present':\\n            # check if content is different than what is in the file\\n            if params['content']:\\n                content = Yedit.parse_value(params['content'], params['content_type'])\\n\\n                # We had no edits to make and the contents are the same\\n                if yamlfile.yaml_dict == content and \\\\\\n                   params['value'] is None:\\n                    return {'changed': False, 'result': yamlfile.yaml_dict, 'state': state}\\n\\n                yamlfile.yaml_dict = content\\n\\n            # If we were passed a key, value then\\n            # we enapsulate it in a list and process it\\n            # Key, Value passed to the module : Converted to Edits list #\\n            edits = []\\n            _edit = {}\\n            if params['value'] is not None:\\n                _edit['value'] = params['value']\\n                _edit['value_type'] = params['value_type']\\n                _edit['key'] = params['key']\\n\\n                if params['update']:\\n                    _edit['action'] = 'update'\\n                    _edit['curr_value'] = params['curr_value']\\n                    _edit['curr_value_format'] = params['curr_value_format']\\n                    _edit['index'] = params['index']\\n\\n                elif params['append']:\\n                    _edit['action'] = 'append'\\n\\n                edits.append(_edit)\\n\\n            elif params['edits'] is not None:\\n                edits = params['edits']\\n\\n            if edits:\\n                results = Yedit.process_edits(edits, yamlfile)\\n\\n                # if there were changes and a src provided to us we need to write\\n                if results['changed'] and params['src']:\\n                    yamlfile.write()\\n\\n                return {'changed': results['changed'], 'result': results['results'], 'state': state}\\n\\n            # no edits to make\\n            if params['src']:\\n                # pylint: disable=redefined-variable-type\\n                rval = yamlfile.write()\\n                return {'changed': rval[0],\\n                        'result': rval[1],\\n                        'state': state}\\n\\n            # We were passed content but no src, key or value, or edits.  Return contents in memory\\n            return {'changed': False, 'result': yamlfile.yaml_dict, 'state': state}\\n        return {'failed': True, 'msg': 'Unkown state passed'}\", 'def locate_oc_binary():\\n    \\'\\'\\' Find and return oc binary file \\'\\'\\'\\n    # https://github.com/openshift/openshift-ansible/issues/3410\\n    # oc can be in /usr/local/bin in some cases, but that may not\\n    # be in $PATH due to ansible/sudo\\n    paths = os.environ.get(\"PATH\", os.defpath).split(os.pathsep) + ADDITIONAL_PATH_LOOKUPS\\n\\n    oc_binary = \\'oc\\'\\n\\n    # Use shutil.which if it is available, otherwise fallback to a naive path search\\n    try:\\n        which_result = shutil.which(oc_binary, path=os.pathsep.join(paths))\\n        if which_result is not None:\\n            oc_binary = which_result\\n    except AttributeError:\\n        for path in paths:\\n            if os.path.exists(os.path.join(path, oc_binary)):\\n                oc_binary = os.path.join(path, oc_binary)\\n                break\\n\\n    return oc_binary', \"def __init__(self,\\n                 namespace,\\n                 kubeconfig='/etc/origin/master/admin.kubeconfig',\\n                 verbose=False,\\n                 all_namespaces=False):\\n        ''' Constructor for OpenshiftCLI '''\\n        self.namespace = namespace\\n        self.verbose = verbose\\n        self.kubeconfig = Utils.create_tmpfile_copy(kubeconfig)\\n        self.all_namespaces = all_namespaces\\n        self.oc_binary = locate_oc_binary()\", \"def _replace_content(self, resource, rname, content, force=False, sep='.'):\\n        ''' replace the current object with the content '''\\n        res = self._get(resource, rname)\\n        if not res['results']:\\n            return res\\n\\n        fname = Utils.create_tmpfile(rname + '-')\\n\\n        yed = Yedit(fname, res['results'][0], separator=sep)\\n        changes = []\\n        for key, value in content.items():\\n            changes.append(yed.put(key, value))\\n\\n        if any([change[0] for change in changes]):\\n            yed.write()\\n\\n            atexit.register(Utils.cleanup, [fname])\\n\\n            return self._replace(fname, force)\\n\\n        return {'returncode': 0, 'updated': False}\", \"def _create_from_content(self, rname, content):\\n        '''create a temporary file and then call oc create on it'''\\n        fname = Utils.create_tmpfile(rname + '-')\\n        yed = Yedit(fname, content=content)\\n        yed.write()\\n\\n        atexit.register(Utils.cleanup, [fname])\\n\\n        return self._create(fname)\", \"def _delete(self, resource, name=None, selector=None):\\n        '''call oc delete on a resource'''\\n        cmd = ['delete', resource]\\n        if selector is not None:\\n            cmd.append('--selector={}'.format(selector))\\n        elif name is not None:\\n            cmd.append(name)\\n        else:\\n            raise OpenShiftCLIError('Either name or selector is required when calling delete.')\\n\\n        return self.openshift_cmd(cmd)\", \"def _get(self, resource, name=None, selector=None):\\n        '''return a resource by name '''\\n        cmd = ['get', resource]\\n        if selector is not None:\\n            cmd.append('--selector={}'.format(selector))\\n        elif name is not None:\\n            cmd.append(name)\\n\\n        cmd.extend(['-o', 'json'])\\n\\n        rval = self.openshift_cmd(cmd, output=True)\\n\\n        # Ensure results are retuned in an array\\n        if 'items' in rval:\\n            rval['results'] = rval['items']\\n        elif not isinstance(rval['results'], list):\\n            rval['results'] = [rval['results']]\\n\\n        return rval\", \"def _list_pods(self, node=None, selector=None, pod_selector=None):\\n        ''' perform oadm list pods\\n\\n            node: the node in which to list pods\\n            selector: the label selector filter if provided\\n            pod_selector: the pod selector filter if provided\\n        '''\\n        cmd = ['manage-node']\\n        if node:\\n            cmd.extend(node)\\n        else:\\n            cmd.append('--selector={}'.format(selector))\\n\\n        if pod_selector:\\n            cmd.append('--pod-selector={}'.format(pod_selector))\\n\\n        cmd.extend(['--list-pods', '-o', 'json'])\\n\\n        return self.openshift_cmd(cmd, oadm=True, output=True, output_type='raw')\", \"def _evacuate(self, node=None, selector=None, pod_selector=None, dry_run=False, grace_period=None, force=False):\\n        ''' perform oadm manage-node evacuate '''\\n        cmd = ['manage-node']\\n        if node:\\n            cmd.extend(node)\\n        else:\\n            cmd.append('--selector={}'.format(selector))\\n\\n        if dry_run:\\n            cmd.append('--dry-run')\\n\\n        if pod_selector:\\n            cmd.append('--pod-selector={}'.format(pod_selector))\\n\\n        if grace_period:\\n            cmd.append('--grace-period={}'.format(int(grace_period)))\\n\\n        if force:\\n            cmd.append('--force')\\n\\n        cmd.append('--evacuate')\\n\\n        return self.openshift_cmd(cmd, oadm=True, output=True, output_type='raw')\", \"def _import_image(self, url=None, name=None, tag=None):\\n        ''' perform image import '''\\n        cmd = ['import-image']\\n\\n        image = '{0}'.format(name)\\n        if tag:\\n            image += ':{0}'.format(tag)\\n\\n        cmd.append(image)\\n\\n        if url:\\n            cmd.append('--from={0}/{1}'.format(url, image))\\n\\n        cmd.append('-n{0}'.format(self.namespace))\\n\\n        cmd.append('--confirm')\\n        return self.openshift_cmd(cmd)\", 'def openshift_cmd(self, cmd, oadm=False, output=False, output_type=\\'json\\', input_data=None):\\n        \\'\\'\\'Base command for oc \\'\\'\\'\\n        cmds = [self.oc_binary]\\n\\n        if oadm:\\n            cmds.append(\\'adm\\')\\n\\n        cmds.extend(cmd)\\n\\n        if self.all_namespaces:\\n            cmds.extend([\\'--all-namespaces\\'])\\n        elif self.namespace is not None and self.namespace.lower() not in [\\'none\\', \\'emtpy\\']:  # E501\\n            cmds.extend([\\'-n\\', self.namespace])\\n\\n        rval = {}\\n        results = \\'\\'\\n        err = None\\n\\n        if self.verbose:\\n            print(\\' \\'.join(cmds))\\n\\n        try:\\n            returncode, stdout, stderr = self._run(cmds, input_data)\\n        except OSError as ex:\\n            returncode, stdout, stderr = 1, \\'\\', \\'Failed to execute {}: {}\\'.format(subprocess.list2cmdline(cmds), ex)\\n\\n        rval = {\"returncode\": returncode,\\n                \"results\": results,\\n                \"cmd\": \\' \\'.join(cmds)}\\n\\n        if returncode == 0:\\n            if output:\\n                if output_type == \\'json\\':\\n                    try:\\n                        rval[\\'results\\'] = json.loads(stdout)\\n                    except ValueError as verr:\\n                        if \"No JSON object could be decoded\" in verr.args:\\n                            err = verr.args\\n                elif output_type == \\'raw\\':\\n                    rval[\\'results\\'] = stdout\\n\\n            if self.verbose:\\n                print(\"STDOUT: {0}\".format(stdout))\\n                print(\"STDERR: {0}\".format(stderr))\\n\\n            if err:\\n                rval.update({\"err\": err,\\n                             \"stderr\": stderr,\\n                             \"stdout\": stdout,\\n                             \"cmd\": cmds})\\n\\n        else:\\n            rval.update({\"stderr\": stderr,\\n                         \"stdout\": stdout,\\n                         \"results\": {}})\\n\\n        return rval', \"def _write(filename, contents):\\n        ''' Actually write the file contents to disk. This helps with mocking. '''\\n\\n        with open(filename, 'w') as sfd:\\n            sfd.write(contents)\", \"def create_tmp_file_from_contents(rname, data, ftype='yaml'):\\n        ''' create a file in tmp with name and contents'''\\n\\n        tmp = Utils.create_tmpfile(prefix=rname)\\n\\n        if ftype == 'yaml':\\n            # AUDIT:no-member makes sense here due to ruamel.YAML/PyYAML usage\\n            # pylint: disable=no-member\\n            if hasattr(yaml, 'RoundTripDumper'):\\n                Utils._write(tmp, yaml.dump(data, Dumper=yaml.RoundTripDumper))\\n            else:\\n                Utils._write(tmp, yaml.safe_dump(data, default_flow_style=False))\\n\\n        elif ftype == 'json':\\n            Utils._write(tmp, json.dumps(data))\\n        else:\\n            Utils._write(tmp, data)\\n\\n        # Register cleanup when module is done\\n        atexit.register(Utils.cleanup, [tmp])\\n        return tmp\", \"def create_tmpfile_copy(inc_file):\\n        '''create a temporary copy of a file'''\\n        tmpfile = Utils.create_tmpfile('lib_openshift-')\\n        Utils._write(tmpfile, open(inc_file).read())\\n\\n        # Cleanup the tmpfile\\n        atexit.register(Utils.cleanup, [tmpfile])\\n\\n        return tmpfile\", \"def create_tmpfile(prefix='tmp'):\\n        ''' Generates and returns a temporary file name '''\\n\\n        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False) as tmp:\\n            return tmp.name\", \"def create_tmp_files_from_contents(content, content_type=None):\\n        '''Turn an array of dict: filename, content into a files array'''\\n        if not isinstance(content, list):\\n            content = [content]\\n        files = []\\n        for item in content:\\n            path = Utils.create_tmp_file_from_contents(item['path'] + '-',\\n                                                       item['data'],\\n                                                       ftype=content_type)\\n            files.append({'name': os.path.basename(item['path']),\\n                          'path': path})\\n        return files\", \"def cleanup(files):\\n        '''Clean up on exit '''\\n        for sfile in files:\\n            if os.path.exists(sfile):\\n                if os.path.isdir(sfile):\\n                    shutil.rmtree(sfile)\\n                elif os.path.isfile(sfile):\\n                    os.remove(sfile)\", \"def exists(results, _name):\\n        ''' Check to see if the results include the name '''\\n        if not results:\\n            return False\\n\\n        if Utils.find_result(results, _name):\\n            return True\\n\\n        return False\", \"def find_result(results, _name):\\n        ''' Find the specified result by name'''\\n        rval = None\\n        for result in results:\\n            if 'metadata' in result and result['metadata']['name'] == _name:\\n                rval = result\\n                break\\n\\n        return rval\", \"def get_resource_file(sfile, sfile_type='yaml'):\\n        ''' return the service file '''\\n        contents = None\\n        with open(sfile) as sfd:\\n            contents = sfd.read()\\n\\n        if sfile_type == 'yaml':\\n            # AUDIT:no-member makes sense here due to ruamel.YAML/PyYAML usage\\n            # pylint: disable=no-member\\n            if hasattr(yaml, 'RoundTripLoader'):\\n                contents = yaml.load(contents, yaml.RoundTripLoader)\\n            else:\\n                contents = yaml.safe_load(contents)\\n        elif sfile_type == 'json':\\n            contents = json.loads(contents)\\n\\n        return contents\", 'def filter_versions(stdout):\\n        \\'\\'\\' filter the oc version output \\'\\'\\'\\n\\n        version_dict = {}\\n        version_search = [\\'oc\\', \\'openshift\\', \\'kubernetes\\']\\n\\n        for line in stdout.strip().split(\\'\\\\n\\'):\\n            for term in version_search:\\n                if not line:\\n                    continue\\n                if line.startswith(term):\\n                    version_dict[term] = line.split()[-1]\\n\\n        # horrible hack to get openshift version in Openshift 3.2\\n        #  By default \"oc version in 3.2 does not return an \"openshift\" version\\n        if \"openshift\" not in version_dict:\\n            version_dict[\"openshift\"] = version_dict[\"oc\"]\\n\\n        return version_dict', 'def add_custom_versions(versions):\\n        \\'\\'\\' create custom versions strings \\'\\'\\'\\n\\n        versions_dict = {}\\n\\n        for tech, version in versions.items():\\n            # clean up \"-\" from version\\n            if \"-\" in version:\\n                version = version.split(\"-\")[0]\\n\\n            if version.startswith(\\'v\\'):\\n                versions_dict[tech + \\'_numeric\\'] = version[1:].split(\\'+\\')[0]\\n                # \"v3.3.0.33\" is what we have, we want \"3.3\"\\n                versions_dict[tech + \\'_short\\'] = version[1:4]\\n\\n        return versions_dict', \"def openshift_installed():\\n        ''' check if openshift is installed '''\\n        import yum\\n\\n        yum_base = yum.YumBase()\\n        if yum_base.rpmdb.searchNevra(name='atomic-openshift'):\\n            return True\\n\\n        return False\", 'def check_def_equal(user_def, result_def, skip_keys=None, debug=False):\\n        \\'\\'\\' Given a user defined definition, compare it with the results given back by our query.  \\'\\'\\'\\n\\n        # Currently these values are autogenerated and we do not need to check them\\n        skip = [\\'metadata\\', \\'status\\']\\n        if skip_keys:\\n            skip.extend(skip_keys)\\n\\n        for key, value in result_def.items():\\n            if key in skip:\\n                continue\\n\\n            # Both are lists\\n            if isinstance(value, list):\\n                if key not in user_def:\\n                    if debug:\\n                        print(\\'User data does not have key [%s]\\' % key)\\n                        print(\\'User data: %s\\' % user_def)\\n                    return False\\n\\n                if not isinstance(user_def[key], list):\\n                    if debug:\\n                        print(\\'user_def[key] is not a list key=[%s] user_def[key]=%s\\' % (key, user_def[key]))\\n                    return False\\n\\n                if len(user_def[key]) != len(value):\\n                    if debug:\\n                        print(\"List lengths are not equal.\")\\n                        print(\"key=[%s]: user_def[%s] != value[%s]\" % (key, len(user_def[key]), len(value)))\\n                        print(\"user_def: %s\" % user_def[key])\\n                        print(\"value: %s\" % value)\\n                    return False\\n\\n                for values in zip(user_def[key], value):\\n                    if isinstance(values[0], dict) and isinstance(values[1], dict):\\n                        if debug:\\n                            print(\\'sending list - list\\')\\n                            print(type(values[0]))\\n                            print(type(values[1]))\\n                        result = Utils.check_def_equal(values[0], values[1], skip_keys=skip_keys, debug=debug)\\n                        if not result:\\n                            print(\\'list compare returned false\\')\\n                            return False\\n\\n                    elif value != user_def[key]:\\n                        if debug:\\n                            print(\\'value should be identical\\')\\n                            print(user_def[key])\\n                            print(value)\\n                        return False\\n\\n            # recurse on a dictionary\\n            elif isinstance(value, dict):\\n                if key not in user_def:\\n                    if debug:\\n                        print(\"user_def does not have key [%s]\" % key)\\n                    return False\\n                if not isinstance(user_def[key], dict):\\n                    if debug:\\n                        print(\"dict returned false: not instance of dict\")\\n                    return False\\n\\n                # before passing ensure keys match\\n                api_values = set(value.keys()) - set(skip)\\n                user_values = set(user_def[key].keys()) - set(skip)\\n                if api_values != user_values:\\n                    if debug:\\n                        print(\"keys are not equal in dict\")\\n                        print(user_values)\\n                        print(api_values)\\n                    return False\\n\\n                result = Utils.check_def_equal(user_def[key], value, skip_keys=skip_keys, debug=debug)\\n                if not result:\\n                    if debug:\\n                        print(\"dict returned false\")\\n                        print(result)\\n                    return False\\n\\n            # Verify each key, value pair is the same\\n            else:\\n                if key not in user_def or value != user_def[key]:\\n                    if debug:\\n                        print(\"value not equal; user_def does not have key\")\\n                        print(key)\\n                        print(value)\\n                        if key in user_def:\\n                            print(user_def[key])\\n                    return False\\n\\n        if debug:\\n            print(\\'returning true\\')\\n        return True', 'def __init__(self, rname, namespace, kubeconfig, options):\\n        self.kubeconfig = kubeconfig\\n        self.name = rname\\n        self.namespace = namespace\\n        self._options = options', \"def config_options(self):\\n        ''' return config options '''\\n        return self._options\", \"def stringify(self, ascommalist=''):\\n        ''' return the options hash as cli params in a string\\n            if ascommalist is set to the name of a key, and\\n            the value of that key is a dict, format the dict\\n            as a list of comma delimited key=value pairs '''\\n        rval = []\\n        for key in sorted(self.config_options.keys()):\\n            data = self.config_options[key]\\n            if data['include'] \\\\\\n               and (data['value'] or isinstance(data['value'], int)):\\n                if key == ascommalist:\\n                    val = ','.join(['{}={}'.format(kk, vv) for kk, vv in sorted(data['value'].items())])\\n                else:\\n                    val = data['value']\\n                rval.append('--{}={}'.format(key.replace('_', '-'), val))\\n\\n        return rval\", \"def __init__(self, content=None):\\n        ''' Constructor for deploymentconfig '''\\n        if not content:\\n            content = DeploymentConfig.default_deployment_config\\n\\n        super(DeploymentConfig, self).__init__(content=content)\", \"def exists_env_value(self, key, value):\\n        ''' return whether a key, value  pair exists '''\\n        results = self.get_env_vars()\\n        if not results:\\n            return False\\n\\n        for result in results:\\n            if result['name'] == key and result['value'] == value:\\n                return True\\n\\n        return False\", \"def get_env_var(self, key):\\n        '''return a environment variables '''\\n        results = self.get(DeploymentConfig.env_path) or []\\n        if not results:\\n            return None\\n\\n        for env_var in results:\\n            if env_var['name'] == key:\\n                return env_var\\n\\n        return None\", \"def delete_env_var(self, keys):\\n        '''delete a list of keys '''\\n        if not isinstance(keys, list):\\n            keys = [keys]\\n\\n        env_vars_array = self.get_env_vars()\\n        modified = False\\n        idx = None\\n        for key in keys:\\n            for env_idx, env_var in enumerate(env_vars_array):\\n                if env_var['name'] == key:\\n                    idx = env_idx\\n                    break\\n\\n            if idx:\\n                modified = True\\n                del env_vars_array[idx]\\n\\n        if modified:\\n            return True\\n\\n        return False\", \"def exists_volume_mount(self, volume_mount):\\n        ''' return whether a volume mount exists '''\\n        exist_volume_mounts = self.get_volume_mounts()\\n\\n        if not exist_volume_mounts:\\n            return False\\n\\n        volume_mount_found = False\\n        for exist_volume_mount in exist_volume_mounts:\\n            if exist_volume_mount['name'] == volume_mount['name']:\\n                volume_mount_found = True\\n                break\\n\\n        return volume_mount_found\", \"def find_volume_by_name(self, volume, mounts=False):\\n        ''' return the index of a volume '''\\n        volumes = []\\n        if mounts:\\n            volumes = self.get_volume_mounts()\\n        else:\\n            volumes = self.get_volumes()\\n        for exist_volume in volumes:\\n            if exist_volume['name'] == volume['name']:\\n                return exist_volume\\n\\n        return None\", \"def get_volume_mounts(self):\\n        '''return volume mount information '''\\n        return self.get_volumes(mounts=True)\", \"def delete_volume_by_name(self, volume):\\n        '''delete a volume '''\\n        modified = False\\n        exist_volume_mounts = self.get_volume_mounts()\\n        exist_volumes = self.get_volumes()\\n        del_idx = None\\n        for idx, exist_volume in enumerate(exist_volumes):\\n            if 'name' in exist_volume and exist_volume['name'] == volume['name']:\\n                del_idx = idx\\n                break\\n\\n        if del_idx != None:\\n            del exist_volumes[del_idx]\\n            modified = True\\n\\n        del_idx = None\\n        for idx, exist_volume_mount in enumerate(exist_volume_mounts):\\n            if 'name' in exist_volume_mount and exist_volume_mount['name'] == volume['name']:\\n                del_idx = idx\\n                break\\n\\n        if del_idx != None:\\n            del exist_volume_mounts[idx]\\n            modified = True\\n\\n        return modified\", \"def add_volume(self, volume):\\n        ''' add a volume or volume mount to the proper location '''\\n        exist_volumes = self.get_volumes()\\n        if not volume:\\n            return\\n\\n        if not exist_volumes:\\n            self.put(DeploymentConfig.volumes_path, [volume])\\n        else:\\n            exist_volumes.append(volume)\", \"def update_volume(self, volume):\\n        '''place an env in the env var list'''\\n        exist_volumes = self.get_volumes()\\n\\n        if not volume:\\n            return False\\n\\n        # update the volume\\n        update_idx = None\\n        for idx, exist_vol in enumerate(exist_volumes):\\n            if exist_vol['name'] == volume['name']:\\n                update_idx = idx\\n                break\\n\\n        if update_idx != None:\\n            exist_volumes[update_idx] = volume\\n        else:\\n            self.add_volume(volume)\\n\\n        return True\", \"def needs_update_volume(self, volume, volume_mount):\\n        ''' verify a volume update is needed '''\\n        exist_volume = self.find_volume_by_name(volume)\\n        exist_volume_mount = self.find_volume_by_name(volume, mounts=True)\\n        results = []\\n        results.append(exist_volume['name'] == volume['name'])\\n\\n        if 'secret' in volume:\\n            results.append('secret' in exist_volume)\\n            results.append(exist_volume['secret']['secretName'] == volume['secret']['secretName'])\\n            results.append(exist_volume_mount['name'] == volume_mount['name'])\\n            results.append(exist_volume_mount['mountPath'] == volume_mount['mountPath'])\\n\\n        elif 'emptyDir' in volume:\\n            results.append(exist_volume_mount['name'] == volume['name'])\\n            results.append(exist_volume_mount['mountPath'] == volume_mount['mountPath'])\\n\\n        elif 'persistentVolumeClaim' in volume:\\n            pvc = 'persistentVolumeClaim'\\n            results.append(pvc in exist_volume)\\n            if results[-1]:\\n                results.append(exist_volume[pvc]['claimName'] == volume[pvc]['claimName'])\\n\\n                if 'claimSize' in volume[pvc]:\\n                    results.append(exist_volume[pvc]['claimSize'] == volume[pvc]['claimSize'])\\n\\n        elif 'hostpath' in volume:\\n            results.append('hostPath' in exist_volume)\\n            results.append(exist_volume['hostPath']['path'] == volume_mount['mountPath'])\\n\\n        return not all(results)\", \"def __init__(self,\\n                 sname,\\n                 namespace,\\n                 kubeconfig,\\n                 secrets=None):\\n        ''' constructor for handling secret options '''\\n        self.kubeconfig = kubeconfig\\n        self.name = sname\\n        self.namespace = namespace\\n        self.secrets = secrets\\n        self.data = {}\\n\\n        self.create_dict()\", \"def __init__(self, content):\\n        '''secret constructor'''\\n        super(Secret, self).__init__(content=content)\\n        self._secrets = None\", \"def secrets(self):\\n        '''secret property getter'''\\n        if self._secrets is None:\\n            self._secrets = self.get_secrets()\\n        return self._secrets\", \"def secrets(self):\\n        '''secret property setter'''\\n        if self._secrets is None:\\n            self._secrets = self.get_secrets()\\n        return self._secrets\", \"def add_secret(self, key, value):\\n        ''' add a secret '''\\n        if self.secrets:\\n            self.secrets[key] = value\\n        else:\\n            self.put(Secret.secret_path, {key: value})\\n\\n        return True\", \"def find_secret(self, key):\\n        ''' find secret'''\\n        rval = None\\n        try:\\n            rval = self.secrets[key]\\n        except KeyError as _:\\n            return None\\n\\n        return {'key': key, 'value': rval}\", \"def __init__(self,\\n                 sname,\\n                 namespace,\\n                 ports,\\n                 selector=None,\\n                 labels=None,\\n                 cluster_ip=None,\\n                 portal_ip=None,\\n                 session_affinity=None,\\n                 service_type=None,\\n                 external_ips=None):\\n        ''' constructor for handling service options '''\\n        self.name = sname\\n        self.namespace = namespace\\n        self.ports = ports\\n        self.selector = selector\\n        self.labels = labels\\n        self.cluster_ip = cluster_ip\\n        self.portal_ip = portal_ip\\n        self.session_affinity = session_affinity\\n        self.service_type = service_type\\n        self.external_ips = external_ips\\n        self.data = {}\\n\\n        self.create_dict()\", \"def __init__(self, content):\\n        '''Service constructor'''\\n        super(Service, self).__init__(content=content)\", \"def get_selector(self):\\n        ''' get the service selector'''\\n        return self.get(Service.selector_path) or {}\", \"def find_ports(self, inc_port):\\n        ''' find a specific port '''\\n        for port in self.get_ports():\\n            if port['port'] == inc_port['port']:\\n                return port\\n\\n        return None\", \"def add_cluster_ip(self, sip):\\n        '''add cluster ip'''\\n        self.put(Service.cluster_ip, sip)\", \"def get_external_ips(self):\\n        ''' get a list of external_ips '''\\n        return self.get(Service.external_ips) or []\", \"def find_external_ips(self, inc_external_ip):\\n        ''' find a specific external IP '''\\n        val = None\\n        try:\\n            idx = self.get_external_ips().index(inc_external_ip)\\n            val = self.get_external_ips()[idx]\\n        except ValueError:\\n            pass\\n\\n        return val\", \"def create_volume_structure(volume_info):\\n        ''' return a properly structured volume '''\\n        volume_mount = None\\n        volume = {'name': volume_info['name']}\\n        volume_type = volume_info['type'].lower()\\n        if volume_type == 'secret':\\n            volume['secret'] = {}\\n            volume[volume_info['type']] = {'secretName': volume_info['secret_name']}\\n            volume_mount = {'mountPath': volume_info['path'],\\n                            'name': volume_info['name']}\\n        elif volume_type == 'emptydir':\\n            volume['emptyDir'] = {}\\n            volume_mount = {'mountPath': volume_info['path'],\\n                            'name': volume_info['name']}\\n        elif volume_type == 'pvc' or volume_type == 'persistentvolumeclaim':\\n            volume['persistentVolumeClaim'] = {}\\n            volume['persistentVolumeClaim']['claimName'] = volume_info['claimName']\\n            volume['persistentVolumeClaim']['claimSize'] = volume_info['claimSize']\\n        elif volume_type == 'hostpath':\\n            volume['hostPath'] = {}\\n            volume['hostPath']['path'] = volume_info['path']\\n        elif volume_type == 'configmap':\\n            volume['configMap'] = {}\\n            volume['configMap']['name'] = volume_info['configmap_name']\\n            volume_mount = {'mountPath': volume_info['path'],\\n                            'name': volume_info['name']}\\n\\n        return (volume, volume_mount)\", \"def __init__(self,\\n                 config,\\n                 debug):\\n        ''' Constructor for OCVersion '''\\n        super(OCVersion, self).__init__(None, config)\\n        self.debug = debug\", \"def run_ansible(params):\\n        '''run the idempotent ansible code'''\\n        oc_version = OCVersion(params['kubeconfig'], params['debug'])\\n\\n        if params['state'] == 'list':\\n\\n            #pylint: disable=protected-access\\n            result = oc_version.get()\\n            return {'state': params['state'],\\n                    'results': result,\\n                    'changed': False}\", 'def __init__(self, rname, namespace, kubeconfig, registry_options):\\n        super(RegistryConfig, self).__init__(rname, namespace, kubeconfig, registry_options)', \"def __init__(self,\\n                 registry_config,\\n                 verbose=False):\\n        ''' Constructor for Registry\\n\\n           a registry consists of 3 or more parts\\n           - dc/docker-registry\\n           - svc/docker-registry\\n\\n           Parameters:\\n           :registry_config:\\n           :verbose:\\n        '''\\n        super(Registry, self).__init__(registry_config.namespace, registry_config.kubeconfig, verbose)\\n        self.version = OCVersion(registry_config.kubeconfig, verbose)\\n        self.svc_ip = None\\n        self.portal_ip = None\\n        self.config = registry_config\\n        self.verbose = verbose\\n        self.registry_parts = [{'kind': 'dc', 'name': self.config.name},\\n                               {'kind': 'svc', 'name': self.config.name},\\n                              ]\\n\\n        self.__prepared_registry = None\\n        self.volume_mounts = []\\n        self.volumes = []\\n        if self.config.config_options['volume_mounts']['value']:\\n            for volume in self.config.config_options['volume_mounts']['value']:\\n                volume_info = {'secret_name': volume.get('secret_name', None),\\n                               'name':        volume.get('name', None),\\n                               'type':        volume.get('type', None),\\n                               'path':        volume.get('path', None),\\n                               'claimName':   volume.get('claim_name', None),\\n                               'claimSize':   volume.get('claim_size', None),\\n                              }\\n\\n                vol, vol_mount = Volume.create_volume_structure(volume_info)\\n                self.volumes.append(vol)\\n                self.volume_mounts.append(vol_mount)\\n\\n        self.dconfig = None\\n        self.svc = None\", \"def deploymentconfig(self):\\n        ''' deploymentconfig property '''\\n        return self.dconfig\", \"def deploymentconfig(self, config):\\n        ''' setter for deploymentconfig property '''\\n        self.dconfig = config\", \"def service(self):\\n        ''' service property '''\\n        return self.svc\", \"def service(self, config):\\n        ''' setter for service property '''\\n        self.svc = config\", \"def prepared_registry(self):\\n        ''' prepared_registry property '''\\n        if not self.__prepared_registry:\\n            results = self.prepare_registry()\\n            if not results or ('returncode' in results and results['returncode'] != 0):\\n                raise RegistryException('Could not perform registry preparation. {}'.format(results))\\n            self.__prepared_registry = results\\n\\n        return self.__prepared_registry\", \"def prepared_registry(self, data):\\n        ''' setter method for prepared_registry attribute '''\\n        self.__prepared_registry = data\", \"def exists(self):\\n        '''does the object exist?'''\\n        if self.deploymentconfig and self.service:\\n            return True\\n\\n        return False\", 'def prepare_registry(self):\\n        \\'\\'\\' prepare a registry for instantiation \\'\\'\\'\\n        options = self.config.to_option_list(ascommalist=\\'labels\\')\\n\\n        cmd = [\\'registry\\']\\n        cmd.extend(options)\\n        cmd.extend([\\'--dry-run=True\\', \\'-o\\', \\'json\\'])\\n\\n        results = self.openshift_cmd(cmd, oadm=True, output=True, output_type=\\'json\\')\\n        # probably need to parse this\\n        # pylint thinks results is a string\\n        # pylint: disable=no-member\\n        if results[\\'returncode\\'] != 0 and \\'items\\' not in results[\\'results\\']:\\n            raise RegistryException(\\'Could not perform registry preparation. {}\\'.format(results))\\n\\n        service = None\\n        deploymentconfig = None\\n        # pylint: disable=invalid-sequence-index\\n        for res in results[\\'results\\'][\\'items\\']:\\n            if res[\\'kind\\'] == \\'DeploymentConfig\\':\\n                deploymentconfig = DeploymentConfig(res)\\n            elif res[\\'kind\\'] == \\'Service\\':\\n                service = Service(res)\\n\\n        # Verify we got a service and a deploymentconfig\\n        if not service or not deploymentconfig:\\n            return results\\n\\n        # results will need to get parsed here and modifications added\\n        deploymentconfig = DeploymentConfig(self.add_modifications(deploymentconfig))\\n\\n        # modify service ip\\n        if self.svc_ip:\\n            service.put(\\'spec.clusterIP\\', self.svc_ip)\\n        if self.portal_ip:\\n            service.put(\\'spec.portalIP\\', self.portal_ip)\\n\\n        # the dry-run doesn\\'t apply the selector correctly\\n        if self.service:\\n            service.put(\\'spec.selector\\', self.service.get_selector())\\n\\n        # need to create the service and the deploymentconfig\\n        service_file = Utils.create_tmp_file_from_contents(\\'service\\', service.yaml_dict)\\n        deployment_file = Utils.create_tmp_file_from_contents(\\'deploymentconfig\\', deploymentconfig.yaml_dict)\\n\\n        return {\"service\": service,\\n                \"service_file\": service_file,\\n                \"service_update\": False,\\n                \"deployment\": deploymentconfig,\\n                \"deployment_file\": deployment_file,\\n                \"deployment_update\": False}', \"def update(self):\\n        '''run update for the registry.  This performs a replace if required'''\\n        # Store the current service IP\\n        if self.service:\\n            svcip = self.service.get('spec.clusterIP')\\n            if svcip:\\n                self.svc_ip = svcip\\n            portip = self.service.get('spec.portalIP')\\n            if portip:\\n                self.portal_ip = portip\\n\\n        results = []\\n        if self.prepared_registry['deployment_update']:\\n            results.append(self._replace(self.prepared_registry['deployment_file']))\\n        if self.prepared_registry['service_update']:\\n            results.append(self._replace(self.prepared_registry['service_file']))\\n\\n        # Clean up returned results\\n        rval = 0\\n        for result in results:\\n            if result['returncode'] != 0:\\n                rval = result['returncode']\\n\\n        return {'returncode': rval, 'results': results}\", \"def needs_update(self):\\n        ''' check to see if we need to update '''\\n        exclude_list = ['clusterIP', 'portalIP', 'type', 'protocol']\\n        if self.service is None or \\\\\\n                not Utils.check_def_equal(self.prepared_registry['service'].yaml_dict,\\n                                          self.service.yaml_dict,\\n                                          exclude_list,\\n                                          debug=self.verbose):\\n            self.prepared_registry['service_update'] = True\\n\\n        exclude_list = ['dnsPolicy',\\n                        'terminationGracePeriodSeconds',\\n                        'restartPolicy', 'timeoutSeconds',\\n                        'livenessProbe', 'readinessProbe',\\n                        'terminationMessagePath',\\n                        'securityContext',\\n                        'imagePullPolicy',\\n                        'protocol', # ports.portocol: TCP\\n                        'type', # strategy: {'type': 'rolling'}\\n                        'defaultMode', # added on secrets\\n                        'activeDeadlineSeconds', # added in 1.5 for timeouts\\n                       ]\\n\\n        if self.deploymentconfig is None or \\\\\\n                not Utils.check_def_equal(self.prepared_registry['deployment'].yaml_dict,\\n                                          self.deploymentconfig.yaml_dict,\\n                                          exclude_list,\\n                                          debug=self.verbose):\\n            self.prepared_registry['deployment_update'] = True\\n\\n        return self.prepared_registry['deployment_update'] or self.prepared_registry['service_update'] or False\", \"def run_ansible(params, check_mode):\\n        '''run idempotent ansible code'''\\n\\n        registry_options = {'images': {'value': params['images'], 'include': True},\\n                            'latest_images': {'value': params['latest_images'], 'include': True},\\n                            'labels': {'value': params['labels'], 'include': True},\\n                            'ports': {'value': ','.join(params['ports']), 'include': True},\\n                            'replicas': {'value': params['replicas'], 'include': True},\\n                            'selector': {'value': params['selector'], 'include': True},\\n                            'service_account': {'value': params['service_account'], 'include': True},\\n                            'mount_host': {'value': params['mount_host'], 'include': True},\\n                            'env_vars': {'value': params['env_vars'], 'include': False},\\n                            'volume_mounts': {'value': params['volume_mounts'], 'include': False},\\n                            'edits': {'value': params['edits'], 'include': False},\\n                            'tls_key': {'value': params['tls_key'], 'include': True},\\n                            'tls_certificate': {'value': params['tls_certificate'], 'include': True},\\n                           }\\n\\n        # Do not always pass the daemonset and enforce-quota parameters because they are not understood\\n        # by old versions of oc.\\n        # Default value is false. So, it's safe to not pass an explicit false value to oc versions which\\n        # understand these parameters.\\n        if params['daemonset']:\\n            registry_options['daemonset'] = {'value': params['daemonset'], 'include': True}\\n        if params['enforce_quota']:\\n            registry_options['enforce_quota'] = {'value': params['enforce_quota'], 'include': True}\\n\\n        rconfig = RegistryConfig(params['name'],\\n                                 params['namespace'],\\n                                 params['kubeconfig'],\\n                                 registry_options)\\n\\n\\n        ocregistry = Registry(rconfig, params['debug'])\\n\\n        api_rval = ocregistry.get()\\n\\n        state = params['state']\\n        ########\\n        # get\\n        ########\\n        if state == 'list':\\n\\n            if api_rval['returncode'] != 0:\\n                return {'failed': True, 'msg': api_rval}\\n\\n            return {'changed': False, 'results': api_rval, 'state': state}\\n\\n        ########\\n        # Delete\\n        ########\\n        if state == 'absent':\\n            if not ocregistry.exists():\\n                return {'changed': False, 'state': state}\\n\\n            if check_mode:\\n                return {'changed': True, 'msg': 'CHECK_MODE: Would have performed a delete.'}\\n\\n            # Unsure as to why this is angry with the return type.\\n            # pylint: disable=redefined-variable-type\\n            api_rval = ocregistry.delete()\\n\\n            if api_rval['returncode'] != 0:\\n                return {'failed': True, 'msg': api_rval}\\n\\n            return {'changed': True, 'results': api_rval, 'state': state}\\n\\n        if state == 'present':\\n            ########\\n            # Create\\n            ########\\n            if not ocregistry.exists():\\n\\n                if check_mode:\\n                    return {'changed': True, 'msg': 'CHECK_MODE: Would have performed a create.'}\\n\\n                api_rval = ocregistry.create()\\n\\n                if api_rval['returncode'] != 0:\\n                    return {'failed': True, 'msg': api_rval}\\n\\n                return {'changed': True, 'results': api_rval, 'state': state}\\n\\n            ########\\n            # Update\\n            ########\\n            if not params['force'] and not ocregistry.needs_update():\\n                return {'changed': False, 'state': state}\\n\\n            if check_mode:\\n                return {'changed': True, 'msg': 'CHECK_MODE: Would have performed an update.'}\\n\\n            api_rval = ocregistry.update()\\n\\n            if api_rval['returncode'] != 0:\\n                return {'failed': True, 'msg': api_rval}\\n\\n            return {'changed': True, 'results': api_rval, 'state': state}\\n\\n        return {'failed': True, 'msg': 'Unknown state passed. %s' % state}\", \"def main():\\n    '''\\n    ansible oc module for registry\\n    '''\\n\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            state=dict(default='present', type='str',\\n                       choices=['present', 'absent']),\\n            debug=dict(default=False, type='bool'),\\n            namespace=dict(default='default', type='str'),\\n            name=dict(default=None, required=True, type='str'),\\n\\n            kubeconfig=dict(default='/etc/origin/master/admin.kubeconfig', type='str'),\\n            images=dict(default=None, type='str'),\\n            latest_images=dict(default=False, type='bool'),\\n            labels=dict(default=None, type='dict'),\\n            ports=dict(default=['5000'], type='list'),\\n            replicas=dict(default=1, type='int'),\\n            selector=dict(default=None, type='str'),\\n            service_account=dict(default='registry', type='str'),\\n            mount_host=dict(default=None, type='str'),\\n            volume_mounts=dict(default=None, type='list'),\\n            env_vars=dict(default={}, type='dict'),\\n            edits=dict(default=[], type='list'),\\n            enforce_quota=dict(default=False, type='bool'),\\n            force=dict(default=False, type='bool'),\\n            daemonset=dict(default=False, type='bool'),\\n            tls_key=dict(default=None, type='str'),\\n            tls_certificate=dict(default=None, type='str'),\\n        ),\\n\\n        supports_check_mode=True,\\n    )\\n\\n    results = Registry.run_ansible(module.params, module.check_mode)\\n    if 'failed' in results:\\n        module.fail_json(**results)\\n\\n    module.exit_json(**results)\"]}, {'features': [], 'snippets': [\"def main(unused_argv):\\n  tpu_cluster_resolver = contrib_cluster_resolver.TPUClusterResolver(\\n      FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\\n\\n  config = contrib_tpu.RunConfig(\\n      cluster=tpu_cluster_resolver,\\n      model_dir=FLAGS.model_dir,\\n      save_checkpoints_steps=FLAGS.iterations_per_loop,\\n      keep_checkpoint_max=None,\\n      tpu_config=contrib_tpu.TPUConfig(\\n          iterations_per_loop=FLAGS.iterations_per_loop,\\n          num_shards=FLAGS.num_cores,\\n          per_host_input_for_training=contrib_tpu.InputPipelineConfig.PER_HOST_V2))  # pylint: disable=line-too-long\\n\\n  # Input pipelines are slightly different (with regards to shuffling and\\n  # preprocessing) between training and evaluation.\\n  imagenet_train = imagenet_input.ImageNetInput(\\n      is_training=True,\\n      data_dir=FLAGS.data_dir,\\n      use_bfloat16=True,\\n      transpose_input=FLAGS.transpose_input)\\n  imagenet_eval = imagenet_input.ImageNetInput(\\n      is_training=False,\\n      data_dir=FLAGS.data_dir,\\n      use_bfloat16=True,\\n      transpose_input=FLAGS.transpose_input)\\n\\n  if FLAGS.use_fast_lr:\\n    resnet_main.LR_SCHEDULE = [    # (multiplier, epoch to start) tuples\\n        (1.0, 4), (0.1, 21), (0.01, 35), (0.001, 43)\\n    ]\\n    imagenet_train_small = imagenet_input.ImageNetInput(\\n        is_training=True,\\n        image_size=128,\\n        data_dir=FLAGS.data_dir_small,\\n        num_parallel_calls=FLAGS.num_parallel_calls,\\n        use_bfloat16=True,\\n        transpose_input=FLAGS.transpose_input,\\n        cache=True)\\n    imagenet_eval_small = imagenet_input.ImageNetInput(\\n        is_training=False,\\n        image_size=128,\\n        data_dir=FLAGS.data_dir_small,\\n        num_parallel_calls=FLAGS.num_parallel_calls,\\n        use_bfloat16=True,\\n        transpose_input=FLAGS.transpose_input,\\n        cache=True)\\n    imagenet_train_large = imagenet_input.ImageNetInput(\\n        is_training=True,\\n        image_size=288,\\n        data_dir=FLAGS.data_dir,\\n        num_parallel_calls=FLAGS.num_parallel_calls,\\n        use_bfloat16=True,\\n        transpose_input=FLAGS.transpose_input)\\n    imagenet_eval_large = imagenet_input.ImageNetInput(\\n        is_training=False,\\n        image_size=288,\\n        data_dir=FLAGS.data_dir,\\n        num_parallel_calls=FLAGS.num_parallel_calls,\\n        use_bfloat16=True,\\n        transpose_input=FLAGS.transpose_input)\\n\\n  resnet_classifier = contrib_tpu.TPUEstimator(\\n      use_tpu=FLAGS.use_tpu,\\n      model_fn=resnet_main.resnet_model_fn,\\n      config=config,\\n      train_batch_size=FLAGS.train_batch_size,\\n      eval_batch_size=FLAGS.eval_batch_size)\\n\\n  if FLAGS.mode == 'train':\\n    current_step = estimator._load_global_step_from_checkpoint_dir(FLAGS.model_dir)  # pylint: disable=protected-access,line-too-long\\n    batches_per_epoch = NUM_TRAIN_IMAGES / FLAGS.train_batch_size\\n    tf.logging.info('Training for %d steps (%.2f epochs in total). Current'\\n                    ' step %d.' % (FLAGS.train_steps,\\n                                   FLAGS.train_steps / batches_per_epoch,\\n                                   current_step))\\n\\n    start_timestamp = time.time()  # This time will include compilation time\\n\\n    # Write a dummy file at the start of training so that we can measure the\\n    # runtime at each checkpoint from the file write time.\\n    tf.gfile.MkDir(FLAGS.model_dir)\\n    if not tf.gfile.Exists(os.path.join(FLAGS.model_dir, 'START')):\\n      with tf.gfile.GFile(os.path.join(FLAGS.model_dir, 'START'), 'w') as f:\\n        f.write(str(start_timestamp))\\n\\n    if FLAGS.use_fast_lr:\\n      small_steps = int(18 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size)\\n      normal_steps = int(41 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size)\\n      large_steps = int(min(50 * NUM_TRAIN_IMAGES / FLAGS.train_batch_size,\\n                            FLAGS.train_steps))\\n\\n      resnet_classifier.train(\\n          input_fn=imagenet_train_small.input_fn, max_steps=small_steps)\\n      resnet_classifier.train(\\n          input_fn=imagenet_train.input_fn, max_steps=normal_steps)\\n      resnet_classifier.train(\\n          input_fn=imagenet_train_large.input_fn,\\n          max_steps=large_steps)\\n    else:\\n      resnet_classifier.train(\\n          input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)\\n\\n  else:\\n    assert FLAGS.mode == 'eval'\\n\\n    start_timestamp = tf.gfile.Stat(\\n        os.path.join(FLAGS.model_dir, 'START')).mtime_nsec\\n    results = []\\n    eval_steps = NUM_EVAL_IMAGES // FLAGS.eval_batch_size\\n\\n    ckpt_steps = set()\\n    all_files = tf.gfile.ListDirectory(FLAGS.model_dir)\\n    for f in all_files:\\n      mat = re.match(CKPT_PATTERN, f)\\n      if mat is not None:\\n        ckpt_steps.add(int(mat.group('gs')))\\n    ckpt_steps = sorted(list(ckpt_steps))\\n    tf.logging.info('Steps to be evaluated: %s' % str(ckpt_steps))\\n\\n    for step in ckpt_steps:\\n      ckpt = os.path.join(FLAGS.model_dir, 'model.ckpt-%d' % step)\\n\\n      batches_per_epoch = NUM_TRAIN_IMAGES // FLAGS.train_batch_size\\n      current_epoch = step // batches_per_epoch\\n\\n      if FLAGS.use_fast_lr:\\n        if current_epoch < 18:\\n          eval_input_fn = imagenet_eval_small.input_fn\\n        if current_epoch >= 18 and current_epoch < 41:\\n          eval_input_fn = imagenet_eval.input_fn\\n        if current_epoch >= 41:  # 41:\\n          eval_input_fn = imagenet_eval_large.input_fn\\n      else:\\n        eval_input_fn = imagenet_eval.input_fn\\n\\n      end_timestamp = tf.gfile.Stat(ckpt + '.index').mtime_nsec\\n      elapsed_hours = (end_timestamp - start_timestamp) / (1e9 * 3600.0)\\n\\n      tf.logging.info('Starting to evaluate.')\\n      eval_start = time.time()  # This time will include compilation time\\n      eval_results = resnet_classifier.evaluate(\\n          input_fn=eval_input_fn,\\n          steps=eval_steps,\\n          checkpoint_path=ckpt)\\n      eval_time = int(time.time() - eval_start)\\n      tf.logging.info('Eval results: %s. Elapsed seconds: %d' %\\n                      (eval_results, eval_time))\\n      results.append([\\n          current_epoch,\\n          elapsed_hours,\\n          '%.2f' % (eval_results['top_1_accuracy'] * 100),\\n          '%.2f' % (eval_results['top_5_accuracy'] * 100),\\n      ])\\n\\n      time.sleep(60)\\n\\n    with tf.gfile.GFile(os.path.join(FLAGS.model_dir, 'results.tsv'), 'wb') as tsv_file:   # pylint: disable=line-too-long\\n      writer = csv.writer(tsv_file, delimiter='\\\\t')\\n      writer.writerow(['epoch', 'hours', 'top1Accuracy', 'top5Accuracy'])\\n      writer.writerows(results)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_engine_module_name():\\n    engine = salt.engines.Engine({}, \"foobar.start\", {}, {}, {}, {}, name=\"foobar\")\\n    assert engine.name == \"foobar\"']}, {'features': [], 'snippets': ['def resource_setup(cls):\\n        super(VolumesActionsTest, cls).resource_setup()\\n\\n        # Create a test shared volume for attach/detach tests\\n        cls.volume = cls.create_volume()', 'def test_attach_detach_volume_to_instance(self):\\n        \"\"\"Test attaching and detaching volume to instance\"\"\"\\n        # Create a server\\n        server = self.create_server()\\n        # Volume is attached and detached successfully from an instance\\n        self.volumes_client.attach_volume(self.volume[\\'id\\'],\\n                                          instance_uuid=server[\\'id\\'],\\n                                          mountpoint=\\'/dev/%s\\' %\\n                                          CONF.compute.volume_device_name)\\n        waiters.wait_for_volume_resource_status(self.volumes_client,\\n                                                self.volume[\\'id\\'], \\'in-use\\')\\n        self.volumes_client.detach_volume(self.volume[\\'id\\'])\\n        waiters.wait_for_volume_resource_status(self.volumes_client,\\n                                                self.volume[\\'id\\'], \\'available\\')', 'def test_volume_bootable(self):\\n        \"\"\"Test setting and retrieving bootable flag of a volume\"\"\"\\n        for bool_bootable in [True, False]:\\n            self.volumes_client.set_bootable_volume(self.volume[\\'id\\'],\\n                                                    bootable=bool_bootable)\\n            fetched_volume = self.volumes_client.show_volume(\\n                self.volume[\\'id\\'])[\\'volume\\']\\n            # Get Volume information\\n            # NOTE(masayukig): \\'bootable\\' is \"true\" or \"false\" in the current\\n            # cinder implementation. So we need to cast boolean values to str\\n            # and make it lower to compare here.\\n            self.assertEqual(str(bool_bootable).lower(),\\n                             fetched_volume[\\'bootable\\'])', 'def test_get_volume_attachment(self):\\n        \"\"\"Test getting volume attachments\\n\\n        Attach a volume to a server, and then retrieve volume\\'s attachments\\n        info.\\n        \"\"\"\\n        # Create a server\\n        server = self.create_server()\\n        # Verify that a volume\\'s attachment information is retrieved\\n        self.volumes_client.attach_volume(self.volume[\\'id\\'],\\n                                          instance_uuid=server[\\'id\\'],\\n                                          mountpoint=\\'/dev/%s\\' %\\n                                          CONF.compute.volume_device_name)\\n        waiters.wait_for_volume_resource_status(self.volumes_client,\\n                                                self.volume[\\'id\\'],\\n                                                \\'in-use\\')\\n        self.addCleanup(waiters.wait_for_volume_resource_status,\\n                        self.volumes_client,\\n                        self.volume[\\'id\\'], \\'available\\')\\n        self.addCleanup(self.volumes_client.detach_volume, self.volume[\\'id\\'])\\n        volume = self.volumes_client.show_volume(self.volume[\\'id\\'])[\\'volume\\']\\n        attachment = volume[\\'attachments\\'][0]\\n\\n        self.assertEqual(\\'/dev/%s\\' %\\n                         CONF.compute.volume_device_name,\\n                         attachment[\\'device\\'])\\n        self.assertEqual(server[\\'id\\'], attachment[\\'server_id\\'])\\n        self.assertEqual(self.volume[\\'id\\'], attachment[\\'id\\'])\\n        self.assertEqual(self.volume[\\'id\\'], attachment[\\'volume_id\\'])', 'def test_volume_upload(self):\\n        \"\"\"Test uploading volume to create an image\"\"\"\\n        # NOTE(gfidente): the volume uploaded in Glance comes from setUpClass,\\n        # it is shared with the other tests. After it is uploaded in Glance,\\n        # there is no way to delete it from Cinder, so we delete it from Glance\\n        # using the Glance images_client and from Cinder via tearDownClass.\\n        image_name = data_utils.rand_name(self.__class__.__name__ + \\'-Image\\')\\n        body = self.volumes_client.upload_volume(\\n            self.volume[\\'id\\'], image_name=image_name,\\n            disk_format=CONF.volume.disk_format)[\\'os-volume_upload_image\\']\\n        image_id = body[\"image_id\"]\\n        self.addCleanup(test_utils.call_and_ignore_notfound_exc,\\n                        self.images_client.delete_image,\\n                        image_id)\\n        waiters.wait_for_image_status(self.images_client, image_id, \\'active\\')\\n        waiters.wait_for_volume_resource_status(self.volumes_client,\\n                                                self.volume[\\'id\\'], \\'available\\')\\n\\n        image_info = self.images_client.show_image(image_id)\\n        self.assertEqual(image_name, image_info[\\'name\\'])\\n        self.assertEqual(CONF.volume.disk_format, image_info[\\'disk_format\\'])', 'def test_reserve_unreserve_volume(self):\\n        \"\"\"Test reserving and unreserving volume\"\"\"\\n        # Mark volume as reserved.\\n        self.volumes_client.reserve_volume(self.volume[\\'id\\'])\\n        # To get the volume info\\n        body = self.volumes_client.show_volume(self.volume[\\'id\\'])[\\'volume\\']\\n        self.assertIn(\\'attaching\\', body[\\'status\\'])\\n        # Unmark volume as reserved.\\n        self.volumes_client.unreserve_volume(self.volume[\\'id\\'])\\n        # To get the volume info\\n        body = self.volumes_client.show_volume(self.volume[\\'id\\'])[\\'volume\\']\\n        self.assertIn(\\'available\\', body[\\'status\\'])']}, {'features': [], 'snippets': ['def Start(self):\\n    \"\"\".\"\"\"\\n    client = aff4.FACTORY.Open(self.client_id, token=self.token)\\n    self.state.Register(\"knowledge_base\",\\n                        client.Get(client.Schema.KNOWLEDGE_BASE))\\n    self.state.Register(\"labels\", client.GetLabels())\\n    self.state.Register(\"artifacts_wanted\", set())\\n    self.state.Register(\"artifacts_fetched\", set())\\n    self.state.Register(\"checks_run\", [])\\n    self.state.Register(\"checks_with_findings\", [])\\n    self.state.Register(\"results_store\", None)\\n    self.state.Register(\"host_data\", {})\\n    self.CallState(next_state=\"MapArtifactData\")', 'def MapArtifactData(self, responses):\\n    \"\"\"Get processed data, mapped to artifacts.\"\"\"\\n    self.state.artifacts_wanted = checks.CheckRegistry.SelectArtifacts(\\n        os=self.state.knowledge_base.os)\\n    # Fetch Artifacts and map results to the artifacts that generated them.\\n    # This is an inefficient collection, but necessary because results need to\\n    # be mapped to the originating artifact. An alternative would be to have\\n    # rdfvalues labeled with originating artifact ids.\\n    for artifact_id in self.state.artifacts_wanted:\\n      self.CallFlow(\"ArtifactCollectorFlow\", artifact_list=[artifact_id],\\n                    request_data={\"artifact_id\": artifact_id},\\n                    next_state=\"AddResponses\")\\n    self.CallState(next_state=\"RunChecks\")', 'def AddResponses(self, responses):\\n    artifact_id = responses.request_data[\"artifact_id\"]\\n    # TODO(user): Check whether artifact collection succeeded.\\n    self.state.host_data[artifact_id] = list(responses)']}, {'features': [], 'snippets': ['def LoadFromFile(self, file):\\n        self.fs, self.s = wav.read(file)\\n        self.sLength, self.nChans = self.s.shape', 'def __init__(self, *args):']}, {'features': [], 'snippets': ['def testAssertNoMsg(self):\\n    self.assertEqual((0, \\'AssertionError()\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        try:\\n          assert False\\n        except AssertionError as e:\\n          print repr(e)\"\"\")))', 'def testBareAssert(self):\\n    # Assertion errors at the top level of a block should raise:\\n    # https://github.com/google/grumpy/issues/18\\n    want = (0, \\'ok\\\\n\\')\\n    self.assertEqual(want, _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        def foo():\\n         assert False\\n        try:\\n         foo()\\n        except AssertionError:\\n         print \\'ok\\'\\n        else:\\n         print \\'bad\\'\"\"\")))', 'def testAssignName(self):\\n    self.assertEqual((0, \\'bar\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = \\'bar\\'\\n        print foo\"\"\")))', 'def testAssignSubscript(self):\\n    self.assertEqual((0, \"{\\'bar\\': None}\\\\n\"), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = {}\\n        foo[\\'bar\\'] = None\\n        print foo\"\"\")))', 'def testAugAssign(self):\\n    self.assertEqual((0, \\'42\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = 41\\n        foo += 1\\n        print foo\"\"\")))', 'def testAugAssignPow(self):\\n    self.assertEqual((0, \\'64\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = 8\\n        foo **= 2\\n        print foo\"\"\")))', 'def testClassDefWithVar(self):\\n    self.assertEqual((0, \\'abc\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        class Foo(object):\\n          bar = \\'abc\\'\\n        print Foo.bar\"\"\")))', 'def testDeleteClassLocal(self):\\n    self.assertEqual((0, \\'False\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        class Foo(object):\\n          bar = \\'baz\\'\\n          del bar\\n        print hasattr(Foo, \\'bar\\')\"\"\")))', 'def testDeleteLocal(self):\\n    self.assertEqual((0, \\'ok\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        def foo():\\n          bar = 123\\n          del bar\\n          try:\\n            print bar\\n            raise AssertionError\\n          except UnboundLocalError:\\n            print \\'ok\\'\\n        foo()\"\"\")))', 'def testDeleteSubscript(self):\\n    self.assertEqual((0, \\'{}\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = {\\'bar\\': \\'baz\\'}\\n        del foo[\\'bar\\']\\n        print foo\"\"\")))', \"def foo():\\n          print 'bar'\", 'def testExprNameGlobal(self):\\n    self.assertEqual((0, \\'\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        foo = 42\\n        foo\"\"\")))', 'def bar():\\n          foo', 'def testFor(self):\\n    self.assertEqual((0, \\'1\\\\n2\\\\n3\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        for i in (1, 2, 3):\\n          print i\"\"\")))', 'def testForContinue(self):\\n    self.assertEqual((0, \\'1\\\\n2\\\\n3\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        for i in (1, 2, 3):\\n          print i\\n          continue\\n          raise AssertionError\"\"\")))', 'def testForElseBreakNotNested(self):\\n    self.assertRaisesRegexp(\\n        util.ParseError, \"\\'continue\\' not in loop\",\\n        _ParseAndVisit, \\'for i in (1,):\\\\n  pass\\\\nelse:\\\\n  continue\\')', 'def testFunctionDecorator(self):\\n    self.assertEqual((0, \\'<b>foo</b>\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        def bold(fn):\\n          return lambda: \\'<b>\\' + fn() + \\'</b>\\'\\n        @bold\\n        def foo():\\n          return \\'foo\\'\\n        print foo()\"\"\")))', \"def tag(name):\\n          def bold(fn):\\n            return lambda: '<b id=' + name + '>' + fn() + '</b>'\\n          return bold\", \"def foo():\\n          return 'foo'\", 'def testFunctionDef(self):\\n    self.assertEqual((0, \\'bar baz\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        def foo(a, b):\\n          print a, b\\n        foo(\\'bar\\', \\'baz\\')\"\"\")))', \"def gen():\\n          yield 'foo'\\n          yield 'bar'\", \"def testFunctionDefGeneratorReturnValue(self):\\n    self.assertRaisesRegexp(\\n        util.ParseError, 'returning a value in a generator function',\\n        _ParseAndVisit, 'def foo():\\\\n  yield 1\\\\n  return 2')\", \"def foo():\\n          def bar():\\n            print 'baz'\\n          bar()\", 'def testIf(self):\\n    self.assertEqual((0, \\'foo\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        if 123:\\n          print \\'foo\\'\\n        if \\'\\':\\n          print \\'bar\\'\"\"\")))', 'def testIfElse(self):\\n    self.assertEqual((0, \\'foo\\\\nbar\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        if True:\\n          print \\'foo\\'\\n        else:\\n          print \\'bar\\'\\n        if False:\\n          print \\'foo\\'\\n        else:\\n          print \\'bar\\'\"\"\")))', \"def testImportFutureLateRaises(self):\\n    regexp = 'from __future__ imports must occur at the beginning of the file'\\n    self.assertRaisesRegexp(util.ImportError, regexp, _ParseAndVisit,\\n                            'foo = bar\\\\nfrom __future__ import print_function')\", 'def testImportMember(self):\\n    self.assertEqual((0, \"<type \\'dict\\'>\\\\n\"), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        from sys import modules\\n        print type(modules)\"\"\")))', 'def testImportNative(self):\\n    self.assertEqual((0, \\'1 1000000000\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        from \"__go__/time\" import Nanosecond, Second\\n        print Nanosecond, Second\"\"\")))', 'def testImportNativeType(self):\\n    self.assertEqual((0, \"<type \\'Duration\\'>\\\\n\"), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        from \"__go__/time\" import Duration\\n        print Duration\"\"\")))', 'def testPrintStatement(self):\\n    self.assertEqual((0, \\'abc 123\\\\nfoo bar\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        print \\'abc\\',\\n        print \\'123\\'\\n        print \\'foo\\', \\'bar\\'\"\"\")))', \"def testRaiseExitStatus(self):\\n    self.assertEqual(1, _GrumpRun('raise Exception')[0])\", 'def testRaiseTypeAndArg(self):\\n    self.assertEqual((0, \\'foo\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        try:\\n          raise KeyError(\\'foo\\')\\n          print \\'bad\\'\\n        except KeyError as e:\\n          print e\"\"\")))', 'def testRaiseTraceback(self):\\n    self.assertEqual((0, \\'\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        import sys\\n        try:\\n          try:\\n            raise Exception\\n          except:\\n            e, _, tb = sys.exc_info()\\n            raise e, None, tb\\n        except:\\n          e2, _, tb2 = sys.exc_info()\\n        assert e is e2\\n        assert tb is tb2\"\"\")))', \"def foo():\\n          return 'bar'\", 'def testTryBareExcept(self):\\n    self.assertEqual((0, \\'\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        try:\\n          raise AssertionError\\n        except:\\n          pass\"\"\")))', 'def testTryMultipleExcept(self):\\n    self.assertEqual((0, \\'bar\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        try:\\n          raise AssertionError\\n        except RuntimeError:\\n          print \\'foo\\'\\n        except AssertionError:\\n          print \\'bar\\'\\n        except:\\n          print \\'baz\\'\"\"\")))', 'def testWhile(self):\\n    self.assertEqual((0, \\'2\\\\n1\\\\n\\'), _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        i = 2\\n        while i:\\n          print i\\n          i -= 1\"\"\")))', 'def testWith(self):\\n    self.assertEqual((0, \\'enter\\\\n1\\\\nexit\\\\nenter\\\\n2\\\\nexit\\\\n3\\\\n\\'),\\n                     _GrumpRun(textwrap.dedent(\"\"\"\\\\\\n        class ContextManager(object):\\n          def __enter__(self):\\n            print \"enter\"\\n\\n          def __exit__(self, exc_type, value, traceback):\\n            print \"exit\"\\n\\n        a = ContextManager()\\n\\n        with a:\\n          print 1\\n\\n        try:\\n          with a:\\n            print 2\\n            raise RuntimeError\\n        except RuntimeError:\\n          print 3\\n        \"\"\")))', 'def __enter__(self):\\n            return (1, (2, 3))', \"def testWriteExceptDispatcherBareExcept(self):\\n    visitor = stmt.StatementVisitor(_MakeModuleBlock())\\n    handlers = [ast.ExceptHandler(type=ast.Name(id='foo')),\\n                ast.ExceptHandler(type=None)]\\n    self.assertEqual(visitor._write_except_dispatcher(  # pylint: disable=protected-access\\n        'exc', 'tb', handlers), [1, 2])\\n    expected = re.compile(r'ResolveGlobal\\\\(.*foo.*\\\\bIsInstance\\\\(.*'\\n                          r'goto Label1.*goto Label2', re.DOTALL)\\n    self.assertRegexpMatches(visitor.writer.getvalue(), expected)\", \"def testWriteExceptDispatcherMultipleExcept(self):\\n    visitor = stmt.StatementVisitor(_MakeModuleBlock())\\n    handlers = [ast.ExceptHandler(type=ast.Name(id='foo')),\\n                ast.ExceptHandler(type=ast.Name(id='bar'))]\\n    self.assertEqual(visitor._write_except_dispatcher(  # pylint: disable=protected-access\\n        'exc', 'tb', handlers), [1, 2])\\n    expected = re.compile(\\n        r'ResolveGlobal\\\\(.*foo.*\\\\bif .*\\\\bIsInstance\\\\(.*\\\\{.*goto Label1.*'\\n        r'ResolveGlobal\\\\(.*bar.*\\\\bif .*\\\\bIsInstance\\\\(.*\\\\{.*goto Label2.*'\\n        r'\\\\bRaise\\\\(exc\\\\.ToObject\\\\(\\\\), nil, tb\\\\.ToObject\\\\(\\\\)\\\\)', re.DOTALL)\\n    self.assertRegexpMatches(visitor.writer.getvalue(), expected)\", \"def _ParseAndVisit(source):\\n  mod = pythonparser.parse(source)\\n  _, future_features = imputil.parse_future_features(mod)\\n  importer = imputil.Importer(None, 'foo', 'foo.py', False)\\n  b = block.ModuleBlock(importer, '__main__', '<test>',\\n                        source, future_features)\\n  visitor = stmt.StatementVisitor(b)\\n  visitor.visit(mod)\\n  return visitor\"]}, {'features': [], 'snippets': ['def to_locale(language, to_lower=False):\\r\\n    \"\"\"\\r\\n    Turns a language name (en-us) into a locale name (en_US). If \\'to_lower\\' is\\r\\n    True, the last component is lower-cased (en_us).\\r\\n    \"\"\"\\r\\n    p = language.find(\\'-\\')\\r\\n    if p >= 0:\\r\\n        if to_lower:\\r\\n            return language[:p].lower()+\\'_\\'+language[p+1:].lower()\\r\\n        else:\\r\\n            return language[:p].lower()+\\'_\\'+language[p+1:].upper()\\r\\n    else:\\r\\n        return language.lower()', 'def to_language(locale):\\r\\n    \"\"\"Turns a locale name (en_US) into a language name (en-us).\"\"\"\\r\\n    p = locale.find(\\'_\\')\\r\\n    if p >= 0:\\r\\n        return locale[:p].lower()+\\'-\\'+locale[p+1:].lower()\\r\\n    else:\\r\\n        return locale.lower()', \"def __init__(self, *args, **kw):\\r\\n        from django.conf import settings\\r\\n        gettext_module.GNUTranslations.__init__(self, *args, **kw)\\r\\n        # Starting with Python 2.4, there's a function to define\\r\\n        # the output charset. Before 2.4, the output charset is\\r\\n        # identical with the translation file charset.\\r\\n        try:\\r\\n            self.set_output_charset('utf-8')\\r\\n        except AttributeError:\\r\\n            pass\\r\\n        self.django_output_charset = 'utf-8'\\r\\n        self.__language = '??'\", 'def merge(self, other):\\r\\n        self._catalog.update(other._catalog)', 'def set_language(self, language):\\r\\n        self.__language = language', 'def language(self):\\r\\n        return self.__language', 'def __repr__(self):\\r\\n        return \"<DjangoTranslation lang:%s>\" % self.__language', 'def gettext(self, msgid):\\r\\n        res = self.ugettext(msgid)\\r\\n        return res.encode(self.django_output_charset)', 'def ngettext(self, msgid1, msgid2, n):\\r\\n        res = self.ungettext(msgid1, msgid2, n)\\r\\n        return res.encode(self.django_output_charset)', 'def translation(language):\\r\\n    \"\"\"\\r\\n    Returns a translation object.', 'def _fetch(lang, fallback=None):', \"def _translation(path):\\r\\n            try:\\r\\n                t = gettext_module.translation('django', path, [loc], klass)\\r\\n                t.set_language(lang)\\r\\n                return t\\r\\n            except IOError, e:\\r\\n                return None\", 'def _merge(path):\\r\\n            t = _translation(path)\\r\\n            if t is not None:\\r\\n                if res is None:\\r\\n                    return t\\r\\n                else:\\r\\n                    res.merge(t)\\r\\n            return res', 'def activate(language):\\r\\n    \"\"\"\\r\\n    Fetches the translation object for a given tuple of application name and\\r\\n    language and installs it as the current translation object for the current\\r\\n    thread.\\r\\n    \"\"\"\\r\\n    _active[currentThread()] = translation(language)', 'def deactivate():\\r\\n    \"\"\"\\r\\n    Deinstalls the currently active translation object so that further _ calls\\r\\n    will resolve against the default translation object, again.\\r\\n    \"\"\"\\r\\n    global _active\\r\\n    if currentThread() in _active:\\r\\n        del _active[currentThread()]', 'def deactivate_all():\\r\\n    \"\"\"\\r\\n    Makes the active translation object a NullTranslations() instance. This is\\r\\n    useful when we want delayed translations to appear as the original string\\r\\n    for some reason.\\r\\n    \"\"\"\\r\\n    _active[currentThread()] = gettext_module.NullTranslations()', 'def get_language():\\r\\n    \"\"\"Returns the currently selected language.\"\"\"\\r\\n    t = _active.get(currentThread(), None)\\r\\n    if t is not None:\\r\\n        try:\\r\\n            return to_language(t.language())\\r\\n        except AttributeError:\\r\\n            pass\\r\\n    # If we don\\'t have a real translation object, assume it\\'s the default language.\\r\\n    from django.conf import settings\\r\\n    return settings.LANGUAGE_CODE', 'def get_language_bidi():\\r\\n    \"\"\"\\r\\n    Returns selected language\\'s BiDi layout.\\r\\n    False = left-to-right layout\\r\\n    True = right-to-left layout\\r\\n    \"\"\"\\r\\n    from django.conf import settings', 'def catalog():\\r\\n    \"\"\"\\r\\n    Returns the current active catalog for further processing.\\r\\n    This can be used if you need to modify the catalog or want to access the\\r\\n    whole message catalog instead of just translating one string.\\r\\n    \"\"\"\\r\\n    global _default, _active\\r\\n    t = _active.get(currentThread(), None)\\r\\n    if t is not None:\\r\\n        return t\\r\\n    if _default is None:\\r\\n        from django.conf import settings\\r\\n        _default = translation(settings.LANGUAGE_CODE)\\r\\n    return _default', 'def do_translate(message, translation_function):\\r\\n    \"\"\"\\r\\n    Translates \\'message\\' using the given \\'translation_function\\' name -- which\\r\\n    will be either gettext or ugettext. It uses the current thread to find the\\r\\n    translation object to use. If no current translation is activated, the\\r\\n    message will be run through the default translation object.\\r\\n    \"\"\"\\r\\n    global _default, _active\\r\\n    t = _active.get(currentThread(), None)\\r\\n    if t is not None:\\r\\n        result = getattr(t, translation_function)(message)\\r\\n    else:\\r\\n        if _default is None:\\r\\n            from django.conf import settings\\r\\n            _default = translation(settings.LANGUAGE_CODE)\\r\\n        result = getattr(_default, translation_function)(message)\\r\\n    if isinstance(message, SafeData):\\r\\n        return mark_safe(result)\\r\\n    return result', \"def gettext(message):\\r\\n    return do_translate(message, 'gettext')\", \"def ugettext(message):\\r\\n    return do_translate(message, 'ugettext')\", 'def gettext_noop(message):\\r\\n    \"\"\"\\r\\n    Marks strings for translation but doesn\\'t translate them now. This can be\\r\\n    used to store strings in global variables that should stay in the base\\r\\n    language (because they might be used externally) and will be translated\\r\\n    later.\\r\\n    \"\"\"\\r\\n    return message', 'def do_ntranslate(singular, plural, number, translation_function):\\r\\n    global _default, _active', 'def ngettext(singular, plural, number):\\r\\n    \"\"\"\\r\\n    Returns a UTF-8 bytestring of the translation of either the singular or\\r\\n    plural, based on the number.\\r\\n    \"\"\"\\r\\n    return do_ntranslate(singular, plural, number, \\'ngettext\\')', 'def ungettext(singular, plural, number):\\r\\n    \"\"\"\\r\\n    Returns a unicode strings of the translation of either the singular or\\r\\n    plural, based on the number.\\r\\n    \"\"\"\\r\\n    return do_ntranslate(singular, plural, number, \\'ungettext\\')', 'def check_for_language(lang_code):\\r\\n    \"\"\"\\r\\n    Checks whether there is a global language file for the given language\\r\\n    code. This is used to decide whether a user-provided language is\\r\\n    available. This is only used for language codes from either the cookies or\\r\\n    session.\\r\\n    \"\"\"\\r\\n    from django.conf import settings\\r\\n    globalpath = os.path.join(os.path.dirname(sys.modules[settings.__module__].__file__), \\'locale\\')\\r\\n    if gettext_module.find(\\'django\\', globalpath, [to_locale(lang_code)]) is not None:\\r\\n        return True\\r\\n    else:\\r\\n        return False', 'def get_language_from_request(request):\\r\\n    \"\"\"\\r\\n    Analyzes the request to find what language the user wants the system to\\r\\n    show. Only languages listed in settings.LANGUAGES are taken into account.\\r\\n    If the user requests a sublanguage where we have a main language, we send\\r\\n    out the main language.\\r\\n    \"\"\"\\r\\n    global _accepted\\r\\n    from django.conf import settings\\r\\n    globalpath = os.path.join(os.path.dirname(sys.modules[settings.__module__].__file__), \\'locale\\')\\r\\n    supported = dict(settings.LANGUAGES)', 'def get_date_formats():\\r\\n    \"\"\"\\r\\n    Checks whether translation files provide a translation for some technical\\r\\n    message ID to store date and time formats. If it doesn\\'t contain one, the\\r\\n    formats provided in the settings will be used.\\r\\n    \"\"\"\\r\\n    from django.conf import settings\\r\\n    date_format = ugettext(\\'DATE_FORMAT\\')\\r\\n    datetime_format = ugettext(\\'DATETIME_FORMAT\\')\\r\\n    time_format = ugettext(\\'TIME_FORMAT\\')\\r\\n    if date_format == \\'DATE_FORMAT\\':\\r\\n        date_format = settings.DATE_FORMAT\\r\\n    if datetime_format == \\'DATETIME_FORMAT\\':\\r\\n        datetime_format = settings.DATETIME_FORMAT\\r\\n    if time_format == \\'TIME_FORMAT\\':\\r\\n        time_format = settings.TIME_FORMAT\\r\\n    return date_format, datetime_format, time_format', 'def get_partial_date_formats():\\r\\n    \"\"\"\\r\\n    Checks whether translation files provide a translation for some technical\\r\\n    message ID to store partial date formats. If it doesn\\'t contain one, the\\r\\n    formats provided in the settings will be used.\\r\\n    \"\"\"\\r\\n    from django.conf import settings\\r\\n    year_month_format = ugettext(\\'YEAR_MONTH_FORMAT\\')\\r\\n    month_day_format = ugettext(\\'MONTH_DAY_FORMAT\\')\\r\\n    if year_month_format == \\'YEAR_MONTH_FORMAT\\':\\r\\n        year_month_format = settings.YEAR_MONTH_FORMAT\\r\\n    if month_day_format == \\'MONTH_DAY_FORMAT\\':\\r\\n        month_day_format = settings.MONTH_DAY_FORMAT\\r\\n    return year_month_format, month_day_format', 'def blankout(src, char):\\r\\n    \"\"\"\\r\\n    Changes every non-whitespace character to the given char.\\r\\n    Used in the templatize function.\\r\\n    \"\"\"\\r\\n    return dot_re.sub(char, src)', 'def templatize(src):\\r\\n    \"\"\"\\r\\n    Turns a Django template into something that is understood by xgettext. It\\r\\n    does so by translating the Django translation tags into standard gettext\\r\\n    function invocations.\\r\\n    \"\"\"\\r\\n    from django.template import Lexer, TOKEN_TEXT, TOKEN_VAR, TOKEN_BLOCK\\r\\n    out = StringIO()\\r\\n    intrans = False\\r\\n    inplural = False\\r\\n    singular = []\\r\\n    plural = []\\r\\n    for t in Lexer(src, None).tokenize():\\r\\n        if intrans:\\r\\n            if t.token_type == TOKEN_BLOCK:\\r\\n                endbmatch = endblock_re.match(t.contents)\\r\\n                pluralmatch = plural_re.match(t.contents)\\r\\n                if endbmatch:\\r\\n                    if inplural:\\r\\n                        out.write(\\' ngettext(%r,%r,count) \\' % (\\'\\'.join(singular), \\'\\'.join(plural)))\\r\\n                        for part in singular:\\r\\n                            out.write(blankout(part, \\'S\\'))\\r\\n                        for part in plural:\\r\\n                            out.write(blankout(part, \\'P\\'))\\r\\n                    else:\\r\\n                        out.write(\\' gettext(%r) \\' % \\'\\'.join(singular))\\r\\n                        for part in singular:\\r\\n                            out.write(blankout(part, \\'S\\'))\\r\\n                    intrans = False\\r\\n                    inplural = False\\r\\n                    singular = []\\r\\n                    plural = []\\r\\n                elif pluralmatch:\\r\\n                    inplural = True\\r\\n                else:\\r\\n                    raise SyntaxError(\"Translation blocks must not include other block tags: %s\" % t.contents)\\r\\n            elif t.token_type == TOKEN_VAR:\\r\\n                if inplural:\\r\\n                    plural.append(\\'%%(%s)s\\' % t.contents)\\r\\n                else:\\r\\n                    singular.append(\\'%%(%s)s\\' % t.contents)\\r\\n            elif t.token_type == TOKEN_TEXT:\\r\\n                if inplural:\\r\\n                    plural.append(t.contents)\\r\\n                else:\\r\\n                    singular.append(t.contents)\\r\\n        else:\\r\\n            if t.token_type == TOKEN_BLOCK:\\r\\n                imatch = inline_re.match(t.contents)\\r\\n                bmatch = block_re.match(t.contents)\\r\\n                cmatches = constant_re.findall(t.contents)\\r\\n                if imatch:\\r\\n                    g = imatch.group(1)\\r\\n                    if g[0] == \\'\"\\': g = g.strip(\\'\"\\')\\r\\n                    elif g[0] == \"\\'\": g = g.strip(\"\\'\")\\r\\n                    out.write(\\' gettext(%r) \\' % g)\\r\\n                elif bmatch:\\r\\n                    for fmatch in constant_re.findall(t.contents):\\r\\n                        out.write(\\' _(%s) \\' % fmatch)\\r\\n                    intrans = True\\r\\n                    inplural = False\\r\\n                    singular = []\\r\\n                    plural = []\\r\\n                elif cmatches:\\r\\n                    for cmatch in cmatches:\\r\\n                        out.write(\\' _(%s) \\' % cmatch)\\r\\n                else:\\r\\n                    out.write(blankout(t.contents, \\'B\\'))\\r\\n            elif t.token_type == TOKEN_VAR:\\r\\n                parts = t.contents.split(\\'|\\')\\r\\n                cmatch = constant_re.match(parts[0])\\r\\n                if cmatch:\\r\\n                    out.write(\\' _(%s) \\' % cmatch.group(1))\\r\\n                for p in parts[1:]:\\r\\n                    if p.find(\\':_(\\') >= 0:\\r\\n                        out.write(\\' %s \\' % p.split(\\':\\',1)[1])\\r\\n                    else:\\r\\n                        out.write(blankout(p, \\'F\\'))\\r\\n            else:\\r\\n                out.write(blankout(t.contents, \\'X\\'))\\r\\n    return out.getvalue()', 'def parse_accept_lang_header(lang_string):\\r\\n    \"\"\"\\r\\n    Parses the lang_string, which is the body of an HTTP Accept-Language\\r\\n    header, and returns a list of (lang, q-value), ordered by \\'q\\' values.']}, {'features': [], 'snippets': ['def __init__(self, ebox_data, sensor_type, name):\\n        \"\"\"Initialize the sensor.\"\"\"\\n        self.client_name = name\\n        self.type = sensor_type\\n        self._name = SENSOR_TYPES[sensor_type][0]\\n        self._unit_of_measurement = SENSOR_TYPES[sensor_type][1]\\n        self._icon = SENSOR_TYPES[sensor_type][2]\\n        self.ebox_data = ebox_data\\n        self._state = None', 'def name(self):\\n        \"\"\"Return the name of the sensor.\"\"\"\\n        return f\"{self.client_name} {self._name}\"', 'def state(self):\\n        \"\"\"Return the state of the sensor.\"\"\"\\n        return self._state', 'def unit_of_measurement(self):\\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\\n        return self._unit_of_measurement', 'def icon(self):\\n        \"\"\"Icon to use in the frontend, if any.\"\"\"\\n        return self._icon', 'def __init__(self, username, password, httpsession):\\n        \"\"\"Initialize the data object.\"\"\"\\n        from pyebox import EboxClient\\n\\n        self.client = EboxClient(username, password, REQUESTS_TIMEOUT, httpsession)\\n        self.data = {}']}, {'features': [], 'snippets': ['def testFormatDateTime(self):\\n    \"\"\"Tests the _FormatDateTime function with dynamic time.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n\\n    date_time_string = test_helper._FormatDateTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(date_time_string, \\'2012-06-27T18:17:01.000000+00:00\\')\\n\\n    output_mediator.SetTimezone(\\'Europe/Amsterdam\\')\\n\\n    date_time_string = test_helper._FormatDateTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(date_time_string, \\'2012-06-27T20:17:01.000000+02:00\\')\\n\\n    output_mediator.SetTimezone(\\'UTC\\')\\n    event.date_time = dfdatetime_semantic_time.InvalidTime()\\n\\n    date_time_string = test_helper._FormatDateTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(date_time_string, \\'Invalid\\')', 'def testFormatDisplayName(self):\\n    \"\"\"Tests the _FormatDisplayName function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n    display_name_string = test_helper._FormatDisplayName(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(display_name_string, \\'FAKE:log/syslog.1\\')', 'def testFormatHostname(self):\\n    \"\"\"Tests the _FormatHostname function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n    hostname_string = test_helper._FormatHostname(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(hostname_string, \\'ubuntu\\')', 'def testFormatMACB(self):\\n    \"\"\"Tests the _FormatMACB function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n    macb_string = test_helper._FormatMACB(event, event_data, event_data_stream)\\n    self.assertEqual(macb_string, \\'..C.\\')', 'def testFormatMessageShort(self):\\n    \"\"\"Tests the _FormatMessageShort function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n\\n    formatters_directory_path = self._GetTestFilePath([\\'formatters\\'])\\n    output_mediator.ReadMessageFormattersFromDirectory(\\n        formatters_directory_path)\\n\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n\\n    message_short_string = test_helper._FormatMessageShort(\\n        event, event_data, event_data_stream)\\n\\n    expected_message_short_string = (\\n        \\'Reporter <CRON> PID: 8442 (pam_unix(cron:session): session closed \\'\\n        \\'for user root)\\')\\n    self.assertEqual(message_short_string, expected_message_short_string)', 'def testFormatSourceShort(self):\\n    \"\"\"Tests the _FormatSourceShort function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n\\n    source_short_string = test_helper._FormatSourceShort(\\n        event, event_data, event_data_stream)\\n\\n    self.assertEqual(source_short_string, \\'FILE\\')', 'def testFormatTime(self):\\n    \"\"\"Tests the _FormatTime function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n\\n    # Test with event.date_time\\n    time_string = test_helper._FormatTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(time_string, \\'18:17:01\\')\\n\\n    output_mediator.SetTimezone(\\'Europe/Amsterdam\\')\\n\\n    time_string = test_helper._FormatTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(time_string, \\'20:17:01\\')\\n\\n    output_mediator.SetTimezone(\\'UTC\\')\\n\\n    # Test with event.timestamp\\n    event.date_time = None\\n    time_string = test_helper._FormatTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(time_string, \\'18:17:01\\')\\n\\n    event.timestamp = 0\\n    time_string = test_helper._FormatTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(time_string, \\'--:--:--\\')\\n\\n    event.timestamp = -9223372036854775808\\n    time_string = test_helper._FormatTime(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(time_string, \\'--:--:--\\')', 'def testFormatUsername(self):\\n    \"\"\"Tests the _FormatUsername function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = formatting_helper.FieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n    username_string = test_helper._FormatUsername(\\n        event, event_data, event_data_stream)\\n    self.assertEqual(username_string, \\'-\\')', 'def testGetFormattedField(self):\\n    \"\"\"Tests the GetFormattedField function.\"\"\"\\n    output_mediator = self._CreateOutputMediator()\\n    test_helper = TestFieldFormattingHelper(output_mediator)\\n\\n    event, event_data, event_data_stream = (\\n        containers_test_lib.CreateEventFromValues(self._TEST_EVENTS[0]))\\n    zone_string = test_helper.GetFormattedField(\\n        \\'zone\\', event, event_data, event_data_stream, None)\\n    self.assertEqual(zone_string, \\'UTC\\')']}, {'features': [], 'snippets': ['def increase(rank):\\n    pass']}, {'features': [], 'snippets': ['def setup_clients(cls):\\n        super(MigrationsAdminTest, cls).setup_clients()\\n        cls.client = cls.os_admin.migrations_client', 'def test_list_migrations(self):\\n        \"\"\"Test admin user can get the migrations list\"\"\"\\n        self.client.list_migrations()', 'def test_list_migrations_in_flavor_resize_situation(self):\\n        \"\"\"Admin can get the migrations list containing the resized server\"\"\"\\n        server = self.create_test_server(wait_until=\"ACTIVE\")\\n        server_id = server[\\'id\\']\\n\\n        self.resize_server(server_id, self.flavor_ref_alt)\\n\\n        body = self.client.list_migrations()[\\'migrations\\']\\n\\n        instance_uuids = [x[\\'instance_uuid\\'] for x in body]\\n        self.assertIn(server_id, instance_uuids)', 'def test_resize_server_revert_deleted_flavor(self):\\n        \"\"\"Test reverting resized server with original flavor deleted\\n\\n        Tests that we can revert the resize on an instance whose original\\n        flavor has been deleted.\\n        \"\"\"\\n\\n        # First we have to create a flavor that we can delete so make a copy\\n        # of the normal flavor from which we\\'d create a server.\\n        flavor = self.admin_flavors_client.show_flavor(\\n            self.flavor_ref)[\\'flavor\\']\\n        flavor = self.admin_flavors_client.create_flavor(\\n            name=data_utils.rand_name(\\'test_resize_flavor_\\'),\\n            ram=flavor[\\'ram\\'],\\n            disk=flavor[\\'disk\\'],\\n            vcpus=flavor[\\'vcpus\\']\\n        )[\\'flavor\\']\\n        self.addCleanup(self._flavor_clean_up, flavor[\\'id\\'])\\n\\n        # Set extra specs same as self.flavor_ref for the created flavor,\\n        # because the environment may need some special extra specs to\\n        # create server which should have been contained in\\n        # self.flavor_ref.\\n        extra_spec_keys = self.admin_flavors_client.list_flavor_extra_specs(\\n            self.flavor_ref)[\\'extra_specs\\']\\n        if extra_spec_keys:\\n            self.admin_flavors_client.set_flavor_extra_spec(\\n                flavor[\\'id\\'], **extra_spec_keys)\\n\\n        # Now boot a server with the copied flavor.\\n        server = self.create_test_server(\\n            wait_until=\\'ACTIVE\\', flavor=flavor[\\'id\\'])\\n        server = self.servers_client.show_server(server[\\'id\\'])[\\'server\\']\\n\\n        # If \\'id\\' not in server[\\'flavor\\'], we can only compare the flavor\\n        # details, so here we should save the to-be-deleted flavor\\'s details,\\n        # for the flavor comparison after the server resizing.\\n        if not server[\\'flavor\\'].get(\\'id\\'):\\n            pre_flavor = {}\\n            body = self.flavors_client.show_flavor(flavor[\\'id\\'])[\\'flavor\\']\\n            for key in [\\'name\\', \\'ram\\', \\'vcpus\\', \\'disk\\']:\\n                pre_flavor[key] = body[key]\\n\\n        # Delete the flavor we used to boot the instance.\\n        self._flavor_clean_up(flavor[\\'id\\'])\\n\\n        # Now resize the server and wait for it to go into verify state.\\n        self.servers_client.resize_server(server[\\'id\\'], self.flavor_ref_alt)\\n        waiters.wait_for_server_status(self.servers_client, server[\\'id\\'],\\n                                       \\'VERIFY_RESIZE\\')\\n\\n        # Now revert the resize, it should be OK even though the original\\n        # flavor used to boot the server was deleted.\\n        self.servers_client.revert_resize_server(server[\\'id\\'])\\n        waiters.wait_for_server_status(self.servers_client, server[\\'id\\'],\\n                                       \\'ACTIVE\\')\\n\\n        server = self.servers_client.show_server(server[\\'id\\'])[\\'server\\']\\n        if server[\\'flavor\\'].get(\\'id\\'):\\n            msg = (\\'server flavor is not same as flavor!\\')\\n            self.assertEqual(flavor[\\'id\\'], server[\\'flavor\\'][\\'id\\'], msg)\\n        else:\\n            self.assertEqual(pre_flavor[\\'name\\'],\\n                             server[\\'flavor\\'][\\'original_name\\'],\\n                             \"original_name in server flavor is not same as \"\\n                             \"flavor name!\")\\n            for key in [\\'ram\\', \\'vcpus\\', \\'disk\\']:\\n                msg = (\\'attribute %s in server flavor is not same as \\'\\n                       \\'flavor!\\' % key)\\n                self.assertEqual(pre_flavor[key], server[\\'flavor\\'][key], msg)', 'def test_cold_migration(self):\\n        \"\"\"Test cold migrating server and then confirm the migration\"\"\"\\n        self._test_cold_migrate_server(revert=False)']}, {'features': [], 'snippets': [\"def get_absolute_saagie_url(saagie_url):\\n    if saagie_url.startswith('/'):\\n        return SAAGIE_ROOT_URL + saagie_url\\n    return saagie_url\", 'def __init__(self, status_code):\\n        self.status_code = status_code\\n        super(ResponseError, self).__init__(status_code)', \"def handle_request(self, method):\\n        data = {k: v[0].decode() for k, v in self.request.arguments.items()}\\n        if 'view' not in data:\\n            self.send_error(404)\\n            return\\n        view_name = data.pop('view')\\n        notebook_path = data.pop('notebook_path', None)\\n        notebook_json = data.pop('notebook_json', None)\\n        notebook = Notebook(notebook_path, notebook_json)\\n        try:\\n            template_name, template_data = views.render(\\n                view_name, notebook=notebook, data=data, method=method)\\n        except ResponseError as e:\\n            self.send_error(e.status_code)\\n            return\\n        except:\\n            template_name = 'internal_error.html'\\n            template_data = {'error': traceback.format_exc()}\\n            self.set_status(500)\\n        template_data.update(\\n            notebook=notebook,\\n        )\\n        template = env.get_template(template_name)\\n        self.finish(template.render(template_data))\", \"def post(self):\\n        self.handle_request('POST')\", 'def get(self):\\n        self.finish()', \"def __init__(self, job, run_data):\\n        self.job = job\\n        self.id = run_data['id']\\n        self.status = run_data['status']\\n        self.stderr = run_data.get('logs_err', '')\\n        self.stdout = run_data.get('logs_out', '')\", 'def from_id(cls, notebook, platform_id, job_id):\\n        return SaagieJob(\\n            notebook,\\n            requests.get(JOB_URL_PATTERN % (platform_id, job_id), auth=SAAGIE_BASIC_AUTH_TOKEN).json())', 'def set_as_current(self):\\n        self.notebook.current_job = self', \"def url(self):\\n        return (JOBS_URL_PATTERN + '/%s') % (self.platform_id, self.id)\", \"def admin_url(self):\\n        return get_absolute_saagie_url('/#/manager/%s/job/%s'\\n                                       % (self.platform_id, self.id))\", \"def logs_url(self):\\n        return self.admin_url + '/logs'\", 'def is_started(self):\\n        return self.last_run is not None', \"def details_template_name(self):\\n        return 'include/python_job_details.html'\", 'def __eq__(self, other):\\n        if other is None:\\n            return False\\n        return self.platform_id == other.platform_id and self.id == other.id', \"def __init__(self, notebook, platform_data):\\n        self.notebook = notebook\\n        self.id = platform_data['id']\\n        self.name = platform_data['name']\\n        self.capsule_types = {c['code'] for c in platform_data['capsules']}\", 'def is_supported(self):\\n        return not self.capsule_types.isdisjoint(self.SUPPORTED_CAPSULE_TYPES)', 'def __eq__(self, other):\\n        return self.id == other.id', 'def __new__(cls, path, json):\\n        if path in cls.CACHE:\\n            return cls.CACHE[path]\\n        cls.CACHE[path] = new = super(Notebook, cls).__new__(cls)\\n        return new', 'def name(self):\\n        return os.path.splitext(os.path.basename(self.path))[0]', \"def kernel_name(self):\\n        return self.json['metadata']['kernelspec']['name']\", \"def kernel_display_name(self):\\n        return self.json['metadata']['kernelspec']['display_name']\", \"def get_code(self, indices=None):\\n        cells = self.get_code_cells()\\n        if indices is None:\\n            indices = list(range(len(cells)))\\n        return '\\\\n\\\\n\\\\n'.join([cells[i] for i in indices])\", 'def add(self, func):\\n        self[func.__name__] = func\\n        return func', 'def modal(method, notebook, data):\\n    return {}', \"def is_logged():\\n    if SAAGIE_ROOT_URL is None or SAAGIE_BASIC_AUTH_TOKEN is None:\\n        return False\\n    else:\\n        # Check if Basic token is still valid\\n        is_logged_in = False\\n        try:\\n            response = requests.get(SAAGIE_ROOT_URL + '/api/v1/user-current', auth=SAAGIE_BASIC_AUTH_TOKEN, allow_redirects=False)\\n            is_logged_in = response.ok\\n        except (requests.ConnectionError, requests.RequestException, requests.HTTPError, requests.Timeout) as err:\\n            print ('Error while trying to connect to Saagie: ', err)\\n\\n        if is_logged_in is not True:\\n            # Remove Basic Auth token from globals. It will force a new login phase.\\n            clear_basic_auth_token()\\n\\n        return is_logged_in\", \"def login_form(method, notebook, data):\\n    if method == 'POST':\\n        # check if the given Saagie URL is well formed\\n        if not validators.url(data['saagie_root_url']):\\n            return {'error': 'Invalid URL', 'saagie_root_url': data['saagie_root_url'] or '', 'username': data['username'] or ''}\\n\\n        define_globals(data['saagie_root_url'], data['username'])\\n\\n        try:\\n            basic_token = HTTPBasicAuth(data['username'], data['password'])\\n            current_user_response = requests.get(SAAGIE_ROOT_URL + '/api/v1/user-current', auth=basic_token, allow_redirects=False)\\n\\n            if current_user_response.ok:\\n                # Login succeeded, keep the basic token for future API calls\\n                global SAAGIE_BASIC_AUTH_TOKEN\\n                SAAGIE_BASIC_AUTH_TOKEN = basic_token\\n\\n        except (requests.ConnectionError, requests.RequestException, requests.HTTPError, requests.Timeout) as err:\\n            print ('Error while trying to connect to Saagie: ', err)\\n            return {'error': 'Connection error', 'saagie_root_url': SAAGIE_ROOT_URL, 'username': SAAGIE_USERNAME or ''}\\n\\n        if SAAGIE_BASIC_AUTH_TOKEN is not None:\\n            return views.render('capsule_type_chooser', notebook)\\n\\n        return {'error': 'Invalid URL, username or password.', 'saagie_root_url': SAAGIE_ROOT_URL, 'username': SAAGIE_USERNAME or ''}\\n    if is_logged():\\n        return views.render('capsule_type_chooser', notebook)\\n    return {'error': None, 'saagie_root_url': SAAGIE_ROOT_URL or '', 'username': SAAGIE_USERNAME or ''}\", \"def inner(method, notebook, data, *args, **kwargs):\\n        if not is_logged():\\n            return views.render('login_form', notebook)\\n        return view(method, notebook, data, *args, **kwargs)\", \"def capsule_type_chooser(method, notebook, data):\\n    return {'username': SAAGIE_USERNAME}\", \"def create_job_base_data(data):\\n    return {\\n        'platform_id': data['saagie-platform'],\\n        'category': 'processing',\\n        'name': data['job-name'],\\n        'description': data['description'],\\n        'current': {\\n            'cpu': data['cpu'],\\n            'disk': data['disk'],\\n            'memory': data['ram'],\\n            'isInternalSubDomain': False,\\n            'isInternalPort': False,\\n            'options': {}\\n        }\\n    }\", \"def python_job_form(method, notebook, data):\\n    if method == 'POST':\\n        platform_id = data['saagie-platform']\\n\\n        job_data = create_job_base_data(data)\\n        job_data['capsule_code'] = 'python'\\n        job_data['always_email'] = False\\n        job_data['manual'] = True\\n        job_data['retry'] = ''\\n\\n        current = job_data['current']\\n        current['options']['language_version'] = data['language-version']\\n        current['releaseNote'] = data['release-note']\\n        current['template'] = data['shell-command']\\n        current['file'] = upload_python_script(notebook, data)\\n\\n        new_job_data = requests.post(JOBS_URL_PATTERN % platform_id,\\n                                    json=job_data, auth=SAAGIE_BASIC_AUTH_TOKEN).json()\\n        job = SaagieJob(notebook, new_job_data)\\n        job.set_as_current()\\n        return views.render('starting_job', notebook, {'job': job})\\n\\n    context = get_job_form(method, notebook, data)\\n    context['action'] = '/saagie?view=python_job_form'\\n    context['username'] = SAAGIE_USERNAME\\n    return context\", \"def update_python_job(method, notebook, data):\\n    if method == 'POST':\\n        job = notebook.current_job\\n        platform_id = job.platform_id\\n        data['saagie-platform'] = platform_id\\n        data['job-name'] = job.name\\n        data['description'] = ''\\n        current = create_job_base_data(data)['current']\\n        current['options']['language_version'] = data['language-version']\\n        current['releaseNote'] = data['release-note']\\n        current['template'] = data['shell-command']\\n        current['file'] = upload_python_script(notebook, data)\\n\\n        requests.post(JOB_UPGRADE_URL_PATTERN % (platform_id, job.id),\\n                     json={'current': current}, auth=SAAGIE_BASIC_AUTH_TOKEN)\\n        job.last_run = None\\n        return views.render('starting_job', notebook, {'job': job})\\n\\n    context = get_job_form(method, notebook, data)\\n    context['action'] = '/saagie?view=update_python_job'\\n    context['username'] = SAAGIE_USERNAME\\n    return context\", \"def select_python_job(method, notebook, data):\\n    if method == 'POST':\\n        platform_id, job_id = data['job'].split('-')\\n        notebook.current_job = SaagieJob.from_id(notebook, platform_id, job_id)\\n        return views.render('update_python_job', notebook, data)\\n    jobs_by_platform = []\\n    for platform in notebook.get_platforms():\\n        jobs = platform.get_jobs()\\n        if jobs:\\n            jobs_by_platform.append((platform,\\n                                     list(sorted(jobs, reverse=True))))\\n    return {'jobs_by_platform': jobs_by_platform,\\n            'action': '/saagie?view=select_python_job', 'username': SAAGIE_USERNAME}\", \"def unsupported_kernel(method, notebook, data):\\n    return {'username': SAAGIE_USERNAME}\", \"def starting_job(method, notebook, data):\\n    job = notebook.current_job\\n    job.fetch_logs()\\n    if job.is_started:\\n        return views.render('started_job', notebook, {'job': job})\\n    return {'job': job, 'username': SAAGIE_USERNAME}\", \"def started_job(method, notebook, data):\\n    return {'job': notebook.current_job, 'username': SAAGIE_USERNAME}\", 'def logout(method, notebook, data):\\n    global SAAGIE_BASIC_AUTH_TOKEN\\n    global SAAGIE_ROOT_URL\\n    global SAAGIE_USERNAME\\n    SAAGIE_BASIC_AUTH_TOKEN = None\\n    SAAGIE_ROOT_URL = None\\n    SAAGIE_USERNAME = None\\n    return {}']}, {'features': [], 'snippets': ['def __init__(self, vocab, clusters):\\n        self.vocab = vocab\\n        self.clusters = clusters', 'def __getitem__(self, word):\\n        return self.get_cluster(word)', 'def get_words_on_cluster(self, cluster):\\n        return self.vocab[self.clusters == cluster]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def unescape(s):\\n    s = s.replace(\"&lt;\", \"<\")\\n    s = s.replace(\"&gt;\", \">\")\\n    # this has to be last:\\n    s = s.replace(\"&amp;\", \"&\")\\n    return s', 'def h_fs_get(_,path,eltName=\\'\\'):\\n        from stat import S_ISDIR\\n        data = (escape(open(path).read())\\n                if not S_ISDIR(os.stat(path).st_mode)\\n                else [(p,S_ISDIR(os.stat(path+\\'/\\'+p).st_mode))\\n                        for p in os.listdir(path)])\\n        _.ws.send(json.dumps({\"method\":\"fs_get\",\"result\":[path,data,eltName]}))\\n        pass', 'def h_fs_system(_,path,eltName=\\'\\',cwd=None):\\n        import subprocess as sp\\n        import shlex\\n        data=sp.Popen(shlex.split(path),cwd=cwd,stdout=sp.PIPE, stderr=sp.PIPE).communicate()\\n        _.ws.send(json.dumps({\"method\":\"fs_system\",\"result\":[path,data,eltName]}));\\n        pass', 'def h_fs_rmdir (_,path): os.rmdir(path)', 'def h_fs_unlink(_,path): os.unlink(path)']}, {'features': [], 'snippets': ['def setUp(self):\\n        super(TestCatalog, self).setUp(capture_output=True)', \"def test_add_not_exist(self, isfile_mock):\\n        isfile_mock.return_value = False\\n        self.assertRaisesRegexp(ConfigurationError,\\n                                'Configuration for catalog dummy not found',\\n                                catalog.add, 'dummy')\", \"def test_add_exists(self, isfile_mock, deploy_mock, validate_mock):\\n        isfile_mock.return_value = True\\n        catalog.add('tpch')\\n        filenames = ['tpch.properties']\\n        deploy_mock.assert_called_with(filenames,\\n                                       get_catalog_directory(),\\n                                       constants.REMOTE_CATALOG_DIR,\\n                                       PRESTO_STANDALONE_USER_GROUP)\\n        validate_mock.assert_called_with(filenames)\", \"def test_add_all(self, mock_validate, listdir_mock, isdir_mock,\\n                     deploy_mock):\\n        catalogs = ['tpch.properties', 'another.properties']\\n        listdir_mock.return_value = catalogs\\n        catalog.add()\\n        deploy_mock.assert_called_with(catalogs,\\n                                       get_catalog_directory(),\\n                                       constants.REMOTE_CATALOG_DIR,\\n                                       PRESTO_STANDALONE_USER_GROUP)\", \"def test_add_all_fails_if_dir_not_there(self, isdir_mock, deploy_mock):\\n        isdir_mock.return_value = False\\n        self.assertRaisesRegexp(ConfigFileNotFoundError,\\n                                r'Cannot add catalogs because directory .+'\\n                                r' does not exist',\\n                                catalog.add)\\n        self.assertFalse(deploy_mock.called)\", 'def test_remove(self, local_rm_mock, exists_mock, sudo_mock):\\n        script = (\\'if [ -f /etc/presto/catalog/tpch.properties ] ; \\'\\n                  \\'then rm /etc/presto/catalog/tpch.properties ; \\'\\n                  \\'else echo \"Could not remove catalog \\\\\\'tpch\\\\\\'. \\'\\n                  \\'No such file \\\\\\'/etc/presto/catalog/tpch.properties\\\\\\'\"; fi\\')\\n        exists_mock.return_value = True\\n        fabric.api.env.host = \\'localhost\\'\\n        catalog.remove(\\'tpch\\')\\n        sudo_mock.assert_called_with(script)\\n        local_rm_mock.assert_called_with(get_catalog_directory() +\\n                                         \\'/tpch.properties\\')', \"def test_remove_failure(self, exists_mock, sudo_mock):\\n        exists_mock.return_value = False\\n        fabric.api.env.host = 'localhost'\\n        out = _AttributeString()\\n        out.succeeded = False\\n        sudo_mock.return_value = out\\n        self.assertRaisesRegexp(SystemExit,\\n                                '\\\\\\\\[localhost\\\\\\\\] Failed to remove catalog tpch.',\\n                                catalog.remove,\\n                                'tpch')\", \"def test_remove_no_such_file(self, exists_mock, sudo_mock):\\n        exists_mock.return_value = False\\n        fabric.api.env.host = 'localhost'\\n        error_msg = ('Could not remove catalog tpch: No such file ' +\\n                     os.path.join(get_catalog_directory(), 'tpch.properties'))\\n        out = _AttributeString(error_msg)\\n        out.succeeded = True\\n        sudo_mock.return_value = out\\n        self.assertRaisesRegexp(SystemExit,\\n                                '\\\\\\\\[localhost\\\\\\\\] %s' % error_msg,\\n                                catalog.remove,\\n                                'tpch')\", \"def test_warning_if_connector_dir_empty(self, isdir_mock, listdir_mock):\\n        isdir_mock.return_value = True\\n        listdir_mock.return_value = []\\n        catalog.add()\\n        self.assertEqual('\\\\nWarning: Directory %s is empty. No catalogs will'\\n                         ' be deployed\\\\n\\\\n' % get_catalog_directory(),\\n                         self.test_stderr.getvalue())\", \"def test_add_permission_denied(self, isdir_mock, listdir_mock):\\n        isdir_mock.return_value = True\\n        error_msg = ('Permission denied')\\n        listdir_mock.side_effect = OSError(13, error_msg)\\n        fabric.api.env.host = 'localhost'\\n        self.assertRaisesRegexp(SystemExit, '\\\\[localhost\\\\] %s' % error_msg,\\n                                catalog.add)\", \"def test_remove_os_error(self, remove_file_mock, remove_mock):\\n        fabric.api.env.host = 'localhost'\\n        error = OSError(13, 'Permission denied')\\n        remove_mock.side_effect = error\\n        self.assertRaisesRegexp(OSError, 'Permission denied',\\n                                catalog.remove, 'tpch')\", \"def test_deploy_files(self, put_mock, create_dir_mock):\\n        local_dir = '/my/local/dir'\\n        remote_dir = '/my/remote/dir'\\n        catalog.deploy_files(['a', 'b'], local_dir, remote_dir,\\n                             PRESTO_STANDALONE_USER_GROUP)\\n        create_dir_mock.assert_called_with(remote_dir, PRESTO_STANDALONE_USER_GROUP)\\n        put_mock.assert_any_call('/my/local/dir/a', remote_dir, use_sudo=True,\\n                                 mode=0600)\\n        put_mock.assert_any_call('/my/local/dir/b', remote_dir, use_sudo=True,\\n                                 mode=0600)\", \"def test_validate(self, open_mock, is_file_mock):\\n        is_file_mock.return_value = True\\n        file_obj = open_mock.return_value.__enter__.return_value\\n        file_obj.read.return_value = 'connector.noname=example'\\n\\n        self.assertRaisesRegexp(ConfigurationError,\\n                                'Catalog configuration example.properties '\\n                                'does not contain connector.name',\\n                                catalog.add, 'example')\", \"def test_validate_fail(self, is_file_mock):\\n        is_file_mock.return_value = True\\n\\n        self.assertRaisesRegexp(\\n            SystemExit,\\n            'Error validating ' + os.path.join(get_catalog_directory(), 'example.properties') + '\\\\n\\\\n'\\n            'Underlying exception:\\\\n    No such file or directory',\\n            catalog.add, 'example')\"]}, {'features': [], 'snippets': ['def initConfig(controllerObject):\\n    global controller\\n    controller = controllerObject\\n    logging.debug(\"Adding MySQL OpenStack configuration\")\\n    paramsList = [\\n                  {\"CMD_OPTION\"      : \"mysql-host\",\\n                   \"USAGE\"           : \"The IP address of the server on which to install MySQL\",\\n                   \"PROMPT\"          : \"Enter the IP address of the MySQL server\",\\n                   \"OPTION_LIST\"     : [],\\n                   \"VALIDATORS\"      : [validators.validate_ssh],\\n                   \"DEFAULT_VALUE\"   : utils.get_localhost_ip(),\\n                   \"MASK_INPUT\"      : False,\\n                   \"LOOSE_VALIDATION\": True,\\n                   \"CONF_NAME\"       : \"CONFIG_MYSQL_HOST\",\\n                   \"USE_DEFAULT\"     : False,\\n                   \"NEED_CONFIRM\"    : False,\\n                   \"CONDITION\"       : False },\\n                  {\"CMD_OPTION\"      : \"mysql-user\",\\n                   \"USAGE\"           : \"Username for the MySQL admin user\",\\n                   \"PROMPT\"          : \"Enter the username for the MySQL admin user\",\\n                   \"OPTION_LIST\"     : [],\\n                   \"VALIDATORS\"      : [validators.validate_not_empty],\\n                   \"DEFAULT_VALUE\"   : \"root\",\\n                   \"MASK_INPUT\"      : False,\\n                   \"LOOSE_VALIDATION\": False,\\n                   \"CONF_NAME\"       : \"CONFIG_MYSQL_USER\",\\n                   \"USE_DEFAULT\"     : True,\\n                   \"NEED_CONFIRM\"    : False,\\n                   \"CONDITION\"       : False },\\n                  {\"CMD_OPTION\"      : \"mysql-pw\",\\n                   \"USAGE\"           : \"Password for the MySQL admin user\",\\n                   \"PROMPT\"          : \"Enter the password for the MySQL admin user\",\\n                   \"OPTION_LIST\"     : [],\\n                   \"VALIDATORS\"      : [validators.validate_not_empty],\\n                   \"DEFAULT_VALUE\"   : uuid.uuid4().hex[:16],\\n                   \"MASK_INPUT\"      : True,\\n                   \"LOOSE_VALIDATION\": True,\\n                   \"CONF_NAME\"       : \"CONFIG_MYSQL_PW\",\\n                   \"USE_DEFAULT\"     : False,\\n                   \"NEED_CONFIRM\"    : True,\\n                   \"CONDITION\"       : False },\\n                 ]\\n\\n    groupDict = { \"GROUP_NAME\"            : \"MYSQL\",\\n                  \"DESCRIPTION\"           : \"MySQL Config parameters\",\\n                  \"PRE_CONDITION\"         : lambda x: \\'yes\\',\\n                  \"PRE_CONDITION_MATCH\"   : \"yes\",\\n                  \"POST_CONDITION\"        : False,\\n                  \"POST_CONDITION_MATCH\"  : True}\\n\\n    controller.addGroup(groupDict, paramsList)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, region, name, retention_in_days=7):\\n    super(LogGroup, self).__init__()\\n    self.region = region\\n    self.name = name\\n    self.retention_in_days = retention_in_days', 'def _Delete(self):\\n    \"\"\"Delete the log group.\"\"\"\\n    delete_cmd = util.AWS_PREFIX + [\\n        \\'--region\\', self.region,\\n        \\'logs\\', \\'delete-log-group\\',\\n        \\'--log-group-name\\', self.name\\n    ]\\n    vm_util.IssueCommand(delete_cmd, raise_on_failure=False)', 'def _PostCreate(self):\\n    \"\"\"Set the retention policy.\"\"\"\\n    put_cmd = util.AWS_PREFIX + [\\n        \\'--region\\', self.region,\\n        \\'logs\\', \\'put-retention-policy\\',\\n        \\'--log-group-name\\', self.name,\\n        \\'--retention-in-days\\', str(self.retention_in_days)\\n    ]\\n    vm_util.IssueCommand(put_cmd)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(\\n            self,\\n            poke_interval=60,\\n            timeout=60*60*24*7,\\n            soft_fail=False,\\n            *args, **kwargs):\\n        super(BaseSensorOperator, self).__init__(*args, **kwargs)\\n        self.poke_interval = poke_interval\\n        self.soft_fail = soft_fail\\n        self.timeout = timeout', 'def execute(self, context):\\n        started_at = datetime.now()\\n        while not self.poke(context):\\n            if (datetime.now() - started_at).total_seconds() > self.timeout:\\n                if self.soft_fail:\\n                    raise AirflowSkipException(\\'Snap. Time is OUT.\\')\\n                else:\\n                    raise AirflowSensorTimeout(\\'Snap. Time is OUT.\\')\\n            sleep(self.poke_interval)\\n        logging.info(\"Success criteria met. Exiting.\")', 'def __init__(self, conn_id, sql, *args, **kwargs):\\n        self.sql = sql\\n        self.conn_id = conn_id\\n        super(SqlSensor, self).__init__(*args, **kwargs)', 'def __init__(\\n            self, table, partition_name, schema=\"default\",\\n            mysql_conn_id=\"metastore_mysql\",\\n            *args, **kwargs):\\n\\n        self.partition_name = partition_name\\n        self.table = table\\n        self.schema = schema\\n        self.first_poke = True\\n        self.conn_id = mysql_conn_id\\n        super(SqlSensor, self).__init__(*args, **kwargs)', \"def __init__(\\n            self,\\n            external_dag_id,\\n            external_task_id,\\n            allowed_states=None,\\n            execution_delta=None,\\n            execution_date_fn=None,\\n            *args, **kwargs):\\n        super(ExternalTaskSensor, self).__init__(*args, **kwargs)\\n        self.allowed_states = allowed_states or [State.SUCCESS]\\n        if execution_delta is not None and execution_date_fn is not None:\\n            raise ValueError(\\n                'Only one of `execution_date` or `execution_date_fn` may'\\n                'be provided to ExternalTaskSensor; not both.')\\n\\n        self.execution_delta = execution_delta\\n        self.execution_date_fn = execution_date_fn\\n        self.external_dag_id = external_dag_id\\n        self.external_task_id = external_task_id\", \"def __init__(\\n            self,\\n            partition_names,\\n            metastore_conn_id='metastore_default',\\n            poke_interval=60*3,\\n            *args,\\n            **kwargs):\\n        super(NamedHivePartitionSensor, self).__init__(\\n            poke_interval=poke_interval, *args, **kwargs)\\n\\n        if isinstance(partition_names, basestring):\\n            raise TypeError('partition_names must be an array of strings')\\n\\n        self.metastore_conn_id = metastore_conn_id\\n        self.partition_names = partition_names\\n        self.next_poke_idx = 0\", \"def poke(self, context):\\n\\n        if not hasattr(self, 'hook'):\\n            self.hook = airflow.hooks.hive_hooks.HiveMetastoreHook(\\n                metastore_conn_id=self.metastore_conn_id)\\n\\n        def poke_partition(partition):\\n\\n            schema, table, partition = self.parse_partition_name(partition)\\n\\n            logging.info(\\n                'Poking for {schema}.{table}/{partition}'.format(**locals())\\n            )\\n            return self.hook.check_for_named_partition(\\n                schema, table, partition)\\n\\n        while self.next_poke_idx < len(self.partition_names):\\n            if poke_partition(self.partition_names[self.next_poke_idx]):\\n                self.next_poke_idx += 1\\n            else:\\n                return False\\n\\n        return True\", 'def __init__(\\n            self,\\n            table, partition=\"ds=\\'{{ ds }}\\'\",\\n            metastore_conn_id=\\'metastore_default\\',\\n            schema=\\'default\\',\\n            poke_interval=60*3,\\n            *args, **kwargs):\\n        super(HivePartitionSensor, self).__init__(\\n            poke_interval=poke_interval, *args, **kwargs)\\n        if not partition:\\n            partition = \"ds=\\'{{ ds }}\\'\"\\n        self.metastore_conn_id = metastore_conn_id\\n        self.table = table\\n        self.partition = partition\\n        self.schema = schema', \"def __init__(\\n            self,\\n            filepath,\\n            hdfs_conn_id='hdfs_default',\\n            *args, **kwargs):\\n        super(HdfsSensor, self).__init__(*args, **kwargs)\\n        self.filepath = filepath\\n        self.hdfs_conn_id = hdfs_conn_id\", \"def __init__(\\n            self,\\n            filepath,\\n            webhdfs_conn_id='webhdfs_default',\\n            *args, **kwargs):\\n        super(WebHdfsSensor, self).__init__(*args, **kwargs)\\n        self.filepath = filepath\\n        self.webhdfs_conn_id = webhdfs_conn_id\", 'def __init__(\\n            self, bucket_key,\\n            bucket_name=None,\\n            wildcard_match=False,\\n            s3_conn_id=\\'s3_default\\',\\n            *args, **kwargs):\\n        super(S3KeySensor, self).__init__(*args, **kwargs)\\n        session = settings.Session()\\n        db = session.query(DB).filter(DB.conn_id == s3_conn_id).first()\\n        if not db:\\n            raise AirflowException(\"conn_id doesn\\'t exist in the repository\")\\n        # Parse\\n        if bucket_name is None:\\n            parsed_url = urlparse(bucket_key)\\n            if parsed_url.netloc == \\'\\':\\n                raise AirflowException(\\'Please provide a bucket_name\\')\\n            else:\\n                bucket_name = parsed_url.netloc\\n                if parsed_url.path[0] == \\'/\\':\\n                    bucket_key = parsed_url.path[1:]\\n                else:\\n                    bucket_key = parsed_url.path\\n        self.bucket_name = bucket_name\\n        self.bucket_key = bucket_key\\n        self.wildcard_match = wildcard_match\\n        self.s3_conn_id = s3_conn_id\\n        session.commit()\\n        session.close()', 'def __init__(\\n            self, bucket_name,\\n            prefix, delimiter=\\'/\\',\\n            s3_conn_id=\\'s3_default\\',\\n            *args, **kwargs):\\n        super(S3PrefixSensor, self).__init__(*args, **kwargs)\\n        session = settings.Session()\\n        db = session.query(DB).filter(DB.conn_id == s3_conn_id).first()\\n        if not db:\\n            raise AirflowException(\"conn_id doesn\\'t exist in the repository\")\\n        # Parse\\n        self.bucket_name = bucket_name\\n        self.prefix = prefix\\n        self.delimiter = delimiter\\n        self.full_url = \"s3://\" + bucket_name + \\'/\\' + prefix\\n        self.s3_conn_id = s3_conn_id\\n        session.commit()\\n        session.close()', 'def __init__(self, target_time, *args, **kwargs):\\n        super(TimeSensor, self).__init__(*args, **kwargs)\\n        self.target_time = target_time', 'def __init__(self, delta, *args, **kwargs):\\n        super(TimeDeltaSensor, self).__init__(*args, **kwargs)\\n        self.delta = delta', \"def __init__(self,\\n                 endpoint,\\n                 http_conn_id='http_default',\\n                 params=None,\\n                 headers=None,\\n                 response_check=None,\\n                 extra_options=None, *args, **kwargs):\\n        super(HttpSensor, self).__init__(*args, **kwargs)\\n        self.endpoint = endpoint\\n        self.http_conn_id = http_conn_id\\n        self.params = params or {}\\n        self.headers = headers or {}\\n        self.extra_options = extra_options or {}\\n        self.response_check = response_check\\n\\n        self.hook = hooks.http_hook.HttpHook(method='GET', http_conn_id=http_conn_id)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def JOB_STATES(state):\\n    if state == 'failed':\\n        return BOLD() + RED() + state + ENDC()\\n    elif state == 'done':\\n        return BOLD() + GREEN() + state + ENDC()\\n    elif state in ['running', 'in_progress']:\\n        return GREEN() + state + ENDC()\\n    elif state == 'partially_failed':\\n        return RED() + state + ENDC()\\n    else:\\n        return YELLOW() + state + ENDC()\", 'def get_size_str(size):\\n    \"\"\"\\n    Formats a byte size as a string.\\n\\n    The returned string is no more than 9 characters long.\\n    \"\"\"\\n    if size is None:\\n        return \"0 \" + SIZE_LEVEL[0]\\n    if size == 0:\\n        magnitude = 0\\n        level = 0\\n    else:\\n        magnitude = math.floor(math.log(size, 10))\\n        level = int(min(math.floor(magnitude // 3), 4))\\n    return (\\'%d\\' if level == 0 else \\'%.2f\\') % (float(size) / 2**(level*10)) + \\' \\' + SIZE_LEVEL[level]', 'def get_io_desc(parameter, include_class=True, show_opt=True, app_help_version=False):\\n    # For interactive help, format array:CLASS inputs as:\\n    #   -iNAME=CLASS [-iNAME=... [...]]   # If input is required (needs >=1 inputs)\\n    #   [-iNAME=CLASS [...]]              # If input is optional (needs >=0 inputs\\n    if app_help_version and parameter[\"class\"].startswith(\"array\"):\\n        scalar_parameter = parameter.copy()\\n        # Munge the parameter dict (strip off \"array:\" to turn it into a\\n        # scalar) and recurse\\n        scalar_parameter[\"class\"] = scalar_parameter[\"class\"][6:]\\n        if \"default\" in parameter or parameter.get(\"optional\"):\\n            return \"[\" + get_io_desc(scalar_parameter, include_class=include_class, show_opt=False, app_help_version=app_help_version) + \" [-i%s=... [...]]]\" % (parameter[\"name\"],)\\n        else:\\n            return get_io_desc(scalar_parameter, include_class=include_class, show_opt=False, app_help_version=app_help_version) + \" [-i%s=... [...]]\" % (parameter[\"name\"],)\\n\\n    desc = \"\"\\n    is_optional = False\\n    if show_opt:\\n        if \"default\" in parameter or parameter.get(\"optional\"):\\n            is_optional = True\\n            desc += \"[\"\\n    desc += (\\'-i\\' if app_help_version else \\'\\') + parameter[\"name\"]\\n    include_parens = include_class or \\'type\\' in parameter or \\'default\\' in parameter\\n    if include_parens:\\n        desc += (\"=\" if app_help_version else \" \") + \"(\"\\n    is_first = True\\n    if include_class:\\n        desc += parameter[\"class\"]\\n        is_first = False\\n    if \"type\" in parameter:\\n        if not is_first:\\n            desc += \", \"\\n        else:\\n            is_first = False\\n        desc += \"type \" + parse_typespec(parameter[\"type\"])\\n    if \"default\" in parameter:\\n        if not is_first:\\n            desc += \\', \\'\\n        desc += \\'default=\\' + json.dumps(parameter[\\'default\\'])\\n    if include_parens:\\n        desc += \")\"\\n    if show_opt and is_optional:\\n        desc += \"]\"\\n    return desc', \"def is_job_ref(thing, reftype=dict):\\n    '''\\n    :param thing: something that might be a job-based object reference hash\\n    :param reftype: type that a job-based object reference would be (default is dict)\\n    '''\\n    return isinstance(thing, reftype) and \\\\\\n        ((len(thing) == 2 and \\\\\\n              isinstance(thing.get('field'), basestring) and \\\\\\n              isinstance(thing.get('job'), basestring)) or \\\\\\n             (len(thing) == 1 and \\\\\\n                  isinstance(thing.get('$dnanexus_link'), reftype) and \\\\\\n                  isinstance(thing['$dnanexus_link'].get('field'), basestring) and \\\\\\n                  isinstance(thing['$dnanexus_link'].get('job'), basestring)))\", \"def get_field_from_jbor(thing):\\n    '''\\n    :returns: Output field name from a JBOR\\n\\n    Assumes :func:`is_job_ref` evaluates to True\\n    '''\\n    if '$dnanexus_link' in thing:\\n        return thing['$dnanexus_link']['field']\\n    else:\\n        return thing['field']\", \"def is_metadata_ref(thing, reftype=dict):\\n    return isinstance(thing, reftype) and \\\\\\n        len(thing) == 1 and \\\\\\n        isinstance(thing.get('$dnanexus_link'), reftype) and \\\\\\n        isinstance(thing['$dnanexus_link'].get('metadata'), basestring)\", \"def io_val_to_str(val):\\n    if is_job_ref(val):\\n        # Job-based object references\\n        return jbor_to_str(val)\\n    elif isinstance(val, dict) and '$dnanexus_link' in val:\\n        # DNAnexus link\\n        if isinstance(val['$dnanexus_link'], basestring):\\n            # simple link\\n            return val['$dnanexus_link']\\n        elif 'project' in val['$dnanexus_link'] and 'id' in val['$dnanexus_link']:\\n            return val['$dnanexus_link']['project'] + ':' + val['$dnanexus_link']['id']\\n        else:\\n            return json.dumps(val)\\n    elif isinstance(val, list):\\n        if len(val) == 0:\\n            return '[]'\\n        else:\\n            return '[ ' + ', '.join([io_val_to_str(item) for item in val]) + ' ]'\\n    elif isinstance(val, dict):\\n        return '{ ' + ', '.join([key + ': ' + io_val_to_str(value) for key, value in val.items()]) + ' }'\\n    else:\\n        return json.dumps(val)\", \"def get_io_field(io_hash, defaults=None, delim='=', highlight_fields=()):\\n\\n    def highlight_value(key, value):\\n        if key in highlight_fields:\\n            return YELLOW() + value + ENDC()\\n        else:\\n            return value\\n\\n    if defaults is None:\\n        defaults = {}\\n    if io_hash is None:\\n        return '-'\\n    if len(io_hash) == 0 and len(defaults) == 0:\\n        return '-'\\n    if get_delimiter() is not None:\\n        return ('\\\\n' + get_delimiter()).join([(key + delim + highlight_value(key, io_val_to_str(value))) for key, value in io_hash.items()] +\\n                                             [('[' + key + delim + io_val_to_str(value) + ']') for key, value in defaults.items()])\\n    else:\\n        lines = [fill(key + ' ' + delim + ' ' + highlight_value(key, io_val_to_str(value)),\\n                      initial_indent=' ' * FIELD_NAME_WIDTH,\\n                      subsequent_indent=' ' * (FIELD_NAME_WIDTH + 1),\\n                      break_long_words=False)\\n                 for key, value in io_hash.items()]\\n        lines.extend([fill('[' + key + ' ' + delim + ' ' + io_val_to_str(value) + ']',\\n                           initial_indent=' ' * FIELD_NAME_WIDTH,\\n                           subsequent_indent=' ' * (FIELD_NAME_WIDTH + 1),\\n                           break_long_words=False)\\n                      for key, value in defaults.items()])\\n        return '\\\\n'.join(lines)[FIELD_NAME_WIDTH:]\", 'def render_bundleddepends(thing):\\n    from ..bindings.search import find_one_data_object\\n    from ..exceptions import DXError\\n    bundles = []\\n    for item in thing:\\n        bundle_asset_record = dxpy.DXFile(item[\"id\"][\"$dnanexus_link\"]).get_properties().get(\"AssetBundle\")\\n        asset = None\\n\\n        if bundle_asset_record:\\n            asset = dxpy.DXRecord(bundle_asset_record)\\n\\n        if asset:\\n            try:\\n                bundles.append(asset.describe().get(\"name\") + \" (\" + asset.get_id() + \")\")\\n            except DXError:\\n                asset = None\\n\\n        if not asset:\\n            bundles.append(item[\"name\"] + \" (\" + item[\"id\"][\"$dnanexus_link\"] + \")\")\\n\\n    return bundles', 'def render_stage(title, stage, as_stage_of=None):\\n    lines_to_print = []\\n\\n    if stage[\\'name\\'] is not None:\\n        lines_to_print.append((title, \"{name} ({id})\".format(name=stage[\\'name\\'], id=stage[\\'id\\'])))\\n    else:\\n        lines_to_print.append((title, stage[\\'id\\']))\\n\\n    lines_to_print.append((\\'  Executable\\', stage[\\'executable\\'] + \\\\\\n                           (\" (\" + RED() + \"inaccessible\" + ENDC() + \")\" \\\\\\n                            if stage.get(\\'accessible\\') is False else \"\")))\\n\\n    if \\'execution\\' in stage:\\n        is_cached_result = as_stage_of is not None and \\'parentAnalysis\\' in stage[\\'execution\\'] and \\\\\\n                           stage[\\'execution\\'][\\'parentAnalysis\\'] != as_stage_of\\n        execution_id_str = stage[\\'execution\\'][\\'id\\']\\n        if is_cached_result:\\n            execution_id_str = \"[\" + execution_id_str + \"]\"\\n\\n        if \\'state\\' in stage[\\'execution\\']:\\n            lines_to_print.append((\\'  Execution\\', execution_id_str + \\' (\\' + JOB_STATES(stage[\\'execution\\'][\\'state\\']) + \\')\\'))\\n        else:\\n            lines_to_print.append((\\'  Execution\\', execution_id_str))\\n\\n        if is_cached_result:\\n            lines_to_print.append((\\'  Cached from\\', stage[\\'execution\\'][\\'parentAnalysis\\']))\\n\\n    for line in lines_to_print:\\n        print_field(line[0], line[1])', 'def render_timestamp(timestamp):\\n    return datetime.datetime.fromtimestamp(timestamp//1000).ctime()', 'def print_field(label, value):\\n    if get_delimiter() is not None:\\n        sys.stdout.write(label + get_delimiter() + value + \\'\\\\n\\')\\n    else:\\n        sys.stdout.write(\\n            label + \" \" * (FIELD_NAME_WIDTH-len(label)) + fill(value,\\n                                                               subsequent_indent=\\' \\'*FIELD_NAME_WIDTH,\\n                                                               width_adjustment=-FIELD_NAME_WIDTH) +\\n            \\'\\\\n\\')', \"def print_list_field(label, values):\\n    print_field(label, ('-' if len(values) == 0 else DELIMITER(', ').join(values)))\", 'def print_project_desc(desc, verbose=False):\\n    recognized_fields = [\\n        \\'id\\', \\'class\\', \\'name\\', \\'summary\\', \\'description\\', \\'protected\\', \\'restricted\\', \\'created\\', \\'modified\\',\\n        \\'dataUsage\\', \\'sponsoredDataUsage\\', \\'tags\\', \\'level\\', \\'folders\\', \\'objects\\', \\'permissions\\', \\'properties\\',\\n        \\'appCaches\\', \\'billTo\\', \\'version\\', \\'createdBy\\', \\'totalSponsoredEgressBytes\\', \\'consumedSponsoredEgressBytes\\',\\n        \\'containsPHI\\', \\'databaseUIViewOnly\\', \\'region\\', \\'storageCost\\', \\'pendingTransfer\\',\\'atSpendingLimit\\',\\n        # Following are app container-specific\\n        \\'destroyAt\\', \\'project\\', \\'type\\', \\'app\\', \\'appName\\'\\n    ]\\n\\n    # Basic metadata\\n    print_field(\"ID\", desc[\"id\"])\\n    print_field(\"Class\", desc[\"class\"])\\n    if \"name\" in desc:\\n        print_field(\"Name\", desc[\"name\"])\\n    if \\'summary\\' in desc:\\n        print_field(\"Summary\", desc[\"summary\"])\\n    if \\'description\\' in desc and (verbose or \\'summary\\' not in desc):\\n        print_field(\"Description\", desc[\\'description\\'])\\n    if \\'version\\' in desc and verbose:\\n        print_field(\"Version\", str(desc[\\'version\\']))\\n\\n    # Ownership and permissions\\n    if \\'billTo\\' in desc:\\n        print_field(\"Billed to\",  desc[\\'billTo\\'][5 if desc[\\'billTo\\'].startswith(\\'user-\\') else 0:])\\n    if \\'pendingTransfer\\' in desc and (verbose or desc[\\'pendingTransfer\\'] is not None):\\n        print_json_field(\\'Pending transfer to\\', desc[\\'pendingTransfer\\'])\\n    if \"level\" in desc:\\n        print_field(\"Access level\", desc[\"level\"])\\n    if \\'region\\' in desc:\\n        print_field(\\'Region\\', desc[\\'region\\'])\\n\\n    # Project settings\\n    if \\'protected\\' in desc:\\n        print_json_field(\"Protected\", desc[\"protected\"])\\n    if \\'restricted\\' in desc:\\n        print_json_field(\"Restricted\", desc[\"restricted\"])\\n    if \\'containsPHI\\' in desc:\\n        print_json_field(\\'Contains PHI\\', desc[\\'containsPHI\\'])\\n    if \\'databaseUIViewOnly\\' in desc and desc[\\'databaseUIViewOnly\\']:\\n        print_json_field(\\'Database UI View Only\\', desc[\\'databaseUIViewOnly\\'])\\n\\n    # Usage\\n    print_field(\"Created\", render_timestamp(desc[\\'created\\']))\\n    if \\'createdBy\\' in desc:\\n        print_field(\"Created by\", desc[\\'createdBy\\'][\\'user\\'][desc[\\'createdBy\\'][\\'user\\'].find(\\'-\\') + 1:])\\n    print_field(\"Last modified\", render_timestamp(desc[\\'modified\\']))\\n    print_field(\"Data usage\", (\\'%.2f\\' % desc[\"dataUsage\"]) + \\' GB\\')\\n    if \\'sponsoredDataUsage\\' in desc:\\n        print_field(\"Sponsored data\", (\\'%.2f\\' % desc[\"sponsoredDataUsage\"]) + \\' GB\\')\\n    if \\'storageCost\\' in desc:\\n        print_field(\"Storage cost\", \"$%.3f/month\" % desc[\"storageCost\"])\\n    if \\'totalSponsoredEgressBytes\\' in desc or \\'consumedSponsoredEgressBytes\\' in desc:\\n        total_egress_str = \\'%.2f GB\\' % (desc[\\'totalSponsoredEgressBytes\\'] / 1073741824.,) \\\\\\n                           if \\'totalSponsoredEgressBytes\\' in desc else \\'??\\'\\n        consumed_egress_str = \\'%.2f GB\\' % (desc[\\'consumedSponsoredEgressBytes\\'] / 1073741824.,) \\\\\\n                              if \\'consumedSponsoredEgressBytes\\' in desc else \\'??\\'\\n        print_field(\\'Sponsored egress\\',\\n                    (\\'%s used of %s total\\' % (consumed_egress_str, total_egress_str)))\\n    if \\'atSpendingLimit\\' in desc:\\n        print_json_field(\"At spending limit?\", desc[\\'atSpendingLimit\\'])\\n\\n    # Misc metadata\\n    if \"objects\" in desc:\\n        print_field(\"# Files\", str(desc[\"objects\"]))\\n    if \"folders\" in desc:\\n        print_list_field(\"Folders\", desc[\"folders\"])\\n    if \"permissions\" in desc:\\n        print_list_field(\\n            \"Permissions\",\\n            [key[5 if key.startswith(\\'user-\\') else 0:] + \\':\\' + value for key, value in desc[\"permissions\"].items()]\\n        )\\n    if \\'tags\\' in desc:\\n        print_list_field(\"Tags\", desc[\"tags\"])\\n    if \"properties\" in desc:\\n        print_list_field(\"Properties\", [key + \\'=\\' + value for key, value in desc[\"properties\"].items()])\\n\\n    if \"appCaches\" in desc:\\n        print_json_field(\"App caches\", desc[\"appCaches\"])\\n\\n    # Container-specific\\n    if \\'type\\' in desc:\\n        print_field(\"Container type\", desc[\"type\"])\\n    if \\'project\\' in desc:\\n        print_field(\"Associated project\", desc[\"project\"])\\n    if \\'destroyAt\\' in desc:\\n        print_field(\"To be destroyed\", render_timestamp(desc[\\'modified\\']))\\n    if \\'app\\' in desc:\\n        print_field(\"Associated App ID\", desc[\"app\"])\\n    if \\'appName\\' in desc:\\n        print_field(\"Associated App\", desc[\"appName\"])\\n\\n    for field in desc:\\n        if field not in recognized_fields:\\n            print_json_field(field, desc[field])', 'def print_app_desc(desc, verbose=False):\\n    recognized_fields = [\\'id\\', \\'class\\', \\'name\\', \\'version\\', \\'aliases\\', \\'createdBy\\', \\'created\\', \\'modified\\', \\'deleted\\', \\'published\\', \\'title\\', \\'subtitle\\', \\'description\\', \\'categories\\', \\'access\\', \\'dxapi\\', \\'inputSpec\\', \\'outputSpec\\', \\'runSpec\\', \\'resources\\', \\'billTo\\', \\'installed\\', \\'openSource\\', \\'summary\\', \\'applet\\', \\'installs\\', \\'billing\\', \\'details\\', \\'developerNotes\\',\\n                         \\'authorizedUsers\\']\\n    print_field(\"ID\", desc[\"id\"])\\n    print_field(\"Class\", desc[\"class\"])\\n    if \\'billTo\\' in desc:\\n        print_field(\"Billed to\", desc[\\'billTo\\'][5 if desc[\\'billTo\\'].startswith(\\'user-\\') else 0:])\\n    print_field(\"Name\", desc[\"name\"])\\n    print_field(\"Version\", desc[\"version\"])\\n    print_list_field(\"Aliases\", desc[\"aliases\"])\\n    print_field(\"Created by\", desc[\"createdBy\"][5 if desc[\\'createdBy\\'].startswith(\\'user-\\') else 0:])\\n    print_field(\"Created\", render_timestamp(desc[\\'created\\']))\\n    print_field(\"Last modified\", render_timestamp(desc[\\'modified\\']))\\n    print_field(\"Created from\", desc[\"applet\"])\\n    print_json_field(\\'Installed\\', desc[\\'installed\\'])\\n    print_json_field(\\'Open source\\', desc[\\'openSource\\'])\\n    print_json_field(\\'Deleted\\', desc[\\'deleted\\'])\\n    if not desc[\\'deleted\\']:\\n        advanced_inputs = []\\n        details = desc[\"details\"]\\n        if isinstance(details, dict) and \"advancedInputs\" in details:\\n            if not verbose:\\n                advanced_inputs = details[\"advancedInputs\"]\\n            del details[\"advancedInputs\"]\\n\\n        if \\'published\\' not in desc or desc[\"published\"] < 0:\\n            print_field(\"Published\", \"-\")\\n        else:\\n            print_field(\"Published\", render_timestamp(desc[\\'published\\']))\\n        if \"title\" in desc and desc[\\'title\\'] is not None:\\n            print_field(\"Title\", desc[\"title\"])\\n        if \"subtitle\" in desc and desc[\\'subtitle\\'] is not None:\\n            print_field(\"Subtitle\", desc[\"subtitle\"])\\n        if \\'summary\\' in desc and desc[\\'summary\\'] is not None:\\n            print_field(\"Summary\", desc[\\'summary\\'])\\n        print_list_field(\"Categories\", desc[\"categories\"])\\n        if \\'details\\' in desc:\\n            print_json_field(\"Details\", desc[\"details\"])\\n        print_json_field(\"Access\", desc[\"access\"])\\n        print_field(\"API version\", desc[\"dxapi\"])\\n        if \\'inputSpec\\' in desc:\\n            print_nofill_field(\"Input Spec\", get_io_spec(desc[\"inputSpec\"], skip_fields=advanced_inputs))\\n            print_nofill_field(\"Output Spec\", get_io_spec(desc[\"outputSpec\"]))\\n            print_field(\"Interpreter\", desc[\"runSpec\"][\"interpreter\"])\\n            if \"resources\" in desc[\"runSpec\"]:\\n                print_json_field(\"Resources\", desc[\"runSpec\"][\"resources\"])\\n            if \"bundledDepends\" in desc[\"runSpec\"]:\\n                print_list_field(\"bundledDepends\", render_bundleddepends(desc[\"runSpec\"][\"bundledDepends\"]))\\n            if \"execDepends\" in desc[\"runSpec\"]:\\n                print_list_field(\"execDepends\", render_execdepends(desc[\"runSpec\"][\"execDepends\"]))\\n            if \"systemRequirements\" in desc[\\'runSpec\\']:\\n                print_json_field(\\'Sys Requirements\\', desc[\\'runSpec\\'][\\'systemRequirements\\'])\\n        if \\'resources\\' in desc:\\n            print_field(\"Resources\", desc[\\'resources\\'])\\n    if \\'installs\\' in desc:\\n        print_field(\\'# Installs\\', str(desc[\\'installs\\']))\\n    if \\'authorizedUsers\\' in desc:\\n        print_list_field(\\'AuthorizedUsers\\', desc[\"authorizedUsers\"])\\n\\n    for field in desc:\\n        if field not in recognized_fields:\\n            print_json_field(field, desc[field])', 'def get_col_str(col_desc):\\n    return col_desc[\\'name\\'] + DELIMITER(\" (\") + col_desc[\\'type\\'] + DELIMITER(\")\")', 'def printable_ssh_host_key(ssh_host_key):\\n    try:\\n        keygen = subprocess.Popen([\"ssh-keygen\", \"-lf\", \"/dev/stdin\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\\n        if USING_PYTHON2:\\n            (stdout, stderr) = keygen.communicate(ssh_host_key)\\n        else:\\n            (stdout, stderr) = keygen.communicate(ssh_host_key.encode())\\n    except:\\n        return ssh_host_key.strip()\\n    else:\\n        if not USING_PYTHON2:\\n            stdout =  stdout.decode()\\n        return stdout.replace(\" no comment\", \"\").strip()', 'def locale_from_currency_code(dx_code):\\n    \"\"\"\\n    This is a (temporary) hardcoded mapping between currency_list.json in nucleus and standard\\n    locale string useful for further formatting\\n\\n    :param dx_code: An id of nucleus/commons/pricing_models/currency_list.json collection\\n    :return: standardised locale, eg \\'en_US\\'; None when no mapping found\\n    \"\"\"\\n    currency_locale_map = {0: \\'en_US\\', 1: \\'en_GB\\'}\\n    return currency_locale_map[dx_code] if dx_code in currency_locale_map else None', 'def format_currency(value, meta, currency_locale=None):\\n    \"\"\"\\n    Formats currency value into properly decorated currency string based on either locale (preferred)\\n    or if that is not available then currency metadata. Until locale is provided from the server\\n    a crude mapping between `currency.dxCode` and a locale string is used instead (eg 0: \\'en_US\\')\\n\\n    :param value: amount\\n    :param meta: server metadata (`currency`)\\n    :return: formatted currency string\\n    \"\"\"\\n    try:\\n        if currency_locale is None:\\n            currency_locale = locale_from_currency_code(meta[\\'dxCode\\'])\\n        if currency_locale is None:\\n            return format_currency_from_meta(value, meta)\\n        else:\\n            locale.setlocale(locale.LC_ALL, currency_locale)\\n            return locale.currency(value, grouping=True)\\n    except locale.Error:\\n        # .. locale is probably not available -> fallback to format manually\\n        return format_currency_from_meta(value, meta)', 'def print_generic_desc(desc):\\n    for field in desc:\\n        print_json_field(field, desc[field])', \"def get_ls_desc(desc, print_id=False):\\n    addendum = ' : ' + desc['id'] if print_id is True else ''\\n    if desc['class'] in ['applet', 'workflow']:\\n        return BOLD() + GREEN() + desc['name'] + ENDC() + addendum\\n    else:\\n        return desc['name'] + addendum\", \"def get_ls_l_header():\\n    return (BOLD() +\\n            'State' + DELIMITER('   ') +\\n            'Last modified' + DELIMITER('       ') +\\n            'Size' + DELIMITER('      ') +\\n            'Name' + DELIMITER(' (') +\\n            'ID' + DELIMITER(')') +\\n            ENDC())\", \"def get_ls_l_desc_fields():\\n    return {\\n        'id': True,\\n        'class': True,\\n        'folder': True,\\n        'length': True,\\n        'modified': True,\\n        'name': True,\\n        'project': True,\\n        'size': True,\\n        'state': True\\n    }\", 'def print_ls_l_desc(desc, **kwargs):\\n    print(get_ls_l_desc(desc, **kwargs))']}, {'features': [], 'snippets': ['def testByteNet(self):\\n    vocab_size = 9\\n    x = np.random.random_integers(1, high=vocab_size - 1, size=(3, 5, 1, 1))\\n    y = np.random.random_integers(1, high=vocab_size - 1, size=(3, 6, 1, 1))\\n    hparams = bytenet.bytenet_base()\\n    p_hparams = problem_hparams.test_problem_hparams(vocab_size, vocab_size)\\n    with self.test_session() as session:\\n      features = {\\n          \"inputs\": tf.constant(x, dtype=tf.int32),\\n          \"targets\": tf.constant(y, dtype=tf.int32),\\n      }\\n      model = bytenet.ByteNet(\\n          hparams, tf.estimator.ModeKeys.TRAIN, p_hparams)\\n      logits, _ = model(features)\\n      session.run(tf.global_variables_initializer())\\n      res = session.run(logits)\\n    self.assertEqual(res.shape, (3, 50, 1, 1, vocab_size))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, rule):\\n        msg = _(\"Policy doesn\\'t allow %s to be performed.\") % rule\\n        super(PolicyNotAuthorized, self).__init__(msg)', 'def load_json(cls, data, default_rule=None):\\n        \"\"\"Allow loading of JSON rule data.\"\"\"\\n\\n        # Suck in the JSON data and parse the rules\\n        rules = dict((k, parse_rule(v)) for k, v in\\n                     jsonutils.loads(data).items())\\n\\n        return cls(rules, default_rule)', 'def __missing__(self, key):\\n        \"\"\"Implements the default rule handling.\"\"\"\\n\\n        # If the default rule isn\\'t actually defined, do something\\n        # reasonably intelligent\\n        if not self.default_rule or self.default_rule not in self:\\n            raise KeyError(key)\\n\\n        return self[self.default_rule]', 'def __init__(self, policy_file=None, rules=None, default_rule=None):\\n        self.rules = Rules(rules)\\n        self.default_rule = default_rule or CONF.policy_default_rule\\n\\n        self.policy_path = None\\n        self.policy_file = policy_file or CONF.policy_file', 'def clear(self):\\n        \"\"\"Clears Enforcer rules, policy\\'s cache and policy\\'s path.\"\"\"\\n        self.set_rules({})\\n        self.policy_path = None', 'def _get_policy_path(self):\\n        \"\"\"Locate the policy json data file.\\n\\n        :param policy_file: Custom policy file to locate.\\n\\n        :returns: The policy path\\n\\n        :raises: ConfigFilesNotFoundError if the file couldn\\'t\\n                 be located.\\n        \"\"\"\\n        policy_file = CONF.find_file(self.policy_file)\\n\\n        if policy_file:\\n            return policy_file\\n\\n        raise cfg.ConfigFilesNotFoundError(path=CONF.policy_file)', 'def __str__(self):\\n        \"\"\"String representation of the Check tree rooted at this node.\"\"\"\\n\\n        pass', 'def __call__(self, target, cred):\\n        \"\"\"Triggers if instance of the class is called.\\n\\n        Performs the check. Returns False to reject the access or a\\n        true value (not necessary True) to accept the access.\\n        \"\"\"\\n\\n        pass', 'def __str__(self):\\n        \"\"\"Return a string representation of this check.\"\"\"\\n\\n        return \"!\"', 'def __str__(self):\\n        \"\"\"Return a string representation of this check.\"\"\"\\n\\n        return \"@\"', 'def __init__(self, kind, match):\\n        \"\"\"Initiates Check instance.\\n\\n        :param kind: The kind of the check, i.e., the field before the\\n                     \\':\\'.\\n        :param match: The match of the check, i.e., the field after\\n                      the \\':\\'.\\n        \"\"\"\\n\\n        self.kind = kind\\n        self.match = match', 'def __init__(self, rule):\\n        \"\"\"Initialize the \\'not\\' check.\\n\\n        :param rule: The rule to negate.  Must be a Check.\\n        \"\"\"\\n\\n        self.rule = rule', 'def __call__(self, target, cred):\\n        \"\"\"Check the policy.\\n\\n        Returns the logical inverse of the wrapped check.\\n        \"\"\"\\n\\n        return not self.rule(target, cred)', 'def __init__(self, rules):\\n        \"\"\"Initialize the \\'and\\' check.\\n\\n        :param rules: A list of rules that will be tested.\\n        \"\"\"\\n\\n        self.rules = rules', 'def __call__(self, target, cred):\\n        \"\"\"Check the policy.\\n\\n        Requires that all rules accept in order to return True.\\n        \"\"\"\\n\\n        for rule in self.rules:\\n            if not rule(target, cred):\\n                return False\\n\\n        return True', 'def __init__(self, rules):\\n        \"\"\"Initialize the \\'or\\' check.\\n\\n        :param rules: A list of rules that will be tested.\\n        \"\"\"\\n\\n        self.rules = rules', 'def __call__(self, target, cred):\\n        \"\"\"Check the policy.\\n\\n        Requires that at least one rule accept in order to return True.\\n        \"\"\"\\n\\n        for rule in self.rules:\\n            if rule(target, cred):\\n                return True\\n\\n        return False', 'def _parse_check(rule):\\n    \"\"\"Parse a single base check rule into an appropriate Check object.\"\"\"\\n\\n    # Handle the special checks\\n    if rule == \\'!\\':\\n        return FalseCheck()\\n    elif rule == \\'@\\':\\n        return TrueCheck()\\n\\n    try:\\n        kind, match = rule.split(\\':\\', 1)\\n    except Exception:\\n        LOG.exception(_(\"Failed to understand rule %s\") % rule)\\n        # If the rule is invalid, we\\'ll fail closed\\n        return FalseCheck()\\n\\n    # Find what implements the check\\n    if kind in _checks:\\n        return _checks[kind](kind, match)\\n    elif None in _checks:\\n        return _checks[None](kind, match)\\n    else:\\n        LOG.error(_(\"No handler for matches of kind %s\") % kind)\\n        return FalseCheck()', 'def _parse_tokenize(rule):\\n    \"\"\"Tokenizer for the policy language.\\n\\n    Most of the single-character tokens are specified in the\\n    _tokenize_re; however, parentheses need to be handled specially,\\n    because they can appear inside a check string.  Thankfully, those\\n    parentheses that appear inside a check string can never occur at\\n    the very beginning or end (\"%(variable)s\" is the correct syntax).\\n    \"\"\"\\n\\n    for tok in _tokenize_re.split(rule):\\n        # Skip empty tokens\\n        if not tok or tok.isspace():\\n            continue\\n\\n        # Handle leading parens on the token\\n        clean = tok.lstrip(\\'(\\')\\n        for i in range(len(tok) - len(clean)):\\n            yield \\'(\\', \\'(\\'\\n\\n        # If it was only parentheses, continue\\n        if not clean:\\n            continue\\n        else:\\n            tok = clean\\n\\n        # Handle trailing parens on the token\\n        clean = tok.rstrip(\\')\\')\\n        trail = len(tok) - len(clean)\\n\\n        # Yield the cleaned token\\n        lowered = clean.lower()\\n        if lowered in (\\'and\\', \\'or\\', \\'not\\'):\\n            # Special tokens\\n            yield lowered, clean\\n        elif clean:\\n            # Not a special token, but not composed solely of \\')\\'\\n            if len(tok) >= 2 and ((tok[0], tok[-1]) in\\n                                  [(\\'\"\\', \\'\"\\'), (\"\\'\", \"\\'\")]):\\n                # It\\'s a quoted string\\n                yield \\'string\\', tok[1:-1]\\n            else:\\n                yield \\'check\\', _parse_check(clean)\\n\\n        # Yield the trailing parens\\n        for i in range(trail):\\n            yield \\')\\', \\')\\'', 'def __new__(mcs, name, bases, cls_dict):\\n        \"\"\"Create the class.\\n\\n        Injects the \\'reducers\\' list, a list of tuples matching token sequences\\n        to the names of the corresponding reduction methods.\\n        \"\"\"\\n\\n        reducers = []\\n\\n        for key, value in cls_dict.items():\\n            if not hasattr(value, \\'reducers\\'):\\n                continue\\n            for reduction in value.reducers:\\n                reducers.append((reduction, key))\\n\\n        cls_dict[\\'reducers\\'] = reducers\\n\\n        return super(ParseStateMeta, mcs).__new__(mcs, name, bases, cls_dict)', \"def decorator(func):\\n        # Make sure we have a list of reducer sequences\\n        if not hasattr(func, 'reducers'):\\n            func.reducers = []\\n\\n        # Add the tokens to the list of reducer sequences\\n        func.reducers.append(list(tokens))\\n\\n        return func\", 'def __init__(self):\\n        \"\"\"Initialize the ParseState.\"\"\"\\n\\n        self.tokens = []\\n        self.values = []', 'def shift(self, tok, value):\\n        \"\"\"Adds one more token to the state.  Calls reduce().\"\"\"\\n\\n        self.tokens.append(tok)\\n        self.values.append(value)\\n\\n        # Do a greedy reduce...\\n        self.reduce()', 'def result(self):\\n        \"\"\"Obtain the final result of the parse.\\n\\n        Raises ValueError if the parse failed to reduce to a single result.\\n        \"\"\"\\n\\n        if len(self.values) != 1:\\n            raise ValueError(\"Could not parse rule\")\\n        return self.values[0]', 'def _wrap_check(self, _p1, check, _p2):\\n        \"\"\"Turn parenthesized expressions into a \\'check\\' token.\"\"\"\\n\\n        return [(\\'check\\', check)]', 'def _make_and_expr(self, check1, _and, check2):\\n        \"\"\"Create an \\'and_expr\\'.\\n\\n        Join two checks by the \\'and\\' operator.\\n        \"\"\"\\n\\n        return [(\\'and_expr\\', AndCheck([check1, check2]))]', 'def _extend_and_expr(self, and_expr, _and, check):\\n        \"\"\"Extend an \\'and_expr\\' by adding one more check.\"\"\"\\n\\n        return [(\\'and_expr\\', and_expr.add_check(check))]', 'def _make_or_expr(self, check1, _or, check2):\\n        \"\"\"Create an \\'or_expr\\'.\\n\\n        Join two checks by the \\'or\\' operator.\\n        \"\"\"\\n\\n        return [(\\'or_expr\\', OrCheck([check1, check2]))]', 'def _extend_or_expr(self, or_expr, _or, check):\\n        \"\"\"Extend an \\'or_expr\\' by adding one more check.\"\"\"\\n\\n        return [(\\'or_expr\\', or_expr.add_check(check))]', 'def _make_not_expr(self, _not, check):\\n        \"\"\"Invert the result of another check.\"\"\"\\n\\n        return [(\\'check\\', NotCheck(check))]', 'def parse_rule(rule):\\n    \"\"\"Parses a policy rule into a tree of Check objects.\"\"\"\\n\\n    # If the rule is a string, it\\'s in the policy language\\n    if isinstance(rule, basestring):\\n        return _parse_text_rule(rule)\\n    return _parse_list_rule(rule)', 'def decorator(func):\\n        _checks[name] = func\\n        return func', 'def __call__(self, target, creds, enforcer):\\n        \"\"\"Recursively checks credentials based on the defined rules.\"\"\"\\n\\n        try:\\n            return enforcer.rules[self.match](target, creds, enforcer)\\n        except KeyError:\\n            # We don\\'t have any matching rule; fail closed\\n            return False', 'def __call__(self, target, creds, enforcer):\\n        \"\"\"Check that there is a matching role in the cred dict.\"\"\"\\n\\n        return self.match.lower() in [x.lower() for x in creds[\\'roles\\']]', 'def __call__(self, target, creds, enforcer):\\n        \"\"\"Check http: rules by calling to a remote server.\\n\\n        This example implementation simply verifies that the response\\n        is exactly \\'True\\'.\\n        \"\"\"\\n\\n        url = (\\'http:\\' + self.match) % target\\n        data = {\\'target\\': jsonutils.dumps(target),\\n                \\'credentials\\': jsonutils.dumps(creds)}\\n        post_data = urllib.urlencode(data)\\n        f = urllib2.urlopen(url, post_data)\\n        return f.read() == \"True\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def stub_vm_utils_with_vdi_attached_here(function, should_return=True):\\n    \"\"\"\\n    vm_utils.with_vdi_attached_here needs to be stubbed out because it\\n    calls down to the filesystem to attach a vdi. This provides a\\n    decorator to handle that.\\n    \"\"\"\\n    @functools.wraps(function)\\n    def decorated_function(self, *args, **kwargs):\\n        @contextlib.contextmanager\\n        def fake_vdi_attached_here(*args, **kwargs):\\n            fake_dev = \\'fakedev\\'\\n            yield fake_dev\\n\\n        def fake_stream_disk(*args, **kwargs):\\n            pass\\n\\n        def fake_is_vdi_pv(*args, **kwargs):\\n            return should_return\\n\\n        orig_vdi_attached_here = vm_utils.vdi_attached_here\\n        orig_stream_disk = vm_utils._stream_disk\\n        orig_is_vdi_pv = vm_utils._is_vdi_pv\\n        try:\\n            vm_utils.vdi_attached_here = fake_vdi_attached_here\\n            vm_utils._stream_disk = fake_stream_disk\\n            vm_utils._is_vdi_pv = fake_is_vdi_pv\\n            return function(self, *args, **kwargs)\\n        finally:\\n            vm_utils._is_vdi_pv = orig_is_vdi_pv\\n            vm_utils._stream_disk = orig_stream_disk\\n            vm_utils.vdi_attached_here = orig_vdi_attached_here\\n\\n    return decorated_function', \"def setUp(self):\\n        super(XenAPIVolumeTestCase, self).setUp()\\n        self.user_id = 'fake'\\n        self.project_id = 'fake'\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n        self.flags(target_host='127.0.0.1',\\n                xenapi_connection_url='test_url',\\n                xenapi_connection_password='test_pass',\\n                firewall_driver='nova.virt.xenapi.firewall.'\\n                                'Dom0IptablesFirewallDriver')\\n        db_fakes.stub_out_db_instance_api(self.stubs)\\n        xenapi_fake.reset()\\n        self.instance_values = {'id': 1,\\n                  'project_id': self.user_id,\\n                  'user_id': 'fake',\\n                  'image_ref': 1,\\n                  'kernel_id': 2,\\n                  'ramdisk_id': 3,\\n                  'root_gb': 20,\\n                  'instance_type_id': '3',  # m1.large\\n                  'os_type': 'linux',\\n                  'architecture': 'x86-64'}\", \"def _make_info():\\n        return {\\n            'driver_volume_type': 'iscsi',\\n            'data': {\\n                'volume_id': 1,\\n                'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\\n                'target_portal': '127.0.0.1:3260,fake',\\n                'target_lun': None,\\n                'auth_method': 'CHAP',\\n                'auth_method': 'fake',\\n                'auth_method': 'fake',\\n            }\\n        }\", 'def test_parse_volume_info_raise_exception(self):\\n        \"\"\"This shows how to test helper classes\\' methods.\"\"\"\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVolumeTests)\\n        session = xenapi_conn.XenAPISession(\\'test_url\\', \\'root\\', \\'test_pass\\')\\n        helper = volume_utils.VolumeHelper\\n        helper.XenAPI = session.get_imported_xenapi()\\n        vol = self._create_volume()\\n        # oops, wrong mount point!\\n        self.assertRaises(volume_utils.StorageError,\\n                          helper.parse_volume_info,\\n                          self._make_info(),\\n                          \\'dev/sd\\'\\n                          )\\n        db.volume_destroy(context.get_admin_context(), vol[\\'id\\'])', 'def test_attach_volume_raise_exception(self):\\n        \"\"\"This shows how to test when exceptions are raised.\"\"\"\\n        stubs.stubout_session(self.stubs,\\n                              stubs.FakeSessionForVolumeFailedTests)\\n        conn = xenapi_conn.get_connection(False)\\n        volume = self._create_volume()\\n        instance = db.instance_create(self.context, self.instance_values)\\n        xenapi_fake.create_vm(instance.name, \\'Running\\')\\n        self.assertRaises(exception.VolumeDriverNotFound,\\n                          conn.attach_volume,\\n                          {\\'driver_volume_type\\': \\'nonexist\\'},\\n                          instance.name,\\n                          \\'/dev/sdc\\')', \"def setUp(self):\\n        super(XenAPIVMTestCase, self).setUp()\\n        self.network = importutils.import_object(FLAGS.network_manager)\\n        self.flags(xenapi_connection_url='test_url',\\n                   xenapi_connection_password='test_pass',\\n                   instance_name_template='%d',\\n                   firewall_driver='nova.virt.xenapi.firewall.'\\n                                   'Dom0IptablesFirewallDriver')\\n        xenapi_fake.reset()\\n        xenapi_fake.create_local_srs()\\n        xenapi_fake.create_local_pifs()\\n        db_fakes.stub_out_db_instance_api(self.stubs)\\n        xenapi_fake.create_network('fake', FLAGS.flat_network_bridge)\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        stubs.stubout_get_this_vm_uuid(self.stubs)\\n        stubs.stubout_stream_disk(self.stubs)\\n        stubs.stubout_is_vdi_pv(self.stubs)\\n        stubs.stub_out_vm_methods(self.stubs)\\n        glance_stubs.stubout_glance_client(self.stubs)\\n        fake_utils.stub_out_utils_execute(self.stubs)\\n        self.user_id = 'fake'\\n        self.project_id = 'fake'\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n        self.conn = xenapi_conn.get_connection(False)\", 'def test_list_instances_0(self):\\n        instances = self.conn.list_instances()\\n        self.assertEquals(instances, [])', \"def test_get_diagnostics(self):\\n        def fake_get_rrd(host, vm_uuid):\\n            with open('xenapi/vm_rrd.xml') as f:\\n                return re.sub(r'\\\\s', '', f.read())\\n        self.stubs.Set(vm_utils, 'get_rrd', fake_get_rrd)\\n\\n        fake_diagnostics = {\\n            'vbd_xvdb_write': '0.0',\\n            'memory_target': '10961792000.0000',\\n            'memory_internal_free': '3612860.6020',\\n            'memory': '10961792000.0000',\\n            'vbd_xvda_write': '0.0',\\n            'cpu0': '0.0110',\\n            'vif_0_tx': '752.4007',\\n            'vbd_xvda_read': '0.0',\\n            'vif_0_rx': '4837.8805'\\n        }\\n        instance = self._create_instance()\\n        expected = self.conn.get_diagnostics(instance)\\n        self.assertDictMatch(fake_diagnostics, expected)\", \"def create_bad_vbd(vm_ref, vdi_ref):\\n            vbd_rec = {'VM': vm_ref,\\n               'VDI': vdi_ref,\\n               'userdevice': 'fake',\\n               'currently_attached': False}\\n            vbd_ref = xenapi_fake._create_object('VBD', vbd_rec)\\n            xenapi_fake.after_VBD_create(vbd_ref, vbd_rec)\\n            return vbd_ref\", 'def test_instance_snapshot(self):\\n        stubs.stubout_instance_snapshot(self.stubs)\\n        stubs.stubout_is_snapshot(self.stubs)\\n        # Stubbing out firewall driver as previous stub sets alters\\n        # xml rpc result parsing\\n        stubs.stubout_firewall_driver(self.stubs, self.conn)\\n        instance = self._create_instance()\\n\\n        name = \"MySnapshot\"\\n        template_vm_ref = self.conn.snapshot(self.context, instance, name)\\n\\n        # Ensure VM was torn down\\n        vm_labels = []\\n        for vm_ref in xenapi_fake.get_all(\\'VM\\'):\\n            vm_rec = xenapi_fake.get_record(\\'VM\\', vm_ref)\\n            if not vm_rec[\"is_control_domain\"]:\\n                vm_labels.append(vm_rec[\"name_label\"])\\n\\n        self.assertEquals(vm_labels, [instance.name])\\n\\n        # Ensure VBDs were torn down\\n        vbd_labels = []\\n        for vbd_ref in xenapi_fake.get_all(\\'VBD\\'):\\n            vbd_rec = xenapi_fake.get_record(\\'VBD\\', vbd_ref)\\n            vbd_labels.append(vbd_rec[\"vm_name_label\"])\\n\\n        self.assertEquals(vbd_labels, [instance.name])\\n\\n        # Ensure VDIs were torn down\\n        for vdi_ref in xenapi_fake.get_all(\\'VDI\\'):\\n            vdi_rec = xenapi_fake.get_record(\\'VDI\\', vdi_ref)\\n            name_label = vdi_rec[\"name_label\"]\\n            self.assert_(not name_label.endswith(\\'snapshot\\'))', \"def check_vm_record(self, conn, check_injection=False):\\n        # Check that m1.large above turned into the right thing.\\n        instance_type = db.instance_type_get_by_name(conn, 'm1.large')\\n        mem_kib = long(instance_type['memory_mb']) << 10\\n        mem_bytes = str(mem_kib << 10)\\n        vcpus = instance_type['vcpus']\\n        self.assertEquals(self.vm_info['max_mem'], mem_kib)\\n        self.assertEquals(self.vm_info['mem'], mem_kib)\\n        self.assertEquals(self.vm['memory_static_max'], mem_bytes)\\n        self.assertEquals(self.vm['memory_dynamic_max'], mem_bytes)\\n        self.assertEquals(self.vm['memory_dynamic_min'], mem_bytes)\\n        self.assertEquals(self.vm['VCPUs_max'], str(vcpus))\\n        self.assertEquals(self.vm['VCPUs_at_startup'], str(vcpus))\\n\\n        # Check that the VM is running according to Nova\\n        self.assertEquals(self.vm_info['state'], power_state.RUNNING)\\n\\n        # Check that the VM is running according to XenAPI.\\n        self.assertEquals(self.vm['power_state'], 'Running')\\n\\n        if check_injection:\\n            xenstore_data = self.vm['xenstore_data']\\n            self.assertEquals(xenstore_data['vm-data/hostname'], 'test')\\n            key = 'vm-data/networking/DEADBEEF0000'\\n            xenstore_value = xenstore_data[key]\\n            tcpip_data = ast.literal_eval(xenstore_value)\\n            self.assertEquals(tcpip_data,\\n                              {'broadcast': '192.168.0.255',\\n                               'dns': ['192.168.0.1'],\\n                               'gateway': '192.168.0.1',\\n                               'gateway_v6': 'dead:beef::1',\\n                               'ip6s': [{'enabled': '1',\\n                                         'ip': 'dead:beef::dcad:beff:feef:0',\\n                                               'netmask': '64'}],\\n                               'ips': [{'enabled': '1',\\n                                        'ip': '192.168.0.100',\\n                                        'netmask': '255.255.255.0'}],\\n                               'dhcp_server': '192.168.0.1',\\n                               'label': 'fake',\\n                               'mac': 'DE:AD:BE:EF:00:00',\\n                               'rxtx_cap': 3})\", \"def check_vm_params_for_linux(self):\\n        self.assertEquals(self.vm['platform']['nx'], 'false')\\n        self.assertEquals(self.vm['PV_args'], '')\\n        self.assertEquals(self.vm['PV_bootloader'], 'pygrub')\\n\\n        # check that these are not set\\n        self.assertEquals(self.vm['PV_kernel'], '')\\n        self.assertEquals(self.vm['PV_ramdisk'], '')\\n        self.assertEquals(self.vm['HVM_boot_params'], {})\\n        self.assertEquals(self.vm['HVM_boot_policy'], '')\", \"def _list_vdis(self):\\n        url = FLAGS.xenapi_connection_url\\n        username = FLAGS.xenapi_connection_username\\n        password = FLAGS.xenapi_connection_password\\n        session = xenapi_conn.XenAPISession(url, username, password)\\n        return session.call_xenapi('VDI.get_all')\", 'def _test_spawn(self, image_ref, kernel_id, ramdisk_id,\\n                    instance_type_id=\"3\", os_type=\"linux\",\\n                    hostname=\"test\", architecture=\"x86-64\", instance_id=1,\\n                    check_injection=False,\\n                    create_record=True, empty_dns=False):\\n        if create_record:\\n            instance_values = {\\'id\\': instance_id,\\n                      \\'project_id\\': self.project_id,\\n                      \\'user_id\\': self.user_id,\\n                      \\'image_ref\\': image_ref,\\n                      \\'kernel_id\\': kernel_id,\\n                      \\'ramdisk_id\\': ramdisk_id,\\n                      \\'root_gb\\': 20,\\n                      \\'instance_type_id\\': instance_type_id,\\n                      \\'os_type\\': os_type,\\n                      \\'hostname\\': hostname,\\n                      \\'architecture\\': architecture}\\n            instance = db.instance_create(self.context, instance_values)\\n        else:\\n            instance = db.instance_get(self.context, instance_id)\\n        network_info = [({\\'bridge\\': \\'fa0\\', \\'id\\': 0,\\n                          \\'injected\\': True,\\n                          \\'cidr\\': \\'192.168.0.0/24\\',\\n                          \\'cidr_v6\\': \\'dead:beef::1/120\\',\\n                          },\\n                          {\\'broadcast\\': \\'192.168.0.255\\',\\n                           \\'dns\\': [\\'192.168.0.1\\'],\\n                           \\'gateway\\': \\'192.168.0.1\\',\\n                           \\'gateway_v6\\': \\'dead:beef::1\\',\\n                           \\'ip6s\\': [{\\'enabled\\': \\'1\\',\\n                                     \\'ip\\': \\'dead:beef::dcad:beff:feef:0\\',\\n                                           \\'netmask\\': \\'64\\'}],\\n                           \\'ips\\': [{\\'enabled\\': \\'1\\',\\n                                    \\'ip\\': \\'192.168.0.100\\',\\n                                    \\'netmask\\': \\'255.255.255.0\\'}],\\n                           \\'dhcp_server\\': \\'192.168.0.1\\',\\n                           \\'label\\': \\'fake\\',\\n                           \\'mac\\': \\'DE:AD:BE:EF:00:00\\',\\n                           \\'rxtx_cap\\': 3})]\\n        if empty_dns:\\n            network_info[0][1][\\'dns\\'] = []\\n\\n        # admin_pass isn\\'t part of the DB model, but it does get set as\\n        # an attribute for spawn to use\\n        instance.admin_pass = \\'herp\\'\\n        image_meta = {\\'id\\': glance_stubs.FakeGlance.IMAGE_VHD,\\n                      \\'disk_format\\': \\'vhd\\'}\\n        self.conn.spawn(self.context, instance, image_meta, network_info)\\n        self.create_vm_record(self.conn, os_type, instance[\\'name\\'])\\n        self.check_vm_record(self.conn, check_injection)\\n        self.assertTrue(instance.os_type)\\n        self.assertTrue(instance.architecture)', 'def test_spawn_not_enough_memory(self):\\n        self.assertRaises(exception.InsufficientFreeMemory,\\n                          self._test_spawn,\\n                          1, 2, 3, \"4\")  # m1.xlarge', 'def test_spawn_fail_cleanup_2(self):\\n        \"\"\"Simulates an error while creating VM record.\\n\\n        It verifies that VDIs created are properly cleaned up.\\n\\n        \"\"\"\\n        vdi_recs_start = self._list_vdis()\\n        stubs.stubout_create_vm(self.stubs)\\n        self.assertRaises(xenapi_fake.Failure,\\n                          self._test_spawn, 1, 2, 3)\\n        # No additional VDI should be found.\\n        vdi_recs_end = self._list_vdis()\\n        self._check_vdis(vdi_recs_start, vdi_recs_end)', 'def test_spawn_raw_glance(self):\\n        self._test_spawn(glance_stubs.FakeGlance.IMAGE_RAW, None, None)\\n        self.check_vm_params_for_linux()', \"def test_spawn_vhd_glance_swapdisk(self):\\n        # Change the default host_call_plugin to one that'll return\\n        # a swap disk\\n        orig_func = stubs.FakeSessionForVMTests.host_call_plugin\\n        _host_call_plugin = stubs.FakeSessionForVMTests.host_call_plugin_swap\\n        stubs.FakeSessionForVMTests.host_call_plugin = _host_call_plugin\\n        # Stubbing out firewall driver as previous stub sets a particular\\n        # stub for async plugin calls\\n        stubs.stubout_firewall_driver(self.stubs, self.conn)\\n        try:\\n            # We'll steal the above glance linux test\\n            self.test_spawn_vhd_glance_linux()\\n        finally:\\n            # Make sure to put this back\\n            stubs.FakeSessionForVMTests.host_call_plugin = orig_func\\n\\n        # We should have 2 VBDs.\\n        self.assertEqual(len(self.vm['VBDs']), 2)\\n        # Now test that we have 1.\\n        self.tearDown()\\n        self.setUp()\\n        self.test_spawn_vhd_glance_linux()\\n        self.assertEqual(len(self.vm['VBDs']), 1)\", 'def test_spawn_iso_glance(self):\\n        self._test_spawn(glance_stubs.FakeGlance.IMAGE_ISO, None, None,\\n                         os_type=\"windows\", architecture=\"i386\")\\n        self.check_vm_params_for_windows()', 'def test_spawn_netinject_file(self):\\n        self.flags(flat_injected=True)\\n        db_fakes.stub_out_db_instance_api(self.stubs, injected=True)\\n\\n        self._tee_executed = False\\n\\n        def _tee_handler(cmd, **kwargs):\\n            input = kwargs.get(\\'process_input\\', None)\\n            self.assertNotEqual(input, None)\\n            config = [line.strip() for line in input.split(\"\\\\n\")]\\n            # Find the start of eth0 configuration and check it\\n            index = config.index(\\'auto eth0\\')\\n            self.assertEquals(config[index + 1:index + 8], [\\n                \\'iface eth0 inet static\\',\\n                \\'address 192.168.0.100\\',\\n                \\'netmask 255.255.255.0\\',\\n                \\'broadcast 192.168.0.255\\',\\n                \\'gateway 192.168.0.1\\',\\n                \\'dns-nameservers 192.168.0.1\\',\\n                \\'\\'])\\n            self._tee_executed = True\\n            return \\'\\', \\'\\'\\n\\n        fake_utils.fake_execute_set_repliers([\\n            # Capture the tee .../etc/network/interfaces command\\n            (r\\'tee.*interfaces\\', _tee_handler),\\n        ])\\n        self._test_spawn(glance_stubs.FakeGlance.IMAGE_MACHINE,\\n                         glance_stubs.FakeGlance.IMAGE_KERNEL,\\n                         glance_stubs.FakeGlance.IMAGE_RAMDISK,\\n                         check_injection=True)\\n        self.assertTrue(self._tee_executed)', \"def _mount_handler(cmd, *ignore_args, **ignore_kwargs):\\n            # When mounting, create real files under the mountpoint to simulate\\n            # files in the mounted filesystem\\n\\n            # mount point will be the last item of the command list\\n            self._tmpdir = cmd[len(cmd) - 1]\\n            LOG.debug(_('Creating files in %s to simulate guest agent'),\\n                      self._tmpdir)\\n            os.makedirs(os.path.join(self._tmpdir, 'usr', 'sbin'))\\n            # Touch the file using open\\n            open(os.path.join(self._tmpdir, 'usr', 'sbin',\\n                'xe-update-networking'), 'w').close()\\n            return '', ''\", \"def _tee_handler(cmd, *ignore_args, **ignore_kwargs):\\n            self._tee_executed = True\\n            return '', ''\", 'def test_spawn_vlanmanager(self):\\n        self.flags(image_service=\\'nova.image.glance.GlanceImageService\\',\\n                   network_manager=\\'nova.network.manager.VlanManager\\',\\n                   vlan_interface=\\'fake0\\')\\n\\n        def dummy(*args, **kwargs):\\n            pass\\n\\n        self.stubs.Set(vmops.VMOps, \\'_create_vifs\\', dummy)\\n        # Reset network table\\n        xenapi_fake.reset_table(\\'network\\')\\n        # Instance id = 2 will use vlan network (see db/fakes.py)\\n        ctxt = self.context.elevated()\\n        instance = self._create_instance(2, False)\\n        networks = self.network.db.network_get_all(ctxt)\\n        for network in networks:\\n            self.network.set_network_host(ctxt, network)\\n\\n        self.network.allocate_for_instance(ctxt,\\n                          instance_id=2,\\n                          instance_uuid=\"00000000-0000-0000-0000-000000000000\",\\n                          host=FLAGS.host,\\n                          vpn=None,\\n                          rxtx_factor=3,\\n                          project_id=self.project_id)\\n        self._test_spawn(glance_stubs.FakeGlance.IMAGE_MACHINE,\\n                         glance_stubs.FakeGlance.IMAGE_KERNEL,\\n                         glance_stubs.FakeGlance.IMAGE_RAMDISK,\\n                         instance_id=2,\\n                         create_record=False)\\n        # TODO(salvatore-orlando): a complete test here would require\\n        # a check for making sure the bridge for the VM\\'s VIF is\\n        # consistent with bridge specified in nova db', \"def test_rescue(self):\\n        instance = self._create_instance()\\n        session = xenapi_conn.XenAPISession('test_url', 'root', 'test_pass')\\n        vm = vm_utils.VMHelper.lookup(session, instance.name)\\n        vbd = xenapi_fake.create_vbd(vm, None)\\n        conn = xenapi_conn.get_connection(False)\\n        image_meta = {'id': glance_stubs.FakeGlance.IMAGE_VHD,\\n                      'disk_format': 'vhd'}\\n        conn.rescue(self.context, instance, [], image_meta)\", 'def test_unrescue_not_in_rescue(self):\\n        instance = self._create_instance()\\n        conn = xenapi_conn.get_connection(False)\\n        # Ensure that it will not unrescue a non-rescued instance.\\n        self.assertRaises(exception.InstanceNotInRescueMode, conn.unrescue,\\n                          instance, None)', 'def __init__(self):\\n                self.finish_revert_migration_called = False', 'def _create_instance(self, instance_id=1, spawn=True):\\n        \"\"\"Creates and spawns a test instance.\"\"\"\\n        instance_values = {\\n            \\'id\\': instance_id,\\n            \\'project_id\\': self.project_id,\\n            \\'user_id\\': self.user_id,\\n            \\'image_ref\\': 1,\\n            \\'kernel_id\\': 2,\\n            \\'ramdisk_id\\': 3,\\n            \\'root_gb\\': 20,\\n            \\'instance_type_id\\': \\'3\\',  # m1.large\\n            \\'os_type\\': \\'linux\\',\\n            \\'architecture\\': \\'x86-64\\'}\\n        instance = db.instance_create(self.context, instance_values)\\n        network_info = [({\\'bridge\\': \\'fa0\\', \\'id\\': 0,\\n                          \\'injected\\': False,\\n                          \\'cidr\\': \\'192.168.0.0/24\\',\\n                          \\'cidr_v6\\': \\'dead:beef::1/120\\',\\n                          },\\n                          {\\'broadcast\\': \\'192.168.0.255\\',\\n                           \\'dns\\': [\\'192.168.0.1\\'],\\n                           \\'gateway\\': \\'192.168.0.1\\',\\n                           \\'gateway_v6\\': \\'dead:beef::1\\',\\n                           \\'ip6s\\': [{\\'enabled\\': \\'1\\',\\n                                     \\'ip\\': \\'dead:beef::dcad:beff:feef:0\\',\\n                                           \\'netmask\\': \\'64\\'}],\\n                           \\'ips\\': [{\\'enabled\\': \\'1\\',\\n                                    \\'ip\\': \\'192.168.0.100\\',\\n                                    \\'netmask\\': \\'255.255.255.0\\'}],\\n                           \\'dhcp_server\\': \\'192.168.0.1\\',\\n                           \\'label\\': \\'fake\\',\\n                           \\'mac\\': \\'DE:AD:BE:EF:00:00\\',\\n                           \\'rxtx_cap\\': 3})]\\n        image_meta = {\\'id\\': glance_stubs.FakeGlance.IMAGE_VHD,\\n                      \\'disk_format\\': \\'vhd\\'}\\n        if spawn:\\n            instance.admin_pass = \\'herp\\'\\n            self.conn.spawn(self.context, instance, image_meta, network_info)\\n        return instance', 'def setUp(self):\\n        super(XenAPIDiffieHellmanTestCase, self).setUp()\\n        self.alice = vmops.SimpleDH()\\n        self.bob = vmops.SimpleDH()', \"def _test_encryption(self, message):\\n        enc = self.alice.encrypt(message)\\n        self.assertFalse(enc.endswith('\\\\n'))\\n        dec = self.bob.decrypt(enc)\\n        self.assertEquals(dec, message)\", \"def test_encrypt_message_with_newlines_at_end(self):\\n        self._test_encryption('This message has a newline at the end.\\\\n')\", \"def test_encrypt_newlines_inside_message(self):\\n        self._test_encryption('Message\\\\nwith\\\\ninterior\\\\nnewlines.')\", \"def test_encrypt_really_long_message(self):\\n        self._test_encryption(''.join(['abcd' for i in xrange(1024)]))\", \"def setUp(self):\\n        super(XenAPIMigrateInstance, self).setUp()\\n        self.flags(target_host='127.0.0.1',\\n                xenapi_connection_url='test_url',\\n                xenapi_connection_password='test_pass',\\n                firewall_driver='nova.virt.xenapi.firewall.'\\n                                'Dom0IptablesFirewallDriver')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        db_fakes.stub_out_db_instance_api(self.stubs)\\n        xenapi_fake.reset()\\n        xenapi_fake.create_network('fake', FLAGS.flat_network_bridge)\\n        self.user_id = 'fake'\\n        self.project_id = 'fake'\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n        self.instance_values = {'id': 1,\\n                  'project_id': self.project_id,\\n                  'user_id': self.user_id,\\n                  'image_ref': 1,\\n                  'kernel_id': None,\\n                  'ramdisk_id': None,\\n                  'root_gb': 5,\\n                  'instance_type_id': '3',  # m1.large\\n                  'os_type': 'linux',\\n                  'architecture': 'x86-64'}\\n\\n        migration_values = {\\n            'source_compute': 'nova-compute',\\n            'dest_compute': 'nova-compute',\\n            'dest_host': '10.127.5.114',\\n            'status': 'post-migrating',\\n            'instance_uuid': '15f23e6a-cc6e-4d22-b651-d9bdaac316f7',\\n            'old_instance_type_id': 5,\\n            'new_instance_type_id': 1\\n        }\\n        self.migration = db.migration_create(\\n            context.get_admin_context(), migration_values)\\n\\n        fake_utils.stub_out_utils_execute(self.stubs)\\n        stubs.stub_out_migration_methods(self.stubs)\\n        stubs.stubout_get_this_vm_uuid(self.stubs)\\n        glance_stubs.stubout_glance_client(self.stubs)\", \"def fake_vdi_resize(*args, **kwargs):\\n            called['resize'] = True\", \"def test_migrate_disk_and_power_off(self):\\n        instance = db.instance_create(self.context, self.instance_values)\\n        xenapi_fake.create_vm(instance.name, 'Running')\\n        instance_type = db.instance_type_get_by_name(self.context, 'm1.large')\\n        conn = xenapi_conn.get_connection(False)\\n        conn.migrate_disk_and_power_off(self.context, instance,\\n                                        '127.0.0.1', instance_type, None)\", \"def fake_raise(*args, **kwargs):\\n            raise exception.MigrationError(reason='test failure')\", 'def test_revert_migrate(self):\\n        instance = db.instance_create(self.context, self.instance_values)\\n        self.called = False\\n        self.fake_vm_start_called = False\\n        self.fake_finish_revert_migration_called = False\\n\\n        def fake_vm_start(*args, **kwargs):\\n            self.fake_vm_start_called = True\\n\\n        def fake_vdi_resize(*args, **kwargs):\\n            self.called = True\\n\\n        def fake_finish_revert_migration(*args, **kwargs):\\n            self.fake_finish_revert_migration_called = True\\n\\n        self.stubs.Set(stubs.FakeSessionForVMTests,\\n                       \"VDI_resize_online\", fake_vdi_resize)\\n        self.stubs.Set(vmops.VMOps, \\'_start\\', fake_vm_start)\\n        self.stubs.Set(vmops.VMOps, \\'finish_revert_migration\\',\\n                       fake_finish_revert_migration)\\n\\n        conn = xenapi_conn.get_connection(False)\\n        network_info = [({\\'bridge\\': \\'fa0\\', \\'id\\': 0, \\'injected\\': False},\\n                          {\\'broadcast\\': \\'192.168.0.255\\',\\n                           \\'dns\\': [\\'192.168.0.1\\'],\\n                           \\'gateway\\': \\'192.168.0.1\\',\\n                           \\'gateway_v6\\': \\'dead:beef::1\\',\\n                           \\'ip6s\\': [{\\'enabled\\': \\'1\\',\\n                                     \\'ip\\': \\'dead:beef::dcad:beff:feef:0\\',\\n                                     \\'netmask\\': \\'64\\'}],\\n                           \\'ips\\': [{\\'enabled\\': \\'1\\',\\n                                    \\'ip\\': \\'192.168.0.100\\',\\n                                    \\'netmask\\': \\'255.255.255.0\\'}],\\n                           \\'label\\': \\'fake\\',\\n                           \\'mac\\': \\'DE:AD:BE:EF:00:00\\',\\n                           \\'rxtx_cap\\': 3})]\\n        image_meta = {\\'id\\': instance.image_ref, \\'disk_format\\': \\'vhd\\'}\\n        base = xenapi_fake.create_vdi(\\'hurr\\', \\'fake\\')\\n        base_uuid = xenapi_fake.get_record(\\'VDI\\', base)[\\'uuid\\']\\n        cow = xenapi_fake.create_vdi(\\'durr\\', \\'fake\\')\\n        cow_uuid = xenapi_fake.get_record(\\'VDI\\', cow)[\\'uuid\\']\\n        conn.finish_migration(self.context, self.migration, instance,\\n                              dict(base_copy=base_uuid, cow=cow_uuid),\\n                              network_info, image_meta, resize_instance=True)\\n        self.assertEqual(self.called, True)\\n        self.assertEqual(self.fake_vm_start_called, True)\\n\\n        conn.finish_revert_migration(instance, network_info)\\n        self.assertEqual(self.fake_finish_revert_migration_called, True)', 'def fake_vm_start(*args, **kwargs):\\n            self.fake_vm_start_called = True', 'def test_finish_migrate_no_local_storage(self):\\n        tiny_type = instance_types.get_instance_type_by_name(\\'m1.tiny\\')\\n        tiny_type_id = tiny_type[\\'id\\']\\n        self.instance_values.update({\\'instance_type_id\\': tiny_type_id,\\n                                     \\'root_gb\\': 0})\\n        instance = db.instance_create(self.context, self.instance_values)\\n\\n        def fake_vdi_resize(*args, **kwargs):\\n            raise Exception(\"This shouldn\\'t be called\")\\n\\n        self.stubs.Set(stubs.FakeSessionForVMTests,\\n                       \"VDI_resize_online\", fake_vdi_resize)\\n        conn = xenapi_conn.get_connection(False)\\n        network_info = [({\\'bridge\\': \\'fa0\\', \\'id\\': 0, \\'injected\\': False},\\n                          {\\'broadcast\\': \\'192.168.0.255\\',\\n                           \\'dns\\': [\\'192.168.0.1\\'],\\n                           \\'gateway\\': \\'192.168.0.1\\',\\n                           \\'gateway_v6\\': \\'dead:beef::1\\',\\n                           \\'ip6s\\': [{\\'enabled\\': \\'1\\',\\n                                     \\'ip\\': \\'dead:beef::dcad:beff:feef:0\\',\\n                                           \\'netmask\\': \\'64\\'}],\\n                           \\'ips\\': [{\\'enabled\\': \\'1\\',\\n                                    \\'ip\\': \\'192.168.0.100\\',\\n                                    \\'netmask\\': \\'255.255.255.0\\'}],\\n                           \\'label\\': \\'fake\\',\\n                           \\'mac\\': \\'DE:AD:BE:EF:00:00\\',\\n                           \\'rxtx_cap\\': 3})]\\n        image_meta = {\\'id\\': instance.image_ref, \\'disk_format\\': \\'vhd\\'}\\n        conn.finish_migration(self.context, self.migration, instance,\\n                              dict(base_copy=\\'hurr\\', cow=\\'durr\\'),\\n                              network_info, image_meta, resize_instance=True)', 'def fake_vdi_resize(*args, **kwargs):\\n            raise Exception(\"This shouldn\\'t be called\")', 'def test_to_string(self):\\n        \"\"\"Can convert from type id to type string.\"\"\"\\n        self.assertEquals(\\n            vm_utils.ImageType.to_string(vm_utils.ImageType.KERNEL),\\n            vm_utils.ImageType.KERNEL_STR)', \"def setUp(self):\\n        super(XenAPIDetermineDiskImageTestCase, self).setUp()\\n        glance_stubs.stubout_glance_client(self.stubs)\\n\\n        class FakeInstance(object):\\n            pass\\n\\n        self.fake_instance = FakeInstance()\\n        self.fake_instance.id = 42\\n        self.fake_instance.os_type = 'linux'\\n        self.fake_instance.architecture = 'x86-64'\", \"def test_machine(self):\\n        image_meta = {'id': 'a', 'disk_format': 'ami'}\\n        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK)\", \"def test_vhd(self):\\n        image_meta = {'id': 'a', 'disk_format': 'vhd'}\\n        self.assert_disk_type(image_meta, vm_utils.ImageType.DISK_VHD)\", 'def test_less_than(self):\\n        \"\"\"Test that cmp_version compares a as less than b\"\"\"\\n        self.assertTrue(vmops.cmp_version(\\'1.2.3.4\\', \\'1.2.3.5\\') < 0)', 'def test_equal(self):\\n        \"\"\"Test that cmp_version compares a as equal to b\"\"\"\\n        self.assertTrue(vmops.cmp_version(\\'1.2.3.4\\', \\'1.2.3.4\\') == 0)', 'def test_length(self):\\n        \"\"\"Test that cmp_version compares by length as last resort\"\"\"\\n        self.assertTrue(vmops.cmp_version(\\'1.2.3\\', \\'1.2.3.4\\') < 0)', \"def setUp(self):\\n        super(XenAPIHostTestCase, self).setUp()\\n        self.flags(xenapi_connection_url='test_url',\\n                   xenapi_connection_password='test_pass')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        xenapi_fake.reset()\\n        xenapi_fake.create_local_srs()\\n        self.conn = xenapi_conn.get_connection(False)\", \"def _test_host_action(self, method, action, expected=None):\\n        result = method('host', action)\\n        if not expected:\\n            expected = action\\n        self.assertEqual(result, expected)\", \"def test_host_shutdown(self):\\n        self._test_host_action(self.conn.host_power_action, 'shutdown')\", \"def test_host_maintenance_on(self):\\n        self._test_host_action(self.conn.host_maintenance_mode,\\n                               True, 'on_maintenance')\", \"def test_set_enable_host_enable(self):\\n        self._test_host_action(self.conn.set_host_enabled, True, 'enabled')\", 'def setUp(self):\\n        super(XenAPIAutoDiskConfigTestCase, self).setUp()\\n        self.flags(target_host=\\'127.0.0.1\\',\\n                   xenapi_connection_url=\\'test_url\\',\\n                   xenapi_connection_password=\\'test_pass\\',\\n                   firewall_driver=\\'nova.virt.xenapi.firewall.\\'\\n                                   \\'Dom0IptablesFirewallDriver\\')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        xenapi_fake.reset()\\n        self.conn = xenapi_conn.get_connection(False)\\n\\n        self.user_id = \\'fake\\'\\n        self.project_id = \\'fake\\'\\n\\n        self.instance_values = {\\'id\\': 1,\\n                  \\'project_id\\': self.project_id,\\n                  \\'user_id\\': self.user_id,\\n                  \\'image_ref\\': 1,\\n                  \\'kernel_id\\': 2,\\n                  \\'ramdisk_id\\': 3,\\n                  \\'root_gb\\': 20,\\n                  \\'instance_type_id\\': \\'3\\',  # m1.large\\n                  \\'os_type\\': \\'linux\\',\\n                  \\'architecture\\': \\'x86-64\\'}\\n\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n\\n        @classmethod\\n        def fake_create_vbd(cls, session, vm_ref, vdi_ref, userdevice,\\n                            vbd_type=\\'disk\\', read_only=False, bootable=True):\\n            pass\\n\\n        self.stubs.Set(vm_utils.VMHelper,\\n                       \"create_vbd\",\\n                       fake_create_vbd)', 'def fake_resize_part_and_fs(dev, start, old, new):\\n            marker[\"partition_called\"] = True', 'def test_instance_not_auto_disk_config(self):\\n        \"\"\"Should not partition unless instance is marked as\\n        auto_disk_config.\\n        \"\"\"\\n        self.instance_values[\\'auto_disk_config\\'] = False\\n        self.assertIsPartitionCalled(False)', 'def test_instance_auto_disk_config_doesnt_pass_fail_safes(self):\\n        \"\"\"Should not partition unless fail safes pass\"\"\"\\n        self.instance_values[\\'auto_disk_config\\'] = True\\n\\n        def fake_get_partitions(dev):\\n            return [(1, 0, 100, \\'ext4\\'), (2, 100, 200, \\'ext4\\')]\\n        self.stubs.Set(vm_utils, \"_get_partitions\",\\n                       fake_get_partitions)\\n\\n        self.assertIsPartitionCalled(False)', 'def test_instance_auto_disk_config_passes_fail_safes(self):\\n        \"\"\"Should partition if instance is marked as auto_disk_config=True and\\n        virt-layer specific fail-safe checks pass.\\n        \"\"\"\\n        self.instance_values[\\'auto_disk_config\\'] = True\\n\\n        def fake_get_partitions(dev):\\n            return [(1, 0, 100, \\'ext4\\')]\\n        self.stubs.Set(vm_utils, \"_get_partitions\",\\n                       fake_get_partitions)\\n\\n        self.assertIsPartitionCalled(True)', 'def setUp(self):\\n        super(XenAPIGenerateLocal, self).setUp()\\n        self.flags(target_host=\\'127.0.0.1\\',\\n                   xenapi_connection_url=\\'test_url\\',\\n                   xenapi_connection_password=\\'test_pass\\',\\n                   xenapi_generate_swap=True,\\n                   firewall_driver=\\'nova.virt.xenapi.firewall.\\'\\n                                   \\'Dom0IptablesFirewallDriver\\')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        db_fakes.stub_out_db_instance_api(self.stubs)\\n        xenapi_fake.reset()\\n        self.conn = xenapi_conn.get_connection(False)\\n\\n        self.user_id = \\'fake\\'\\n        self.project_id = \\'fake\\'\\n\\n        self.instance_values = {\\'id\\': 1,\\n                  \\'project_id\\': self.project_id,\\n                  \\'user_id\\': self.user_id,\\n                  \\'image_ref\\': 1,\\n                  \\'kernel_id\\': 2,\\n                  \\'ramdisk_id\\': 3,\\n                  \\'root_gb\\': 20,\\n                  \\'instance_type_id\\': \\'3\\',  # m1.large\\n                  \\'os_type\\': \\'linux\\',\\n                  \\'architecture\\': \\'x86-64\\'}\\n\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n\\n        @classmethod\\n        def fake_create_vbd(cls, session, vm_ref, vdi_ref, userdevice,\\n                            vbd_type=\\'disk\\', read_only=False, bootable=True):\\n            pass\\n\\n        self.stubs.Set(vm_utils.VMHelper,\\n                       \"create_vbd\",\\n                       fake_create_vbd)', 'def test_generate_swap(self):\\n        \"\"\"Test swap disk generation.\"\"\"\\n        instance = db.instance_create(self.context, self.instance_values)\\n        instance = db.instance_update(self.context, instance[\\'id\\'],\\n                                      {\\'instance_type_id\\': 5})\\n\\n        @classmethod\\n        def fake_generate_swap(cls, *args, **kwargs):\\n            self.called = True\\n        self.stubs.Set(vm_utils.VMHelper, \\'generate_swap\\',\\n                       fake_generate_swap)\\n\\n        self.assertCalled(instance)', 'def fake_generate_ephemeral(cls, *args):\\n            self.called = True', 'def setUp(self):\\n        super(XenAPIBWUsageTestCase, self).setUp()\\n        self.stubs.Set(vm_utils.VMHelper, \"compile_metrics\",\\n                       XenAPIBWUsageTestCase._fake_compile_metrics)\\n        self.flags(target_host=\\'127.0.0.1\\',\\n                   xenapi_connection_url=\\'test_url\\',\\n                   xenapi_connection_password=\\'test_pass\\',\\n                   firewall_driver=\\'nova.virt.xenapi.firewall.\\'\\n                                   \\'Dom0IptablesFirewallDriver\\')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        xenapi_fake.reset()\\n        self.conn = xenapi_conn.get_connection(False)', 'def _fake_compile_metrics(cls, start_time, stop_time=None):\\n        raise exception.CouldNotFetchMetrics()', 'def __init__(self):\\n                self.name = \"instance-0001\"\\n                self.uuid = \"1-2-3-4-5\"', \"def setUp(self):\\n        super(XenAPIDom0IptablesFirewallTestCase, self).setUp()\\n        self.flags(xenapi_connection_url='test_url',\\n                   xenapi_connection_password='test_pass',\\n                   instance_name_template='%d',\\n                   firewall_driver='nova.virt.xenapi.firewall.'\\n                                   'Dom0IptablesFirewallDriver')\\n        xenapi_fake.reset()\\n        xenapi_fake.create_local_srs()\\n        xenapi_fake.create_local_pifs()\\n        self.user_id = 'mappin'\\n        self.project_id = 'fake'\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForFirewallTests,\\n                              test_case=self)\\n        self.context = context.RequestContext(self.user_id, self.project_id)\\n        self.network = importutils.import_object(FLAGS.network_manager)\\n        self.conn = xenapi_conn.get_connection(False)\\n        self.fw = self.conn._vmops.firewall_driver\", \"def _create_test_security_group(self):\\n        admin_ctxt = context.get_admin_context()\\n        secgroup = db.security_group_create(admin_ctxt,\\n                                {'user_id': self.user_id,\\n                                 'project_id': self.project_id,\\n                                 'name': 'testgroup',\\n                                 'description': 'test group'})\\n        db.security_group_rule_create(admin_ctxt,\\n                                      {'parent_group_id': secgroup['id'],\\n                                       'protocol': 'icmp',\\n                                       'from_port': -1,\\n                                       'to_port': -1,\\n                                       'cidr': '192.168.11.0/24'})\\n\\n        db.security_group_rule_create(admin_ctxt,\\n                                      {'parent_group_id': secgroup['id'],\\n                                       'protocol': 'icmp',\\n                                       'from_port': 8,\\n                                       'to_port': -1,\\n                                       'cidr': '192.168.11.0/24'})\\n\\n        db.security_group_rule_create(admin_ctxt,\\n                                      {'parent_group_id': secgroup['id'],\\n                                       'protocol': 'tcp',\\n                                       'from_port': 80,\\n                                       'to_port': 81,\\n                                       'cidr': '192.168.10.0/24'})\\n        return secgroup\", 'def test_static_filters(self):\\n        instance_ref = self._create_instance_ref()\\n        src_instance_ref = self._create_instance_ref()\\n        admin_ctxt = context.get_admin_context()\\n        secgroup = self._create_test_security_group()\\n\\n        src_secgroup = db.security_group_create(admin_ctxt,\\n                                                {\\'user_id\\': self.user_id,\\n                                                 \\'project_id\\': self.project_id,\\n                                                 \\'name\\': \\'testsourcegroup\\',\\n                                                 \\'description\\': \\'src group\\'})\\n        db.security_group_rule_create(admin_ctxt,\\n                                      {\\'parent_group_id\\': secgroup[\\'id\\'],\\n                                       \\'protocol\\': \\'tcp\\',\\n                                       \\'from_port\\': 80,\\n                                       \\'to_port\\': 81,\\n                                       \\'group_id\\': src_secgroup[\\'id\\']})\\n\\n        db.instance_add_security_group(admin_ctxt, instance_ref[\\'uuid\\'],\\n                                       secgroup[\\'id\\'])\\n        db.instance_add_security_group(admin_ctxt, src_instance_ref[\\'uuid\\'],\\n                                       src_secgroup[\\'id\\'])\\n        instance_ref = db.instance_get(admin_ctxt, instance_ref[\\'id\\'])\\n        src_instance_ref = db.instance_get(admin_ctxt, src_instance_ref[\\'id\\'])\\n\\n        network_model = fake_network.fake_get_instance_nw_info(self.stubs,\\n                                                      1, spectacular=True)\\n\\n        fake_network.stub_out_nw_api_get_instance_nw_info(self.stubs,\\n                                      lambda *a, **kw: network_model)\\n\\n        network_info = compute_utils.legacy_network_info(network_model)\\n        self.fw.prepare_instance_filter(instance_ref, network_info)\\n        self.fw.apply_instance_filter(instance_ref, network_info)\\n\\n        self._validate_security_group()\\n        # Extra test for TCP acceptance rules\\n        for ip in network_model.fixed_ips():\\n            if ip[\\'version\\'] != 4:\\n                continue\\n            regex = re.compile(\\'-A .* -j ACCEPT -p tcp\\'\\n                               \\' --dport 80:81 -s %s\\' % ip[\\'address\\'])\\n            self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,\\n                            \"TCP port 80/81 acceptance rule wasn\\'t added\")\\n\\n        db.instance_destroy(admin_ctxt, instance_ref[\\'id\\'])', 'def test_filters_for_instance_without_ip_v6(self):\\n        self.flags(use_ipv6=False)\\n        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1)\\n        rulesv4, rulesv6 = self.fw._filters_for_instance(\"fake\", network_info)\\n        self.assertEquals(len(rulesv4), 2)\\n        self.assertEquals(len(rulesv6), 0)', 'def test_do_refresh_security_group_rules(self):\\n        admin_ctxt = context.get_admin_context()\\n        instance_ref = self._create_instance_ref()\\n        network_info = fake_network.fake_get_instance_nw_info(self.stubs, 1, 1)\\n        secgroup = self._create_test_security_group()\\n        db.instance_add_security_group(admin_ctxt, instance_ref[\\'uuid\\'],\\n                                       secgroup[\\'id\\'])\\n        self.fw.prepare_instance_filter(instance_ref, network_info)\\n        self.fw.instances[instance_ref[\\'id\\']] = instance_ref\\n        self._validate_security_group()\\n        # add a rule to the security group\\n        db.security_group_rule_create(admin_ctxt,\\n                                      {\\'parent_group_id\\': secgroup[\\'id\\'],\\n                                       \\'protocol\\': \\'udp\\',\\n                                       \\'from_port\\': 200,\\n                                       \\'to_port\\': 299,\\n                                       \\'cidr\\': \\'192.168.99.0/24\\'})\\n        #validate the extra rule\\n        self.fw.refresh_security_group_rules(secgroup)\\n        regex = re.compile(\\'-A .* -j ACCEPT -p udp --dport 200:299\\'\\n                           \\' -s 192.168.99.0/24\\')\\n        self.assertTrue(len(filter(regex.match, self._out_rules)) > 0,\\n                        \"Rules were not updated properly.\"\\n                        \"The rule for UDP acceptance is missing\")', 'def setUp(self):\\n        super(XenAPISRSelectionTestCase, self).setUp()\\n        xenapi_fake.reset()', 'def test_safe_find_sr_local_storage(self):\\n        \"\"\"Ensure the default local-storage is found.\"\"\"\\n        self.flags(sr_matching_filter=\\'other-config:i18n-key=local-storage\\')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        session = xenapi_conn.XenAPISession(\\'test_url\\', \\'root\\', \\'test_pass\\')\\n        helper = vm_utils.VMHelper\\n        helper.XenAPI = session.get_imported_xenapi()\\n        host_ref = xenapi_fake.get_all(\\'host\\')[0]\\n        local_sr = xenapi_fake.create_sr(\\n                              name_label=\\'Fake Storage\\',\\n                              type=\\'lvm\\',\\n                              other_config={\\'i18n-original-value-name_label\\':\\n                                            \\'Local storage\\',\\n                                            \\'i18n-key\\': \\'local-storage\\'},\\n                              host_ref=host_ref)\\n        expected = helper.safe_find_sr(session)\\n        self.assertEqual(local_sr, expected)', 'def test_safe_find_sr_default(self):\\n        \"\"\"Ensure the default SR is found regardless of other-config.\"\"\"\\n        self.flags(sr_matching_filter=\\'default-sr:true\\')\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        session = xenapi_conn.XenAPISession(\\'test_url\\', \\'root\\', \\'test_pass\\')\\n        helper = vm_utils.VMHelper\\n        pool_ref = xenapi_fake.create_pool(\\'\\')\\n        helper.XenAPI = session.get_imported_xenapi()\\n        expected = helper.safe_find_sr(session)\\n        self.assertEqual(session.call_xenapi(\\'pool.get_default_SR\\', pool_ref),\\n                         expected)', \"def setUp(self):\\n        super(XenAPIAggregateTestCase, self).setUp()\\n        self.flags(xenapi_connection_url='http://test_url',\\n                   xenapi_connection_username='test_user',\\n                   xenapi_connection_password='test_pass',\\n                   instance_name_template='%d',\\n                   firewall_driver='nova.virt.xenapi.firewall.'\\n                                   'Dom0IptablesFirewallDriver',\\n                   host='host')\\n        xenapi_fake.reset()\\n        host_ref = xenapi_fake.get_all('host')[0]\\n        stubs.stubout_session(self.stubs, stubs.FakeSessionForVMTests)\\n        self.context = context.get_admin_context()\\n        self.conn = xenapi_conn.get_connection(False)\\n        self.fake_metadata = {'main_compute': 'host',\\n                              'host': xenapi_fake.get_record('host',\\n                                                             host_ref)['uuid']}\", 'def fake_add_to_aggregate(context, aggregate, host):\\n            fake_add_to_aggregate.called = True', 'def test_add_to_aggregate_for_first_host_sets_metadata(self):\\n        def fake_init_pool(id, name):\\n            fake_init_pool.called = True\\n        self.stubs.Set(self.conn._pool, \"_init_pool\", fake_init_pool)\\n\\n        aggregate = self._aggregate_setup()\\n        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")\\n        result = db.aggregate_get(self.context, aggregate.id)\\n        self.assertTrue(fake_init_pool.called)\\n        self.assertDictMatch(self.fake_metadata, result.metadetails)\\n        self.assertEqual(aggregate_states.ACTIVE, result.operational_state)', 'def fake_join_subordinate(id, compute_uuid, host, url, user, password):\\n            fake_join_subordinate.called = True', 'def test_add_to_aggregate_first_host(self):\\n        def fake_pool_set_name_label(self, session, pool_ref, name):\\n            fake_pool_set_name_label.called = True\\n        self.stubs.Set(xenapi_fake.SessionBase, \"pool_set_name_label\",\\n                       fake_pool_set_name_label)\\n        self.conn._session.call_xenapi(\"pool.create\", {\"name\": \"asdf\"})\\n\\n        values = {\"name\": \\'fake_aggregate\\',\\n                  \"availability_zone\": \\'fake_zone\\'}\\n        result = db.aggregate_create(self.context, values)\\n        db.aggregate_host_add(self.context, result.id, \"host\")\\n        aggregate = db.aggregate_get(self.context, result.id)\\n        self.assertEqual([\"host\"], aggregate.hosts)\\n        self.assertEqual({}, aggregate.metadetails)\\n\\n        self.conn._pool.add_to_aggregate(self.context, aggregate, \"host\")\\n        self.assertTrue(fake_pool_set_name_label.called)', 'def fake_remove_from_aggregate(context, aggregate, host):\\n            fake_remove_from_aggregate.called = True', 'def test_remove_from_empty_aggregate(self):\\n        values = {\"name\": \\'fake_aggregate\\',\\n                  \"availability_zone\": \\'fake_zone\\'}\\n        result = db.aggregate_create(self.context, values)\\n        self.assertRaises(exception.AggregateError,\\n                          self.conn._pool.remove_from_aggregate,\\n                          None, result, \"test_host\")', 'def fake_eject_subordinate(id, compute_uuid, host_uuid):\\n            fake_eject_subordinate.called = True', 'def test_remove_main_solo(self):\\n        \"\"\"Ensure metadata are cleared after removal.\"\"\"\\n        def fake_clear_pool(id):\\n            fake_clear_pool.called = True\\n        self.stubs.Set(self.conn._pool, \"_clear_pool\", fake_clear_pool)\\n\\n        aggregate = self._aggregate_setup(aggr_state=aggregate_states.ACTIVE,\\n                                          metadata=self.fake_metadata)\\n        self.conn._pool.remove_from_aggregate(self.context, aggregate, \"host\")\\n        result = db.aggregate_get(self.context, aggregate.id)\\n        self.assertTrue(fake_clear_pool.called)\\n        self.assertDictMatch({}, result.metadetails)\\n        self.assertEqual(aggregate_states.ACTIVE, result.operational_state)']}, {'features': [], 'snippets': [\"def register_options(cls, register):\\n    super(GoThriftGen, cls).register_options(register)\\n\\n    register('--strict', default=True, fingerprint=True, type=bool,\\n             help='Run thrift compiler with strict warnings.')\\n    register('--gen-options', advanced=True, fingerprint=True,\\n            help='Use these apache thrift go gen options.')\\n    register('--thrift-import', advanced=True,\\n             help='Use this thrift-import gen option to thrift.')\\n    register('--thrift-import-target', advanced=True,\\n             help='Use this thrift import on symbolic defs.')\", 'def subsystem_dependencies(cls):\\n    return (super(GoThriftGen, cls).subsystem_dependencies() +\\n            (ThriftDefaults, ThriftBinary.Factory.scoped(cls)))', 'def _thrift_binary(self):\\n    thrift_binary = ThriftBinary.Factory.scoped_instance(self).create()\\n    return thrift_binary.path', 'def _deps(self):\\n    thrift_import_target = self.get_options().thrift_import_target\\n    thrift_imports = self.context.resolve(thrift_import_target)\\n    return thrift_imports', \"def _service_deps(self):\\n    service_deps = self.get_options().get('service_deps')\\n    return list(self.resolve_deps(service_deps)) if service_deps else self._deps\", 'def _declares_service(self, source):\\n    with open(source) as thrift:\\n      return any(line for line in thrift if self.SERVICE_PARSER.search(line))', 'def synthetic_target_extra_dependencies(self, target, target_workdir):\\n    for source in target.sources_relative_to_buildroot():\\n      if self._declares_service(os.path.join(get_buildroot(), source)):\\n        return self._service_deps\\n    return self._deps', 'def is_gentarget(self, target):\\n    return isinstance(target, GoThriftLibrary)', \"def _thrift_cmd(self):\\n    cmd = [self._thrift_binary]\\n    thrift_import = 'thrift_import={}'.format(self.get_options().thrift_import)\\n    gen_options = self.get_options().gen_options\\n    if gen_options:\\n      gen_options += ',' + thrift_import\\n    else:\\n      gen_options = thrift_import\\n    cmd.extend(('--gen', 'go:{}'.format(gen_options)))\\n\\n    if self.get_options().strict:\\n      cmd.append('-strict')\\n    if self.get_options().level == 'debug':\\n      cmd.append('-verbose')\\n    return cmd\", \"def product_types(cls):\\n    return ['go']\", 'def _copy_target_attributes(self):\\n    \"\"\"Override `_copy_target_attributes` to exclude `provides`.\"\"\"\\n    return [a for a in super(GoThriftGen, self)._copy_target_attributes if a != \\'provides\\']']}, {'features': [], 'snippets': [\"def test_compile_1(self):\\n        compiler = PatternCompiler(pattern_set=dict(\\n            TEST=r'\\\\w+'\\n        ))\\n\\n        try:\\n            c1 = compiler.compile('$1{TEST}')\\n        except Exception as exc:\\n            self.assertTrue(1)\\n\\n        c1 = compiler.compile('$1{TEST}', ['test'])\\n        self.assertEqual(c1, r'(?:(?P<test>(\\\\w+)))')\"]}, {'features': [], 'snippets': ['def __init__(self, base_controller: Controller, sim: LungEnv, pid_K=[0.0, 0.0], decay=0.1, **kwargs):\\n        self.base_controller = base_controller\\n        self.sim = sim\\n        self.I = 0.0\\n        self.K = pid_K\\n        self.decay = decay\\n\\n        self.reset()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def each(f):\\n        if f.body:\\n            f.hashes = []\\n            for hash_type, h in HashFile.extract_hashes(f.body.contents):\\n                hash_object = Hash.get_or_create(value=h.hexdigest())\\n                hash_object.add_source(\"analytics\")\\n                hash_object.save()\\n                f.active_link_to(\\n                    hash_object,\\n                    \"{} hash\".format(hash_type.upper()),\\n                    \"HashFile\",\\n                    clean_old=False,\\n                )\\n                f.hashes.append({\"hash\": hash_type, \"value\": h.hexdigest()})\\n            f.save()']}, {'features': [], 'snippets': ['def __init__(self, queue, level, formatter):\\n        super(QueueingLogHandler, self).__init__()\\n        self._queue = queue\\n        self.setLevel(level)\\n        self.setFormatter(formatter)', 'def emit(self, record):\\n        msg = self.format(record)\\n        self._queue.put_nowait(msg)', 'def close(self):\\n        super(QueueingLogHandler, self).close()\\n        self._queue.put_nowait(None)', 'def emitted(self):\\n        return self._queue', 'def __init__(self):\\n        self._logging_handlers = set()', 'def test(self, logger_name, logger_level, message):\\n        logger = logging.getLogger(logger_name)\\n        getattr(logger, logger_level.lower())(message)', 'def close_log_streams(self):\\n        \"\"\" Closes all log_stream streams. \"\"\"\\n        while self._logging_handlers:\\n            self._logging_handlers.pop().close()', 'def log_stream(self, logger_name, level_name, format_str):\\n        \"\"\" Attaches a log handler to the specified logger and sends emitted logs \\n            back as stream. \\n        \"\"\"\\n        if logger_name != \"\" and logger_name not in self.available_loggers():\\n            raise ValueError(\"logger {0} is not available\".format(logger_name))\\n\\n        level_name_upper = level_name.upper() if level_name else \"NOTSET\"\\n        try:\\n            level = getattr(logging, level_name_upper)\\n        except AttributeError, e:\\n            raise AttributeError(\"log level {0} is not available\".format(level_name_upper))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def get(self):\\n            self.response.write('CSRF Token:%s' % self.csrf_token)\", 'def put(self):\\n            pass', \"def setUp(self):\\n        super(CsrfTests, self).setUp()\\n        # The CSRF library uses the time, so we mock it out.\\n        self.time_mock = mock.Mock()\\n        csrf.time = self.time_mock\\n        self.time_mock.time = mock.Mock(return_value=MOCKED_TIME)\\n        # The handler tests need a WSGIApplication.\\n        app = webapp2.WSGIApplication([('/', self.TestHandler)])\\n        self.testapp = webtest.TestApp(app)\", \"def test_tokens_are_equal(self):\\n        # It should fail if the tokens aren't equal length.\\n        self.assertFalse(csrf._tokens_are_equal('a', 'ab'))\\n        # It should fail if the tokens are different.\\n        self.assertFalse(csrf._tokens_are_equal('abcde', 'abcdf'))\\n        # It should succeed if the tokens are the same.\\n        self.assertTrue(csrf._tokens_are_equal('abcde', 'abcde'))\", \"def test_make_token_includes_time(self):\\n        self.login()\\n        # It should get the current time.\\n        token1 = csrf.make_token()\\n        self.assertEqual(token1.split()[-1], str(MOCKED_TIME))\\n        # It should use the provided time.\\n        token2 = csrf.make_token(token_time='456')\\n        self.assertEqual(token2.split()[-1], '456')\\n        # Different time should cause the digest to be different.\\n        self.assertNotEqual(token1.split()[0], token2.split()[0])\\n        token3 = csrf.make_token(token_time='456')\\n        self.assertEqual(token2, token3)\", \"def test_make_token_includes_path(self):\\n        self.login()\\n        # It should get the current path.\\n        self.testbed.setup_env(PATH_INFO='/action/1', overwrite=True)\\n        token1 = csrf.make_token(token_time='123')\\n        self.testbed.setup_env(PATH_INFO='/action/23', overwrite=True)\\n        token2 = csrf.make_token(token_time='123')\\n        token3 = csrf.make_token(token_time='123')\\n        self.assertNotEqual(token1, token2)\\n        self.assertEqual(token2, token3)\\n        # It should let the client pass in a path.\\n        token4 = csrf.make_token(path='/action/4', token_time='123')\\n        token5 = csrf.make_token(path='/action/56', token_time='123')\\n        token6 = csrf.make_token(path='/action/56', token_time='123')\\n        self.assertNotEqual(token4, token5)\\n        self.assertEqual(token5, token6)\", \"def test_token_is_valid(self):\\n        self.login()\\n        # Token is required.\\n        self.assertFalse(csrf.token_is_valid(None))\\n        # Token needs to have a timestamp on it.\\n        self.assertFalse(csrf.token_is_valid('hello'))\\n        # The timestamp needs to be within the current date range.\\n        self.time_mock.time = mock.Mock(return_value=9999999999999)\\n        self.assertFalse(csrf.token_is_valid('hello 123'))\\n        # The user needs to be logged in.\\n        token = csrf.make_token()\\n        self.logout()\\n        self.assertFalse(csrf.token_is_valid(token))\\n        self.login()\\n        # Modifying the token should break everything.\\n        modified_token = '0' + token[1:]\\n        if token == modified_token:\\n            modified_token = '1' + token[1:]\\n        self.assertFalse(csrf.token_is_valid(modified_token))\\n        # The original token that we got should work.\\n        self.assertTrue(csrf.token_is_valid(token))\", \"def test_mutators_require_csrf_token(self):\\n        self.login()\\n        self.testapp.put('/', status=403)\\n        self.testapp.post('/', status=403)\\n        self.testapp.delete('/', status=403)\\n        csrf_param = 'csrf_token=' + csrf.make_token(path='/')\\n        self.testapp.put('/', params=csrf_param, status=200)\\n        self.testapp.post('/', params=csrf_param, status=200)\\n        # Though the spec allows DELETE to have a body, it tends to be ignored\\n        # by servers (http://stackoverflow.com/questions/299628), and webapp2\\n        # ignores it as well, so we have to put the params in the URL.\\n        self.testapp.delete('/?' + csrf_param, status=200)\"]}, {'features': [], 'snippets': ['def getContainedObjectInterface(self):\\n        return IPublication']}, {'features': [], 'snippets': ['def setUp(self):\\n        self.fd, self.path = tempfile.mkstemp()']}, {'features': [], 'snippets': [\"def count(corpus, output_file):\\n    debug = False\\n    dic = defaultdict(int)\\n    other = set()\\n    fout = codecs.open(output_file, 'w', 'utf8')\\n    for line in open(corpus, 'r'):\\n        words = line.split()\\n        for word in words:\\n            if len(word) % 3 == 0:\\n                for i in xrange(len(word) / 3):\\n                    dic[word[i:i+3]] += 1\\n            else:\\n                other.add(word)\\n    fout.write('%i %i\\\\n' % (len(dic), len(other)))\"]}, {'features': [], 'snippets': ['def __init__(\\n        self, client, name, offset, read_rows_kwargs, retry_delay_callback=None', 'def __iter__(self):\\n        \"\"\"An iterable of messages.\\n\\n        Returns:\\n            Iterable[ \\\\\\n                ~google.cloud.bigquery_storage_v1.types.ReadRowsResponse \\\\\\n            ]:\\n                A sequence of row messages.\\n        \"\"\"\\n        # Infinite loop to reconnect on reconnectable errors while processing\\n        # the row stream.\\n\\n        if self._wrapped is None:\\n            self._reconnect()\\n\\n        while True:\\n            try:\\n                for message in self._wrapped:\\n                    rowcount = message.row_count\\n                    self._offset += rowcount\\n                    yield message\\n\\n                return  # Made it through the whole stream.\\n            except google.api_core.exceptions.InternalServerError as exc:\\n                resumable_error = any(\\n                    resumable_message in exc.message\\n                    for resumable_message in _STREAM_RESUMPTION_INTERNAL_ERROR_MESSAGES\\n                )\\n                if not resumable_error:\\n                    raise\\n            except _STREAM_RESUMPTION_EXCEPTIONS:\\n                # Transient error, so reconnect to the stream.\\n                pass\\n            except Exception as exc:\\n                if not self._resource_exhausted_exception_is_retryable(exc):\\n                    raise\\n\\n            self._reconnect()', 'def _resource_exhausted_exception_is_retryable(self, exc):\\n        if isinstance(exc, google.api_core.exceptions.ResourceExhausted):\\n            # ResourceExhausted errors are only retried if a valid\\n            # RetryInfo is provided with the error.\\n            #\\n            # TODO: Remove hasattr logic when we require google-api-core >= 2.2.0.\\n            #       ResourceExhausted added details/_details in google-api-core 2.2.0.\\n            details = None\\n            if hasattr(exc, \"details\"):\\n                details = exc.details\\n            elif hasattr(exc, \"_details\"):\\n                details = exc._details\\n            if details is not None:\\n                for detail in details:\\n                    if isinstance(detail, google.rpc.error_details_pb2.RetryInfo):\\n                        retry_delay = detail.retry_delay\\n                        if retry_delay is not None:\\n                            delay = max(\\n                                0,\\n                                float(retry_delay.seconds)\\n                                + (float(retry_delay.nanos) / 1e9),\\n                            )\\n                            if self._retry_delay_callback:\\n                                self._retry_delay_callback(delay)\\n                            time.sleep(delay)\\n                            return True\\n        return False', 'def to_arrow(self, read_session=None):\\n        \"\"\"Create a :class:`pyarrow.Table` of all rows in the stream.\\n\\n        This method requires the pyarrow library and a stream using the Arrow\\n        format.\\n\\n        Args:\\n            read_session ( \\\\\\n                ~google.cloud.bigquery_storage_v1.types.ReadSession \\\\\\n            ):\\n                DEPRECATED.\\n\\n                This argument was used to specify the schema of the rows in the\\n                stream, but now the first message in a read stream contains\\n                this information.\\n\\n        Returns:\\n            pyarrow.Table:\\n                A table of all rows in the stream.\\n        \"\"\"\\n        return self.rows(read_session=read_session).to_arrow()', 'def __init__(self, reader, read_session=None):\\n        self._reader = reader\\n        if read_session is not None:\\n            self._stream_parser = _StreamParser.from_read_session(read_session)\\n        else:\\n            self._stream_parser = None', 'def pages(self):\\n        \"\"\"A generator of all pages in the stream.\\n\\n        Returns:\\n            types.GeneratorType[google.cloud.bigquery_storage_v1.ReadRowsPage]:\\n                A generator of pages.\\n        \"\"\"\\n        # Each page is an iterator of rows. But also has num_items, remaining,\\n        # and to_dataframe.\\n        for message in self._reader:\\n            # Only the first message contains the schema, which is needed to\\n            # decode the messages.\\n            if not self._stream_parser:\\n                self._stream_parser = _StreamParser.from_read_rows_response(message)\\n            yield ReadRowsPage(self._stream_parser, message)', 'def to_arrow(self):\\n        \"\"\"Create a :class:`pyarrow.Table` of all rows in the stream.\\n\\n        This method requires the pyarrow library and a stream using the Arrow\\n        format.\\n\\n        Returns:\\n            pyarrow.Table:\\n                A table of all rows in the stream.\\n        \"\"\"\\n        record_batches = []\\n        for page in self.pages:\\n            record_batches.append(page.to_arrow())\\n\\n        if record_batches:\\n            return pyarrow.Table.from_batches(record_batches)\\n\\n        # No data, return an empty Table.\\n        self._stream_parser._parse_arrow_schema()\\n        return pyarrow.Table.from_batches([], schema=self._stream_parser._schema)', 'def _dtypes_from_avro(self, avro_fields):\\n        \"\"\"Determine Pandas dtypes for columns in Avro schema.\\n\\n        Args:\\n            avro_fields (Iterable[Mapping[str, Any]]):\\n                Avro fields\\' metadata.\\n\\n        Returns:\\n            colelctions.OrderedDict[str, str]:\\n                Column names with their corresponding Pandas dtypes.\\n        \"\"\"\\n        result = collections.OrderedDict()\\n\\n        type_map = {\"long\": \"int64\", \"double\": \"float64\", \"boolean\": \"bool\"}\\n\\n        for field_info in avro_fields:\\n            # If a type is an union of multiple types, pick the first type\\n            # that is not \"null\".\\n            if isinstance(field_info[\"type\"], list):\\n                type_info = next(item for item in field_info[\"type\"] if item != \"null\")\\n\\n            if isinstance(type_info, str):\\n                field_dtype = type_map.get(type_info, \"object\")\\n            else:\\n                logical_type = type_info.get(\"logicalType\")\\n                if logical_type == \"timestamp-micros\":\\n                    field_dtype = \"datetime64[ns, UTC]\"\\n                else:\\n                    field_dtype = \"object\"\\n\\n            result[field_info[\"name\"]] = field_dtype\\n\\n        return result', 'def __init__(self, stream_parser, message):\\n        self._stream_parser = stream_parser\\n        self._message = message\\n        self._iter_rows = None\\n        self._num_items = self._message.row_count\\n        self._remaining = self._message.row_count', 'def num_items(self):\\n        \"\"\"int: Total items in the page.\"\"\"\\n        return self._num_items', 'def remaining(self):\\n        \"\"\"int: Remaining items in the page.\"\"\"\\n        return self._remaining', 'def next(self):\\n        \"\"\"Get the next row in the page.\"\"\"\\n        self._parse_rows()\\n        if self._remaining > 0:\\n            self._remaining -= 1\\n        return next(self._iter_rows)', 'def to_arrow(self):\\n        \"\"\"Create an :class:`pyarrow.RecordBatch` of rows in the page.\\n\\n        Returns:\\n            pyarrow.RecordBatch:\\n                Rows from the message, as an Arrow record batch.\\n        \"\"\"\\n        return self._stream_parser.to_arrow(self._message)', 'def to_arrow(self, message):\\n        raise NotImplementedError(\"Not implemented.\")', 'def to_rows(self, message):\\n        raise NotImplementedError(\"Not implemented.\")', 'def _parse_arrow_schema(self):\\n        raise NotImplementedError(\"Not implemented.\")', 'def from_read_session(read_session):\\n        schema_type = read_session._pb.WhichOneof(\"schema\")\\n        if schema_type == \"avro_schema\":\\n            return _AvroStreamParser(read_session)\\n        elif schema_type == \"arrow_schema\":\\n            return _ArrowStreamParser(read_session)\\n        else:\\n            raise TypeError(\\n                \"Unsupported schema type in read_session: {0}\".format(schema_type)\\n            )', 'def from_read_rows_response(message):\\n        schema_type = message._pb.WhichOneof(\"schema\")\\n        if schema_type == \"avro_schema\":\\n            return _AvroStreamParser(message)\\n        elif schema_type == \"arrow_schema\":\\n            return _ArrowStreamParser(message)\\n        else:\\n            raise TypeError(\\n                \"Unsupported schema type in message: {0}\".format(schema_type)\\n            )', 'def __init__(self, message):\\n        \"\"\"Construct an _AvroStreamParser.\\n\\n        Args:\\n            message (Union[\\n                google.cloud.bigquery_storage_v1.types.ReadSession, \\\\\\n                google.cloud.bigquery_storage_v1.types.ReadRowsResponse, \\\\\\n            ]):\\n                Either the first message of data from a read rows stream or a\\n                read session. Both types contain a oneof \"schema\" field, which\\n                can be used to determine how to deserialize rows.\\n        \"\"\"\\n        if fastavro is None:\\n            raise ImportError(_FASTAVRO_REQUIRED)\\n\\n        self._first_message = message\\n        self._avro_schema_json = None\\n        self._fastavro_schema = None\\n        self._column_names = None', 'def to_dataframe(self, message, dtypes=None):\\n        \"\"\"Create a :class:`pandas.DataFrame` of rows in the page.\\n\\n        This method requires the pandas libary to create a data frame and the\\n        fastavro library to parse row messages.\\n\\n        .. warning::\\n            DATETIME columns are not supported. They are currently parsed as\\n            strings in the fastavro library.\\n\\n        Args:\\n            message ( \\\\\\n                ~google.cloud.bigquery_storage_v1.types.ReadRowsResponse \\\\\\n            ):\\n                A message containing Avro bytes to parse into a pandas DataFrame.\\n            dtypes ( \\\\\\n                Map[str, Union[str, pandas.Series.dtype]] \\\\\\n            ):\\n                Optional. A dictionary of column names pandas ``dtype``s. The\\n                provided ``dtype`` is used when constructing the series for\\n                the column specified. Otherwise, the default pandas behavior\\n                is used.\\n\\n        Returns:\\n            pandas.DataFrame:\\n                A data frame of all rows in the stream.\\n        \"\"\"\\n        self._parse_avro_schema()\\n\\n        if dtypes is None:\\n            dtypes = {}\\n\\n        columns = collections.defaultdict(list)\\n        for row in self.to_rows(message):\\n            for column in row:\\n                columns[column].append(row[column])\\n        for column in dtypes:\\n            columns[column] = pandas.Series(columns[column], dtype=dtypes[column])\\n        return pandas.DataFrame(columns, columns=self._column_names)', 'def _parse_fastavro(self):\\n        \"\"\"Convert parsed Avro schema to fastavro format.\"\"\"\\n        self._parse_avro_schema()\\n        self._fastavro_schema = fastavro.parse_schema(self._avro_schema_json)', 'def __init__(self, message):\\n        \"\"\"Construct an _ArrowStreamParser.\\n\\n        Args:\\n            message (Union[\\n                google.cloud.bigquery_storage_v1.types.ReadSession, \\\\\\n                google.cloud.bigquery_storage_v1.types.ReadRowsResponse, \\\\\\n            ]):\\n                Either the first message of data from a read rows stream or a\\n                read session. Both types contain a oneof \"schema\" field, which\\n                can be used to determine how to deserialize rows.\\n        \"\"\"\\n        if pyarrow is None:\\n            raise ImportError(_PYARROW_REQUIRED)\\n\\n        self._first_message = message\\n        self._schema = None', 'def to_rows(self, message):\\n        record_batch = self._parse_arrow_message(message)\\n\\n        # Iterate through each column simultaneously, and make a dict from the\\n        # row values\\n        for row in zip(*record_batch.columns):\\n            yield dict(zip(self._column_names, row))', 'def _parse_arrow_message(self, message):\\n        self._parse_arrow_schema()\\n\\n        return pyarrow.ipc.read_record_batch(\\n            pyarrow.py_buffer(message.arrow_record_batch.serialized_record_batch),\\n            self._schema,\\n        )']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, name, params):\\n        base.InstantiationValidationBenchmark.__init__(self, name, params)\\n\\n        if common.RELEASE == 'liberty':\\n            temp_name = 'stress_workload_liberty.yaml'\\n        else:\\n            temp_name = 'stress_workload.yaml'\\n\\n        self.template_file = common.get_template_dir() + \\\\\\n            temp_name\\n        self.stack_name = 'neighbour'\\n        self.neighbor_stack_names = list()\", 'def init(self):\\n        super(InstantiationValidationNoisyNeighborsBenchmark, self).init()\\n        common.replace_in_file(self.lua_file, \\'local out_file = \"\"\\',\\n                               \\'local out_file = \"\\' +\\n                               self.results_file + \\'\"\\')\\n        heat_param = dict()\\n        heat_param[\\'network\\'] = self.params[NETWORK_NAME]\\n        heat_param[\\'subnet\\'] = self.params[SUBNET_NAME]\\n        heat_param[\\'cores\\'] = self.params[\\'number_of_cores\\']\\n        heat_param[\\'memory\\'] = self.params[\\'amount_of_ram\\']\\n        for i in range(0, int(self.params[\\'num_of_neighbours\\'])):\\n            stack_name = self.stack_name + str(i)\\n            common.DEPLOYMENT_UNIT.deploy_heat_template(self.template_file,\\n                                                        stack_name,\\n                                                        heat_param)\\n            self.neighbor_stack_names.append(stack_name)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def create_router_precommit(self, context, router_context):\\n        pass', 'def update_router_precommit(self, context, router_context):\\n        pass', 'def delete_router_precommit(self, context, router_context):\\n        pass', 'def schedule_router_precommit(self, context, router_context):\\n        pass', 'def unschedule_router_precommit(self, context, router_context):\\n        pass', \"def add_router_interface_precommit(self, context, r_port_context):\\n        # Inside an ASR1k, VLAN sub-interfaces are used to connect to internal\\n        # neutron networks. Only one such sub-interface can be created for each\\n        # VLAN. As the VLAN sub-interface is added to the VRF representing the\\n        # Neutron router, we must only allow one Neutron router to attach to a\\n        # particular Neutron subnet/network.\\n        if (r_port_context.router_context.current[routerrole.ROUTER_ROLE_ATTR]\\n                == ROUTER_ROLE_HA_REDUNDANCY):\\n            # redundancy routers can be exempt as we check the user visible\\n            # routers and the request will be rejected there.\\n            return\\n        e_context = context.elevated()\\n        if r_port_context.current is None:\\n            sn = self._core_plugin.get_subnet(e_context,\\n                                              r_port_context.current_subnet_id)\\n            net_id = sn['network_id']\\n        else:\\n            net_id = r_port_context.current['network_id']\\n        filters = {'network_id': [net_id],\\n                   'device_owner': [bc.constants.DEVICE_OWNER_ROUTER_INTF]}\\n        for port in self._core_plugin.get_ports(e_context,\\n                                                filters=filters):\\n            router_id = port['device_id']\\n            if router_id is None:\\n                continue\\n            router = self._l3_plugin.get_router(e_context, router_id)\\n            if router[routerrole.ROUTER_ROLE_ATTR] is None:\\n                raise TopologyNotSupportedByRouterError()\", 'def remove_router_interface_precommit(self, context, r_port_context):\\n        pass', 'def create_floatingip_precommit(self, context, fip_context):\\n        pass', 'def update_floatingip_precommit(self, context, fip_context):\\n        pass', 'def delete_floatingip_precommit(self, context, fip_context):\\n        pass', \"def ha_interface_ip_address_needed(self, context, router, port,\\n                                       ha_settings_db, ha_group_uuid):\\n        if port['device_owner'] == bc.constants.DEVICE_OWNER_ROUTER_GW:\\n            return False\\n        else:\\n            return True\", 'def pre_backlog_processing(self, context):\\n        filters = {routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_GLOBAL]}\\n        global_routers = self._l3_plugin.get_routers(context, filters=filters)\\n        if not global_routers:\\n            LOG.debug(\"There are no global routers\")\\n            return\\n        for gr in global_routers:\\n            filters = {\\n                HOSTING_DEVICE_ATTR: [gr[HOSTING_DEVICE_ATTR]],\\n                routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_HA_REDUNDANCY, None]\\n            }\\n            invert_filters = {\\'gw_port_id\\': [None]}\\n            num_rtrs = self._l3_plugin.get_routers_count_extended(\\n                context, filters=filters, invert_filters=invert_filters)\\n            LOG.debug(\"Global router %(name)s[%(id)s] with hosting_device \"\\n                      \"%(hd)s has %(num)d routers with gw_port set on that \"\\n                      \"device\",\\n                      {\\'name\\': gr[\\'name\\'], \\'id\\': gr[\\'id\\'],\\n                       \\'hd\\': gr[HOSTING_DEVICE_ATTR], \\'num\\': num_rtrs, })\\n            if num_rtrs == 0:\\n                LOG.warning(\\n                    _LW(\"Global router:%(name)s[id:%(id)s] is present for \"\\n                        \"hosting device:%(hd)s but there are no tenant or \"\\n                        \"redundancy routers with gateway set on that hosting \"\\n                        \"device. Proceeding to delete global router.\"),\\n                    {\\'name\\': gr[\\'name\\'], \\'id\\': gr[\\'id\\'],\\n                     \\'hd\\': gr[HOSTING_DEVICE_ATTR]})\\n                self._delete_global_router(context, gr[\\'id\\'])\\n                filters = {\\n                    #TODO(bmelande): Filter on routertype of global router\\n                    #routertype.TYPE_ATTR: [routertype_id],\\n                    routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_LOGICAL_GLOBAL]}\\n                log_global_routers = self._l3_plugin.get_routers(\\n                    context, filters=filters)\\n                if log_global_routers:\\n                    log_global_router_id = log_global_routers[0][\\'id\\']\\n                    self._delete_global_router(context, log_global_router_id,\\n                                               logical=True)', \"def _conditionally_add_global_router(self, context, tenant_router):\\n        # We could filter on hosting device id but we don't so we get all\\n        # global routers for this router type. We can then use that count to\\n        # determine which ha priority a new global router should get.\\n        filters = {\\n            routertype.TYPE_ATTR: [tenant_router[routertype.TYPE_ATTR]],\\n            routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_GLOBAL]}\\n        global_routers = self._l3_plugin.get_routers(\\n            context, filters=filters)\\n        hd_to_gr_dict = {r[HOSTING_DEVICE_ATTR]: r for r in global_routers}\\n        hosting_device_id = tenant_router[HOSTING_DEVICE_ATTR]\\n        ext_nw_id = tenant_router[l3.EXTERNAL_GW_INFO]['network_id']\\n        global_router = hd_to_gr_dict.get(hosting_device_id)\\n        logical_global_router = self._get_logical_global_router(context,\\n                                                                tenant_router)\\n        self._conditionally_add_auxiliary_external_gateway_port(\\n            context, logical_global_router, ext_nw_id, tenant_router, True)\\n        if global_router is None:\\n            # must create global router on hosting device\\n            global_router = self._create_global_router(\\n                context, hosting_device_id, hd_to_gr_dict, tenant_router,\\n                logical_global_router)\\n        self._conditionally_add_auxiliary_external_gateway_port(\\n            context, global_router, ext_nw_id, tenant_router)\\n        self._l3_plugin.add_type_and_hosting_device_info(context,\\n                                                         global_router)\\n        for ni in self._l3_plugin.get_notifiers(context, [global_router]):\\n            if ni['notifier']:\\n                ni['notifier'].routers_updated(context, ni['routers'])\", \"def _create_auxiliary_external_gateway_port(\\n            self, context, global_router, ext_net_id, tenant_router,\\n            port_type=DEVICE_OWNER_GLOBAL_ROUTER_GW):\\n        # When a global router is connected to an external network then a\\n        # special type of gateway port is created on that network. Such a\\n        # port is called auxiliary gateway ports. It has an ip address on\\n        # each subnet of the external network. A (logical) global router\\n        # never has a traditional Neutron gateway port.\\n        filters = {\\n            'device_id': [tenant_router['id']],\\n            'device_owner': [l3_constants.DEVICE_OWNER_ROUTER_GW]}\\n        # fetch the gateway port of the *tenant* router so we can determine\\n        # the CIDR of that port's subnet\\n        gw_port = self._core_plugin.get_ports(context,\\n                                              filters=filters)[0]\\n        fixed_ips = self._get_fixed_ips_subnets(context, gw_port)\\n        global_router_id = global_router['id']\\n        with context.session.begin(subtransactions=True):\\n            aux_gw_port = self._core_plugin.create_port(context, {\\n                'port': {\\n                    'tenant_id': '',  # intentionally not set\\n                    'network_id': ext_net_id,\\n                    'mac_address': bc.constants.ATTR_NOT_SPECIFIED,\\n                    'fixed_ips': fixed_ips,\\n                    'device_id': global_router_id,\\n                    'device_owner': port_type,\\n                    'admin_state_up': True,\\n                    'name': ''}})\\n            router_port = bc.RouterPort(\\n                port_id=aux_gw_port['id'],\\n                router_id=global_router_id,\\n                port_type=port_type)\\n            context.session.add(router_port)\\n        return aux_gw_port\", 'def _get_logical_global_router(self, context, tenant_router):\\n        # Since HA is also enabled on the global routers on each hosting device\\n        # those global routers need HA settings and VIPs. We represent that\\n        # using a Neutron router that is never instantiated/hosted. That\\n        # Neutron router is referred to as the \"logical global\" router.\\n        filters = {routertype.TYPE_ATTR: [tenant_router[routertype.TYPE_ATTR]],\\n                   routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_LOGICAL_GLOBAL]}\\n        logical_global_routers = self._l3_plugin.get_routers(\\n            context, filters=filters)\\n        if not logical_global_routers:\\n            # must create logical global router\\n            logical_global_router = self._create_logical_global_router(\\n                context, tenant_router)\\n        else:\\n            logical_global_router = logical_global_routers[0]\\n            self._update_ha_redundancy_level(context, logical_global_router, 1)\\n        return logical_global_router', \"def _get_fixed_ips_subnets(self, context, gw_port):\\n        nw = self._core_plugin.get_network(context, gw_port['network_id'])\\n        subnets = [{'subnet_id': s} for s in nw['subnets']]\\n        return subnets\", 'def _get_ha_binding(self, context, router_id):\\n        with context.session.begin(subtransactions=True):\\n            query = context.session.query(ha_db.RouterHASetting)\\n            query = query.filter(\\n                ha_db.RouterHASetting.router_id == router_id)\\n            return query.first()', \"def _conditionally_remove_global_router(self, context, tenant_router,\\n                                            update_operation=False):\\n        filters = {routertype.TYPE_ATTR: [tenant_router[routertype.TYPE_ATTR]],\\n                   routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_GLOBAL],\\n                   HOSTING_DEVICE_ATTR: [tenant_router[HOSTING_DEVICE_ATTR]]}\\n        global_routers = self._l3_plugin.get_routers(context,\\n                                                     filters=filters)\\n        hd_to_gr_dict = {r[HOSTING_DEVICE_ATTR]: r for r in global_routers}\\n        if global_routers:\\n            global_router_id = global_routers[0]['id']\\n            if not tenant_router or not tenant_router[l3.EXTERNAL_GW_INFO]:\\n                # let l3 plugin's periodic backlog processing take care of the\\n                # clean up of the global router\\n                return\\n            ext_net_id = tenant_router[l3.EXTERNAL_GW_INFO]['network_id']\\n            routertype_id = tenant_router[routertype.TYPE_ATTR]\\n            hd_id = tenant_router[HOSTING_DEVICE_ATTR]\\n            global_router = hd_to_gr_dict.get(hd_id)\\n            port_deleted = self._conditionally_remove_auxiliary_gateway_port(\\n                context, global_router_id, ext_net_id, routertype_id, hd_id,\\n                update_operation)\\n            if port_deleted is False:\\n                # since no auxiliary gateway port was deleted we can\\n                # abort no since auxiliary gateway port count cannot\\n                # have reached zero\\n                return\\n            filters = {\\n                'device_id': [global_router_id],\\n                'device_owner': [DEVICE_OWNER_GLOBAL_ROUTER_GW]}\\n            num_aux_gw_ports = self._core_plugin.get_ports_count(\\n                context, filters=filters)\\n            if num_aux_gw_ports == 0:\\n                # global router not needed any more so we delete it\\n                self._delete_global_router(context, global_router_id)\\n                do_notify = False\\n            else:\\n                do_notify = True\\n            # process logical global router to remove its port\\n            self._conditionally_remove_auxiliary_gateway_vip_port(\\n                context, ext_net_id, routertype_id)\\n            self._l3_plugin.add_type_and_hosting_device_info(context,\\n                                                             global_router)\\n            if do_notify is True:\\n                for ni in self._l3_plugin.get_notifiers(context,\\n                                                        [global_router]):\\n                    if ni['notifier']:\\n                        ni['notifier'].routers_updated(context, ni['routers'])\", \"def _conditionally_remove_auxiliary_gateway_vip_port(\\n            self, context, ext_net_id, routertype_id):\\n        filters = {routertype.TYPE_ATTR: [routertype_id],\\n                   routerrole.ROUTER_ROLE_ATTR: [ROUTER_ROLE_LOGICAL_GLOBAL]}\\n        log_global_routers = self._l3_plugin.get_routers(context,\\n                                                         filters=filters)\\n        if not log_global_routers:\\n            return\\n        self._update_ha_redundancy_level(context, log_global_routers[0], -1)\\n        log_global_router_id = log_global_routers[0]['id']\\n        num_global_rtrs = self._get_gateway_routers_count(\\n            context, ext_net_id, routertype_id, ROUTER_ROLE_GLOBAL)\\n        if num_global_rtrs == 0:\\n            # there are no global routers *on ext_net_id* that are serviced by\\n            # this logical global router so it's aux gw VIP port can be deleted\\n            self._delete_auxiliary_gateway_ports(context, log_global_router_id,\\n                                                 ext_net_id)\\n        filters[routerrole.ROUTER_ROLE_ATTR] = [ROUTER_ROLE_GLOBAL]\\n        total_num_global_rtrs = self._l3_plugin.get_routers_count(\\n            context, filters=filters)\\n        if total_num_global_rtrs == 0:\\n            # there are no global routers left that are serviced by this\\n            # logical global router so it can be deleted\\n            self._delete_global_router(context, log_global_router_id, True)\\n        return False\", 'def _delete_global_router(self, context, global_router_id, logical=False):\\n        # ensure we clean up any stale auxiliary gateway ports\\n        self._delete_auxiliary_gateway_ports(context, global_router_id)\\n        try:\\n            if logical is True:\\n                # We use parent class method as no special operations beyond\\n                # what the base implemenation does are needed for logical\\n                # global router\\n                super(L3RouterApplianceDBMixin, self._l3_plugin).delete_router(\\n                        context, global_router_id)\\n            else:\\n                self._l3_plugin.delete_router(\\n                    context, global_router_id, unschedule=False)\\n        except (exc.ObjectDeletedError, l3.RouterNotFound) as e:\\n            LOG.warning(e)', \"def _update_ha_redundancy_level(self, context, logical_global_router,\\n                                    delta):\\n        with context.session.begin(subtransactions=True):\\n            log_g_router_db = self._l3_plugin._get_router(\\n                context, logical_global_router['id'])\\n            log_g_router_db.ha_settings.redundancy_level += delta\\n            context.session.add(log_g_router_db.ha_settings)\", \"def _global_router_name(self, hosting_device_id, logical=False):\\n        if logical is True:\\n            return cisco_constants.LOGICAL_ROUTER_ROLE_NAME\\n        else:\\n            return '%s-%s' % (cisco_constants.ROUTER_ROLE_NAME_PREFIX,\\n                              hosting_device_id[-cisco_constants.ROLE_ID_LEN:])\", 'def _core_plugin(self):\\n        return bc.get_plugin()']}, {'features': [], 'snippets': [\"def setUp(self):\\n        with mock.patch(\\n            'airflow.providers.google.cloud.hooks.vision.CloudVisionHook.__init__',\\n            new=mock_base_gcp_hook_default_project_id,\\n        ):\\n            self.hook = CloudVisionHook(gcp_conn_id='test')\", 'def test_product_search_client_creation(self, mock_client, mock_get_creds, mock_client_info):\\n        result = self.hook.get_conn()\\n        mock_client.assert_called_once_with(\\n            credentials=mock_get_creds.return_value, client_info=mock_client_info.return_value\\n        )\\n        assert mock_client.return_value == result\\n        assert self.hook._client == result', 'def test_create_productset_explicit_id(self, get_conn):\\n        # Given\\n        create_product_set_method = get_conn.return_value.create_product_set\\n        create_product_set_method.return_value = None\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product_set = ProductSet()\\n        # When\\n        result = self.hook.create_product_set(\\n            location=LOC_ID_TEST,\\n            product_set_id=PRODUCTSET_ID_TEST,\\n            product_set=product_set,\\n            project_id=PROJECT_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )\\n\\n        # Then\\n        # ProductSet ID was provided explicitly in the method call above, should be returned from the method\\n        assert result == PRODUCTSET_ID_TEST\\n        create_product_set_method.assert_called_once_with(\\n            parent=parent,\\n            product_set=product_set,\\n            product_set_id=PRODUCTSET_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )', \"def test_create_productset_autogenerated_id(self, get_conn):\\n        # Given\\n        autogenerated_id = 'autogen-id'\\n        response_product_set = ProductSet(\\n            name=ProductSearchClient.product_set_path(PROJECT_ID_TEST, LOC_ID_TEST, autogenerated_id)\\n        )\\n        create_product_set_method = get_conn.return_value.create_product_set\\n        create_product_set_method.return_value = response_product_set\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product_set = ProductSet()\\n        # When\\n        result = self.hook.create_product_set(\\n            location=LOC_ID_TEST, product_set_id=None, product_set=product_set, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        # ProductSet ID was not provided in the method call above. Should be extracted from the API response\\n        # and returned.\\n        assert result == autogenerated_id\\n        create_product_set_method.assert_called_once_with(\\n            parent=parent,\\n            product_set=product_set,\\n            product_set_id=None,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )\", \"def test_create_productset_autogenerated_id_wrong_api_response(self, get_conn):\\n        # Given\\n        response_product_set = None\\n        create_product_set_method = get_conn.return_value.create_product_set\\n        create_product_set_method.return_value = response_product_set\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product_set = ProductSet()\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.create_product_set(\\n                location=LOC_ID_TEST,\\n                product_set_id=None,\\n                product_set=product_set,\\n                project_id=PROJECT_ID_TEST,\\n                retry=None,\\n                timeout=None,\\n                metadata=None,\\n            )\\n        # Then\\n        # API response was wrong (None) and thus ProductSet ID extraction should fail.\\n        err = ctx.value\\n        assert 'Unable to get name from response...' in str(err)\\n        create_product_set_method.assert_called_once_with(\\n            parent=parent,\\n            product_set=product_set,\\n            product_set_id=None,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )\", 'def test_get_productset(self, get_conn):\\n        # Given\\n        name = ProductSearchClient.product_set_path(PROJECT_ID_TEST, LOC_ID_TEST, PRODUCTSET_ID_TEST)\\n        response_product_set = ProductSet(name=name)\\n        get_product_set_method = get_conn.return_value.get_product_set\\n        get_product_set_method.return_value = response_product_set\\n        # When\\n        response = self.hook.get_product_set(\\n            location=LOC_ID_TEST, product_set_id=PRODUCTSET_ID_TEST, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        assert response\\n        assert response == MessageToDict(response_product_set)\\n        get_product_set_method.assert_called_once_with(name=name, retry=None, timeout=None, metadata=None)', 'def test_update_productset_no_explicit_name(self, get_conn):\\n        # Given\\n        product_set = ProductSet()\\n        update_product_set_method = get_conn.return_value.update_product_set\\n        update_product_set_method.return_value = product_set\\n        productset_name = ProductSearchClient.product_set_path(\\n            PROJECT_ID_TEST, LOC_ID_TEST, PRODUCTSET_ID_TEST\\n        )\\n        # When\\n        result = self.hook.update_product_set(\\n            location=LOC_ID_TEST,\\n            product_set_id=PRODUCTSET_ID_TEST,\\n            product_set=product_set,\\n            update_mask=None,\\n            project_id=PROJECT_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )\\n        # Then\\n        assert result == MessageToDict(product_set)\\n        update_product_set_method.assert_called_once_with(\\n            product_set=ProductSet(name=productset_name),\\n            metadata=None,\\n            retry=None,\\n            timeout=None,\\n            update_mask=None,\\n        )', 'def test_update_productset_no_explicit_name_and_missing_params_for_constructed_name(\\n        self, location, product_set_id, get_conn', 'def test_update_productset_explicit_name_missing_params_for_constructed_name(\\n        self, location, product_set_id, get_conn', 'def test_update_productset_explicit_name_different_from_constructed(self, get_conn):\\n        # Given\\n        update_product_set_method = get_conn.return_value.update_product_set\\n        update_product_set_method.return_value = None\\n        explicit_ps_name = ProductSearchClient.product_set_path(\\n            PROJECT_ID_TEST_2, LOC_ID_TEST_2, PRODUCTSET_ID_TEST_2\\n        )\\n        product_set = ProductSet(name=explicit_ps_name)\\n        template_ps_name = ProductSearchClient.product_set_path(\\n            PROJECT_ID_TEST, LOC_ID_TEST, PRODUCTSET_ID_TEST\\n        )\\n        # When\\n        # Location and product_set_id are passed in addition to a ProductSet with an explicit name,\\n        # but both names differ (constructed != explicit).\\n        # Should throw AirflowException in this case.\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.update_product_set(\\n                location=LOC_ID_TEST,\\n                product_set_id=PRODUCTSET_ID_TEST,\\n                product_set=product_set,\\n                update_mask=None,\\n                project_id=PROJECT_ID_TEST,\\n                retry=None,\\n                timeout=None,\\n                metadata=None,\\n            )\\n        err = ctx.value\\n        # self.assertIn(\"The required parameter \\'project_id\\' is missing\", str(err))\\n        assert err\\n        assert (\\n            ERR_DIFF_NAMES.format(\\n                explicit_name=explicit_ps_name,\\n                constructed_name=template_ps_name,\\n                label=\"ProductSet\",\\n                id_label=\"productset_id\",\\n            )\\n            in str(err)\\n        )\\n        update_product_set_method.assert_not_called()', 'def test_delete_productset(self, get_conn):\\n        # Given\\n        delete_product_set_method = get_conn.return_value.delete_product_set\\n        delete_product_set_method.return_value = None\\n        name = ProductSearchClient.product_set_path(PROJECT_ID_TEST, LOC_ID_TEST, PRODUCTSET_ID_TEST)\\n        # When\\n        response = self.hook.delete_product_set(\\n            location=LOC_ID_TEST, product_set_id=PRODUCTSET_ID_TEST, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        assert response is None\\n        delete_product_set_method.assert_called_once_with(name=name, retry=None, timeout=None, metadata=None)', 'def test_create_reference_image_explicit_id(self, get_conn):\\n        # Given\\n        create_reference_image_method = get_conn.return_value.create_reference_image\\n\\n        # When\\n        result = self.hook.create_reference_image(\\n            project_id=PROJECT_ID_TEST,\\n            location=LOC_ID_TEST,\\n            product_id=PRODUCT_ID_TEST,\\n            reference_image=REFERENCE_IMAGE_WITHOUT_ID_NAME,\\n            reference_image_id=REFERENCE_IMAGE_ID_TEST,\\n        )\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        assert result == REFERENCE_IMAGE_ID_TEST\\n        create_reference_image_method.assert_called_once_with(\\n            parent=PRODUCT_NAME,\\n            reference_image=REFERENCE_IMAGE_WITHOUT_ID_NAME,\\n            reference_image_id=REFERENCE_IMAGE_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )', 'def test_create_reference_image_autogenerated_id(self, get_conn):\\n        # Given\\n        create_reference_image_method = get_conn.return_value.create_reference_image\\n\\n        # When\\n        result = self.hook.create_reference_image(\\n            project_id=PROJECT_ID_TEST,\\n            location=LOC_ID_TEST,\\n            product_id=PRODUCT_ID_TEST,\\n            reference_image=REFERENCE_IMAGE_TEST,\\n            reference_image_id=REFERENCE_IMAGE_ID_TEST,\\n        )\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        assert result == REFERENCE_IMAGE_GEN_ID_TEST\\n        create_reference_image_method.assert_called_once_with(\\n            parent=PRODUCT_NAME,\\n            reference_image=REFERENCE_IMAGE_TEST,\\n            reference_image_id=REFERENCE_IMAGE_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )', 'def test_add_product_to_product_set(self, get_conn):\\n        # Given\\n        add_product_to_product_set_method = get_conn.return_value.add_product_to_product_set\\n\\n        # When\\n        self.hook.add_product_to_product_set(\\n            product_set_id=PRODUCTSET_ID_TEST,\\n            product_id=PRODUCT_ID_TEST,\\n            location=LOC_ID_TEST,\\n            project_id=PROJECT_ID_TEST,\\n        )\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        add_product_to_product_set_method.assert_called_once_with(\\n            name=PRODUCTSET_NAME_TEST, product=PRODUCT_NAME_TEST, retry=None, timeout=None, metadata=None\\n        )', 'def test_remove_product_from_product_set(self, get_conn):\\n        # Given\\n        remove_product_from_product_set_method = get_conn.return_value.remove_product_from_product_set\\n\\n        # When\\n        self.hook.remove_product_from_product_set(\\n            product_set_id=PRODUCTSET_ID_TEST,\\n            product_id=PRODUCT_ID_TEST,\\n            location=LOC_ID_TEST,\\n            project_id=PROJECT_ID_TEST,\\n        )\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        remove_product_from_product_set_method.assert_called_once_with(\\n            name=PRODUCTSET_NAME_TEST, product=PRODUCT_NAME_TEST, retry=None, timeout=None, metadata=None\\n        )', 'def test_annotate_image(self, annotator_client_mock):\\n        # Given\\n        annotate_image_method = annotator_client_mock.annotate_image\\n\\n        # When\\n        self.hook.annotate_image(request=ANNOTATE_IMAGE_REQUEST)\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        annotate_image_method.assert_called_once_with(\\n            request=ANNOTATE_IMAGE_REQUEST, retry=None, timeout=None\\n        )', 'def test_batch_annotate_images(self, annotator_client_mock):\\n        # Given\\n        batch_annotate_images_method = annotator_client_mock.batch_annotate_images\\n\\n        # When\\n        self.hook.batch_annotate_images(requests=BATCH_ANNOTATE_IMAGE_REQUEST)\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        batch_annotate_images_method.assert_called_once_with(\\n            requests=BATCH_ANNOTATE_IMAGE_REQUEST, retry=None, timeout=None\\n        )', 'def test_create_product_explicit_id(self, get_conn):\\n        # Given\\n        create_product_method = get_conn.return_value.create_product\\n        create_product_method.return_value = None\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product = Product()\\n        # When\\n        result = self.hook.create_product(\\n            location=LOC_ID_TEST, product_id=PRODUCT_ID_TEST, product=product, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        # Product ID was provided explicitly in the method call above, should be returned from the method\\n        assert result == PRODUCT_ID_TEST\\n        create_product_method.assert_called_once_with(\\n            parent=parent,\\n            product=product,\\n            product_id=PRODUCT_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )', \"def test_create_product_autogenerated_id(self, get_conn):\\n        # Given\\n        autogenerated_id = 'autogen-p-id'\\n        response_product = Product(\\n            name=ProductSearchClient.product_path(PROJECT_ID_TEST, LOC_ID_TEST, autogenerated_id)\\n        )\\n        create_product_method = get_conn.return_value.create_product\\n        create_product_method.return_value = response_product\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product = Product()\\n        # When\\n        result = self.hook.create_product(\\n            location=LOC_ID_TEST, product_id=None, product=product, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        # Product ID was not provided in the method call above. Should be extracted from the API response\\n        # and returned.\\n        assert result == autogenerated_id\\n        create_product_method.assert_called_once_with(\\n            parent=parent, product=product, product_id=None, retry=None, timeout=None, metadata=None\\n        )\", \"def test_create_product_autogenerated_id_wrong_name_in_response(self, get_conn):\\n        # Given\\n        wrong_name = 'wrong_name_not_a_correct_path'\\n        response_product = Product(name=wrong_name)\\n        create_product_method = get_conn.return_value.create_product\\n        create_product_method.return_value = response_product\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product = Product()\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.create_product(\\n                location=LOC_ID_TEST, product_id=None, product=product, project_id=PROJECT_ID_TEST\\n            )\\n        # Then\\n        # API response was wrong (wrong name format) and thus ProductSet ID extraction should fail.\\n        err = ctx.value\\n        assert 'Unable to get id from name' in str(err)\\n        create_product_method.assert_called_once_with(\\n            parent=parent, product=product, product_id=None, retry=None, timeout=None, metadata=None\\n        )\", \"def test_create_product_autogenerated_id_wrong_api_response(self, get_conn):\\n        # Given\\n        response_product = None\\n        create_product_method = get_conn.return_value.create_product\\n        create_product_method.return_value = response_product\\n        parent = ProductSearchClient.location_path(PROJECT_ID_TEST, LOC_ID_TEST)\\n        product = Product()\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.create_product(\\n                location=LOC_ID_TEST, product_id=None, product=product, project_id=PROJECT_ID_TEST\\n            )\\n        # Then\\n        # API response was wrong (None) and thus ProductSet ID extraction should fail.\\n        err = ctx.value\\n        assert 'Unable to get name from response...' in str(err)\\n        create_product_method.assert_called_once_with(\\n            parent=parent, product=product, product_id=None, retry=None, timeout=None, metadata=None\\n        )\", 'def test_update_product_no_explicit_name(self, get_conn):\\n        # Given\\n        product = Product()\\n        update_product_method = get_conn.return_value.update_product\\n        update_product_method.return_value = product\\n        product_name = ProductSearchClient.product_path(PROJECT_ID_TEST, LOC_ID_TEST, PRODUCT_ID_TEST)\\n        # When\\n        result = self.hook.update_product(\\n            location=LOC_ID_TEST,\\n            product_id=PRODUCT_ID_TEST,\\n            product=product,\\n            update_mask=None,\\n            project_id=PROJECT_ID_TEST,\\n            retry=None,\\n            timeout=None,\\n            metadata=None,\\n        )\\n        # Then\\n        assert result == MessageToDict(product)\\n        update_product_method.assert_called_once_with(\\n            product=Product(name=product_name), metadata=None, retry=None, timeout=None, update_mask=None\\n        )', 'def test_update_product_no_explicit_name_and_missing_params_for_constructed_name(\\n        self, location, product_id, get_conn', 'def test_update_product_explicit_name_missing_params_for_constructed_name(\\n        self, location, product_id, get_conn', 'def test_update_product_explicit_name_different_from_constructed(self, get_conn):\\n        # Given\\n        update_product_method = get_conn.return_value.update_product\\n        update_product_method.return_value = None\\n        explicit_p_name = ProductSearchClient.product_path(\\n            PROJECT_ID_TEST_2, LOC_ID_TEST_2, PRODUCT_ID_TEST_2\\n        )\\n        product = Product(name=explicit_p_name)\\n        template_p_name = ProductSearchClient.product_path(PROJECT_ID_TEST, LOC_ID_TEST, PRODUCT_ID_TEST)\\n        # When\\n        # Location and product_id are passed in addition to a Product with an explicit name,\\n        # but both names differ (constructed != explicit).\\n        # Should throw AirflowException in this case.\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.update_product(\\n                location=LOC_ID_TEST,\\n                product_id=PRODUCT_ID_TEST,\\n                product=product,\\n                update_mask=None,\\n                project_id=PROJECT_ID_TEST,\\n                retry=None,\\n                timeout=None,\\n                metadata=None,\\n            )\\n        err = ctx.value\\n        assert err\\n        assert (\\n            ERR_DIFF_NAMES.format(\\n                explicit_name=explicit_p_name,\\n                constructed_name=template_p_name,\\n                label=\"Product\",\\n                id_label=\"product_id\",\\n            )\\n            in str(err)\\n        )\\n        update_product_method.assert_not_called()', 'def test_delete_product(self, get_conn):\\n        # Given\\n        delete_product_method = get_conn.return_value.delete_product\\n        delete_product_method.return_value = None\\n        name = ProductSearchClient.product_path(PROJECT_ID_TEST, LOC_ID_TEST, PRODUCT_ID_TEST)\\n        # When\\n        response = self.hook.delete_product(\\n            location=LOC_ID_TEST, product_id=PRODUCT_ID_TEST, project_id=PROJECT_ID_TEST\\n        )\\n        # Then\\n        assert response is None\\n        delete_product_method.assert_called_once_with(name=name, retry=None, timeout=None, metadata=None)', 'def test_detect_text(self, annotator_client_mock):\\n        # Given\\n        detect_text_method = annotator_client_mock.text_detection\\n        detect_text_method.return_value = AnnotateImageResponse(\\n            text_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.text_detection(image=DETECT_TEST_IMAGE)\\n\\n        # Then\\n        detect_text_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None\\n        )', 'def test_detect_text_with_additional_properties(self, annotator_client_mock):\\n        # Given\\n        detect_text_method = annotator_client_mock.text_detection\\n        detect_text_method.return_value = AnnotateImageResponse(\\n            text_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.text_detection(\\n            image=DETECT_TEST_IMAGE, additional_properties={\"prop1\": \"test1\", \"prop2\": \"test2\"}\\n        )\\n\\n        # Then\\n        detect_text_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None, prop1=\"test1\", prop2=\"test2\"\\n        )', 'def test_detect_text_with_error_response(self, annotator_client_mock):\\n        # Given\\n        detect_text_method = annotator_client_mock.text_detection\\n        detect_text_method.return_value = AnnotateImageResponse(\\n            error={\"code\": 3, \"message\": \"test error message\"}\\n        )\\n\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.text_detection(image=DETECT_TEST_IMAGE)\\n\\n        err = ctx.value\\n        assert \"test error message\" in str(err)', 'def test_document_text_detection(self, annotator_client_mock):\\n        # Given\\n        document_text_detection_method = annotator_client_mock.document_text_detection\\n        document_text_detection_method.return_value = AnnotateImageResponse(\\n            text_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.document_text_detection(image=DETECT_TEST_IMAGE)\\n\\n        # Then\\n        document_text_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None\\n        )', 'def test_document_text_detection_with_additional_properties(self, annotator_client_mock):\\n        # Given\\n        document_text_detection_method = annotator_client_mock.document_text_detection\\n        document_text_detection_method.return_value = AnnotateImageResponse(\\n            text_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.document_text_detection(\\n            image=DETECT_TEST_IMAGE, additional_properties={\"prop1\": \"test1\", \"prop2\": \"test2\"}\\n        )\\n\\n        # Then\\n        document_text_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None, prop1=\"test1\", prop2=\"test2\"\\n        )', 'def test_detect_document_text_with_error_response(self, annotator_client_mock):\\n        # Given\\n        detect_text_method = annotator_client_mock.document_text_detection\\n        detect_text_method.return_value = AnnotateImageResponse(\\n            error={\"code\": 3, \"message\": \"test error message\"}\\n        )\\n\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.document_text_detection(image=DETECT_TEST_IMAGE)\\n\\n        err = ctx.value\\n        assert \"test error message\" in str(err)', 'def test_label_detection(self, annotator_client_mock):\\n        # Given\\n        label_detection_method = annotator_client_mock.label_detection\\n        label_detection_method.return_value = AnnotateImageResponse(\\n            label_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.label_detection(image=DETECT_TEST_IMAGE)\\n\\n        # Then\\n        label_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None\\n        )', 'def test_label_detection_with_additional_properties(self, annotator_client_mock):\\n        # Given\\n        label_detection_method = annotator_client_mock.label_detection\\n        label_detection_method.return_value = AnnotateImageResponse(\\n            label_annotations=[EntityAnnotation(description=\"test\", score=0.5)]\\n        )\\n\\n        # When\\n        self.hook.label_detection(\\n            image=DETECT_TEST_IMAGE, additional_properties={\"prop1\": \"test1\", \"prop2\": \"test2\"}\\n        )\\n\\n        # Then\\n        label_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None, prop1=\"test1\", prop2=\"test2\"\\n        )', 'def test_label_detection_with_error_response(self, annotator_client_mock):\\n        # Given\\n        detect_text_method = annotator_client_mock.label_detection\\n        detect_text_method.return_value = AnnotateImageResponse(\\n            error={\"code\": 3, \"message\": \"test error message\"}\\n        )\\n\\n        # When\\n        with pytest.raises(AirflowException) as ctx:\\n            self.hook.label_detection(image=DETECT_TEST_IMAGE)\\n\\n        err = ctx.value\\n        assert \"test error message\" in str(err)', 'def test_safe_search_detection(self, annotator_client_mock):\\n        # Given\\n        safe_search_detection_method = annotator_client_mock.safe_search_detection\\n        safe_search_detection_method.return_value = AnnotateImageResponse(\\n            safe_search_annotation=SafeSearchAnnotation(\\n                adult=\"VERY_UNLIKELY\",\\n                spoof=\"VERY_UNLIKELY\",\\n                medical=\"VERY_UNLIKELY\",\\n                violence=\"VERY_UNLIKELY\",\\n                racy=\"VERY_UNLIKELY\",\\n            )\\n        )\\n\\n        # When\\n        self.hook.safe_search_detection(image=DETECT_TEST_IMAGE)\\n\\n        # Then\\n        safe_search_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None\\n        )', 'def test_safe_search_detection_with_additional_properties(self, annotator_client_mock):\\n        # Given\\n        safe_search_detection_method = annotator_client_mock.safe_search_detection\\n        safe_search_detection_method.return_value = AnnotateImageResponse(\\n            safe_search_annotation=SafeSearchAnnotation(\\n                adult=\"VERY_UNLIKELY\",\\n                spoof=\"VERY_UNLIKELY\",\\n                medical=\"VERY_UNLIKELY\",\\n                violence=\"VERY_UNLIKELY\",\\n                racy=\"VERY_UNLIKELY\",\\n            )\\n        )\\n\\n        # When\\n        self.hook.safe_search_detection(\\n            image=DETECT_TEST_IMAGE, additional_properties={\"prop1\": \"test1\", \"prop2\": \"test2\"}\\n        )\\n\\n        # Then\\n        safe_search_detection_method.assert_called_once_with(\\n            image=DETECT_TEST_IMAGE, max_results=None, retry=None, timeout=None, prop1=\"test1\", prop2=\"test2\"\\n        )']}, {'features': [], 'snippets': ['def photo_preview(self):\\n        img = get_thumbnail(self.photo, \\'75x75\\', crop=\\'center\\')\\n        return format_html(\\'<a href=\"{}\" target=\"_blank\"><img style=\"width:75px; height:75px;\" src=\"{}\"></a>\\',\\n                           self.photo.url, img.url)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _get_virtual_variable(variables, key):\\n    \"\"\"Get a virtual variable (e.g., \\'time.year\\') from a dict of xray.Variable\\n    objects (if possible)\\n    \"\"\"\\n    if not isinstance(key, basestring):\\n        raise KeyError(key)\\n\\n    split_key = key.split(\\'.\\', 1)\\n    if len(split_key) != 2:\\n        raise KeyError(key)\\n\\n    ref_name, var_name = split_key\\n    ref_var = variables[ref_name]\\n    if ref_var.ndim == 1:\\n        date = ref_var.to_index()\\n    elif ref_var.ndim == 0:\\n        date = pd.Timestamp(ref_var.values)\\n    else:\\n        raise KeyError(key)\\n\\n    if var_name == \\'season\\':\\n        # TODO: move \\'season\\' into pandas itself\\n        seasons = np.array([\\'DJF\\', \\'MAM\\', \\'JJA\\', \\'SON\\'])\\n        month = date.month\\n        data = seasons[(month // 3) % 4]\\n    else:\\n        data = getattr(date, var_name)\\n    return ref_name, var_name, Variable(ref_var.dims, data)', 'def _align_variables(variables, join=\\'outer\\'):\\n    \"\"\"Align all DataArrays in the provided dict, leaving other values alone.\\n    \"\"\"\\n    alignable = [k for k, v in variables.items() if hasattr(v, \\'indexes\\')]\\n    aligned = align(*[variables[a] for a in alignable],\\n                    join=join, copy=False)\\n    new_variables = OrderedDict(variables)\\n    new_variables.update(zip(alignable, aligned))\\n    return new_variables', 'def maybe_promote_or_replace(name, var):\\n        existing_var = variables[name]\\n        if name not in existing_var.dims:\\n            if name in var.dims:\\n                variables[name] = var\\n            else:\\n                common_dims = OrderedDict(zip(existing_var.dims,\\n                                              existing_var.shape))\\n                common_dims.update(zip(var.dims, var.shape))\\n                variables[name] = existing_var.expand_dims(common_dims)\\n                new_coord_names.update(var.dims)', 'def _calculate_dims(variables):\\n    \"\"\"Calculate the dimensions corresponding to a set of variables.\\n\\n    Returns dictionary mapping from dimension names to sizes. Raises ValueError\\n    if any of the dimension sizes conflict.\\n    \"\"\"\\n    dims = {}\\n    last_used = {}\\n    scalar_vars = set(k for k, v in iteritems(variables) if not v.dims)\\n    for k, var in iteritems(variables):\\n        for dim, size in zip(var.dims, var.shape):\\n            if dim in scalar_vars:\\n                raise ValueError(\\'dimension %s already exists as a scalar \\'\\n                                 \\'variable\\' % dim)\\n            if dim not in dims:\\n                dims[dim] = size\\n                last_used[dim] = k\\n            elif dims[dim] != size:\\n                raise ValueError(\\'conflicting sizes for dimension %r: \\'\\n                                 \\'length %s on %r and length %s on %r\\' %\\n                                 (dim, size, k, dims[dim], last_used[dim]))\\n    return dims', 'def _merge_dataset(self, other, overwrite_vars, compat, join):\\n    aligned_self, other = partial_align(self, other, join=join, copy=False)\\n\\n    replace_vars, new_vars, new_coord_names = _merge_expand(\\n        aligned_self, other._variables, overwrite_vars, compat)\\n    new_coord_names.update(other._coord_names)\\n\\n    return replace_vars, new_vars, new_coord_names', \"def _assert_empty(args, msg='%s'):\\n    if args:\\n        raise ValueError(msg % args)\", 'def __init__(self, dataset):\\n        self._dataset = dataset', 'def __len__(self):\\n        return len(self._dataset._variables) - len(self._dataset._coord_names)', 'def __getitem__(self, key):\\n        if key not in self._dataset._coord_names:\\n            return self._dataset[key]\\n        else:\\n            raise KeyError(key)', 'def __init__(self, dataset):\\n        self.dataset = dataset', 'def __init__(self, variables=None, coords=None, attrs=None,\\n                 compat=\\'broadcast_equals\\'):\\n        \"\"\"To load data from a file or file-like object, use the `open_dataset`\\n        function.\\n\\n        Parameters\\n        ----------\\n        variables : dict-like, optional\\n            A mapping from variable names to :py:class:`~xray.DataArray`\\n            objects, :py:class:`~xray.Variable` objects or tuples of the\\n            form ``(dims, data[, attrs])`` which can be used as arguments to\\n            create a new ``Variable``. Each dimension must have the same length\\n            in all variables in which it appears.\\n        coords : dict-like, optional\\n            Another mapping in the same form as the `variables` argument,\\n            except the each item is saved on the dataset as a \"coordinate\".\\n            These variables have an associated meaning: they describe\\n            constant/fixed/independent quantities, unlike the\\n            varying/measured/dependent quantities that belong in `variables`.\\n            Coordinates values may be given by 1-dimensional arrays or scalars,\\n            in which case `dims` do not need to be supplied: 1D arrays will be\\n            assumed to give index values along the dimension with the same\\n            name.\\n        attrs : dict-like, optional\\n            Global attributes to save on this dataset.\\n        compat : {\\'broadcast_equals\\', \\'equals\\', \\'identical\\'}, optional\\n            String indicating how to compare variables of the same name for\\n            potential conflicts:\\n\\n            - \\'broadcast_equals\\': all values must be equal when variables are\\n              broadcast against each other to ensure common dimensions.\\n            - \\'equals\\': all values and dimensions must be the same.\\n            - \\'identical\\': all values, dimensions and attributes must be the\\n              same.\\n        \"\"\"\\n        self._variables = OrderedDict()\\n        self._coord_names = set()\\n        self._dims = {}\\n        self._attrs = None\\n        self._file_obj = None\\n        if variables is None:\\n            variables = {}\\n        if coords is None:\\n            coords = set()\\n        if variables or coords:\\n            self._set_init_vars_and_dims(variables, coords, compat)\\n        if attrs is not None:\\n            self.attrs = attrs', 'def _update_vars_and_coords(self, new_variables, new_coord_names={},\\n                                needs_copy=True, check_coord_names=True):\\n        \"\"\"Add a dictionary of new variables to this dataset.\\n\\n        Raises a ValueError if any dimensions have conflicting lengths in the\\n        new dataset. Otherwise will update this dataset\\'s _variables and\\n        _dims attributes in-place.\\n\\n        Set `needs_copy=False` only if this dataset is brand-new and hence\\n        can be thrown away if this method fails.\\n        \"\"\"\\n        # default to creating another copy of variables so can unroll if we end\\n        # up with inconsistent dimensions\\n        variables = self._variables.copy() if needs_copy else self._variables\\n\\n        if check_coord_names:\\n            _assert_empty([k for k in self.data_vars if k in new_coord_names],\\n                          \\'coordinates with these names already exist as \\'\\n                          \\'variables: %s\\')\\n\\n        variables.update(new_variables)\\n        dims = _calculate_dims(variables)\\n        # all checks are complete: it\\'s safe to update\\n        self._variables = variables\\n        self._dims = dims\\n        self._add_missing_coords_inplace()\\n        self._coord_names.update(new_coord_names)', 'def load_store(cls, store, decoder=None):\\n        \"\"\"Create a new dataset from the contents of a backends.*DataStore\\n        object\\n        \"\"\"\\n        variables, attributes = store.load()\\n        if decoder:\\n            variables, attributes = decoder(variables, attributes)\\n        obj = cls(variables, attrs=attributes)\\n        obj._file_obj = store\\n        return obj', 'def __enter__(self):\\n        return self', 'def __getstate__(self):\\n        \"\"\"Always load data in-memory before pickling\"\"\"\\n        self.load()\\n        # self.__dict__ is the default pickle object, we don\\'t need to\\n        # implement our own __setstate__ method to make pickle work\\n        state = self.__dict__.copy()\\n        # throw away any references to datastores in the pickle\\n        state[\\'_file_obj\\'] = None\\n        return state', 'def variables(self):\\n        \"\"\"Frozen dictionary of xray.Variable objects constituting this\\n        dataset\\'s data\\n        \"\"\"\\n        return Frozen(self._variables)', 'def attrs(self):\\n        \"\"\"Dictionary of global attributes on this dataset\\n        \"\"\"\\n        if self._attrs is None:\\n            self._attrs = OrderedDict()\\n        return self._attrs', 'def attrs(self, value):\\n        self._attrs = OrderedDict(value)', 'def dims(self):\\n        \"\"\"Mapping from dimension names to lengths.\\n\\n        This dictionary cannot be modified directly, but is updated when adding\\n        new variables.\\n        \"\"\"\\n        return Frozen(SortedKeysDict(self._dims))', \"def load_data(self):  # pragma: no cover\\n        warnings.warn('the Dataset method `load_data` has been deprecated; '\\n                      'use `load` instead',\\n                      FutureWarning, stacklevel=2)\\n        return self.load()\", 'def _construct_direct(cls, variables, coord_names, dims, attrs,\\n                          file_obj=None):\\n        \"\"\"Shortcut around __init__ for internal use when we want to skip\\n        costly validation\\n        \"\"\"\\n        obj = object.__new__(cls)\\n        obj._variables = variables\\n        obj._coord_names = coord_names\\n        obj._dims = dims\\n        obj._attrs = attrs\\n        obj._file_obj = file_obj\\n        return obj', 'def _replace_vars_and_dims(self, variables, coord_names=None,\\n                               attrs=__default_attrs, inplace=False):\\n        \"\"\"Fastpath constructor for internal use.\\n\\n        Preserves coord names and attributes; dimensions are recalculated from\\n        the supplied variables.\\n\\n        The arguments are *not* copied when placed on the new dataset. It is up\\n        to the caller to ensure that they have the right type and are not used\\n        elsewhere.\\n\\n        Parameters\\n        ----------\\n        variables : OrderedDict\\n        coord_names : set or None, optional\\n        attrs : OrderedDict or None, optional\\n\\n        Returns\\n        -------\\n        new : Dataset\\n        \"\"\"\\n        dims = _calculate_dims(variables)\\n        if inplace:\\n            self._dims = dims\\n            self._variables = variables\\n            if coord_names is not None:\\n                self._coord_names = coord_names\\n            if attrs is not self.__default_attrs:\\n                self._attrs = attrs\\n            obj = self\\n        else:\\n            if coord_names is None:\\n                coord_names = self._coord_names.copy()\\n            if attrs is self.__default_attrs:\\n                attrs = self._attrs_copy()\\n            obj = self._construct_direct(variables, coord_names, dims, attrs)\\n        return obj', 'def _copy_listed(self, names, keep_attrs=True):\\n        \"\"\"Create a new Dataset with the listed variables from this dataset and\\n        the all relevant coordinates. Skips all validation.\\n        \"\"\"\\n        variables = OrderedDict()\\n        coord_names = set()\\n\\n        for name in names:\\n            try:\\n                variables[name] = self._variables[name]\\n            except KeyError:\\n                ref_name, var_name, var = _get_virtual_variable(\\n                    self._variables, name)\\n                variables[var_name] = var\\n                if ref_name in self._coord_names:\\n                    coord_names.add(var_name)\\n\\n        needed_dims = set()\\n        for v in variables.values():\\n            needed_dims.update(v._dims)\\n        for k in self._coord_names:\\n            if set(self._variables[k]._dims) <= needed_dims:\\n                variables[k] = self._variables[k]\\n                coord_names.add(k)\\n\\n        dims = dict((k, self._dims[k]) for k in needed_dims)\\n\\n        attrs = self.attrs.copy() if keep_attrs else None\\n\\n        return self._construct_direct(variables, coord_names, dims, attrs)', 'def __deepcopy__(self, memo=None):\\n        # memo does nothing but is required for compatibility with\\n        # copy.deepcopy\\n        return self.copy(deep=True)', 'def __len__(self):\\n        return len(self._variables)', 'def nbytes(self):\\n        return sum(v.nbytes for v in self.variables.values())', 'def loc(self):\\n        \"\"\"Attribute for location based indexing. Only supports __getitem__,\\n        and only when the key is a dict of the form {dim: labels}.\\n        \"\"\"\\n        return _LocIndexer(self)', 'def __setitem__(self, key, value):\\n        \"\"\"Add an array to this dataset.\\n\\n        If value is a `DataArray`, call its `select_vars()` method, rename it\\n        to `key` and merge the contents of the resulting dataset into this\\n        dataset.\\n\\n        If value is an `Variable` object (or tuple of form\\n        ``(dims, data[, attrs])``), add it to this dataset as a new\\n        variable.\\n        \"\"\"\\n        if utils.is_dict_like(key):\\n            raise NotImplementedError(\\'cannot yet use a dictionary as a key \\'\\n                                      \\'to set Dataset values\\')\\n        self.update({key: value})', 'def remove(k):\\n            del self._variables[k]\\n            self._coord_names.discard(k)', 'def _all_compat(self, other, compat_str):\\n        \"\"\"Helper function for equals and identical\"\"\"\\n        # some stores (e.g., scipy) do not seem to preserve order, so don\\'t\\n        # require matching order for equality\\n        compat = lambda x, y: getattr(x, compat_str)(y)\\n        return (self._coord_names == other._coord_names\\n                and utils.dict_equiv(self._variables, other._variables,\\n                                     compat=compat))', 'def equals(self, other):\\n        \"\"\"Two Datasets are equal if they have matching variables and\\n        coordinates, all of which are equal.\\n\\n        Datasets can still be equal (like pandas objects) if they have NaN\\n        values in the same locations.\\n\\n        This method is necessary because `v1 == v2` for ``Dataset``\\n        does element-wise comparisions (like numpy.ndarrays).\\n\\n        See Also\\n        --------\\n        Dataset.broadcast_equals\\n        Dataset.identical\\n        \"\"\"\\n        try:\\n            return self._all_compat(other, \\'equals\\')\\n        except (TypeError, AttributeError):\\n            return False', 'def indexes(self):\\n        \"\"\"OrderedDict of pandas.Index objects used for label based indexing\\n        \"\"\"\\n        return Indexes(self)', 'def coords(self):\\n        \"\"\"Dictionary of xray.DataArray objects corresponding to coordinate\\n        variables\\n        \"\"\"\\n        return DatasetCoordinates(self)', 'def data_vars(self):\\n        \"\"\"Dictionary of xray.DataArray objects corresponding to data variables\\n        \"\"\"\\n        return Variables(self)', \"def vars(self):  # pragma: no cover\\n        warnings.warn('the Dataset property `vars` has been deprecated; '\\n                      'use `data_vars` instead',\\n                      FutureWarning, stacklevel=2)\\n        return self.data_vars\", 'def reset_coords(self, names=None, drop=False, inplace=False):\\n        \"\"\"Given names of coordinates, reset them to become variables\\n\\n        Parameters\\n        ----------\\n        names : str or list of str, optional\\n            Name(s) of non-index coordinates in this dataset to reset into\\n            variables. By default, all non-index coordinates are reset.\\n        drop : bool, optional\\n            If True, remove coordinates instead of converting them into\\n            variables.\\n        inplace : bool, optional\\n            If True, modify this dataset inplace. Otherwise, create a new\\n            object.\\n\\n        Returns\\n        -------\\n        Dataset\\n        \"\"\"\\n        if names is None:\\n            names = self._coord_names - set(self.dims)\\n        else:\\n            if isinstance(names, basestring):\\n                names = [names]\\n            self._assert_all_in_dataset(names)\\n            _assert_empty(\\n                set(names) & set(self.dims),\\n                \\'cannot remove index coordinates with reset_coords: %s\\')\\n        obj = self if inplace else self.copy()\\n        obj._coord_names.difference_update(names)\\n        if drop:\\n            for name in names:\\n                del obj._variables[name]\\n        return obj', 'def to_netcdf(self, path=None, mode=\\'w\\', format=None, group=None,\\n                  engine=None):\\n        \"\"\"Write dataset contents to a netCDF file.\\n\\n        Parameters\\n        ----------\\n        path : str, optional\\n            Path to which to save this dataset. If no path is provided, this\\n            function returns the resulting netCDF file as a bytes object; in\\n            this case, we need to use scipy.io.netcdf, which does not support\\n            netCDF version 4 (the default format becomes NETCDF3_64BIT).\\n        mode : {\\'w\\', \\'a\\'}, optional\\n            Write (\\'w\\') or append (\\'a\\') mode. If mode=\\'w\\', any existing file at\\n            this location will be overwritten.\\n        format : {\\'NETCDF4\\', \\'NETCDF4_CLASSIC\\', \\'NETCDF3_64BIT\\', \\'NETCDF3_CLASSIC\\'}, optional\\n            File format for the resulting netCDF file:\\n\\n            * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\\n              features.\\n            * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\\n              netCDF 3 compatibile API features.\\n            * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\\n              which fully supports 2+ GB files, but is only compatible with\\n              clients linked against netCDF version 3.6.0 or later.\\n            * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\\n              handle 2+ GB files very well.\\n\\n            All formats are supported by the netCDF4-python library.\\n            scipy.io.netcdf only supports the last two formats.\\n\\n            The default format is NETCDF4 if you are saving a file to disk and\\n            have the netCDF4-python library available. Otherwise, xray falls\\n            back to using scipy to write netCDF files and defaults to the\\n            NETCDF3_64BIT format (scipy does not support netCDF4).\\n        group : str, optional\\n            Path to the netCDF4 group in the given file to open (only works for\\n            format=\\'NETCDF4\\'). The group(s) will be created if necessary.\\n        engine : {\\'netcdf4\\', \\'scipy\\', \\'h5netcdf\\'}, optional\\n            Engine to use when writing netCDF files. If not provided, the\\n            default engine is chosen based on available dependencies, with a\\n            preference for \\'netcdf4\\' if writing to a file on disk.\\n        \"\"\"\\n        from ..backends.api import to_netcdf\\n        return to_netcdf(self, path, mode, format, group, engine)', 'def __repr__(self):\\n        return formatting.dataset_repr(self)', 'def chunks(self):\\n        \"\"\"Block dimensions for this dataset\\'s data or None if it\\'s not a dask\\n        array.\\n        \"\"\"\\n        chunks = {}\\n        for v in self.variables.values():\\n            if v.chunks is not None:\\n                new_chunks = list(zip(v.dims, v.chunks))\\n                if any(chunk != chunks[d] for d, chunk in new_chunks\\n                       if d in chunks):\\n                    raise ValueError(\\'inconsistent chunks\\')\\n                chunks.update(new_chunks)\\n        return Frozen(SortedKeysDict(chunks))', 'def selkeys(dict_, keys):\\n            if dict_ is None:\\n                return None\\n            return dict((d, dict_[d]) for d in keys if d in dict_)', 'def isel(self, **indexers):\\n        \"\"\"Returns a new dataset with each array indexed along the specified\\n        dimension(s).\\n\\n        This method selects values from each array using its `__getitem__`\\n        method, except this method does not require knowing the order of\\n        each array\\'s dimensions.\\n\\n        Parameters\\n        ----------\\n        **indexers : {dim: indexer, ...}\\n            Keyword arguments with names matching dimensions and values given\\n            by integers, slice objects or arrays.\\n\\n        Returns\\n        -------\\n        obj : Dataset\\n            A new Dataset with the same contents as this dataset, except each\\n            array and dimension is indexed by the appropriate indexers. In\\n            general, each array\\'s data will be a view of the array\\'s data\\n            in this dataset, unless numpy fancy indexing was triggered by using\\n            an array indexer, in which case the data will be a copy.\\n\\n        See Also\\n        --------\\n        Dataset.sel\\n        DataArray.isel\\n        DataArray.sel\\n        \"\"\"\\n        invalid = [k for k in indexers if not k in self.dims]\\n        if invalid:\\n            raise ValueError(\"dimensions %r do not exist\" % invalid)\\n\\n        # all indexers should be int, slice or np.ndarrays\\n        indexers = [(k, (np.asarray(v)\\n                         if not isinstance(v, (int, np.integer, slice))\\n                         else v))\\n                    for k, v in iteritems(indexers)]\\n\\n        variables = OrderedDict()\\n        for name, var in iteritems(self._variables):\\n            var_indexers = dict((k, v) for k, v in indexers if k in var.dims)\\n            variables[name] = var.isel(**var_indexers)\\n        return self._replace_vars_and_dims(variables)', 'def isel_points(self, dim=\\'points\\', **indexers):\\n        \"\"\"Returns a new dataset with each array indexed pointwise along the\\n        specified dimension(s).\\n\\n        This method selects pointwise values from each array and is akin to\\n        the NumPy indexing behavior of `arr[[0, 1], [0, 1]]`, except this\\n        method does not require knowing the order of each array\\'s dimensions.\\n\\n        Parameters\\n        ----------\\n        dim : str or DataArray or pandas.Index or other list-like object, optional\\n            Name of the dimension to concatenate along. If dim is provided as a\\n            string, it must be a new dimension name, in which case it is added\\n            along axis=0. If dim is provided as a DataArray or Index or\\n            list-like object, its name, which must not be present in the\\n            dataset, is used as the dimension to concatenate along and the\\n            values are added as a coordinate.\\n        **indexers : {dim: indexer, ...}\\n            Keyword arguments with names matching dimensions and values given\\n            by array-like objects. All indexers must be the same length and\\n            1 dimensional.\\n\\n        Returns\\n        -------\\n        obj : Dataset\\n            A new Dataset with the same contents as this dataset, except each\\n            array and dimension is indexed by the appropriate indexers. With\\n            pointwise indexing, the new Dataset will always be a copy of the\\n            original.\\n\\n        See Also\\n        --------\\n        Dataset.sel\\n        DataArray.isel\\n        DataArray.sel\\n        DataArray.isel_points\\n        \"\"\"\\n        indexer_dims = set(indexers)\\n\\n        def relevant_keys(mapping):\\n            return [k for k, v in mapping.items()\\n                    if any(d in indexer_dims for d in v.dims)]\\n\\n        data_vars = relevant_keys(self.data_vars)\\n        coords = relevant_keys(self.coords)\\n\\n        # all the indexers should be iterables\\n        keys = indexers.keys()\\n        indexers = [(k, np.asarray(v)) for k, v in iteritems(indexers)]\\n        # Check that indexers are valid dims, integers, and 1D\\n        for k, v in indexers:\\n            if k not in self.dims:\\n                raise ValueError(\"dimension %s does not exist\" % k)\\n            if v.dtype.kind != \\'i\\':\\n                raise TypeError(\\'Indexers must be integers\\')\\n            if v.ndim != 1:\\n                raise ValueError(\\'Indexers must be 1 dimensional\\')\\n\\n        # all the indexers should have the same length\\n        lengths = set(len(v) for k, v in indexers)\\n        if len(lengths) > 1:\\n            raise ValueError(\\'All indexers must be the same length\\')\\n\\n        # Existing dimensions are not valid choices for the dim argument\\n        if isinstance(dim, basestring):\\n            if dim in self.dims:\\n                # dim is an invalid string\\n                raise ValueError(\\'Existing dimension names are not valid \\'\\n                                 \\'choices for the dim argument in sel_points\\')\\n        elif hasattr(dim, \\'dims\\'):\\n            # dim is a DataArray or Coordinate\\n            if dim.name in self.dims:\\n                # dim already exists\\n                raise ValueError(\\'Existing dimensions are not valid choices \\'\\n                                 \\'for the dim argument in sel_points\\')\\n        else:\\n            # try to cast dim to DataArray with name = points\\n            from .dataarray import DataArray\\n            dim = DataArray(dim, dims=\\'points\\', name=\\'points\\')\\n\\n        # TODO: This would be sped up with vectorized indexing. This will\\n        # require dask to support pointwise indexing as well.\\n        return concat([self.isel(**d) for d in\\n                       [dict(zip(keys, inds)) for inds in\\n                        zip(*[v for k, v in indexers])]],\\n                      dim=dim, coords=coords, data_vars=data_vars)', 'def reindex(self, indexers=None, method=None, copy=True, **kw_indexers):\\n        \"\"\"Conform this object onto a new set of indexes, filling in\\n        missing values with NaN.\\n\\n        Parameters\\n        ----------\\n        indexers : dict. optional\\n            Dictionary with keys given by dimension names and values given by\\n            arrays of coordinates tick labels. Any mis-matched coordinate values\\n            will be filled in with NaN, and any mis-matched dimension names will\\n            simply be ignored.\\n        method : {None, \\'nearest\\', \\'pad\\'/\\'ffill\\', \\'backfill\\'/\\'bfill\\'}, optional\\n            Method to use for filling index values in ``indexers`` not found in\\n            this dataset:\\n\\n            * default: don\\'t fill gaps\\n            * pad / ffill: propgate last valid index value forward\\n            * backfill / bfill: propagate next valid index value backward\\n            * nearest: use nearest valid index value (requires pandas>=0.16)\\n        copy : bool, optional\\n            If `copy=True`, the returned dataset contains only copied\\n            variables. If `copy=False` and no reindexing is required then\\n            original variables from this dataset are returned.\\n        **kw_indexers : optional\\n            Keyword arguments in the same form as ``indexers``.\\n\\n        Returns\\n        -------\\n        reindexed : Dataset\\n            Another dataset, with this dataset\\'s data but replaced coordinates.\\n\\n        See Also\\n        --------\\n        Dataset.reindex_like\\n        align\\n        pandas.Index.get_indexer\\n        \"\"\"\\n        indexers = utils.combine_pos_and_kw_args(indexers, kw_indexers,\\n                                                 \\'reindex\\')\\n        if not indexers:\\n            # shortcut\\n            return self.copy(deep=True) if copy else self\\n\\n        variables = alignment.reindex_variables(\\n            self.variables, self.indexes, indexers, method, copy=copy)\\n        return self._replace_vars_and_dims(variables)', 'def swap_dims(self, dims_dict, inplace=False):\\n        \"\"\"Returns a new object with swapped dimensions.\\n\\n        Parameters\\n        ----------\\n        dims_dict : dict-like\\n            Dictionary whose keys are current dimension names and whose values\\n            are new names. Each value must already be a variable in the\\n            dataset.\\n        inplace : bool, optional\\n            If True, swap dimensions in-place. Otherwise, return a new dataset\\n            object.\\n\\n        Returns\\n        -------\\n        renamed : Dataset\\n            Dataset with swapped dimensions.\\n\\n        See Also\\n        --------\\n\\n        Dataset.rename\\n        DataArray.swap_dims\\n        \"\"\"\\n        for k, v in dims_dict.items():\\n            if k not in self.dims:\\n                raise ValueError(\\'cannot swap from dimension %r because it is \\'\\n                                 \\'not an existing dimension\\' % k)\\n            if self.variables[v].dims != (k,):\\n                raise ValueError(\\'replacement dimension %r is not a 1D \\'\\n                                 \\'variable along the old dimension %r\\'\\n                                 % (v, k))\\n\\n        result_dims = set(dims_dict.get(dim, dim) for dim in self.dims)\\n\\n        variables = OrderedDict()\\n\\n        coord_names = self._coord_names.copy()\\n        coord_names.update(dims_dict.values())\\n\\n        for k, v in iteritems(self.variables):\\n            dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\\n            var = v.to_coord() if k in result_dims else v.to_variable()\\n            var.dims = dims\\n            variables[k] = var\\n\\n        return self._replace_vars_and_dims(variables, coord_names,\\n                                           inplace=inplace)', 'def merge(self, other, inplace=False, overwrite_vars=set(),\\n              compat=\\'broadcast_equals\\', join=\\'outer\\'):\\n        \"\"\"Merge the arrays of two datasets into a single dataset.\\n\\n        This method generally not allow for overriding data, with the exception\\n        of attributes, which are ignored on the second dataset. Variables with\\n        the same name are checked for conflicts via the equals or identical\\n        methods.\\n\\n        Parameters\\n        ----------\\n        other : Dataset or castable to Dataset\\n            Dataset or variables to merge with this dataset.\\n        inplace : bool, optional\\n            If True, merge the other dataset into this dataset in-place.\\n            Otherwise, return a new dataset object.\\n        overwrite_vars : str or sequence, optional\\n            If provided, update variables of these name(s) without checking for\\n            conflicts in this dataset.\\n        compat : {\\'broadcast_equals\\', \\'equals\\', \\'identical\\'}, optional\\n            String indicating how to compare variables of the same name for\\n            potential conflicts:\\n\\n            - \\'broadcast_equals\\': all values must be equal when variables are\\n              broadcast against each other to ensure common dimensions.\\n            - \\'equals\\': all values and dimensions must be the same.\\n            - \\'identical\\': all values, dimensions and attributes must be the\\n              same.\\n        join : {\\'outer\\', \\'inner\\', \\'left\\', \\'right\\'}, optional\\n            Method for joining ``self`` and ``other`` along shared dimensions:\\n\\n            - \\'outer\\': use the union of the indexes\\n            - \\'inner\\': use the intersection of the indexes\\n            - \\'left\\': use indexes from ``self``\\n            - \\'right\\': use indexes from ``other``\\n\\n        Returns\\n        -------\\n        merged : Dataset\\n            Merged dataset.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If any variables conflict (see ``compat``).\\n        \"\"\"\\n        if compat not in [\\'broadcast_equals\\', \\'equals\\', \\'identical\\']:\\n            raise ValueError(\"compat=%r invalid: must be \\'broadcast_equals\\', \"\\n                             \"\\'equals\\' or \\'identical\\'\" % compat)\\n\\n        if isinstance(overwrite_vars, basestring):\\n            overwrite_vars = [overwrite_vars]\\n        overwrite_vars = set(overwrite_vars)\\n\\n        merge = _merge_dataset if isinstance(other, Dataset) else _merge_dict\\n\\n        replace_vars, new_vars, new_coord_names = merge(\\n            self, other, overwrite_vars, compat=compat, join=join)\\n\\n        newly_coords = new_coord_names & (set(self) - set(self.coords))\\n        no_longer_coords = set(self.coords) & (set(new_vars) - new_coord_names)\\n        ambiguous_coords = (newly_coords | no_longer_coords) - overwrite_vars\\n        if ambiguous_coords:\\n            raise ValueError(\\'cannot merge: the following variables are \\'\\n                             \\'coordinates on one dataset but not the other: %s\\'\\n                             % list(ambiguous_coords))\\n\\n        obj = self if inplace else self.copy()\\n        obj._update_vars_and_coords(replace_vars, new_coord_names)\\n        return obj', 'def drop(self, labels, dim=None):\\n        \"\"\"Drop variables or index labels from this dataset.\\n\\n        If a variable corresponding to a dimension is dropped, all variables\\n        that use that dimension are also dropped.\\n\\n        Parameters\\n        ----------\\n        labels : str\\n            Names of variables or index labels to drop.\\n        dim : None or str, optional\\n            Dimension along which to drop index labels. By default (if\\n            ``dim is None``), drops variables rather than index labels.\\n\\n        Returns\\n        -------\\n        dropped : Dataset\\n        \"\"\"\\n        if utils.is_scalar(labels):\\n            labels = [labels]\\n        if dim is None:\\n            return self._drop_vars(labels)\\n        else:\\n            new_index = self.indexes[dim].drop(labels)\\n            return self.loc[{dim: new_index}]', \"def drop_vars(self, *names):  # pragma: no cover\\n        warnings.warn('the Dataset method `drop_vars` has been deprecated; '\\n                      'use `drop` instead',\\n                      FutureWarning, stacklevel=2)\\n        return self.drop(names)\", 'def T(self):\\n        return self.transpose()', 'def dropna(self, dim, how=\\'any\\', thresh=None, subset=None):\\n        \"\"\"Returns a new dataset with dropped labels for missing values along\\n        the provided dimension.\\n\\n        Parameters\\n        ----------\\n        dim : str\\n            Dimension along which to drop missing values. Dropping along\\n            multiple dimensions simultaneously is not yet supported.\\n        how : {\\'any\\', \\'all\\'}, optional\\n            * any : if any NA values are present, drop that label\\n            * all : if all values are NA, drop that label\\n        thresh : int, default None\\n            If supplied, require this many non-NA values.\\n        subset : sequence, optional\\n            Subset of variables to check for missing values. By default, all\\n            variables in the dataset are checked.\\n\\n        Returns\\n        -------\\n        Dataset\\n        \"\"\"\\n        # TODO: consider supporting multiple dimensions? Or not, given that\\n        # there are some ugly edge cases, e.g., pandas\\'s dropna differs\\n        # depending on the order of the supplied axes.\\n\\n        if dim not in self.dims:\\n            raise ValueError(\\'%s must be a single dataset dimension\\' % dim)\\n\\n        if subset is None:\\n            subset = list(self.data_vars)\\n\\n        count = np.zeros(self.dims[dim], dtype=np.int64)\\n        size = 0\\n\\n        for k in subset:\\n            array = self._variables[k]\\n            if dim in array.dims:\\n                dims = [d for d in array.dims if d != dim]\\n                count += array.count(dims)\\n                size += np.prod([self.dims[d] for d in dims])\\n\\n        if thresh is not None:\\n            mask = count >= thresh\\n        elif how == \\'any\\':\\n            mask = count == size\\n        elif how == \\'all\\':\\n            mask = count > 0\\n        elif how is not None:\\n            raise ValueError(\\'invalid how option: %s\\' % how)\\n        else:\\n            raise TypeError(\\'must specify how or thresh\\')\\n\\n        return self.isel(**{dim: mask})', 'def reduce(self, func, dim=None, keep_attrs=False, numeric_only=False,\\n               allow_lazy=False, **kwargs):\\n        \"\"\"Reduce this dataset by applying `func` along some dimension(s).\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Function which can be called in the form\\n            `f(x, axis=axis, **kwargs)` to return the result of reducing an\\n            np.ndarray over an integer valued axis.\\n        dim : str or sequence of str, optional\\n            Dimension(s) over which to apply `func`.  By default `func` is\\n            applied over all dimensions.\\n        keep_attrs : bool, optional\\n            If True, the datasets\\'s attributes (`attrs`) will be copied from\\n            the original object to the new one.  If False (default), the new\\n            object will be returned without attributes.\\n        numeric_only : bool, optional\\n            If True, only apply ``func`` to variables with a numeric dtype.\\n        **kwargs : dict\\n            Additional keyword arguments passed on to ``func``.\\n\\n        Returns\\n        -------\\n        reduced : Dataset\\n            Dataset with this object\\'s DataArrays replaced with new DataArrays\\n            of summarized data and the indicated dimension(s) removed.\\n        \"\"\"\\n        if isinstance(dim, basestring):\\n            dims = set([dim])\\n        elif dim is None:\\n            dims = set(self.dims)\\n        else:\\n            dims = set(dim)\\n\\n        _assert_empty([dim for dim in dims if dim not in self.dims],\\n                      \\'Dataset does not contain the dimensions: %s\\')\\n\\n        variables = OrderedDict()\\n        for name, var in iteritems(self._variables):\\n            reduce_dims = [dim for dim in var.dims if dim in dims]\\n            if reduce_dims or not var.dims:\\n                if name not in self.coords:\\n                    if (not numeric_only\\n                            or np.issubdtype(var.dtype, np.number)\\n                            or var.dtype == np.bool_):\\n                        if len(reduce_dims) == 1:\\n                            # unpack dimensions for the benefit of functions\\n                            # like np.argmin which can\\'t handle tuple arguments\\n                            reduce_dims, = reduce_dims\\n                        elif len(reduce_dims) == var.ndim:\\n                            # prefer to aggregate over axis=None rather than\\n                            # axis=(0, 1) if they will be equivalent, because\\n                            # the former is often more efficient\\n                            reduce_dims = None\\n                        variables[name] = var.reduce(func, dim=reduce_dims,\\n                                                     keep_attrs=keep_attrs,\\n                                                     allow_lazy=allow_lazy,\\n                                                     **kwargs)\\n            else:\\n                variables[name] = var\\n\\n        coord_names = set(k for k in self.coords if k in variables)\\n        attrs = self.attrs if keep_attrs else None\\n        return self._replace_vars_and_dims(variables, coord_names, attrs)', 'def assign(self, **kwargs):\\n        \"\"\"Assign new data variables to a Dataset, returning a new object\\n        with all the original variables in addition to the new ones.\\n\\n        Parameters\\n        ----------\\n        kwargs : keyword, value pairs\\n            keywords are the variables names. If the values are callable, they\\n            are computed on the Dataset and assigned to new data variables. If\\n            the values are not callable, (e.g. a DataArray, scalar, or array),\\n            they are simply assigned.\\n\\n        Returns\\n        -------\\n        ds : Dataset\\n            A new Dataset with the new variables in addition to all the\\n            existing variables.\\n\\n        Notes\\n        -----\\n        Since ``kwargs`` is a dictionary, the order of your arguments may not\\n        be preserved, and so the order of the new variables is not well\\n        defined. Assigning multiple variables within the same ``assign`` is\\n        possible, but you cannot reference other variables created within the\\n        same ``assign`` call.\\n\\n        See Also\\n        --------\\n        pandas.DataFrame.assign\\n        \"\"\"\\n        data = self.copy()\\n        # do all calculations first...\\n        results = data._calc_assign_results(kwargs)\\n        # ... and then assign\\n        data.update(results)\\n        return data', 'def _to_dataframe(self, ordered_dims):\\n        columns = [k for k in self if k not in self.dims]\\n        data = [self._variables[k].expand_dims(ordered_dims).values.reshape(-1)\\n                for k in columns]\\n        index = self.coords.to_index(ordered_dims)\\n        return pd.DataFrame(OrderedDict(zip(columns, data)), index=index)', 'def from_dataframe(cls, dataframe):\\n        \"\"\"Convert a pandas.DataFrame into an xray.Dataset\\n\\n        Each column will be converted into an independent variable in the\\n        Dataset. If the dataframe\\'s index is a MultiIndex, it will be expanded\\n        into a tensor product of one-dimensional indices (filling in missing\\n        values with NaN). This method will produce a Dataset very similar to\\n        that on which the \\'to_dataframe\\' method was called, except with\\n        possibly redundant dimensions (since all dataset variables will have\\n        the same dimensionality).\\n        \"\"\"\\n        # TODO: Add an option to remove dimensions along which the variables\\n        # are constant, to enable consistent serialization to/from a dataframe,\\n        # even if some variables have different dimensionality.\\n\\n        idx = dataframe.index\\n        obj = cls()\\n\\n        if hasattr(idx, \\'levels\\'):\\n            # it\\'s a multi-index\\n            # expand the DataFrame to include the product of all levels\\n            full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)\\n            dataframe = dataframe.reindex(full_idx)\\n            dims = [name if name is not None else \\'level_%i\\' % n\\n                    for n, name in enumerate(idx.names)]\\n            for dim, lev in zip(dims, idx.levels):\\n                obj[dim] = (dim, lev)\\n            shape = [lev.size for lev in idx.levels]\\n        else:\\n            if idx.size:\\n                dims = (idx.name if idx.name is not None else \\'index\\',)\\n                obj[dims[0]] = (dims, idx)\\n            else:\\n                dims = []\\n            shape = -1\\n\\n        for name, series in iteritems(dataframe):\\n            data = series.values.reshape(shape)\\n            obj[name] = (dims, data)\\n        return obj', 'def _unary_op(f):\\n        @functools.wraps(f)\\n        def func(self, *args, **kwargs):\\n            ds = self.coords.to_dataset()\\n            for k in self.data_vars:\\n                ds._variables[k] = f(self._variables[k], *args, **kwargs)\\n            return ds\\n        return func', \"def _binary_op(f, reflexive=False, join='inner', drop_na_vars=True):\\n        @functools.wraps(f)\\n        def func(self, other):\\n            if isinstance(other, groupby.GroupBy):\\n                return NotImplemented\\n            if hasattr(other, 'indexes'):\\n                self, other = align(self, other, join=join, copy=False)\\n                empty_indexes = [d for d, s in self.dims.items() if s == 0]\\n                if empty_indexes:\\n                    raise ValueError('no overlapping labels for some '\\n                                     'dimensions: %s' % empty_indexes)\\n            g = f if not reflexive else lambda x, y: f(y, x)\\n            ds = self._calculate_binary_op(g, other, drop_na_vars=drop_na_vars)\\n            return ds\\n        return func\", \"def _inplace_binary_op(f):\\n        @functools.wraps(f)\\n        def func(self, other):\\n            if isinstance(other, groupby.GroupBy):\\n                raise TypeError('in-place operations between a Dataset and '\\n                                'a grouped object are not permitted')\\n            if hasattr(other, 'indexes'):\\n                other = other.reindex_like(self, copy=False)\\n            # we don't want to actually modify arrays in-place\\n            g = ops.inplace_to_noninplace_op(f)\\n            ds = self._calculate_binary_op(g, other, inplace=True)\\n            self._replace_vars_and_dims(ds._variables, ds._coord_names,\\n                                        ds._attrs, inplace=True)\\n            return self\\n        return func\", \"def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\\n            dest_vars = OrderedDict()\\n            performed_op = False\\n            for k in lhs_data_vars:\\n                if k in rhs_data_vars:\\n                    dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\\n                    performed_op = True\\n                elif inplace:\\n                    raise ValueError(\\n                        'datasets must have the same data variables '\\n                        'for in-place arithmetic operations: %s, %s'\\n                        % (list(lhs_data_vars), list(rhs_data_vars)))\\n                elif not drop_na_vars:\\n                    # this shortcuts left alignment of variables for fillna\\n                    dest_vars[k] = lhs_vars[k]\\n            if not performed_op:\\n                raise ValueError(\\n                    'datasets have no overlapping data variables: %s, %s'\\n                    % (list(lhs_data_vars), list(rhs_data_vars)))\\n            return dest_vars\"]}, {'features': [], 'snippets': ['def play_one_episode(player, func, verbose=False):\\n    def f(s):\\n        spc = player.get_action_space()\\n        act = func([[s]])[0][0].argmax()\\n        if random.random() < 0.001:\\n            act = spc.sample()\\n        if verbose:\\n            print(act)\\n        return act\\n    return np.mean(player.play_one_episode(f))', 'def eval_with_funcs(predictors, nr_eval, get_player_fn):\\n    class Worker(StoppableThread, ShareSessionThread):\\n        def __init__(self, func, queue):\\n            super(Worker, self).__init__()\\n            self._func = func\\n            self.q = queue\\n\\n        def func(self, *args, **kwargs):\\n            if self.stopped():\\n                raise RuntimeError(\"stopped!\")\\n            return self._func(*args, **kwargs)\\n\\n        def run(self):\\n            with self.default_sess():\\n                player = get_player_fn(train=False)\\n                while not self.stopped():\\n                    try:\\n                        score = play_one_episode(player, self.func)\\n                        # print(\"Score, \", score)\\n                    except RuntimeError:\\n                        return\\n                    self.queue_put_stoppable(self.q, score)\\n\\n    q = queue.Queue()\\n    threads = [Worker(f, q) for f in predictors]\\n\\n    for k in threads:\\n        k.start()\\n        time.sleep(0.1)  # avoid simulator bugs\\n    stat = StatCounter()\\n    try:\\n        for _ in tqdm(range(nr_eval), **get_tqdm_kwargs()):\\n            r = q.get()\\n            stat.feed(r)\\n        logger.info(\"Waiting for all the workers to finish the last run...\")\\n        for k in threads:\\n            k.stop()\\n        for k in threads:\\n            k.join()\\n        while q.qsize():\\n            r = q.get()\\n            stat.feed(r)\\n    except:\\n        logger.exception(\"Eval\")\\n    finally:\\n        if stat.count > 0:\\n            return (stat.average, stat.max)\\n        return (0, 0)', 'def __init__(self, nr_eval, input_names, output_names, get_player_fn):\\n        self.eval_episode = nr_eval\\n        self.input_names = input_names\\n        self.output_names = output_names\\n        self.get_player_fn = get_player_fn', \"def _trigger(self):\\n        t = time.time()\\n        mean, max = eval_with_funcs(\\n            self.pred_funcs, self.eval_episode, self.get_player_fn)\\n        t = time.time() - t\\n        if t > 10 * 60:  # eval takes too long\\n            self.eval_episode = int(self.eval_episode * 0.94)\\n        self.trainer.monitors.put_scalar('mean_score', mean)\\n        self.trainer.monitors.put_scalar('max_score', max)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def choice(question, options, default):\\n    \"Ask the user to choose from a short list of named options\"\\n    while True:\\n        sys.stdout.write(\"{} ({}) [{}]: \".format(question, \"/\".join(options), default))\\n        answer = sys.stdin.readline().strip()\\n        if len(answer) == 0:\\n            return default\\n        for opt in options:\\n            if answer == opt:\\n                return answer', 'def query(question, default=None):\\n    \"Ask the user a question and return the response\"\\n    while True:\\n        if default:\\n            sys.stdout.write(\"{} [{}]: \".format(question, default))\\n        else:\\n            sys.stdout.write(\"%s: \" % question)\\n        answer = sys.stdin.readline().strip().replace(\" \", \"_\")\\n        if answer == \"\":\\n            if default:\\n                return default\\n        else:\\n            return answer', 'def __init__(self, workflowdir, sharedir):\\n        self.jinja = Environment(loader=FileSystemLoader(sharedir), trim_blocks=True)\\n        self.name = os.path.basename(workflowdir)\\n        self.workflowdir = workflowdir\\n        self.sharedir = sharedir\\n        self.properties = {}\\n        self.home = os.environ[\"HOME\"]\\n        self.user = pwd.getpwuid(os.getuid())[0]\\n        self.tutorial = None\\n        self.generate_tutorial = False\\n        self.tutorial_setup = None\\n        self.compute_queue = \"default\"\\n        self.project = \"MYPROJ123\"\\n        sysname, _, _, _, machine = os.uname()\\n        if sysname == \"Darwin\":\\n            self.os = \"MACOSX\"\\n        else:\\n            # Probably Linux\\n            self.os = sysname.upper()\\n        self.arch = machine', 'def copy_dir(self, src, dest):\\n        # self.mkdir(dest)\\n        if not src.startswith(\"/\"):\\n            src = os.path.join(self.sharedir, src)\\n        try:\\n            dest = os.path.join(self.workflowdir, dest)\\n            shutil.copytree(src, dest)\\n        except OSError as exc:  # python >2.5\\n            if exc.errno == errno.ENOTDIR:\\n                shutil.copy(src, dest)\\n            else:\\n                raise', 'def configure(self):\\n        # The tutorial is a special case\\n        if yesno(\"Do you want to generate a tutorial workflow?\", \"n\"):\\n            self.config = \"tutorial\"\\n            self.daxgen = \"tutorial\"\\n            self.generate_tutorial = True\\n\\n            # determine the environment to setup tutorial for\\n            self.tutorial_setup = optionlist(\\n                \"What environment is tutorial to be setup for?\",\\n                [\\n                    TutorialEnv.LOCAL_MACHINE,\\n                    TutorialEnv.USC_HPCC_CLUSTER,\\n                    TutorialEnv.OSG_FROM_ISI,\\n                    TutorialEnv.XSEDE_BOSCO,\\n                    TutorialEnv.BLUEWATERS_GLITE,\\n                    TutorialEnv.TACC_WRANGLER,\\n                    TutorialEnv.OLCF_TITAN,\\n                    TutorialEnv.OLCF_SUMMIT_KUBERNETES_BOSCO,\\n                ],\\n            )\\n\\n            # figure out what example options to provide\\n            examples = [\\n                TutorialExample.PROCESS,\\n                TutorialExample.PIPELINE,\\n                TutorialExample.SPLIT,\\n                TutorialExample.MERGE,\\n                TutorialExample.EPA,\\n                TutorialExample.CONTAINER,\\n            ]\\n            if self.tutorial_setup != \"osg\":\\n                examples.append(TutorialExample.DIAMOND)\\n\\n            if self.tutorial_setup in [\\n                \"bw-glite\",\\n                \"wrangler-glite\",\\n                \"titan-glite\",\\n                \"summit-kub-bosco\",\\n            ]:\\n                examples.append(TutorialExample.MPI)\\n                self.project = query(\\n                    \"What project your jobs should run under. For example on TACC there are like : TG-DDM160003 ?\"\\n                )\\n\\n            self.tutorial = optionlist(\"What tutorial workflow do you want?\", examples)\\n\\n            self.setup_tutorial()\\n            return\\n\\n        # Determine which DAX generator API to use\\n        self.daxgen = choice(\\n            \"What DAX generator API do you want to use?\",\\n            [\"python\", \"perl\", \"java\", \"r\"],\\n            \"python\",\\n        )\\n\\n        # Determine what kind of site catalog we need to generate\\n        self.config = optionlist(\\n            \"What does your computing infrastructure look like?\",\\n            [\\n                (\"Local Machine Condor Pool\", \"condorpool\"),\\n                (\"Remote Cluster using Globus GRAM\", \"globus\"),\\n                (\"Remote Cluster using CREAMCE\", \"creamce\"),\\n                (\"Local PBS Cluster with Glite\", \"glite\"),\\n                (\"Remote PBS Cluster with BOSCO and SSH\", \"bosco\"),\\n            ],\\n        )\\n\\n        # Find out some information about the site\\n        self.sitename = query(\"What do you want to call your compute site?\", \"compute\")\\n        self.os = choice(\\n            \"What OS does your compute site have?\", [\"LINUX\", \"MACOSX\"], self.os\\n        )\\n        self.arch = choice(\\n            \"What architecture does your compute site have?\",\\n            [\"x86_64\", \"x86\"],\\n            self.arch,\\n        )', 'def generate(self):\\n        os.makedirs(self.workflowdir)\\n        if self.tutorial != \"population\":\\n            self.mkdir(\"input\")\\n        self.mkdir(\"output\")\\n\\n        if self.generate_tutorial:\\n            self.copy_template(\"%s/tc.txt\" % self.tutorial, \"tc.txt\")\\n\\n            if self.tutorial == \"r-epa\":\\n                self.copy_template(\"%s/daxgen.R\" % self.tutorial, \"daxgen.R\")\\n            elif self.tutorial != \"mpi-hw\":\\n                self.copy_template(\"%s/daxgen.py\" % self.tutorial, \"daxgen.py\")\\n\\n            if self.tutorial == \"diamond\":\\n\\n                # Executables used by the diamond workflow\\n                self.mkdir(\"bin\")\\n                self.copy_template(\\n                    \"diamond/transformation.py\", \"bin/preprocess\", mode=0o755\\n                )\\n                self.copy_template(\\n                    \"diamond/transformation.py\", \"bin/findrange\", mode=0o755\\n                )\\n                self.copy_template(\\n                    \"diamond/transformation.py\", \"bin/analyze\", mode=0o755\\n                )\\n\\n                # Diamond input file\\n                self.copy_template(\"diamond/f.a\", \"input/f.a\")\\n            elif self.tutorial == \"split\":\\n                # Split workflow input file\\n                self.mkdir(\"bin\")\\n                self.copy_template(\"split/pegasus.html\", \"input/pegasus.html\")\\n            elif self.tutorial == \"r-epa\":\\n                # Executables used by the R-EPA workflow\\n                self.mkdir(\"bin\")\\n                self.copy_template(\\n                    \"r-epa/epa-wrapper.sh\", \"bin/epa-wrapper.sh\", mode=0o755\\n                )\\n                self.copy_template(\"r-epa/setupvar.R\", \"bin/setupvar.R\", mode=0o755)\\n                self.copy_template(\\n                    \"r-epa/weighted.average.R\", \"bin/weighted.average.R\", mode=0o755\\n                )\\n                self.copy_template(\\n                    \"r-epa/cumulative.percentiles.R\",\\n                    \"bin/cumulative.percentiles.R\",\\n                    mode=0o755,\\n                )\\n            elif self.tutorial == \"population\":\\n                self.copy_template(\"%s/Dockerfile\" % self.tutorial, \"Dockerfile\")\\n                self.copy_template(\"%s/Singularity\" % self.tutorial, \"Singularity\")\\n                self.copy_template(\\n                    \"%s/tc.txt.containers\" % self.tutorial, \"tc.txt.containers\"\\n                )\\n                self.copy_dir(\"%s/scripts\" % self.tutorial, \"scripts\")\\n                self.copy_dir(\"%s/data\" % self.tutorial, \"input\")\\n                # copy the mpi wrapper, c code and mpi\\n            elif self.tutorial == \"mpi-hw\":\\n                # copy the mpi wrapper, c code and mpi example\\n                # Executables used by the mpi-hw workflow\\n                self.mkdir(\"bin\")\\n                self.copy_template(\\n                    \"%s/pegasus-mpi-hw.c\" % self.tutorial, \"pegasus-mpi-hw.c\"\\n                )\\n                self.copy_template(\"%s/Makefile\" % self.tutorial, \"Makefile\")\\n                self.copy_template(\"%s/daxgen.py.template\" % self.tutorial, \"daxgen.py\")\\n                self.copy_template(\\n                    \"%s/mpi-hello-world-wrapper\" % self.tutorial,\\n                    \"bin/mpi-hello-world-wrapper\",\\n                    mode=0o755,\\n                )\\n                self.copy_template(\"split/pegasus.html\", \"input/f.in\")\\n\\n        else:\\n            self.copy_template(\"tc.txt\", \"tc.txt\")\\n            if self.daxgen == \"python\":\\n                self.copy_template(\"daxgen/daxgen.py\", \"daxgen.py\")\\n            elif self.daxgen == \"perl\":\\n                self.copy_template(\"daxgen/daxgen.pl\", \"daxgen.pl\")\\n            elif self.daxgen == \"java\":\\n                self.copy_template(\"daxgen/DAXGen.java\", \"DAXGen.java\")\\n            elif self.daxgen == \"r\":\\n                self.copy_template(\"daxgen/daxgen.R\", \"daxgen.R\")\\n            else:\\n                assert False\\n\\n        self.copy_template(\"sites.xml\", \"sites.xml\")\\n        self.copy_template(\"plan_dax.sh\", \"plan_dax.sh\", mode=0o755)\\n        self.copy_template(\"plan_cluster_dax.sh\", \"plan_cluster_dax.sh\", mode=0o755)\\n        self.copy_template(\"generate_dax.sh\", \"generate_dax.sh\", mode=0o755)\\n        self.copy_template(\"README.md\", \"README.md\")\\n        self.copy_template(\"rc.txt\", \"rc.txt\")\\n        self.copy_template(\"pegasus.properties\", \"pegasus.properties\")\\n\\n        if self.tutorial == \"diamond\":\\n            if self.tutorial_setup == \"wrangler-glite\":\\n                self.copy_template(\\n                    \"pmc-wrapper.wrangler\", \"bin/pmc-wrapper\", mode=0o755\\n                )\\n            elif self.tutorial_setup == \"titan-glite\":\\n                self.copy_template(\"pmc-wrapper.titan\", \"bin/pmc-wrapper\", mode=0o755)\\n            elif self.tutorial_setup == \"wrangler-glite\":\\n                self.copy_template(\\n                    \"pmc-wrapper.wrangler\", \"bin/pmc-wrapper\", mode=0o755\\n                )\\n            elif self.tutorial_setup == \"summit-kub-bosco\":\\n                self.copy_template(\"pmc-wrapper.summit\", \"bin/pmc-wrapper\", mode=0o755)\\n\\n        if self.generate_tutorial:\\n            sys.stdout.write(\\n                \"Pegasus Tutorial setup for example workflow - %s for execution on %s in directory %s\\\\n\"\\n                % (self.tutorial, self.tutorial_setup, self.workflowdir)\\n            )']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def get_queryset(self):\\n        queryset = super(PublishListView, self).get_queryset()\\n        self.keyword = self.request.GET.get('keyword', '').strip()\\n        if self.keyword:\\n            queryset = queryset.filter(Q(name__icontains=self.keyword) |\\n                                       Q(address__icontains=self.keyword) |\\n                                       Q(city__icontains=self.keyword))\\n        return queryset\", \"def post(self, request):\\n        form = PublishForm(request.POST)\\n        if form.is_valid():\\n            form.save()\\n            res = {'code': 0, 'result': '添加出版商成功'}\\n        else:\\n            # form.errors会把验证不通过的信息以对象的形式传到前端，前端直接渲染即可\\n            res = {'code': 1, 'errmsg': form.errors}\\n            print form.errors\\n        return JsonResponse(res, safe=True)\", 'def post(self, request, *args, **kwargs):\\n        pk = kwargs.get(\\'pk\\')\\n        p = self.model.objects.get(pk=pk)\\n        form = PublishForm(request.POST, instance=p)\\n        if form.is_valid():\\n            form.save()\\n            res = {\"code\": 0, \"result\": \"更新出版商成功\", \\'next_url\\': self.next_url}\\n        else:\\n            res = {\"code\": 1, \"errmsg\": form.errors, \\'next_url\\': self.next_url}\\n        return render(request, settings.JUMP_PAGE, res)\\n        # return HttpResponseRedirect(reverse(\\'books:publish_detail\\',args=[pk]))']}, {'features': [], 'snippets': ['def read_table(filename):\\n    with open(filename) as fp:\\n        header = next(fp).split()\\n        rows = [line.split()[1:] for line in fp if line.strip()]\\n        columns = zip(*rows)\\n    data = dict(zip(header, columns))\\n    return data']}, {'features': [], 'snippets': ['def coverage(geom, srs):\\n    if isinstance(geom, (list, tuple)):\\n        return BBOXCoverage(geom, srs)\\n    else:\\n        return GeomCoverage(geom, srs)', 'def __init__(self, coverages):\\n        self.coverages = coverages\\n        self.bbox = self.extent.bbox', 'def extent(self):\\n        return reduce(operator.add, [c.extent for c in self.coverages])', 'def contains(self, bbox, srs):\\n        return any(c.contains(bbox, srs) for c in self.coverages)', 'def __eq__(self, other):\\n        if not isinstance(other, MultiCoverage):\\n            return NotImplemented\\n\\n        if self.bbox != other.bbox:\\n            return False\\n\\n        if len(self.coverages) != len(other.coverages):\\n            return False\\n\\n        for a, b in zip(self.coverages, other.coverages):\\n            if a != b:\\n                return False\\n\\n        return True', \"def __repr__(self):\\n        return '<MultiCoverage %r: %r>' % (self.extent.llbbox, self.coverages)\", 'def __init__(self, bbox, srs):\\n        self.bbox = bbox\\n        self.srs = srs\\n        self.geom = None', 'def extent(self):\\n        from mapproxy.layer import MapExtent\\n\\n        return MapExtent(self.bbox, self.srs)', 'def intersects(self, bbox, srs):\\n        bbox = self._bbox_in_coverage_srs(bbox, srs)\\n        return bbox_intersects(self.bbox, bbox)', 'def contains(self, bbox, srs):\\n        bbox = self._bbox_in_coverage_srs(bbox, srs)\\n        return bbox_contains(self.bbox, bbox)', 'def __eq__(self, other):\\n        if not isinstance(other, BBOXCoverage):\\n            return NotImplemented\\n\\n        if self.srs != other.srs:\\n            return False\\n\\n        if self.bbox != other.bbox:\\n            return False\\n\\n        return True', \"def __repr__(self):\\n        return '<BBOXCoverage %r/%r>' % (self.extent.llbbox, self.bbox)\", 'def __init__(self, geom, srs, clip=False):\\n        self.geom = geom\\n        self.bbox = geom.bounds\\n        self.srs = srs\\n        self.clip = clip\\n        self._prep_lock = threading.Lock()\\n        self._prepared_geom = None\\n        self._prepared_counter = 0\\n        self._prepared_max = 10000', 'def extent(self):\\n        from mapproxy.layer import MapExtent\\n        return MapExtent(self.bbox, self.srs)', 'def prepared_geom(self):\\n        # GEOS internal data structure for prepared geometries grows over time,\\n        # recreate to limit memory consumption\\n        if not self._prepared_geom or self._prepared_counter > self._prepared_max:\\n            self._prepared_geom = shapely.prepared.prep(self.geom)\\n            self._prepared_counter = 0\\n        self._prepared_counter += 1\\n        return self._prepared_geom', 'def transform_to(self, srs):\\n        if srs == self.srs:\\n            return self\\n\\n        geom = transform_geometry(self.srs, srs, self.geom)\\n        return GeomCoverage(geom, srs)', 'def intersection(self, bbox, srs):\\n        bbox = self._geom_in_coverage_srs(bbox, srs)\\n        return GeomCoverage(self.geom.intersection(bbox), self.srs)', 'def __eq__(self, other):\\n        if not isinstance(other, GeomCoverage):\\n            return NotImplemented\\n\\n        if self.srs != other.srs:\\n            return False\\n\\n        if self.bbox != other.bbox:\\n            return False\\n\\n        if not self.geom.equals(other.geom):\\n            return False\\n\\n        return True']}, {'features': [], 'snippets': [\"def fake_get_channel_status(self, ch1_status='UP'):\\n        return [{\\n            'datalink': 'mgmt0',\\n            'status': 'UP',\\n            'typeConfig': 'DHCP',\\n            'IP': '172.27.112.125',\\n            'MAC': '00:d0:23:00:15:a6',\\n            'netmask': '255.255.240.0',\\n            'type': 'dhcp',\\n            'gateway': '172.27.127.254'}, {\\n            'datalink': 'CH0',\\n            'status': 'UP',\\n            'typeConfig': 'DHCP',\\n            'IP': self.fake_channel_ip[0],\\n            'MAC': '00:d0:23:80:15:a6',\\n            'netmask': '255.255.240.0',\\n            'type': 'dhcp',\\n            'gateway': '172.27.127.254'}, {\\n            'datalink': 'CH1',\\n            'status': ch1_status,\\n            'typeConfig': 'DHCP',\\n            'IP': self.fake_channel_ip[1],\\n            'MAC': '00:d0:23:40:15:a6',\\n            'netmask': '255.255.240.0',\\n            'type': 'dhcp',\\n            'gateway': '172.27.127.254'}, {\\n            'datalink': 'CH2',\\n            'status': 'DOWN',\\n            'typeConfig': 'DHCP',\\n            'IP': '',\\n            'MAC': '00:d0:23:c0:15:a6',\\n            'netmask': '',\\n            'type': '',\\n            'gateway': ''}, {\\n            'datalink': 'CH3',\\n            'status': 'DOWN',\\n            'typeConfig': 'DHCP',\\n            'IP': '',\\n            'MAC': '00:d0:23:20:15:a6',\\n            'netmask': '',\\n            'type': '',\\n            'gateway': '',\\n        }]\", \"def fake_get_share_status_nfs(self, status=False):\\n        fake_share_status_nfs = [{\\n            'ftp': False,\\n            'cifs': False,\\n            'oss': False,\\n            'sftp': False,\\n            'nfs': status,\\n            'directory': '/LV-1/share-pool-01/' + self.fake_share_name[0],\\n            'exist': True,\\n            'afp': False,\\n            'webdav': False\\n        }]\\n        if status:\\n            fake_share_status_nfs[0]['nfs_detail'] = {\\n                'hostList': [{\\n                    'uid': '65534',\\n                    'insecure': 'insecure',\\n                    'squash': 'all',\\n                    'access': 'ro',\\n                    'host': '*',\\n                    'gid': '65534',\\n                    'mode': 'async',\\n                    'no_subtree_check': 'no_subtree_check',\\n                }]\\n            }\\n        return fake_share_status_nfs\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, *args):\\n    self.custom_objects = args\\n    self.backup = None', 'def __exit__(self, *args, **kwargs):\\n    _GLOBAL_CUSTOM_OBJECTS.clear()\\n    _GLOBAL_CUSTOM_OBJECTS.update(self.backup)', 'def custom_object_scope(*args):\\n  \"\"\"Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\\n\\n  Convenience wrapper for `CustomObjectScope`.\\n  Code within a `with` statement will be able to access custom objects\\n  by name. Changes to global custom objects persist\\n  within the enclosing `with` statement. At end of the `with` statement,\\n  global custom objects are reverted to state\\n  at beginning of the `with` statement.\\n\\n  Example:\\n\\n  Consider a custom object `MyObject`\\n\\n  ```python\\n      with custom_object_scope({\\'MyObject\\':MyObject}):\\n          layer = Dense(..., kernel_regularizer=\\'MyObject\\')\\n          # save, load, etc. will recognize custom object by name\\n  ```\\n\\n  Arguments:\\n      *args: Variable length list of dictionaries of name,\\n          class pairs to add to custom objects.\\n\\n  Returns:\\n      Object of type `CustomObjectScope`.\\n  \"\"\"\\n  return CustomObjectScope(*args)', 'def get_custom_objects():\\n  \"\"\"Retrieves a live reference to the global dictionary of custom objects.\\n\\n  Updating and clearing custom objects using `custom_object_scope`\\n  is preferred, but `get_custom_objects` can\\n  be used to directly access `_GLOBAL_CUSTOM_OBJECTS`.\\n\\n  Example:\\n\\n  ```python\\n      get_custom_objects().clear()\\n      get_custom_objects()[\\'MyObject\\'] = MyObject\\n  ```\\n\\n  Returns:\\n      Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).\\n  \"\"\"\\n  return _GLOBAL_CUSTOM_OBJECTS', \"def serialize_keras_object(instance):\\n  _, instance = tf_decorator.unwrap(instance)\\n  if instance is None:\\n    return None\\n  if hasattr(instance, 'get_config'):\\n    return serialize_keras_class_and_config(instance.__class__.__name__,\\n                                            instance.get_config())\\n  if hasattr(instance, '__name__'):\\n    return instance.__name__\\n  raise ValueError('Cannot serialize', instance)\", \"def deserialize_keras_object(identifier,\\n                             module_objects=None,\\n                             custom_objects=None,\\n                             printable_module_name='object'):\\n  if identifier is None:\\n    return None\\n  if isinstance(identifier, dict):\\n    # In this case we are dealing with a Keras config dictionary.\\n    config = identifier\\n    (cls, cls_config) = class_and_config_for_serialized_keras_object(\\n        config, module_objects, custom_objects, printable_module_name)\\n\\n    if hasattr(cls, 'from_config'):\\n      arg_spec = tf_inspect.getfullargspec(cls.from_config)\\n      custom_objects = custom_objects or {}\\n\\n      if 'custom_objects' in arg_spec.args:\\n        return cls.from_config(\\n            cls_config,\\n            custom_objects=dict(\\n                list(_GLOBAL_CUSTOM_OBJECTS.items()) +\\n                list(custom_objects.items())))\\n      with CustomObjectScope(custom_objects):\\n        return cls.from_config(cls_config)\\n    else:\\n      # Then `cls` may be a function returning a class.\\n      # in this case by convention `config` holds\\n      # the kwargs of the function.\\n      custom_objects = custom_objects or {}\\n      with CustomObjectScope(custom_objects):\\n        return cls(**cls_config)\\n  elif isinstance(identifier, six.string_types):\\n    object_name = identifier\\n    if custom_objects and object_name in custom_objects:\\n      obj = custom_objects.get(object_name)\\n    elif object_name in _GLOBAL_CUSTOM_OBJECTS:\\n      obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\\n    else:\\n      obj = module_objects.get(object_name)\\n      if obj is None:\\n        raise ValueError('Unknown ' + printable_module_name + ':' + object_name)\\n    # Classes passed by name are instantiated with no args, functions are\\n    # returned as-is.\\n    if tf_inspect.isclass(obj):\\n      return obj()\\n    return obj\\n  else:\\n    raise ValueError('Could not interpret serialized ' + printable_module_name +\\n                     ': ' + identifier)\", 'def func_load(code, defaults=None, closure=None, globs=None):\\n  \"\"\"Deserializes a user defined function.\\n\\n  Arguments:\\n      code: bytecode of the function.\\n      defaults: defaults of the function.\\n      closure: closure of the function.\\n      globs: dictionary of global objects.\\n\\n  Returns:\\n      A function object.\\n  \"\"\"\\n  if isinstance(code, (tuple, list)):  # unpack previous dump\\n    code, defaults, closure = code\\n    if isinstance(defaults, list):\\n      defaults = tuple(defaults)\\n\\n  def ensure_value_to_cell(value):\\n    \"\"\"Ensures that a value is converted to a python cell object.\\n\\n    Arguments:\\n        value: Any value that needs to be casted to the cell type\\n\\n    Returns:\\n        A value wrapped as a cell object (see function \"func_load\")\\n    \"\"\"\\n    def dummy_fn():\\n      # pylint: disable=pointless-statement\\n      value  # just access it so it gets captured in .__closure__\\n\\n    cell_value = dummy_fn.__closure__[0]\\n    if not isinstance(value, type(cell_value)):\\n      return cell_value\\n    return value\\n\\n  if closure is not None:\\n    closure = tuple(ensure_value_to_cell(_) for _ in closure)\\n  try:\\n    raw_code = codecs.decode(code.encode(\\'ascii\\'), \\'base64\\')\\n  except (UnicodeEncodeError, binascii.Error):\\n    raw_code = code.encode(\\'raw_unicode_escape\\')\\n  code = marshal.loads(raw_code)\\n  if globs is None:\\n    globs = globals()\\n  return python_types.FunctionType(\\n      code, globs, name=code.co_name, argdefs=defaults, closure=closure)', \"def __init__(self, target, width=30, verbose=1, interval=0.05,\\n               stateful_metrics=None, unit_name='step'):\\n    self.target = target\\n    self.width = width\\n    self.verbose = verbose\\n    self.interval = interval\\n    self.unit_name = unit_name\\n    if stateful_metrics:\\n      self.stateful_metrics = set(stateful_metrics)\\n    else:\\n      self.stateful_metrics = set()\\n\\n    self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\\n                              sys.stdout.isatty()) or\\n                             'ipykernel' in sys.modules or\\n                             'posix' in sys.modules)\\n    self._total_width = 0\\n    self._seen_so_far = 0\\n    # We use a dict + list to avoid garbage collection\\n    # issues found in OrderedDict\\n    self._values = {}\\n    self._values_order = []\\n    self._start = time.time()\\n    self._last_update = 0\", 'def add(self, n, values=None):\\n    self.update(self._seen_so_far + n, values)', 'def slice_arrays(arrays, start=None, stop=None):\\n  \"\"\"Slice an array or list of arrays.\\n\\n  This takes an array-like, or a list of\\n  array-likes, and outputs:\\n      - arrays[start:stop] if `arrays` is an array-like\\n      - [x[start:stop] for x in arrays] if `arrays` is a list\\n\\n  Can also work on list/array of indices: `slice_arrays(x, indices)`\\n\\n  Arguments:\\n      arrays: Single array or list of arrays.\\n      start: can be an integer index (start index)\\n          or a list/array of indices\\n      stop: integer (stop index); should be None if\\n          `start` was a list.\\n\\n  Returns:\\n      A slice of the array(s).\\n\\n  Raises:\\n      ValueError: If the value of start is a list and stop is not None.\\n  \"\"\"\\n  if arrays is None:\\n    return [None]\\n  if isinstance(start, list) and stop is not None:\\n    raise ValueError(\\'The stop argument has to be None if the value of start \\'\\n                     \\'is a list.\\')\\n  elif isinstance(arrays, list):\\n    if hasattr(start, \\'__len__\\'):\\n      # hdf5 datasets only support list objects as indices\\n      if hasattr(start, \\'shape\\'):\\n        start = start.tolist()\\n      return [None if x is None else x[start] for x in arrays]\\n    return [\\n        None if x is None else\\n        None if not hasattr(x, \\'__getitem__\\') else x[start:stop] for x in arrays\\n    ]\\n  else:\\n    if hasattr(start, \\'__len__\\'):\\n      if hasattr(start, \\'shape\\'):\\n        start = start.tolist()\\n      return arrays[start]\\n    if hasattr(start, \\'__getitem__\\'):\\n      return arrays[start:stop]\\n    return [None]', 'def object_list_uid(object_list):\\n  \"\"\"Creates a single string from object ids.\"\"\"\\n  object_list = nest.flatten(object_list)\\n  return \\', \\'.join([str(abs(id(x))) for x in object_list])', \"def is_all_none(structure):\\n  iterable = nest.flatten(structure)\\n  # We cannot use Python's `any` because the iterable may return Tensors.\\n  for element in iterable:\\n    if element is not None:\\n      return False\\n  return True\"]}, {'features': [], 'snippets': ['def __init__(self, config):\\n        self.config = config', \"def decorate_as_label(self, label_type, labels):\\n        allure_label_marker = '{prefix}.{label_type}'.format(prefix=ALLURE_LABEL_PREFIX, label_type=label_type)\\n        allure_label = getattr(pytest.mark, allure_label_marker)\\n        return allure_label(*labels, label_type=label_type)\"]}, {'features': [], 'snippets': ['def __init__(self, *args, **kwargs):\\n        super(FarmworkForm, self).__init__(*args, **kwargs)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def score_dt(model_name, model, X, y, y_actual, output_folder):\\n    \"\"\"\\n    Score a decision tree model.\\n\\n    :param string model_name: title for the model used on the output filename\\n    :param dataframe model: model reference\\n    :param dataframe X: examples\\n    :param dataframe y: targets\\n    :param dataframe y_actual: target results\\n    :param string output_folder: location of the output / results\\n    \"\"\"\\n    print(\"Scoring model...\")\\n    model_score = model.score(X, y)\\n    mse = mean_squared_error(y, y_actual)\\n\\n    mse_score = model_name, \"- Mean Squared Error:\", mse\\n    accuracy = model_name, \"- Accuracy score (%):\", \"{:.2%}\".format(model_score)\\n\\n    # write to file\\n    path = output_folder + \\'/models\\'\\n    create_folder_if_not_exists(path)\\n\\n    filename = path + \\'/score_\\' + model_name + \\'.txt\\'\\n    with open(filename, \\'w\\') as scores:\\n        print(mse_score, file=scores)\\n        print(accuracy, file=scores)\\n    scores.close()\\n    print(\"Scores saved location:\", filename)', 'def save_dt_model(model_name, model, folder):\\n    \"\"\"\\n    Save model using Pickle binary format.\\n\\n    :param dataframe model: model reference\\n    :param string model_name: title for the model used on the output filename\\n    :param string folder: location of model output\\n    \"\"\"\\n    print(\"Saving model...\")\\n    model_file = folder + \\'/models/\\' + model_name + \\'.pkl\\'\\n    path = open(model_file, \\'wb\\')\\n    pickle.dump(model, path)\\n    print(\"Model saved location:\", model_file)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def process(job_config, settings):\\n    for job_id, schedule_time, execution_time, status, runs, uid in get_job_info(job_config.get(\\'id\\'),\\n                                                                                 db_name=settings.metadata):\\n\\n        if status == job_status.get(\\'failed\\'):\\n            if (int(job_config.get(\\'retry\\')) if job_config.get(\\'retry\\') else 0) > int(runs):\\n                settings.logger.debug(\\n                    \\'%s runs %s. set retries %s.\\' % (job_config.get(\\'id\\'), runs, job_config.get(\\'retry\\')))\\n                if dependencies_are_met(job_config, schedule_time, settings):\\n                    set_ready(job_config.get(\\'id\\'), schedule_time, db_name=settings.metadata)\\n                    settings.logger.info(\\'Job \"%s\" \"%s\" set as ready\\' % (job_config.get(\\'id\\'), schedule_time))\\n                    run(job_config, schedule_time, settings)\\n                    continue\\n                else:\\n                    continue\\n            else:\\n                continue\\n        elif status == job_status.get(\\'running\\'):\\n            check_running_job_progress(job_config, schedule_time, uid, settings)\\n            continue\\n        elif status == job_status.get(\\'ready\\'):\\n            run(job_config, schedule_time, settings)\\n        elif status == job_status.get(\\'succeeded\\'):\\n            continue\\n        elif status == job_status.get(\\'not_ready\\'):\\n            if dependencies_are_met(job_config, schedule_time, settings):\\n                set_ready(job_config.get(\\'id\\'), schedule_time, db_name=settings.metadata)\\n                settings.logger.info(\\'Job \"%s\" \"%s\" set as ready\\' % (job_config.get(\\'id\\'), schedule_time))\\n                run(job_config, schedule_time, settings)\\n            else:\\n                continue\\n        else:\\n            settings.logger.error(\\'Unknown job status \"%s\"\\' % status)\\n            sys.exit(1)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        super(SimpleInstanceTest, self).setUp()\\n        db_info = DBInstance(\\n            InstanceTasks.BUILDING, name=\"TestInstance\")\\n        self.instance = SimpleInstance(\\n            None, db_info, InstanceServiceStatus(\\n                ServiceStatuses.BUILDING), ds_version=Mock(), ds=Mock())\\n        db_info.addresses = {\"private\": [{\"addr\": \"123.123.123.123\"}],\\n                             \"internal\": [{\"addr\": \"10.123.123.123\"}],\\n                             \"public\": [{\"addr\": \"15.123.123.123\"}]}\\n        self.orig_conf = CONF.network_label_regex\\n        self.orig_ip_regex = CONF.ip_regex\\n        self.orig_black_list_regex = CONF.black_list_regex', \"def test_get_root_on_create(self):\\n        root_on_create_val = Instance.get_root_on_create(\\n            'redis')\\n        self.assertFalse(root_on_create_val)\", \"def test_filter_ips_black_list(self):\\n        CONF.network_label_regex = '.*'\\n        CONF.ip_regex = '.*'\\n        CONF.black_list_regex = '^10.123.123.*'\\n        ip = self.instance.get_visible_ip_addresses()\\n        ip = filter_ips(\\n            ip, CONF.ip_regex, CONF.black_list_regex)\\n        self.assertEqual(2, len(ip))\\n        self.assertTrue('10.123.123.123' not in ip)\", \"def test_two_network_labels(self):\\n        CONF.network_label_regex = '^(private|public)$'\\n        ip = self.instance.get_visible_ip_addresses()\\n        self.assertEqual(2, len(ip))\\n        self.assertTrue('123.123.123.123' in ip)\\n        self.assertTrue('15.123.123.123' in ip)\", 'def setUp(self):\\n        util.init_db()\\n        self.context = trove_testtools.TroveTestContext(self, is_admin=True)\\n        self.name = \"name\"\\n        self.flavor_id = 5\\n        self.image_id = \"UUID\"\\n        self.databases = []\\n        self.users = []\\n        self.datastore = datastore_models.DBDatastore.create(\\n            id=str(uuid.uuid4()),\\n            name=\\'mysql\\' + str(uuid.uuid4()),\\n        )\\n        self.datastore_version = (\\n            datastore_models.DBDatastoreVersion.create(\\n                id=str(uuid.uuid4()),\\n                datastore_id=self.datastore.id,\\n                name=\"5.5\" + str(uuid.uuid4()),\\n                manager=\"mysql\",\\n                image_id=\"image_id\",\\n                packages=\"\",\\n                active=True))\\n        self.volume_size = 1\\n        self.az = \"az\"\\n        self.nics = None\\n        self.configuration = None\\n        self.tenant_id = \"UUID\"\\n        self.datastore_version_id = str(uuid.uuid4())\\n\\n        self.db_info = DBInstance.create(\\n            name=self.name, flavor_id=self.flavor_id,\\n            tenant_id=self.tenant_id,\\n            volume_size=self.volume_size,\\n            datastore_version_id=self.datastore_version.id,\\n            task_status=InstanceTasks.BUILDING,\\n            configuration_id=self.configuration\\n        )\\n\\n        self.backup_name = \"name\"\\n        self.descr = None\\n        self.backup_state = backup_models.BackupState.COMPLETED\\n        self.instance_id = self.db_info.id\\n        self.parent_id = None\\n        self.deleted = False\\n\\n        self.backup = backup_models.DBBackup.create(\\n            name=self.backup_name,\\n            description=self.descr,\\n            tenant_id=self.tenant_id,\\n            state=self.backup_state,\\n            instance_id=self.instance_id,\\n            parent_id=self.parent_id,\\n            datastore_version_id=self.datastore_version.id,\\n            deleted=False\\n        )\\n        self.backup.size = 1.1\\n        self.backup.save()\\n        self.backup_id = self.backup.id\\n        self.orig_client = models.create_nova_client\\n        models.create_nova_client = nova.fake_create_nova_client\\n        self.orig_api = task_api.API(self.context).create_instance\\n        task_api.API(self.context).create_instance = Mock()\\n        self.run_with_quotas = models.run_with_quotas\\n        models.run_with_quotas = Mock()\\n        self.check = backup_models.DBBackup.check_swift_object_exist\\n        backup_models.DBBackup.check_swift_object_exist = Mock(\\n            return_value=True)\\n        super(CreateInstanceTest, self).setUp()', 'def tearDown(self):\\n        self.db_info.delete()\\n        self.backup.delete()\\n        self.datastore.delete()\\n        self.datastore_version.delete()\\n        models.create_nova_client = self.orig_client\\n        task_api.API(self.context).create_instance = self.orig_api\\n        models.run_with_quotas = self.run_with_quotas\\n        backup_models.DBBackup.check_swift_object_exist = self.check\\n        self.backup.delete()\\n        self.db_info.delete()\\n        super(CreateInstanceTest, self).tearDown()', 'def test_can_restore_from_backup_with_almost_equal_size(self):\\n        # target size equals to \"1Gb\"\\n        self.backup.size = 0.99\\n        self.backup.save()\\n        instance = models.Instance.create(\\n            self.context, self.name, self.flavor_id,\\n            self.image_id, self.databases, self.users,\\n            self.datastore, self.datastore_version,\\n            self.volume_size, self.backup_id,\\n            self.az, self.nics, self.configuration)\\n        self.assertIsNotNone(instance)', 'def setUp(self):\\n        util.init_db()\\n\\n        self.datastore = datastore_models.DBDatastore.create(\\n            id=str(uuid.uuid4()),\\n            name=\\'name\\' + str(uuid.uuid4()),\\n            default_version_id=str(uuid.uuid4()))\\n\\n        self.datastore_version = datastore_models.DBDatastoreVersion.create(\\n            id=self.datastore.default_version_id,\\n            name=\\'name\\' + str(uuid.uuid4()),\\n            image_id=str(uuid.uuid4()),\\n            packages=str(uuid.uuid4()),\\n            datastore_id=self.datastore.id,\\n            manager=\\'mysql\\',\\n            active=1)\\n\\n        self.master = DBInstance(\\n            InstanceTasks.NONE,\\n            id=str(uuid.uuid4()),\\n            name=\"TestMasterInstance\",\\n            datastore_version_id=self.datastore_version.id)\\n        self.master.set_task_status(InstanceTasks.NONE)\\n        self.master.save()\\n        self.master_status = InstanceServiceStatus(\\n            ServiceStatuses.RUNNING,\\n            id=str(uuid.uuid4()),\\n            instance_id=self.master.id)\\n        self.master_status.save()\\n\\n        self.safe_nova_client = models.create_nova_client\\n        models.create_nova_client = nova.fake_create_nova_client\\n        super(TestReplication, self).setUp()', 'def test_replica_of_not_active_master(self, mock_logging):\\n        self.master.set_task_status(InstanceTasks.BUILDING)\\n        self.master.save()\\n        self.master_status.set_status(ServiceStatuses.BUILDING)\\n        self.master_status.save()\\n        self.assertRaises(exception.UnprocessableEntity,\\n                          Instance.create,\\n                          None, \\'name\\', 1, \"UUID\", [], [], None,\\n                          self.datastore_version, 1,\\n                          None, slave_of_id=self.master.id)', 'def test_replica_with_invalid_slave_of_id(self, mock_logging):\\n        self.assertRaises(exception.NotFound,\\n                          Instance.create,\\n                          None, \\'name\\', 1, \"UUID\", [], [], None,\\n                          self.datastore_version, 1,\\n                          None, slave_of_id=str(uuid.uuid4()))']}, {'features': [], 'snippets': [\"def __init__(self, topic):\\n        target = oslo_messaging.Target(topic=topic, version='1.0')\\n        self.client = n_rpc.get_client(target)\", 'def update_status(self, context, status):\\n        \"\"\"Update local status.\\n\\n            This method call updates status attribute of\\n            VPNServices.\\n        \"\"\"\\n        cctxt = self.client.prepare()\\n        return cctxt.call(context, \\'update_status\\', status=status)', 'def __init__(self, vpn_service, host):\\n        self.conf = vpn_service.conf\\n        self.host = host\\n        self.conn = n_rpc.create_connection(new=True)\\n        self.context = context.get_admin_context_without_session()\\n        self.topic = topics.NUAGE_IPSEC_AGENT_TOPIC\\n        self.processes = {}\\n        self.routers = {}\\n        self.process_status_cache = {}\\n        self.endpoints = [self]\\n        self.conn.create_consumer(self.topic, self.endpoints)\\n        self.conn.consume_in_threads()\\n        self.agent_rpc = NuageIPsecVpnDriverApi(\\n            topics.NUAGE_IPSEC_DRIVER_TOPIC)\\n        self.process_status_cache_check = loopingcall.FixedIntervalLoopingCall(\\n            self.report_status, self.context)\\n        self.process_status_cache_check.start(\\n            interval=20)\\n        self.nuage_if_driver = NuageInterfaceDriver(cfg.CONF)', 'def get_namespace(self, router_id):\\n        \"\"\"Get namespace of router.\\n\\n        :router_id: router_id\\n        :returns: namespace string.\\n        \"\"\"\\n        return \\'vpn-\\' + router_id', 'def tracking(self, context, **kwargs):\\n        \"\"\"Handling create router event.\\n\\n        Agent calls this method, when the process namespace is ready.\\n        Note: process_id == router_id == vpnservice_id\\n        \"\"\"\\n        router = kwargs.get(\\'router\\', None)\\n        process_id = router[\\'id\\']\\n        self.routers[process_id] = process_id\\n        if process_id in self.processes:\\n            # In case of vpnservice is created\\n            # before vpn service namespace\\n            process = self.processes[process_id]\\n            process.enable()', 'def ensure_process(self, process_id, vpnservice=None):\\n        \"\"\"Ensuring process.\\n\\n        If the process doesn\\'t exist, it will create process\\n        and store it in self.processs\\n        \"\"\"\\n        process = self.processes.get(process_id)\\n        if not process or not process.namespace:\\n            namespace = self.get_namespace(process_id)\\n            process = self.create_process(\\n                process_id,\\n                vpnservice,\\n                namespace)\\n            self.processes[process_id] = process\\n        elif vpnservice:\\n            process.update_vpnservice(vpnservice)\\n        return process', 'def sync(self, context, routers):\\n        \"\"\"Sync status with server side.\\n\\n        :param context: context object for RPC call\\n        :param routers: Router objects which is created in this sync event\\n\\n        There could be many failure cases should be\\n        considered including the followings.\\n        1) Agent class restarted\\n        2) Failure on process creation\\n        3) VpnService is deleted during agent down\\n        4) RPC failure\\n\\n        In order to handle, these failure cases,\\n        the driver needs to take sync strategies.\\n\\n        \"\"\"\\n        vpnservices = self.agent_rpc.get_vpn_services_on_host(\\n            context, self.host)\\n        router_ids = [vpnservice[\\'router_id\\'] for vpnservice in vpnservices]\\n        sync_router_ids = [router[\\'id\\'] for router in routers]\\n        self._sync_vpn_processes(vpnservices, sync_router_ids)\\n        self._delete_vpn_processes(sync_router_ids, router_ids)\\n        self._cleanup_stale_vpn_processes(router_ids)\\n        self.report_status(context)', 'def report_status(self, context):\\n        status_changed_vpn_services = []\\n        for process in self.processes.values():\\n            previous_status = self.get_process_status_cache(process)\\n            if self.is_status_updated(process, previous_status):\\n                new_status = self.copy_process_status(process)\\n                self.update_downed_connections(process.id, new_status)\\n                status_changed_vpn_services.append(new_status)\\n                self.process_status_cache[process.id] = (\\n                    self.copy_process_status(process))\\n                # We need unset updated_pending status after it\\n                # is reported to the server side\\n                self.unset_updated_pending_status(process)\\n\\n        if status_changed_vpn_services:\\n            self.agent_rpc.update_status(context,\\n                                         status_changed_vpn_services)', 'def _delete_vpn_processes(self, sync_router_ids, vpn_router_ids):\\n        for process_id in sync_router_ids:\\n            if process_id not in vpn_router_ids:\\n                self.destroy_process(process_id)', \"def is_status_updated(self, process, previous_status):\\n        if process.updated_pending_status:\\n            return True\\n        if process.status != previous_status['status']:\\n            return True\\n        if (process.connection_status !=\\n                previous_status['ipsec_site_connections']):\\n            return True\", \"def copy_process_status(self, process):\\n        return {\\n            'id': process.vpnservice['id'],\\n            'status': process.status,\\n            'updated_pending_status': process.updated_pending_status,\\n            'ipsec_site_connections': copy.deepcopy(process.connection_status)\\n        }\", 'def create_router(self, router):\\n        \"\"\"Handling create router event.\"\"\"\\n        pass', 'def destroy_process(self, process_id):\\n        \"\"\"Destroy process.\\n\\n        Disable the process and remove the process\\n        manager for the processes that no longer are running vpn service.\\n        \"\"\"\\n        if process_id in self.processes:\\n            process = self.processes[process_id]\\n            process.disable()\\n            if process_id in self.processes:\\n                del self.processes[process_id]', \"def unplug_from_ovs(self, context, **kwargs):\\n        self.nuage_if_driver.unplug(kwargs['device_name'], 'alubr0',\\n                                    kwargs['ns_name'])\\n        ip = ip_lib.IPWrapper(kwargs['ns_name'])\\n        ip.garbage_collect_namespace()\\n        # On Redhat deployments an additional directory is created named\\n        # 'ip_vti0' in the namespace which prevents the cleanup\\n        # of namespace by the neutron agent in 'ip_lib.py' which we clean.\\n        if kwargs['ns_name'] in ip.get_namespaces():\\n            ip.netns.delete(kwargs['ns_name'])\", 'def create_process(self, process_id, vpnservice, namespace):\\n        return ipsec.OpenSwanProcess(\\n            self.conf,\\n            process_id,\\n            vpnservice,\\n            namespace)', 'def create_process(self, process_id, vpnservice, namespace):\\n        return strongswan_ipsec.StrongSwanProcess(\\n            self.conf,\\n            process_id,\\n            vpnservice,\\n            namespace)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, start_date, end_date, returns,\\n                 benchmark_returns=None):\\n\\n        treasury_curves = trading.environment.treasury_curves\\n        if treasury_curves.index[-1] >= start_date:\\n            mask = ((treasury_curves.index >= start_date) &\\n                    (treasury_curves.index <= end_date))\\n\\n            self.treasury_curves = treasury_curves[mask]\\n        else:\\n            # our test is beyond the treasury curve history\\n            # so we'll use the last available treasury curve\\n            self.treasury_curves = treasury_curves[-1:]\\n\\n        self.start_date = start_date\\n        self.end_date = end_date\\n\\n        if benchmark_returns is None:\\n            br = trading.environment.benchmark_returns\\n            benchmark_returns = br[(br.index >= returns.index[0]) &\\n                                   (br.index <= returns.index[-1])]\\n\\n        self.algorithm_returns = self.mask_returns_to_period(returns)\\n        self.benchmark_returns = self.mask_returns_to_period(benchmark_returns)\\n        self.calculate_metrics()\", 'def to_dict(self):\\n        \"\"\"\\n        Creates a dictionary representing the state of the risk report.\\n        Returns a dict object of the form:\\n        \"\"\"\\n        period_label = self.end_date.strftime(\"%Y-%m\")\\n        rval = {\\n            \\'trading_days\\': self.num_trading_days,\\n            \\'benchmark_volatility\\': self.benchmark_volatility,\\n            \\'algo_volatility\\': self.algorithm_volatility,\\n            \\'treasury_period_return\\': self.treasury_period_return,\\n            \\'algorithm_period_return\\': self.algorithm_period_returns,\\n            \\'benchmark_period_return\\': self.benchmark_period_returns,\\n            \\'sharpe\\': self.sharpe,\\n            \\'sortino\\': self.sortino,\\n            \\'information\\': self.information,\\n            \\'beta\\': self.beta,\\n            \\'alpha\\': self.alpha,\\n            \\'excess_return\\': self.excess_return,\\n            \\'max_drawdown\\': self.max_drawdown,\\n            \\'period_label\\': period_label\\n        }\\n\\n        return {k: None if check_entry(k, v) else v\\n                for k, v in iteritems(rval)}', \"def mask_returns_to_period(self, daily_returns):\\n        if isinstance(daily_returns, list):\\n            returns = pd.Series([x.returns for x in daily_returns],\\n                                index=[x.date for x in daily_returns])\\n        else:  # otherwise we're receiving an index already\\n            returns = daily_returns\\n\\n        trade_days = trading.environment.trading_days\\n        trade_day_mask = returns.index.normalize().isin(trade_days)\\n\\n        mask = ((returns.index >= self.start_date) &\\n                (returns.index <= self.end_date) & trade_day_mask)\\n\\n        returns = returns[mask]\\n        return returns\", 'def calculate_volatility(self, daily_returns):\\n        return np.std(daily_returns, ddof=1) * math.sqrt(self.num_trading_days)', 'def calculate_sortino(self, mar=None):\\n        \"\"\"\\n        http://en.wikipedia.org/wiki/Sortino_ratio\\n        \"\"\"\\n        if mar is None:\\n            mar = self.treasury_period_return\\n\\n        return sortino_ratio(self.algorithm_returns,\\n                             self.algorithm_period_returns,\\n                             mar)', 'def calculate_beta(self):\\n        \"\"\"\\n\\n        .. math::\\n\\n            \\\\\\\\beta_a = \\\\\\\\frac{\\\\mathrm{Cov}(r_a,r_p)}{\\\\mathrm{Var}(r_p)}\\n\\n        http://en.wikipedia.org/wiki/Beta_(finance)\\n        \"\"\"\\n        # it doesn\\'t make much sense to calculate beta for less than two days,\\n        # so return none.\\n        if len(self.algorithm_returns) < 2:\\n            return 0.0, 0.0, 0.0, 0.0, []\\n\\n        returns_matrix = np.vstack([self.algorithm_returns,\\n                                    self.benchmark_returns])\\n        C = np.cov(returns_matrix, ddof=1)\\n        eigen_values = la.eigvals(C)\\n        condition_number = max(eigen_values) / min(eigen_values)\\n        algorithm_covariance = C[0][1]\\n        benchmark_variance = C[1][1]\\n        beta = algorithm_covariance / benchmark_variance\\n\\n        return (\\n            beta,\\n            algorithm_covariance,\\n            benchmark_variance,\\n            condition_number,\\n            eigen_values\\n        )']}, {'features': [], 'snippets': ['def __init__(self, f):\\n        \"\"\"\\n        Constructor que requiere en el parámetro una cadena con el nombre del\\n        cfdi.\\n        \"\"\"\\n        fxml = open(f,\\'r\\').read()\\n        soup                 = Soup(fxml,\\'lxml\\')\\n        #============componentes del cfdi============\\n        emisor        = soup.find(\\'cfdi:emisor\\')\\n        receptor      = soup.find(\\'cfdi:receptor\\')\\n        comprobante   = soup.find(\\'cfdi:comprobante\\')\\n        tfd           = soup.find(\\'tfd:timbrefiscaldigital\\')\\n        self.__version        = comprobante[\\'version\\']\\n        self.__folio          = comprobante[\\'folio\\']\\n        self.__uuid           = tfd[\\'uuid\\']\\n        self.__fechatimbrado  = tfd[\\'fechatimbrado\\']\\n        self.__traslados      = soup.find_all(lambda e: e.name==\\'cfdi:traslado\\' and\\n                                                        sorted(e.attrs.keys())==[\\'importe\\',\\'impuesto\\',\\'tasaocuota\\',\\'tipofactor\\'])\\n        self.__retenciones    = soup.find_all(lambda e: e.name==\\'cfdi:retencion\\' and \\n                                                        sorted(e.attrs.keys())==[\\'importe\\',\\'impuesto\\'])\\n        #============emisor==========================\\n        self.__emisorrfc      = emisor[\\'rfc\\']\\n        try:\\n            self.__emisornombre   = emisor[\\'nombre\\']\\n        except:\\n            self.__emisornombre   = emisor[\\'rfc\\']\\n        #============receptor========================\\n        self.__receptorrfc    = receptor[\\'rfc\\']\\n        try:\\n            self.__receptornombre = receptor[\\'nombre\\']\\n        except:\\n            self.__receptornombre = receptor[\\'rfc\\']\\n        #============comprobante=====================\\n        self.__certificado    = comprobante[\\'certificado\\']\\n        self.__sello          = comprobante[\\'sello\\']\\n        self.__total          = round(float(comprobante[\\'total\\']),2)\\n        self.__subtotal       = round(float(comprobante[\\'subtotal\\']),2)\\n        self.__fecha_cfdi     = comprobante[\\'fecha\\']\\n        self.__conceptos      = soup.find_all(lambda e: e.name==\\'cfdi:concepto\\')\\n        self.__n_conceptos    = len(self.__conceptos)\\n\\n        try:\\n            self.__moneda     = comprobante[\\'moneda\\']\\n        except KeyError as k:\\n            self.__moneda     = \\'MXN\\'\\n\\n        try:\\n            self.__lugar      = comprobante[\\'lugarexpedicion\\']\\n        except KeyError as k:\\n            self.__lugar      = u\\'México\\'\\n        tipo = comprobante[\\'tipodecomprobante\\']\\n\\n        if(float(self.__version)==3.2):\\n            self.__tipo       = tipo\\n        else:\\n            tcomprobantes = {\\'I\\':\\'Ingreso\\', \\'E\\':\\'Egreso\\', \\'N\\':\\'Nomina\\', \\'P\\':\\'Pagado\\'}\\n            self.__tipo       = tcomprobantes[tipo]\\n\\n        try:\\n            self.__tcambio    = float(comprobante[\\'tipocambio\\'])\\n        except:\\n            self.__tcambio    = 1.\\n\\n        triva, trieps, trisr  = self.__calcula_traslados()\\n        self.__triva          = round(triva,2)\\n        self.__trieps         = round(trieps,2)\\n        self.__trisr          = round(trisr,2)\\n        retiva, retisr        = self.__calcula_retenciones()\\n        self.__retiva         = round(retiva,2)\\n        self.__retisr         = round(retisr,2)', \"def __calcula_traslados(self):\\n        triva, trieps, trisr = 0., 0., 0\\n        for t in self.__traslados:\\n            impuesto = t['impuesto']\\n            importe  = float(t['importe'])\\n            if(self.__version=='3.2'):\\n                if impuesto=='IVA':\\n                    triva += importe\\n                elif impuesto=='ISR':\\n                    trisr += importe\\n                elif impuesto=='IEPS':\\n                    trieps += importe\\n            elif(self.__version=='3.3'):\\n                if impuesto=='002':\\n                    triva += importe\\n                elif impuesto=='001':\\n                    trisr += importe\\n                elif impuesto=='003':\\n                    trieps += importe\\n        return triva, trieps, trisr\", 'def lista_valores(self):\\n        v  = [self.__emisornombre,self.__fechatimbrado, self.__tipo, self.__emisorrfc ]\\n        v += [self.__uuid, self.__folio, self.__receptornombre, self.__receptorrfc ]\\n        v += [self.__subtotal, self.__trieps, self.__triva]\\n        v += [self.__retiva, self.__retisr, self.__tcambio, self.__total]\\n        return v', 'def dic_cfdi(self):\\n        d = {}\\n        d[\"Emisor\"]       = self.__emisornombre\\n        d[\"Fecha_CFDI\"]   = self.__fechatimbrado\\n        d[\"Tipo\"]         = self.__tipo\\n        d[\"RFC_Emisor\"]   = self.__emisorrfc\\n        d[\"Folio_fiscal\"] = self.__uuid\\n        d[\"Folio\"]        = self.__folio\\n        d[\"Receptor\"]     = self.__receptornombre\\n        d[\"RFC_Receptor\"] = self.__receptorrfc\\n        d[\"Subtotal\"]     = self.__subtotal\\n        d[\"IEPS\"]         = self.__trieps\\n        d[\"IVA\"]          = self.__triva\\n        d[\"Ret IVA\"]      = self.__retiva\\n        d[\"Ret ISR\"]      = self.__retisr\\n        d[\"TC\"]           = self.__tcambio\\n        d[\"Total\"]        = self.__total\\n        return d', 'def certificado(self):\\n        return self.__certificado', 'def sello(self):\\n        return self.__sello', 'def total(self):\\n        return self.__total', 'def subtotal(self):\\n        return self.__subtotal', 'def fechatimbrado(self):\\n        return self.__fechatimbrado', 'def tipodecambio(self):\\n        return self.__tcambio', 'def lugar(self):\\n        return self.__lugar', 'def moneda(self):\\n        return self.__moneda', 'def traslado_iva(self):\\n        return self.__triva', 'def traslado_isr(self):\\n        return self.__trisr', 'def traslado_ieps(self):\\n        return self.__trieps', 'def n_conceptos(self):\\n        return self.__n_conceptos', 'def conceptos(self):\\n        return self.__conceptos', 'def folio(self):\\n        return self.__folio', 'def columnas():\\n        return [\"Emisor\",\"Fecha_CFDI\",\"Tipo\",\"RFC_Emisor\",\"Folio_fiscal\",\"Folio\",\"Receptor\",\\n                \"RFC_Receptor\", \"Subtotal\",\"IEPS\",\"IVA\",\"Ret IVA\",\"Ret ISR\",\"TC\",\"Total\"]', 'def imprime_reporte(nf, nr):\\n        reporte  = \"Número de archivos procesados:\\\\t {}\\\\n\".format(nf)\\n        reporte += \"Número de filas en tsv:\\\\t {}\\\\n\".format(nr)\\n        if(nf!=nr):\\n            reporte += \"\\\\n\\\\n**** Atención ****\\\\n\"\\n\\n        return reporte']}, {'features': [], 'snippets': [\"def __init__(self,\\n                 tuning=('E', 'A', 'D', 'G'), \\n                 frets_count=24):\\n        self.tuning = tuning\\n        self.frets_count = frets_count\", 'def debug_strings(self):\\n        print(self.strings)', 'def show_me_plz(self,\\n                    seek_note=None,\\n                    seek_string=None):\\n        if (seek_string):\\n            seek_note = self.strings[seek_string[0]][int(seek_string[1]) - 1]']}, {'features': [], 'snippets': ['def test_compute_service_delete_ensure_related_cleanup(self):\\n        \"\"\"Tests deleting a compute service and the related cleanup associated\\n        with that like the compute_nodes table entry, removing the host\\n        from any aggregates, the host mapping in the API DB and the associated\\n        resource provider in Placement.\\n        \"\"\"\\n        compute = self._start_compute(\\'host1\\')\\n        # Make sure our compute host is represented as expected.\\n        services = self.admin_api.get_services(binary=\\'nova-compute\\')\\n        self.assertEqual(1, len(services))\\n        service = services[0]\\n\\n        # Now create a host aggregate and add our host to it.\\n        aggregate = self.admin_api.post_aggregate(\\n            {\\'aggregate\\': {\\'name\\': \\'agg1\\'}})\\n        self.admin_api.add_host_to_aggregate(aggregate[\\'id\\'], service[\\'host\\'])\\n        # Make sure the host is in the aggregate.\\n        aggregate = self.admin_api.api_get(\\n            \\'/os-aggregates/%s\\' % aggregate[\\'id\\']).body[\\'aggregate\\']\\n        self.assertEqual([service[\\'host\\']], aggregate[\\'hosts\\'])\\n\\n        rp_uuid = self._get_provider_uuid_by_host(service[\\'host\\'])\\n\\n        # We\\'ll know there is a host mapping implicitly if os-hypervisors\\n        # returned something in _get_provider_uuid_by_host, but let\\'s also\\n        # make sure the host mapping is there like we expect.\\n        ctxt = nova_context.get_admin_context()\\n        objects.HostMapping.get_by_host(ctxt, service[\\'host\\'])\\n\\n        # Make sure there is a resource provider for that compute node based\\n        # on the uuid.\\n        resp = self.placement_api.get(\\'/resource_providers/%s\\' % rp_uuid)\\n        self.assertEqual(200, resp.status)\\n\\n        # Make sure the resource provider has inventory.\\n        inventories = self._get_provider_inventory(rp_uuid)\\n        # Expect a minimal set of inventory for the fake virt driver.\\n        for resource_class in [orc.VCPU, orc.MEMORY_MB, orc.DISK_GB]:\\n            self.assertIn(resource_class, inventories)\\n\\n        # Now create a server so that the resource provider has some allocation\\n        # records.\\n        flavor = self.api.get_flavors()[0]\\n        server = self._boot_and_check_allocations(flavor, service[\\'host\\'])\\n\\n        # Now the fun part, delete the compute service and make sure related\\n        # resources are cleaned up, like the compute node, host mapping, and\\n        # resource provider. We have to first stop the compute service so\\n        # it doesn\\'t recreate the compute node during the\\n        # update_available_resource periodic task.\\n        self.admin_api.put_service(service[\\'id\\'], {\\'forced_down\\': True})\\n        compute.stop()\\n        # The first attempt should fail since there is an instance on the\\n        # compute host.\\n        ex = self.assertRaises(api_client.OpenStackApiException,\\n                               self.admin_api.api_delete,\\n                               \\'/os-services/%s\\' % service[\\'id\\'])\\n        self.assertIn(\\'Unable to delete compute service that is hosting \\'\\n                      \\'instances.\\', six.text_type(ex))\\n        self.assertEqual(409, ex.response.status_code)\\n\\n        # Now delete the instance and wait for it to be gone.\\n        self._delete_and_check_allocations(server)\\n\\n        # Now we can delete the service.\\n        self.admin_api.api_delete(\\'/os-services/%s\\' % service[\\'id\\'])\\n\\n        # Make sure the service is deleted.\\n        services = self.admin_api.get_services(binary=\\'nova-compute\\')\\n        self.assertEqual(0, len(services))\\n\\n        # Make sure the host was removed from the aggregate.\\n        aggregate = self.admin_api.api_get(\\n            \\'/os-aggregates/%s\\' % aggregate[\\'id\\']).body[\\'aggregate\\']\\n        self.assertEqual([], aggregate[\\'hosts\\'])\\n\\n        # Trying to get the hypervisor should result in a 404.\\n        self.admin_api.api_get(\\n            \\'os-hypervisors?hypervisor_hostname_pattern=%s\\' % service[\\'host\\'],\\n            check_response_status=[404])\\n\\n        # The host mapping should also be gone.\\n        self.assertRaises(exception.HostMappingNotFound,\\n                          objects.HostMapping.get_by_host,\\n                          ctxt, service[\\'host\\'])\\n\\n        # And finally, the resource provider should also be gone. The API\\n        # will perform a cascading delete of the resource provider inventory\\n        # and allocation information.\\n        resp = self.placement_api.get(\\'/resource_providers/%s\\' % rp_uuid)\\n        self.assertEqual(404, resp.status)', 'def test_migrate_confirm_after_deleted_source_compute(self):\\n        \"\"\"Tests a scenario where a server is cold migrated and while in\\n        VERIFY_RESIZE status the admin attempts to delete the source compute\\n        and then the user tries to confirm the resize.\\n        \"\"\"\\n        # Start a compute service and create a server there.\\n        self._start_compute(\\'host1\\')\\n        host1_rp_uuid = self._get_provider_uuid_by_host(\\'host1\\')\\n        flavor = self.api.get_flavors()[0]\\n        server = self._boot_and_check_allocations(flavor, \\'host1\\')\\n        # Start a second compute service so we can cold migrate there.\\n        self._start_compute(\\'host2\\')\\n        host2_rp_uuid = self._get_provider_uuid_by_host(\\'host2\\')\\n        # Cold migrate the server to host2.\\n        self._migrate_and_check_allocations(\\n            server, flavor, host1_rp_uuid, host2_rp_uuid)\\n        # Delete the source compute service.\\n        service = self.admin_api.get_services(\\n            binary=\\'nova-compute\\', host=\\'host1\\')[0]\\n        # We expect the delete request to fail with a 409 error because of the\\n        # instance in VERIFY_RESIZE status even though that instance is marked\\n        # as being on host2 now.\\n        ex = self.assertRaises(api_client.OpenStackApiException,\\n                               self.admin_api.api_delete,\\n                               \\'/os-services/%s\\' % service[\\'id\\'])\\n        self.assertEqual(409, ex.response.status_code)\\n        self.assertIn(\\'Unable to delete compute service that has in-progress \\'\\n                      \\'migrations\\', six.text_type(ex))\\n        self.assertIn(\\'There are 1 in-progress migrations involving the host\\',\\n                      self.stdlog.logger.output)\\n        # The provider is still around because we did not delete the service.\\n        resp = self.placement_api.get(\\'/resource_providers/%s\\' % host1_rp_uuid)\\n        self.assertEqual(200, resp.status)\\n        self.assertFlavorMatchesUsage(host1_rp_uuid, flavor)\\n        # Now try to confirm the migration.\\n        self._confirm_resize(server)\\n        # Delete the host1 service since the migration is confirmed and the\\n        # server is on host2.\\n        self.admin_api.api_delete(\\'/os-services/%s\\' % service[\\'id\\'])\\n        # The host1 resource provider should be gone.\\n        resp = self.placement_api.get(\\'/resource_providers/%s\\' % host1_rp_uuid)\\n        self.assertEqual(404, resp.status)', 'def _update_service(self, service, disabled, forced_down=None):\\n        \"\"\"Update the service using the 2.53 request schema.\\n\\n        :param service: dict representing the service resource in the API\\n        :param disabled: True if the service should be disabled, False if the\\n            service should be enabled\\n        :param forced_down: Optionally change the forced_down value.\\n        \"\"\"\\n        status = \\'disabled\\' if disabled else \\'enabled\\'\\n        req = {\\'status\\': status}\\n        if forced_down is not None:\\n            req[\\'forced_down\\'] = forced_down\\n        self.admin_api.put_service(service[\\'id\\'], req)', 'def _update_service(self, service, disabled, forced_down=None):\\n        \"\"\"Update the service using the 2.11 request schema.\\n\\n        :param service: dict representing the service resource in the API\\n        :param disabled: True if the service should be disabled, False if the\\n            service should be enabled\\n        :param forced_down: Optionally change the forced_down value.\\n        \"\"\"\\n        # Before 2.53 the service is uniquely identified by host and binary.\\n        body = {\\n            \\'host\\': service[\\'host\\'],\\n            \\'binary\\': service[\\'binary\\']\\n        }\\n        # Handle forced_down first if provided since the enable/disable\\n        # behavior in the API depends on it.\\n        if forced_down is not None:\\n            body[\\'forced_down\\'] = forced_down\\n            self.admin_api.api_put(\\'/os-services/force-down\\', body)\\n\\n        if disabled:\\n            self.admin_api.api_put(\\'/os-services/disable\\', body)\\n        else:\\n            self.admin_api.api_put(\\'/os-services/enable\\', body)']}, {'features': [], 'snippets': ['def __init__(self, filename):\\n        self.filename = filename\\n        tfile, members = self.get_archive_object_tar()\\n        self.read_files(tfile, members)', \"def read_files(self, tfile, members):\\n        '''\\n        array with txt data from tarfile object\\n        '''\\n        self.data = [tfile.extractfile(member).read() for member in members if\\n                     tfile.extractfile(member) is not None]\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def foo(x, y):\\n        ...', 'def MyFunc(x, y):\\n    return x + y, x - y', 'def __init__(self, *input_types, **kwargs):\\n    \"\"\"Create a `Defun` decorator.\\n\\n    Args:\\n      *input_types: A list of `tf.DType`\\n      **kwargs: Optional keyword arguments, including\\n         func_name - (optional).  A python string, the name to use to\\n           declare this `Function` in the graph.\\n\\n         grad_func - (optional).  A function implementing the gradient\\n           of the function-to-register.  This is must be a\\n           `_DefinedFunction` object. The gradient\\n           function must satisfy the criterion defined in\\n           function.proto:GradientDef.\\n\\n         python_grad_func - (optional).  A function implementing the\\n           gradient of the function python-side. This function must\\n           take the current op and the gradients w.r.t. its outputs,\\n           and return the gradients w.r.t. the inputs. That is it must\\n           implement the interface expected by `tf.RegisterGradient`).\\n           This will be called by tf.gradients to add the gradient ops\\n           to the graph. At most one of grad_func and python_grad_func\\n           can be specified.\\n\\n         out_names = (optional). A list of strings, one per output\\n           tensor.\\n\\n         shape_func - (optional). A function taking the op and returning a list\\n           of static shapes to set for the function\\'s outputs.\\n    \"\"\"\\n    self._input_types = input_types\\n    self._func_name = kwargs.pop(\"func_name\", None)\\n    self._grad_func = kwargs.pop(\"grad_func\", None)\\n    self._python_grad_func = kwargs.pop(\"python_grad_func\", None)\\n    self._out_names = kwargs.pop(\"out_names\", None)\\n    self._extra_kwargs = kwargs', 'def __init__(self,\\n               func,\\n               argnames,\\n               input_types,\\n               func_name=None,\\n               grad_func=None,\\n               python_grad_func=None,\\n               out_names=None,\\n               shape_func=None,\\n               capture_by_value=False,\\n               **kwargs):\\n    \"\"\"Creates _DefinedFunction.\\n\\n    Args:\\n      func:  A python callable which constructs a tf function body.\\n      argnames: A list of strings for function argument names.\\n      input_types: The function\\'s argument types. Can be a tuple, list of\\n        tf data types.\\n      func_name: The function name. Defaults to None, in which derives from\\n        \\'func\\'.\\n      grad_func: This function\\'s gradient function, if not None. Defaults\\n        to None.\\n      python_grad_func: A python callable implementing the gradient of\\n        the function python-side.\\n      out_names: An optional list of strings for the function return value\\n        names.\\n      shape_func: An optional function mapping an op to a list of static\\n        output shapes.\\n      capture_by_value: Boolean (defaults to False). If True, captured values\\n        will be copied into the function body.\\n      **kwargs: The keyword arguments. **kwargs is passed to every call\\n        site of this function.\\n\\n    Raises:\\n      ValueError: The function definition is invalid.\\n\\n    \"\"\"\\n    self._func = func\\n    self._input_types = input_types\\n    self._func_name = func_name\\n    self._grad_func = grad_func\\n    self._python_grad_func = python_grad_func\\n    self._out_names = out_names\\n    self._shape_func = shape_func\\n    self._capture_by_value = capture_by_value\\n    self._extra_kwargs = kwargs\\n    # Constructed only when C API is disabled, lazily\\n    self._definition = None\\n    # Constructed only when C API is enabled, lazily\\n    self._c_func = None\\n    self._sub_functions = dict()  # Constructed with _definition or _c_func\\n    # pylint: disable=protected-access\\n    device_funcs = ops.get_default_graph()._device_functions_outer_to_inner\\n    # pylint: enable=protected-access\\n\\n    # Get the innermost device if possbile.\\n    self._caller_device = device_funcs[-1] if device_funcs else None\\n\\n    # Cached OpDef for this function. When C API is enabled, this is\\n    # the only part of FunctionDef that we cache in Python. When C API\\n    # is disabled the whole _definition is available and this is simply\\n    # another reference to _definition.signature\\n    self._op_def = None\\n\\n    assert isinstance(input_types, (list, tuple))\\n    self._arg_types = input_types\\n    self._arg_names = [argnames[i] if i < len(argnames) else (\"arg%d\" % i)\\n                       for i in range(len(input_types))]', 'def name(self):\\n    \"\"\"Function name.\"\"\"\\n    self._create_definition_if_needed()\\n    return self._func_name', 'def definition(self):\\n    \"\"\"Function definition proto.\"\"\"\\n    self._create_definition_if_needed()\\n    if self._c_func:\\n      with c_api_util.tf_buffer() as buf:\\n        c_api.TF_FunctionToFunctionDef(self._c_func.func, buf)\\n        fdef = function_pb2.FunctionDef()\\n        proto_data = c_api.TF_GetBuffer(buf)\\n        fdef.ParseFromString(compat.as_bytes(proto_data))\\n      return fdef\\n    return self._definition', 'def _signature(self):\\n    self._create_definition_if_needed()\\n    return self._op_def', 'def grad_func_name(self):\\n    \"\"\"Its gradient function\\'s name.\"\"\"\\n    return self._grad_func.name if self._grad_func else None', 'def python_grad_func(self):\\n    \"\"\"Python gradient function callable.\"\"\"\\n    return self._python_grad_func', 'def declared_input_types(self):\\n    \"\"\"Returns the list of data types of explicit declared inputs.\"\"\"\\n    return self._input_types', 'def captured_inputs(self):\\n    \"\"\"Returns the list of implicitly captured inputs.\"\"\"\\n    self._create_definition_if_needed()\\n    return self._extra_inputs', 'def stateful_ops(self):\\n    \"\"\"Returns the list of stateful ops in function definition.\\n\\n    Returns:\\n      A list of (op.name, op.type) pairs.\\n    \"\"\"\\n    self._create_definition_if_needed()\\n    return self._stateful_ops', 'def _create_definition_if_needed_impl(self):\\n    \"\"\"This is not what you want, see _create_definition_if_needed.\"\"\"\\n    if self._definition is not None or self._c_func is not None:\\n      return\\n\\n    temp_graph = func_graph_from_py_func(\\n        self._func, self._arg_names, self._arg_types, self._func_name,\\n        self._capture_by_value, self._caller_device)\\n\\n    self._extra_inputs = temp_graph.extra_inputs\\n    # pylint: disable=protected-access\\n    self._sub_functions = temp_graph._functions\\n    # pylint: enable=protected-access\\n\\n    # Extra kwargs are treated as attrs on the function def.\\n    if self._func_name:\\n      base_func_name = self._func_name\\n    else:\\n      base_func_name = function_utils.get_func_name(self._func)\\n      if self._grad_func:\\n        base_func_name += (\"_%s\" % self._grad_func.name)\\n    kwargs_attr = _parse_kwargs_as_attrs(base_func_name, **self._extra_kwargs)\\n\\n    if not temp_graph._c_graph:  # pylint: disable=protected-access\\n      # Build the FunctionDef\\n      self._definition = graph_to_function_def.graph_to_function_def(\\n          temp_graph,\\n          temp_graph.get_operations(),\\n          temp_graph.inputs,\\n          temp_graph.outputs,\\n          out_names=self._out_names)\\n\\n      for k in kwargs_attr:\\n        self._definition.attr[k].CopyFrom(kwargs_attr[k])\\n\\n      # Hash the definition and its dependencies.\\n      self._hash_str = self._create_hash_str(\\n          self._definition.signature.input_arg,\\n          self._definition.signature.output_arg, self._definition.node_def)\\n\\n      # Finally, we decide the function name to use.  If not specified,\\n      # make up something which is almost certainly unique (but deterministic).\\n      if not self._func_name:\\n        self._func_name = \"_\".join([base_func_name, self._hash_str])\\n      self._definition.signature.name = self._func_name\\n      if self._func.__doc__:\\n        self._definition.signature.description = self._func.__doc__\\n\\n      self._op_def = self._definition.signature\\n    else:  # C API is enabled\\n      output_names = ([compat.as_bytes(x) for x in self._out_names]\\n                      if self._out_names else [])\\n      description = self._func.__doc__ or None\\n      # pylint: disable=protected-access\\n      c_func = c_api.TF_GraphToFunction_wrapper(\\n          temp_graph._c_graph,\\n          base_func_name,\\n          self._func_name is None,  # append_hash_to_fn_name\\n          None,  # opers\\n          [t._as_tf_output() for t in temp_graph.inputs],\\n          [t._as_tf_output() for t in temp_graph.outputs],\\n          output_names,\\n          None,  # opts\\n          description)\\n      self._c_func = c_api_util.ScopedTFFunction(c_func)\\n      # pylint: enable=protected-access\\n      self._set_c_attrs(kwargs_attr)\\n\\n      # Set cached fields: _op_def and _func_name (if not already set)\\n      self._op_def = self.definition.signature\\n      if self._func_name:\\n        assert self._func_name == self._op_def.name\\n      else:\\n        self._func_name = compat.as_str(self._op_def.name)\\n\\n    self._stateful_ops = [(op.name, op.type)\\n                          for op in temp_graph.get_operations()\\n                          if op.op_def.is_stateful]', 'def _create_hash_str(self, input_arg, output_arg, node_def):\\n    \"\"\"Creates an 8-character string unique to this input.\\n\\n    Args:\\n      input_arg: the input_arg field of an OpDef\\n                 (e.g. self._definition.signature.input_arg)\\n      output_arg: the output_arg field of an OpDef\\n                 (e.g. self._definition.signature.output_arg)\\n      node_def: the node_def field of a FunctionDef\\n                (e.g. self._definition.node_def)\\n\\n    Returns:\\n      The unique string for this input\\n    \"\"\"\\n    hasher = hashlib.sha1()\\n\\n    def update_num(n):\\n      hasher.update(compat.as_bytes(\"%x\" % n))\\n\\n    def update_str(s):\\n      update_num(len(s))\\n      hasher.update(compat.as_bytes(s))\\n\\n    def update_strs(slist):\\n      update_num(len(slist))\\n      for s in slist:\\n        update_str(s)\\n\\n    for adef in input_arg:\\n      update_str(adef.SerializeToString())\\n\\n    for adef in output_arg:\\n      update_str(adef.SerializeToString())\\n\\n    for n in sorted(node_def, key=lambda n: n.name):\\n      update_str(n.name)\\n      update_str(n.op)\\n      update_strs(n.input)\\n      update_num(len(n.attr))\\n      # NOTE: protobuf map serialization does not guarantee ordering.\\n      for k in sorted(n.attr):\\n        update_str(k)\\n        update_str(n.attr[k].SerializeToString())\\n\\n    return hasher.hexdigest()[:8]', 'def __call__(self, *args, **kwargs):\\n    self.add_to_graph(ops.get_default_graph())\\n    args = [ops.convert_to_tensor(_) for _ in args] + self._extra_inputs\\n    ret, op = _call(self._signature, *args, **kwargs)\\n\\n    # Set a hidden attr in \\'op\\' so that gradients_impl can refer back\\n    # to this _DefinedFunction instance to access python_grad_func.\\n    assert isinstance(op, ops.Operation)\\n    setattr(op, \"__defun\", self)\\n\\n    if self._shape_func is not None:\\n      shapes = self._shape_func(op)\\n      if len(shapes) != len(op.outputs):\\n        raise ValueError(\"shape_func produced %d shapes for %d outputs\" %\\n                         (len(shapes), len(op.outputs)))\\n      for (t, shape) in zip(op.outputs, shapes):\\n        t.set_shape(shape)\\n    return ret', 'def __init__(self,\\n               func,\\n               argnames,\\n               func_name=None,\\n               grad_func=None,\\n               python_grad_func=None,\\n               out_names=None,\\n               **kwargs):\\n    \"\"\"Creates _DefinedFunction.\\n\\n    Args:\\n      func:  A python callable which constructs a tf function body.\\n      argnames: A list of strings for function argument names.\\n      func_name: The function name. Defaults to None, in which derives from\\n        \\'func\\'.\\n      grad_func: This function\\'s gradient function, if not None. Defaults\\n        to None.\\n      python_grad_func: A python callable implementing the gradient of\\n        the function python-side.\\n      out_names: A list of strings for the function return value names.\\n      **kwargs: The keyword arguments. **kwargs is passed to every call\\n        site of this function.\\n\\n    Raises:\\n      ValueError: The function definition is invalid.\\n\\n    \"\"\"\\n    self._func = func\\n    self._argnames = argnames\\n    self._func_name = func_name\\n    assert grad_func is None or isinstance(grad_func, _OverloadedFunction)\\n    self._grad_func = grad_func\\n    self._python_grad_func = python_grad_func\\n    self._out_names = out_names\\n    self._extra_kwargs = kwargs\\n    self._overload = {}', 'def __call__(self, *args, **kwargs):\\n    input_types = []\\n    args = list(args)\\n    for (i, x) in enumerate(args):\\n      x = ops.convert_to_tensor(x)\\n      if not isinstance(x, ops.Tensor):\\n        raise ValueError(\"Expect a Tensor but get \", x)\\n      input_types.append(x.dtype)\\n      args[i] = x\\n    return self.instantiate(input_types)(*args, **kwargs)', 'def __init__(self, name, capture_by_value, *args, **kwargs):\\n    super(_FuncGraph, self).__init__(*args, **kwargs)\\n    self._capture_by_value = capture_by_value\\n    self._building_function = True\\n    self._outer_graph = ops.get_default_graph()\\n    self._vscope = vs.get_variable_scope()\\n    self._old_custom_getter = self._vscope.custom_getter\\n\\n    # The name of the function.\\n    self.name = name\\n    # Placeholder tensors representing the inputs to this function. The tensors\\n    # are in this _FuncGraph.\\n    self.inputs = []\\n    # Tensors that will be returned this function. The tensors are in this\\n    # _FuncGraph.\\n    self.outputs = []\\n    # Maps external tensor -> internal tensor (e.g. input placeholder).\\n    self._captured = {}\\n    # The external tensors that have been captured as inputs and must be passed\\n    # to this function (empty if capturing by value, otherwise these are the\\n    # keys of _captured).\\n    self.extra_inputs = []\\n    # Input placeholders that been added for captured values (empty if capturing\\n    # by value).\\n    self.extra_args = []\\n    # Captured variables.\\n    # TODO(skyewm): is this needed?\\n    self.extra_vars = []', 'def container(self, container_name):\\n    \"\"\"Returns a context manager that specifies the resource container to use.\\n\\n    Overridden from `tf.Graph` to update both the init_scope container\\n    and the present inner container. This is necessary to make sure setting\\n    containers applies correctly both to created variables and to stateful\\n    ops.\\n\\n    Args:\\n      container_name: container name string.\\n\\n    Returns:\\n      A context manager for defining resource containers for stateful ops,\\n        yields the container name.\\n    \"\"\"\\n    original_container = self._container\\n    # pylint: disable=protected-access\\n    with ops.init_scope():\\n      original_init_container = ops.get_default_graph()._container\\n    try:\\n      self._container = container_name\\n      with ops.init_scope():\\n        ops.get_default_graph()._container = container_name\\n      yield self._container\\n    finally:\\n      self._container = original_container\\n      with ops.init_scope():\\n        ops.get_default_graph()._container = original_init_container\\n    # pylint: enable=protected-access', 'def getvar(\\n      self,\\n      getter,\\n      name,\\n      shape=None,\\n      dtype=None,\\n      initializer=None,\\n      reuse=None,\\n      trainable=True,\\n      collections=None,  # pylint: disable=redefined-outer-name\\n      use_resource=None,\\n      **kwargs):\\n    \"\"\"A custom variable getter.\"\"\"\\n    # Here, we switch the default graph to the outer graph and ask the\\n    # variable scope in which the function is defined to give us the\\n    # variable. The variable is stashed in extra_vars and returned to\\n    # the caller.\\n    #\\n    # We capture these variables so that the variable definition is\\n    # hoisted upward to the outer most graph.\\n    with self._outer_graph.as_default():\\n      # pylint: disable=protected-access\\n      var = self._vscope.get_variable(\\n          vs._get_default_variable_store(),\\n          name,\\n          shape=shape,\\n          dtype=dtype,\\n          initializer=initializer,\\n          reuse=reuse,\\n          trainable=trainable,\\n          collections=collections,\\n          use_resource=use_resource)\\n      self.extra_vars.append(var)\\n      if isinstance(var, resource_variable_ops.ResourceVariable):\\n        # For resource-based variables read the variable outside the function\\n        # and pass in the value. This ensures that the function is pure and\\n        # differentiable. TODO(apassos) this may have performance problems if\\n        # the function will only do embedding lookups on the variable.\\n        return var.value()\\n      return var', 'def capture(self, tensor, name=None):\\n    \"\"\"Adds the given tensor to this graph and returns the captured tensor.\"\"\"\\n    if tensor in self._captured:\\n      # Captured already.\\n      return self._captured[tensor]\\n    elif self._capture_by_value:\\n      return self._add_tensor_and_parents(tensor)\\n    else:\\n      return self._capture_tensor_as_extra_input(tensor, name)', 'def _add_tensor_and_parents(self, tensor):\\n    op = self._add_op_and_parents(tensor.op)\\n    return op.outputs[tensor.value_index]', 'def func_graph_from_py_func(func, arg_names, arg_types, name=None,\\n                            capture_by_value=False, device=None,\\n                            colocation_stack=None, container=None,\\n                            collections_ref=None, arg_shapes=None):\\n  \"\"\"Returns a _FuncGraph generated from `func`.\\n\\n  Args:\\n    func: A Python callable which constructs a TF function body. The arguments\\n      must correspond to `arg_types`. Returns a value or list/tuple of values.\\n      No returned value can be None.\\n    arg_names: A sequence of strings for the function argument names.\\n    arg_types: A sequence of the function\\'s argument types.\\n    name: The function name. If None, the name is derived from `func`.\\n    capture_by_value: boolean. If True, captured values will be copied into the\\n      function body.\\n    device: device name or function.\\n    colocation_stack: A colocation stack (list) the _FuncGraph should use.\\n    container: A container name the _FuncGraph should start with.\\n    collections_ref: A reference to a collections dict the _FuncGraph should\\n      use internally.\\n    arg_shapes: A sequence of the function\\'s argument shapes.\\n\\n  Returns:\\n    A _FuncGraph.\\n\\n  Raises:\\n    ValueError: if func returns None.\\n  \"\"\"\\n  if not name:\\n    name = function_utils.get_func_name(func)\\n  func_graph = _FuncGraph(name, capture_by_value)\\n\\n  with func_graph.as_default(), ops.device(device):\\n    # pylint: disable=protected-access\\n    if collections_ref is not None:\\n      func_graph._collections = collections_ref\\n    if container is not None:\\n      func_graph._container = container\\n    if colocation_stack is not None:\\n      func_graph._colocation_stack = colocation_stack\\n    # pylint: enable=protected-access\\n\\n    if arg_shapes is None:\\n      arg_shapes = [None] * len(arg_types)\\n\\n    # Create placeholders for the function arguments.\\n    for (argname, argtype, argshape) in zip(arg_names, arg_types, arg_shapes):\\n      argholder = array_ops.placeholder(argtype, shape=argshape, name=argname)\\n      func_graph.inputs.append(argholder)\\n    # Call func and gather the output tensors.\\n    with vs.variable_scope(\"\", custom_getter=func_graph.getvar):\\n      outputs = func(*func_graph.inputs)\\n\\n    # There is no way of distinguishing between a function not returning\\n    # anything and a function returning None in Python.\\n    # We need to allow the former and ideally want to forbid the latter as\\n    # it is most likely user error.\\n    # TODO(iga): Consider adding a @NoOutput decorator on top of @Defun to\\n    # allow users to explicitly mark the function as not returning anything.\\n    # For now, we allow a single None return and interpret it as a function\\n    # with no output.\\n    if outputs is None:\\n      outputs = []\\n    else:\\n      # If func only returned one value, make it a tuple.\\n      if not isinstance(outputs, (list, tuple)):\\n        outputs = (outputs,)\\n      if any([_ is None for _ in outputs]):\\n        raise ValueError(\"Function can not return None.\")\\n    # Ensures each output is a Tensor in the function graph.\\n    outputs = [ops.convert_to_tensor(t) for t in outputs]\\n    outputs = [func_graph.capture(t) if t.graph is not func_graph else t\\n               for t in outputs]\\n    func_graph.outputs = outputs\\n  return func_graph', 'def __init__(self, op, leaving):\\n      self.op = op\\n      self.leaving = leaving', 'def all_inputs_const(op):\\n    # If all inputs of an op are guaranteed constants, then we can infer that\\n    # the op produces a constant as well.\\n    return op.inputs and all(inp.op in constants for inp in op.inputs)', 'def _call(sig, *inputs, **kwargs):\\n  \"\"\"Adds a node calling a function.\\n\\n  This adds a `call` op to the default graph that calls the function\\n  of signature `sig`, passing the tensors in `inputs` as arguments.\\n  It returns the outputs of the call, which are one or more tensors.\\n\\n  `sig` is OpDefArg.a `_DefinedFunction` object.\\n\\n  You can pass an optional keyword parameter `name=string` to name the\\n  added operation.\\n\\n  You can pass an optional keyword parameter `noinline=True|False` to\\n  instruct the runtime not to inline the function body into the call\\n  site.\\n\\n  Args:\\n    sig: OpDefArg. The signature of the function.\\n    *inputs: arguments to the function.\\n    **kwargs: Optional keyword arguments.  Can only contain \\'name\\' or\\n        \\'noinline\\'.\\n\\n  Returns:\\n     A 2-element tuple. First element: a Tensor if the function returns a single\\n     value; a list of Tensors if the function returns multiple value; the\\n     Operation if the function returns no values. Second element: the Operation.\\n\\n  Raises:\\n    ValueError: if the arguments are invalid.\\n  \"\"\"\\n  if len(inputs) != len(sig.input_arg):\\n    raise ValueError(\"Expected number of arguments: %d, received: %d\" % (len(\\n        sig.input_arg), len(inputs)))\\n  name = kwargs.pop(\"name\", None)\\n  g = ops.get_default_graph()\\n  func_name = sig.name\\n  attrs = _parse_kwargs_as_attrs(func_name, **kwargs)\\n  output_types = [dtypes.DType(x.type) for x in sig.output_arg]\\n  with ops.name_scope(name, func_name, inputs) as name:\\n    op = g.create_op(\\n        func_name,\\n        list(inputs),\\n        output_types,\\n        name=name,\\n        attrs=attrs,\\n        op_def=sig,\\n        compute_shapes=False)\\n  if op.outputs:\\n    if len(op.outputs) == 1:\\n      ret = op.outputs[0]\\n    else:\\n      ret = tuple(op.outputs)\\n  else:\\n    ret = op\\n  return ret, op', 'def _from_library(lib):\\n  \"\"\"Creates _DefinedFunctions initialized from a FunctionDefLibrary proto.\\n\\n  This method handles assigning the correct gradient functions to each\\n  function.\\n\\n  Args:\\n    lib: a FunctionDefLibrary\\n\\n  Returns:\\n    A list of _DefinedFunctions\\n\\n  Raises:\\n    ValueError: `lib` is invalid\\n  \"\"\"\\n  if not lib.function and not lib.gradient:\\n    return []\\n\\n  # function name -> FunctionDef proto\\n  funcs = {fdef.signature.name: fdef for fdef in lib.function}\\n\\n  # Validate that all references function names have function defs\\n  for g in lib.gradient:\\n    if g.function_name not in funcs:\\n      raise ValueError(\"FunctionDefLibrary missing \\'%s\\' FunctionDef\\\\n%s\" %\\n                       (g.function_name, str(lib)))\\n    if g.gradient_func not in funcs:\\n      raise ValueError(\"FunctionDefLibrary missing \\'%s\\' FunctionDef\\\\n%s\" %\\n                       (g.gradient_func, str(lib)))\\n\\n  # function name -> gradient function name\\n  func_to_grad = collections.defaultdict(lambda: None)\\n  # gradient function name -> names of functions having that grad function\\n  grad_to_funcs = collections.defaultdict(list)\\n\\n  for gdef in lib.gradient:\\n    func_to_grad[gdef.function_name] = gdef.gradient_func\\n    grad_to_funcs[gdef.gradient_func].append(gdef.function_name)\\n\\n  # Start with functions without gradients\\n  ready = [\\n      fdef for fdef in lib.function if func_to_grad[fdef.signature.name] is None\\n  ]\\n  if not ready:\\n    raise ValueError(\\n        \"FunctionDefLibrary contains cyclic gradient functions!\\\\n\" + str(lib))\\n  # function name -> _DefinedFunction\\n  initialized = {}\\n\\n  while ready:\\n    fdef = ready.pop()\\n    name = fdef.signature.name\\n\\n    grad = initialized.get(func_to_grad[name])\\n    if func_to_grad[name]:\\n      assert grad\\n    defined_func = _from_definition(fdef, grad_func=grad)\\n    initialized[name] = defined_func\\n\\n    ready.extend(funcs[f] for f in grad_to_funcs[name])\\n\\n  return initialized.values()', 'def _parse_kwargs_as_attrs(func_name, **kwargs):\\n  \"\"\"Parses **kwargs into a node\\'s attributes.\"\"\"\\n  attrs = {}\\n\\n  noinline = kwargs.pop(\"noinline\", None)\\n  if noinline is not None:\\n    attrs[\"_noinline\"] = attr_value_pb2.AttrValue(b=bool(noinline))\\n\\n  compiled = kwargs.pop(\"compiled\", None)\\n  separate_compiled_gradients = kwargs.pop(\"separate_compiled_gradients\", None)\\n  if compiled is not None:\\n    attrs[\"_XlaCompile\"] = attr_value_pb2.AttrValue(b=bool(compiled))\\n    attrs[\"_XlaSeparateCompiledGradients\"] = attr_value_pb2.AttrValue(\\n        b=bool(separate_compiled_gradients))\\n    # Forward _XlaScope from enclosing context (if set), otherwise create new.\\n    # pylint: disable=protected-access\\n    if \"_XlaScope\" in ops.get_default_graph()._attr_scope_map:\\n      attrs[\"_XlaScope\"] = ops.get_default_graph()._attr_scope_map[\"_XlaScope\"]\\n    else:\\n      attrs[\"_XlaScope\"] = attr_value_pb2.AttrValue(\\n          s=(\"function_%s\" % func_name).encode())\\n    # pylint: enable=protected-access\\n\\n  kwargs_keys = list(kwargs.keys())\\n  for key in kwargs_keys:\\n    if key.startswith(\"experimental_\"):\\n      attrs[key] = _get_experimental_kwarg_as_attr(key, kwargs[key])\\n      del kwargs[key]\\n\\n  if kwargs:\\n    raise ValueError(\"Unknown keyword arguments: %s\" % kwargs.keys())\\n  return attrs', 'def get_extra_inputs():\\n  \"\"\"Returns the captured input tensors by the function.\\n\\n  Returns:\\n    If the default graph is being used to define a function, the\\n    returned list of tensors are those accessed inside the function body\\n    but defined outside the function body so far. Otherwise, returns an\\n    empty list.\\n  \"\"\"\\n  g = ops.get_default_graph()\\n  if isinstance(g, _FuncGraph):\\n    return g.extra_inputs\\n  else:\\n    return []', 'def _type_list_to_str(types):\\n  if any([_ not in _DTYPE_TO_STR for _ in types]):\\n    raise ValueError(\"Unsupported dtypes: %s\" % types)\\n  return \"\".join([_DTYPE_TO_STR[_] for _ in types])']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def main(ad_exchange_buyer, owner_name, body, is_transient):\\n  try:\\n    # Construct and execute the request.\\n    filter_set = ad_exchange_buyer.bidders().filterSets().create(\\n        ownerName=owner_name, isTransient=is_transient, body=body).execute()\\n    print(f\\'FilterSet created for bidder: \"{owner_name}\".\\')\\n    pprint.pprint(filter_set)\\n  except HttpError as e:\\n    print(e)', 'def time_series_granularity_type(s):\\n    if s not in _VALID_TIME_SERIES_GRANULARITIES:\\n      raise argparse.ArgumentTypeError(\\'Invalid TimeSeriesGranularity \\'\\n                                       f\\'specified: \"{s}\".\\')\\n    return s', 'def format_type(s):\\n    if s not in _VALID_FORMATS:\\n      raise argparse.ArgumentTypeError(f\\'Invalid Format specified: \"{s}\".\\')\\n    return s', 'def valid_date(s):\\n    try:\\n      return datetime.strptime(s, _DATE_FORMAT).date()\\n    except ValueError:\\n      raise argparse.ArgumentTypeError(f\\'Invalid date specified: \"{s}\".\\')']}, {'features': [], 'snippets': [\"def test_dropout(self):\\n    testing_utils.layer_test(\\n        keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))\\n\\n    testing_utils.layer_test(\\n        keras.layers.Dropout,\\n        kwargs={'rate': 0.5,\\n                'noise_shape': [3, 1]},\\n        input_shape=(3, 2))\", \"def test_spatial_dropout_1d(self):\\n    testing_utils.layer_test(\\n        keras.layers.SpatialDropout1D,\\n        kwargs={'rate': 0.5},\\n        input_shape=(2, 3, 4))\", \"def test_spatial_dropout_3d(self):\\n    testing_utils.layer_test(\\n        keras.layers.SpatialDropout3D,\\n        kwargs={'rate': 0.5},\\n        input_shape=(2, 3, 4, 4, 5))\\n\\n    testing_utils.layer_test(\\n        keras.layers.SpatialDropout3D,\\n        kwargs={'rate': 0.5, 'data_format': 'channels_first'},\\n        input_shape=(2, 3, 4, 4, 5))\", \"def test_lambda(self):\\n    testing_utils.layer_test(\\n        keras.layers.Lambda,\\n        kwargs={'function': lambda x: x + 1},\\n        input_shape=(3, 2))\\n\\n    testing_utils.layer_test(\\n        keras.layers.Lambda,\\n        kwargs={\\n            'function': lambda x, a, b: x * a + b,\\n            'arguments': {\\n                'a': 0.6,\\n                'b': 0.4\\n            }\\n        },\\n        input_shape=(3, 2))\\n\\n    # test serialization with function\\n    def f(x):\\n      return x + 1\\n\\n    ld = keras.layers.Lambda(f)\\n    config = ld.get_config()\\n    ld = keras.layers.deserialize({\\n        'class_name': 'Lambda',\\n        'config': config\\n    })\\n\\n    # test with lambda\\n    ld = keras.layers.Lambda(\\n        lambda x: keras.backend.concatenate([math_ops.square(x), x]))\\n    config = ld.get_config()\\n    ld = keras.layers.Lambda.from_config(config)\", \"def test_lambda_output_shape(self):\\n    l = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))\\n    l(keras.backend.variable(np.ones((1, 1))))\\n    self.assertEqual((1, 1), l.get_config()['output_shape'])\", 'def get_output_shape(input_shape):\\n      return 1 * input_shape', 'def test_lambda_output_shape_autocalculate_multiple_inputs(self):\\n\\n    def lambda_fn(x):\\n      return math_ops.matmul(x[0], x[1])\\n\\n    l = keras.layers.Lambda(lambda_fn)\\n    output_shape = l.compute_output_shape([(10, 10), (10, 20)])\\n    self.assertAllEqual((10, 20), output_shape)', 'def lambda_fn(x):\\n      return x', 'def test_lambda_output_shape_tuple_with_none(self):\\n\\n    def lambda_fn(x):\\n      return x\\n\\n    l = keras.layers.Lambda(lambda_fn, output_shape=(None, 10))\\n    output_shape = l.compute_output_shape((5, 10, 20))\\n    self.assertAllEqual([5, None, 10], output_shape.as_list())', 'def lambda_fn(x):\\n      return x', \"def test_lambda_config_serialization(self):\\n    # Test serialization with output_shape and output_shape_type\\n    layer = keras.layers.Lambda(lambda x: x + 1, output_shape=(1, 1))\\n    layer(keras.backend.variable(np.ones((1, 1))))\\n    config = layer.get_config()\\n    layer = keras.layers.deserialize({\\n        'class_name': 'Lambda',\\n        'config': config\\n    })\\n    layer = keras.layers.Lambda.from_config(config)\", 'def test_masking(self):\\n    testing_utils.layer_test(\\n        keras.layers.Masking, kwargs={}, input_shape=(3, 2, 3))', \"def test_activation(self):\\n    # with string argument\\n    testing_utils.layer_test(\\n        keras.layers.Activation,\\n        kwargs={'activation': 'relu'},\\n        input_shape=(3, 2))\\n\\n    # with function argument\\n    testing_utils.layer_test(\\n        keras.layers.Activation,\\n        kwargs={'activation': keras.backend.relu},\\n        input_shape=(3, 2))\", \"def test_permute(self):\\n    testing_utils.layer_test(\\n        keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))\", \"def test_permute_errors_on_invalid_set_of_dims_indices(self):\\n    with self.assertRaisesRegexp(ValueError, r'Invalid permutation .*dims.*'):\\n      testing_utils.layer_test(\\n          keras.layers.Permute,\\n          kwargs={'dims': (1, 4, 2)}, input_shape=(3, 2, 4))\", \"def test_flatten_scalar_channels(self):\\n    testing_utils.layer_test(\\n        keras.layers.Flatten, kwargs={}, input_shape=(3,))\\n\\n    # Test channels_first\\n    inputs = np.random.random((10,)).astype('float32')\\n    outputs = testing_utils.layer_test(\\n        keras.layers.Flatten,\\n        kwargs={'data_format': 'channels_first'},\\n        input_data=inputs)\\n    target_outputs = np.expand_dims(inputs, -1)\\n    self.assertAllClose(outputs, target_outputs)\", \"def test_dense(self):\\n    testing_utils.layer_test(\\n        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 2))\\n\\n    testing_utils.layer_test(\\n        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 2))\\n\\n    testing_utils.layer_test(\\n        keras.layers.Dense, kwargs={'units': 3}, input_shape=(None, None, 2))\\n\\n    testing_utils.layer_test(\\n        keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 5, 2))\", \"def test_dense_with_policy(self):\\n    inputs = ops.convert_to_tensor(\\n        np.random.randint(low=0, high=7, size=(2, 2)), dtype='float16')\\n    layer = keras.layers.Dense(5, dtype=policy.Policy('infer_float32_vars'))\\n    outputs = layer(inputs)\\n    self.assertEqual(outputs.dtype, 'float16')\\n    self.assertEqual(layer.kernel.dtype, 'float32')\", 'def test_dense_constraints(self):\\n    k_constraint = keras.constraints.max_norm(0.01)\\n    b_constraint = keras.constraints.max_norm(0.01)\\n    layer = keras.layers.Dense(\\n        3, kernel_constraint=k_constraint, bias_constraint=b_constraint)\\n    layer(keras.backend.variable(np.ones((2, 4))))\\n    self.assertEqual(layer.kernel.constraint, k_constraint)\\n    self.assertEqual(layer.bias.constraint, b_constraint)', 'def test_numpy_inputs(self):\\n    if context.executing_eagerly():\\n      layer = keras.layers.RepeatVector(2)\\n      x = np.ones((10, 10))\\n      self.assertAllEqual(np.ones((10, 2, 10)), layer(x))\\n\\n      layer = keras.layers.Concatenate()\\n      x, y = np.ones((10, 10)), np.ones((10, 10))\\n      self.assertAllEqual(np.ones((10, 20)), layer([x, y]))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def takephoto():\\n\\tcamera.capture(imagefile)\\n\\timage1 = Image.open(imagefile)\\n\\treturn image1', 'def newphoto():\\n   global image1\\n   image1 =  takephoto()\\n\\n   tkimage1 = ImageTk.PhotoImage(image1)\\n   panel1.configure(image=tkimage1)\\n   panel1.image = tkimage1', 'def grayscale():\\n   global image1\\n   r, g, b = image1.split()\\n   image1 = Image.merge(\"RGB\", (g,g,g))\\n\\n   tkimage1 = ImageTk.PhotoImage(image1)\\n   panel1.configure(image=tkimage1)\\n   panel1.image = tkimage1']}, {'features': [], 'snippets': ['def __init__(\\n        self,\\n        hass,\\n        device_id,\\n        friendly_name,\\n        state_template,\\n        icon_template,\\n        entity_picture_template,\\n        availability_template,\\n        on_action,\\n        off_action,\\n        unique_id,', 'def _update_state(self, result):\\n        super()._update_state(result)\\n        if isinstance(result, TemplateError):\\n            self._state = None\\n            return\\n        self._state = result.lower() in (\"true\", STATE_ON)', 'def name(self):\\n        \"\"\"Return the name of the switch.\"\"\"\\n        return self._name', 'def unique_id(self):\\n        \"\"\"Return the unique id of this switch.\"\"\"\\n        return self._unique_id', 'def is_on(self):\\n        \"\"\"Return true if device is on.\"\"\"\\n        return self._state', 'def should_poll(self):\\n        \"\"\"Return the polling state.\"\"\"\\n        return False']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self,apiKey,licenseId):    \\n    self.headers = {\"content-type\": \"application/json\",\\n                    \"Authorization\": apiKey}\\n    self.params = {\"licenseId\" : licenseId }\\n    retries = Retry(total=5,\\n                    backoff_factor=0.75)\\n    self.session = requests.Session()\\n    self.session.mount(KANGROUTER_WEBSERVICE_APPLICATION_ROOT, \\n                       HTTPAdapter(max_retries=retries))', 'def create(self,problem,**kwargs):\\n    path = self.pathbase\\n    payload=json.dumps(problem)\\n    params = self.params.copy()\\n    params.update(kwargs)\\n    req = self.session.post(path,\\n                        params=params, \\n                        headers=self.headers,\\n                        data=payload)    \\n    self.validateReply(req)\\n    return req.text', 'def delete(self,solverId):\\n    path = \"{base}/{solverId}\".format(base=self.pathbase,\\n                                      solverId=str(solverId))\\n    req = self.session.delete(path,\\n                          params=self.params,\\n                          headers=self.headers)\\n    self.validateReply(req)\\n    return True', 'def stop(self,solverId):\\n    path = \"{base}/{solverId}/stop\".format(base=self.pathbase,\\n                                      solverId=str(solverId))\\n    req = self.session.put(path,\\n                       params=self.params,\\n                       headers=self.headers)\\n    self.validateReply(req)\\n    return True', 'def getStatus(self,solverId):\\n    path = \"{base}/{solverId}/status\".format(base=self.pathbase,\\n                                      solverId=str(solverId))\\n    req = self.session.get(path,\\n                       params=self.params,\\n                       headers=self.headers)\\n    self.validateReply(req)\\n    return req.json()', 'def createAndWait(self,problem,cancel,**kwargs):\\n    solverId = self.create(problem,**kwargs)\\n    timeout = 300\\n    while not cancel() and timeout>0:\\n      status = self.getStatus(solverId)\\n      if status[\"execStatus\"] ==\"invalid\":\\n        raise exception.solverError(json.dumps(status[\"errors\"]))\\n      if status[\"execStatus\"] ==\"completed\":\\n        return self.getSolution(solverId)\\n      time.sleep(1)\\n      timeout -= 1\\n    if timeout == 0:\\n      raise exception.InternalError(\"Timed out waiting for solver\")\\n    raise exception.UserCancelled()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        super(NovaProxyRequestHandlerBaseTestCase, self).setUp()\\n\\n        self.wh = websocketproxy.NovaProxyRequestHandlerBase()\\n        self.wh.socket = mock.MagicMock()\\n        self.wh.msg = mock.MagicMock()\\n        self.wh.do_proxy = mock.MagicMock()\\n        self.wh.headers = mock.MagicMock()', 'def test_new_websocket_client(self, check_token):\\n        check_token.return_value = {\\n            \\'host\\': \\'node1\\',\\n            \\'port\\': \\'10000\\'\\n        }\\n        self.wh.socket.return_value = \\'<socket>\\'\\n        self.wh.path = \"ws://127.0.0.1/?token=123-456-789\"\\n\\n        self.wh.new_websocket_client()\\n\\n        check_token.assert_called_with(mock.ANY, token=\"123-456-789\")\\n        self.wh.socket.assert_called_with(\\'node1\\', 10000, connect=True)\\n        self.wh.do_proxy.assert_called_with(\\'<socket>\\')', 'def test_new_websocket_client_token_invalid(self, check_token):\\n        check_token.return_value = False\\n\\n        self.wh.path = \"ws://127.0.0.1/?token=XXX\"\\n\\n        self.assertRaises(exception.InvalidToken,\\n                          self.wh.new_websocket_client)\\n        check_token.assert_called_with(mock.ANY, token=\"XXX\")', 'def test_new_websocket_client_novnc(self, check_token):\\n        check_token.return_value = {\\n            \\'host\\': \\'node1\\',\\n            \\'port\\': \\'10000\\'\\n        }\\n        self.wh.socket.return_value = \\'<socket>\\'\\n        self.wh.path = \"http://127.0.0.1/\"\\n        self.wh.headers.getheader.return_value = \"token=123-456-789\"\\n\\n        self.wh.new_websocket_client()\\n\\n        check_token.assert_called_with(mock.ANY, token=\"123-456-789\")\\n        self.wh.socket.assert_called_with(\\'node1\\', 10000, connect=True)\\n        self.wh.do_proxy.assert_called_with(\\'<socket>\\')', 'def test_new_websocket_client_novnc_token_invalid(self, check_token):\\n        check_token.return_value = False\\n\\n        self.wh.path = \"http://127.0.0.1/\"\\n        self.wh.headers.getheader.return_value = \"token=XXX\"\\n\\n        self.assertRaises(exception.InvalidToken,\\n                          self.wh.new_websocket_client)\\n        check_token.assert_called_with(mock.ANY, token=\"XXX\")', 'def test_new_websocket_client_internal_access_path(self, check_token):\\n        check_token.return_value = {\\n            \\'host\\': \\'node1\\',\\n            \\'port\\': \\'10000\\',\\n            \\'internal_access_path\\': \\'vmid\\'\\n        }\\n\\n        tsock = mock.MagicMock()\\n        tsock.recv.return_value = \"HTTP/1.1 200 OK\\\\r\\\\n\\\\r\\\\n\"\\n\\n        self.wh.socket.return_value = tsock\\n        self.wh.path = \"ws://127.0.0.1/?token=123-456-789\"\\n\\n        self.wh.new_websocket_client()\\n\\n        check_token.assert_called_with(mock.ANY, token=\"123-456-789\")\\n        self.wh.socket.assert_called_with(\\'node1\\', 10000, connect=True)\\n        self.wh.do_proxy.assert_called_with(tsock)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _get_ovn_controller(self, install_method=\"sandbox\"):\\n        ovn_nbctl = self.controller_client(\"ovn-nbctl\")\\n        ovn_nbctl.set_sandbox(\"controller-sandbox\", install_method,\\n                              self.context[\\'controller\\'][\\'host_container\\'])\\n        ovn_nbctl.set_daemon_socket(self.context.get(\"daemon_socket\", None))\\n        return ovn_nbctl', 'def _stop_daemon(self):\\n        ovn_nbctl = self._get_ovn_controller(self.install_method)\\n        ovn_nbctl.stop_daemon()', 'def _create_lswitches(self, lswitch_create_args, num_switches=-1):\\n        self.RESOURCE_NAME_FORMAT = \"lswitch_XXXXXX_XXXXXX\"\\n\\n        if (num_switches == -1):\\n            num_switches = lswitch_create_args.get(\"amount\", 1)\\n        batch = lswitch_create_args.get(\"batch\", num_switches)\\n\\n        start_cidr = lswitch_create_args.get(\"start_cidr\", \"\")\\n        if start_cidr:\\n            start_cidr = netaddr.IPNetwork(start_cidr)\\n\\n        mcast_snoop = lswitch_create_args.get(\"mcast_snoop\", \"true\")\\n        mcast_idle = lswitch_create_args.get(\"mcast_idle_timeout\", 300)\\n        mcast_table_size = lswitch_create_args.get(\"mcast_table_size\", 2048)\\n\\n        LOG.info(\"Create lswitches method: %s\" % self.install_method)\\n        ovn_nbctl = self._get_ovn_controller(self.install_method)\\n        ovn_nbctl.enable_batch_mode()\\n\\n        flush_count = batch\\n        lswitches = []\\n        for i in range(num_switches):\\n            name = self.generate_random_name()\\n            if start_cidr:\\n                cidr = start_cidr.next(i)\\n                name = \"lswitch_%s\" % cidr\\n            else:\\n                name = self.generate_random_name()\\n\\n            other_cfg = {\\n                \\'mcast_snoop\\': mcast_snoop,\\n                \\'mcast_idle_timeout\\': mcast_idle,\\n                \\'mcast_table_size\\': mcast_table_size\\n            }\\n\\n            lswitch = ovn_nbctl.lswitch_add(name, other_cfg)\\n            if start_cidr:\\n                lswitch[\"cidr\"] = cidr\\n\\n            LOG.info(\"create %(name)s %(cidr)s\" % \\\\\\n                      {\"name\": name, \"cidr\": lswitch.get(\"cidr\", \"\")})\\n            lswitches.append(lswitch)\\n\\n            flush_count -= 1\\n            if flush_count < 1:\\n                ovn_nbctl.flush()\\n                flush_count = batch\\n\\n        ovn_nbctl.flush() # ensure all commands be run\\n        ovn_nbctl.enable_batch_mode(False)\\n        return lswitches', 'def _connect_network_to_router(self, router, network):\\n        LOG.info(\"Connect network %s to router %s\" % (network[\"name\"], router[\"name\"]))\\n\\n        ovn_nbctl = self.controller_client(\"ovn-nbctl\")\\n        ovn_nbctl.set_sandbox(\"controller-sandbox\", self.install_method,\\n                              self.context[\\'controller\\'][\\'host_container\\'])\\n        ovn_nbctl.enable_batch_mode(False)\\n\\n\\n        base_mac = [i[:2] for i in self.task[\"uuid\"].split(\\'-\\')]\\n        base_mac[0] = str(hex(int(base_mac[0], 16) & 254))\\n        base_mac[3:] = [\\'00\\']*3\\n        mac = utils.get_random_mac(base_mac)\\n\\n        lrouter_port = ovn_nbctl.lrouter_port_add(router[\"name\"], network[\"name\"], mac,\\n                                                  str(network[\"cidr\"]))\\n        ovn_nbctl.flush()\\n\\n\\n        switch_router_port = \"rp-\" + network[\"name\"]\\n        lport = ovn_nbctl.lswitch_port_add(network[\"name\"], switch_router_port)\\n        ovn_nbctl.db_set(\\'Logical_Switch_Port\\', switch_router_port,\\n                         (\\'options\\', {\"router-port\":network[\"name\"]}),\\n                         (\\'type\\', \\'router\\'),\\n                         (\\'address\\', \\'router\\'))\\n        ovn_nbctl.flush()']}, {'features': [], 'snippets': ['def __init__(self, channel):\\n        \"\"\"Constructor.', 'def CreateSession(self, request, context):\\n        \"\"\"Creates a new session. A session can be used to perform', 'def BatchCreateSessions(self, request, context):\\n        \"\"\"Creates multiple new sessions.', 'def GetSession(self, request, context):\\n        \"\"\"Gets a session. Returns `NOT_FOUND` if the session does not exist.', 'def ListSessions(self, request, context):\\n        \"\"\"Lists all sessions in a given database.', 'def DeleteSession(self, request, context):\\n        \"\"\"Ends a session, releasing server resources associated with it. This will', 'def ExecuteSql(self, request, context):\\n        \"\"\"Executes an SQL statement, returning all results in a single reply. This', 'def ExecuteStreamingSql(self, request, context):\\n        \"\"\"Like [ExecuteSql][google.spanner.v1.Spanner.ExecuteSql], except returns the result', 'def ExecuteBatchDml(self, request, context):\\n        \"\"\"Executes a batch of SQL DML statements. This method allows many statements', 'def Read(self, request, context):\\n        \"\"\"Reads rows from the database using key lookups and scans, as a', 'def StreamingRead(self, request, context):\\n        \"\"\"Like [Read][google.spanner.v1.Spanner.Read], except returns the result set as a', 'def BeginTransaction(self, request, context):\\n        \"\"\"Begins a new transaction. This step can often be skipped:', 'def Commit(self, request, context):\\n        \"\"\"Commits a transaction. The request includes the mutations to be', 'def Rollback(self, request, context):\\n        \"\"\"Rolls back a transaction, releasing any locks it holds. It is a good', 'def PartitionQuery(self, request, context):\\n        \"\"\"Creates a set of partition tokens that can be used to execute a query', 'def PartitionRead(self, request, context):\\n        \"\"\"Creates a set of partition tokens that can be used to execute a read']}, {'features': [], 'snippets': ['def __init__(self, driver):\\n        super(BrowseMoviePage, self).__init__(driver)\\n        self.nav = NavBlock(driver)', 'def click_edit_button(self):\\n        \"\"\"\\n        :rtype: EditMoviePage\\n        \"\"\"\\n        self._click(BrowseMoviePageLocators.EDIT_BUTTON_LOCATOR)\\n        return EditMoviePage(self._driver)', 'def click_remove_button(self):\\n        \"\"\"\\n        :rtype: HomePage\\n        \"\"\"\\n        self._click(BrowseMoviePageLocators.REMOVE_BUTTON_LOCATOR)\\n        self.alert_accept()\\n        from .home import HomePage\\n        return HomePage(self._driver)', 'def __init__(self, driver):\\n        super(AddMoviePage, self).__init__(driver)\\n        self.nav = NavBlock(driver)', 'def click_save_button(self):\\n        \"\"\"\\n        :rtype: BrowseMoviePage\\n        \"\"\"\\n        self._click(AddMoviePageLocators.SAVE_BUTTON_LOCATOR)\\n        return BrowseMoviePage(self._driver)', 'def year_field_is_required_present(self):\\n        \"\"\"\\n        :rtype: bool\\n        \"\"\"\\n        return self._is_element_present(AddMoviePageLocators.YEAR_INPUT_ERROR_LOCATOR)']}, {'features': [], 'snippets': ['def __init__(self, conf, topic_name):\\n        self.topic_name = topic_name\\n        self.producer = Producer(conf)\\n        self.counter = 0\\n        self.running = True', \"def on_delivery(self, err, msg):\\n        if err:\\n            print('Delivery report: Failed sending message {0}'.format(msg.value()))\\n            print(err)\\n            # We could retry sending the message\\n        else:\\n            print('Message produced, offset: {0}'.format(msg.offset()))\"]}, {'features': [], 'snippets': ['def __init__(self):\\n        super(TestAppModelSimple, self).__init__()\\n        self.my_key = \"\"\\n        self.stringField = \"\"']}, {'features': [], 'snippets': [\"def get_key(opts):\\n    if opts['transport'] in ('zeromq', 'tcp'):\\n        return Key(opts)\\n    else:\\n        return RaetKey(opts)\", \"def __init__(self, opts):\\n        self.opts = opts\\n        if self.opts['transport'] in ('zeromq', 'tcp'):\\n            self.key = Key(opts)\\n        else:\\n            self.key = RaetKey(opts)\", \"def list_all(self):\\n        '''\\n        Print out all keys\\n        '''\\n        salt.output.display_output(\\n                self.key.list_keys(),\\n                'key',\\n                self.opts)\", \"def _print_accepted(matches, after_match):\\n            if self.key.ACC in after_match:\\n                accepted = sorted(\\n                    set(after_match[self.key.ACC]).difference(\\n                        set(matches.get(self.key.ACC, []))\\n                    )\\n                )\\n                for key in accepted:\\n                    print('Key for minion {0} accepted.'.format(key))\", \"def accept_all(self, include_rejected=False):\\n        '''\\n        Accept all keys\\n\\n        :param bool include_rejected: Whether or not to accept a matched key that was formerly rejected\\n        '''\\n        self.accept('*', include_rejected=include_rejected)\", \"def _print_deleted(matches, after_match):\\n            deleted = []\\n            for keydir in (self.key.ACC, self.key.PEND, self.key.REJ):\\n                deleted.extend(list(\\n                    set(matches.get(keydir, [])).difference(\\n                        set(after_match.get(keydir, []))\\n                    )\\n                ))\\n            for key in sorted(deleted):\\n                print('Key for minion {0} deleted.'.format(key))\", \"def delete_all(self):\\n        '''\\n        Delete all keys\\n        '''\\n        self.delete('*')\", \"def _print_rejected(matches, after_match):\\n            if self.key.REJ in after_match:\\n                rejected = sorted(\\n                    set(after_match[self.key.REJ]).difference(\\n                        set(matches.get(self.key.REJ, []))\\n                    )\\n                )\\n                for key in rejected:\\n                    print('Key for minion {0} rejected.'.format(key))\", \"def reject_all(self, include_accepted=False):\\n        '''\\n        Reject all keys\\n\\n        :param bool include_accepted: Whether or not to accept a matched key that was formerly accepted\\n        '''\\n        self.reject('*', include_accepted=include_accepted)\", \"def print_all(self):\\n        '''\\n        Print out all managed keys\\n        '''\\n        self.print_key('*')\", \"def finger_all(self):\\n        '''\\n        Print out all fingerprints\\n        '''\\n        matches = self.key.finger('*')\\n        salt.output.display_output(\\n                matches,\\n                'key',\\n                self.opts)\", \"def run(self):\\n        '''\\n        Run the logic for saltkey\\n        '''\\n        if self.opts['gen_keys']:\\n            self.key.gen_keys()\\n            return\\n        elif self.opts['gen_signature']:\\n            self.prep_signature()\\n            return\\n        if self.opts['list']:\\n            self.list_status(self.opts['list'])\\n        elif self.opts['list_all']:\\n            self.list_all()\\n        elif self.opts['print']:\\n            self.print_key(self.opts['print'])\\n        elif self.opts['print_all']:\\n            self.print_all()\\n        elif self.opts['accept']:\\n            self.accept(\\n                self.opts['accept'],\\n                include_rejected=self.opts['include_all']\\n            )\\n        elif self.opts['accept_all']:\\n            self.accept_all(include_rejected=self.opts['include_all'])\\n        elif self.opts['reject']:\\n            self.reject(\\n                self.opts['reject'],\\n                include_accepted=self.opts['include_all']\\n            )\\n        elif self.opts['reject_all']:\\n            self.reject_all(include_accepted=self.opts['include_all'])\\n        elif self.opts['delete']:\\n            self.delete(self.opts['delete'])\\n        elif self.opts['delete_all']:\\n            self.delete_all()\\n        elif self.opts['finger']:\\n            self.finger(self.opts['finger'])\\n        elif self.opts['finger_all']:\\n            self.finger_all()\\n        else:\\n            self.list_all()\", \"def __init__(self, opts):\\n        opts['__multi_key'] = True\\n        super(MultiKeyCLI, self).__init__(opts)\\n        # Remove the key attribute set in KeyCLI.__init__\\n        delattr(self, 'key')\\n        zopts = copy.copy(opts)\\n        ropts = copy.copy(opts)\\n        self.keys = {}\\n        zopts['transport'] = 'zeromq'\\n        self.keys['ZMQ Keys'] = KeyCLI(zopts)\\n        ropts['transport'] = 'raet'\\n        self.keys['RAET Keys'] = KeyCLI(ropts)\", \"def list_status(self, status):\\n        self._call_all('list_status', status)\", \"def accept(self, match, include_rejected=False):\\n        self._call_all('accept', match, include_rejected)\", \"def delete(self, match):\\n        self._call_all('delete', match)\", \"def reject(self, match, include_accepted=False):\\n        self._call_all('reject', match, include_accepted)\", \"def print_key(self, match):\\n        self._call_all('print_key', match)\", \"def finger(self, match):\\n        self._call_all('finger', match)\", \"def prep_signature(self):\\n        self._call_all('prep_signature')\", 'def __init__(self, opts):\\n        self.opts = opts\\n        kind = self.opts.get(\\'__role\\', \\'\\')  # application kind\\n        if kind not in kinds.APPL_KINDS:\\n            emsg = (\"Invalid application kind = \\'{0}\\'.\".format(kind))\\n            log.error(emsg + \\'\\\\n\\')\\n            raise ValueError(emsg)\\n        self.event = salt.utils.event.get_event(\\n                kind,\\n                opts[\\'sock_dir\\'],\\n                opts[\\'transport\\'],\\n                opts=opts,\\n                listen=False)', \"def gen_keys(self):\\n        '''\\n        Generate minion RSA public keypair\\n        '''\\n        salt.crypt.gen_keys(\\n                self.opts['gen_keys_dir'],\\n                self.opts['gen_keys'],\\n                self.opts['keysize'])\\n        return\", \"def check_minion_cache(self, preserve_minions=None):\\n        '''\\n        Check the minion cache to make sure that old minion data is cleared\\n\\n        Optionally, pass in a list of minions which should have their caches\\n        preserved. To preserve all caches, set __opts__['preserve_minion_cache']\\n        '''\\n        if preserve_minions is None:\\n            preserve_minions = []\\n        m_cache = os.path.join(self.opts['cachedir'], self.ACC)\\n        if not os.path.isdir(m_cache):\\n            return\\n        keys = self.list_keys()\\n        minions = []\\n        for key, val in six.iteritems(keys):\\n            minions.extend(val)\\n        if not self.opts.get('preserve_minion_cache', False) or not preserve_minions:\\n            for minion in os.listdir(m_cache):\\n                if minion not in minions and minion not in preserve_minions:\\n                    shutil.rmtree(os.path.join(m_cache, minion))\", \"def name_match(self, match, full=False):\\n        '''\\n        Accept a glob which to match the of a key and return the key's location\\n        '''\\n        if full:\\n            matches = self.all_keys()\\n        else:\\n            matches = self.list_keys()\\n        ret = {}\\n        if ',' in match and isinstance(match, str):\\n            match = match.split(',')\\n        for status, keys in six.iteritems(matches):\\n            for key in salt.utils.isorted(keys):\\n                if isinstance(match, list):\\n                    for match_item in match:\\n                        if fnmatch.fnmatch(key, match_item):\\n                            if status not in ret:\\n                                ret[status] = []\\n                            ret[status].append(key)\\n                else:\\n                    if fnmatch.fnmatch(key, match):\\n                        if status not in ret:\\n                            ret[status] = []\\n                        ret[status].append(key)\\n        return ret\", \"def local_keys(self):\\n        '''\\n        Return a dict of local keys\\n        '''\\n        ret = {'local': []}\\n        for fn_ in salt.utils.isorted(os.listdir(self.opts['pki_dir'])):\\n            if fn_.endswith('.pub') or fn_.endswith('.pem'):\\n                path = os.path.join(self.opts['pki_dir'], fn_)\\n                if os.path.isfile(path):\\n                    ret['local'].append(fn_)\\n        return ret\", \"def all_keys(self):\\n        '''\\n        Merge managed keys with local keys\\n        '''\\n        keys = self.list_keys()\\n        keys.update(self.local_keys())\\n        return keys\", \"def key_str(self, match):\\n        '''\\n        Return the specified public key or keys based on a glob\\n        '''\\n        ret = {}\\n        for status, keys in six.iteritems(self.name_match(match)):\\n            ret[status] = {}\\n            for key in salt.utils.isorted(keys):\\n                path = os.path.join(self.opts['pki_dir'], status, key)\\n                with salt.utils.fopen(path, 'r') as fp_:\\n                    ret[status][key] = fp_.read()\\n        return ret\", 'def accept(self, match=None, match_dict=None, include_rejected=False):\\n        \\'\\'\\'\\n        Accept public keys. If \"match\" is passed, it is evaluated as a glob.\\n        Pre-gathered matches can also be passed via \"match_dict\".\\n        \\'\\'\\'\\n        if match is not None:\\n            matches = self.name_match(match)\\n        elif match_dict is not None and isinstance(match_dict, dict):\\n            matches = match_dict\\n        else:\\n            matches = {}\\n        keydirs = [self.PEND]\\n        if include_rejected:\\n            keydirs.append(self.REJ)\\n        for keydir in keydirs:\\n            for key in matches.get(keydir, []):\\n                try:\\n                    shutil.move(\\n                            os.path.join(\\n                                self.opts[\\'pki_dir\\'],\\n                                keydir,\\n                                key),\\n                            os.path.join(\\n                                self.opts[\\'pki_dir\\'],\\n                                self.ACC,\\n                                key)\\n                            )\\n                    eload = {\\'result\\': True,\\n                             \\'act\\': \\'accept\\',\\n                             \\'id\\': key}\\n                    self.event.fire_event(eload, tagify(prefix=\\'key\\'))\\n                except (IOError, OSError):\\n                    pass\\n        return (\\n            self.name_match(match) if match is not None\\n            else self.dict_match(matches)\\n        )', 'def delete_key(self, match=None, match_dict=None, preserve_minions=False):\\n        \\'\\'\\'\\n        Delete public keys. If \"match\" is passed, it is evaluated as a glob.\\n        Pre-gathered matches can also be passed via \"match_dict\".\\n\\n        To preserve the master caches of minions who are matched, set preserve_minions\\n        \\'\\'\\'\\n        if match is not None:\\n            matches = self.name_match(match)\\n        elif match_dict is not None and isinstance(match_dict, dict):\\n            matches = match_dict\\n        else:\\n            matches = {}\\n        for status, keys in six.iteritems(matches):\\n            for key in keys:\\n                try:\\n                    os.remove(os.path.join(self.opts[\\'pki_dir\\'], status, key))\\n                    eload = {\\'result\\': True,\\n                             \\'act\\': \\'delete\\',\\n                             \\'id\\': key}\\n                    self.event.fire_event(eload, tagify(prefix=\\'key\\'))\\n                except (OSError, IOError):\\n                    pass\\n        self.check_minion_cache(preserve_minions=matches.get(\\'minions\\', []))\\n        if self.opts.get(\\'rotate_aes_key\\'):\\n            salt.crypt.dropfile(self.opts[\\'cachedir\\'], self.opts[\\'user\\'])\\n        return (\\n            self.name_match(match) if match is not None\\n            else self.dict_match(matches)\\n        )', 'def reject(self, match=None, match_dict=None, include_accepted=False):\\n        \\'\\'\\'\\n        Reject public keys. If \"match\" is passed, it is evaluated as a glob.\\n        Pre-gathered matches can also be passed via \"match_dict\".\\n        \\'\\'\\'\\n        if match is not None:\\n            matches = self.name_match(match)\\n        elif match_dict is not None and isinstance(match_dict, dict):\\n            matches = match_dict\\n        else:\\n            matches = {}\\n        keydirs = [self.PEND]\\n        if include_accepted:\\n            keydirs.append(self.ACC)\\n        for keydir in keydirs:\\n            for key in matches.get(keydir, []):\\n                try:\\n                    shutil.move(\\n                            os.path.join(\\n                                self.opts[\\'pki_dir\\'],\\n                                keydir,\\n                                key),\\n                            os.path.join(\\n                                self.opts[\\'pki_dir\\'],\\n                                self.REJ,\\n                                key)\\n                            )\\n                    eload = {\\'result\\': True,\\n                            \\'act\\': \\'reject\\',\\n                            \\'id\\': key}\\n                    self.event.fire_event(eload, tagify(prefix=\\'key\\'))\\n                except (IOError, OSError):\\n                    pass\\n        self.check_minion_cache()\\n        if self.opts.get(\\'rotate_aes_key\\'):\\n            salt.crypt.dropfile(self.opts[\\'cachedir\\'], self.opts[\\'user\\'])\\n        return (\\n            self.name_match(match) if match is not None\\n            else self.dict_match(matches)\\n        )', \"def finger(self, match):\\n        '''\\n        Return the fingerprint for a specified key\\n        '''\\n        matches = self.name_match(match, True)\\n        ret = {}\\n        for status, keys in six.iteritems(matches):\\n            ret[status] = {}\\n            for key in keys:\\n                if status == 'local':\\n                    path = os.path.join(self.opts['pki_dir'], key)\\n                else:\\n                    path = os.path.join(self.opts['pki_dir'], status, key)\\n                ret[status][key] = salt.utils.pem_finger(path, sum_type=self.opts['hash_type'])\\n        return ret\", 'def __init__(self, opts):\\n        Key.__init__(self, opts)\\n        self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)\\n        self.serial = salt.payload.Serial(self.opts)', 'def check_minion_cache(self, preserve_minions=False):\\n        \\'\\'\\'\\n        Check the minion cache to make sure that old minion data is cleared\\n        \\'\\'\\'\\n        keys = self.list_keys()\\n        minions = []\\n        for key, val in six.iteritems(keys):\\n            minions.extend(val)\\n\\n        m_cache = os.path.join(self.opts[\\'cachedir\\'], \\'minions\\')\\n        if os.path.isdir(m_cache):\\n            for minion in os.listdir(m_cache):\\n                if minion not in minions:\\n                    shutil.rmtree(os.path.join(m_cache, minion))\\n\\n        kind = self.opts.get(\\'__role\\', \\'\\')  # application kind\\n        if kind not in kinds.APPL_KINDS:\\n            emsg = (\"Invalid application kind = \\'{0}\\'.\".format(kind))\\n            log.error(emsg + \\'\\\\n\\')\\n            raise ValueError(emsg)\\n        role = self.opts.get(\\'id\\', \\'\\')\\n        if not role:\\n            emsg = (\"Invalid id.\")\\n            log.error(emsg + \"\\\\n\")\\n            raise ValueError(emsg)\\n\\n        name = \"{0}_{1}\".format(role, kind)\\n        road_cache = os.path.join(self.opts[\\'cachedir\\'],\\n                                  \\'raet\\',\\n                                  name,\\n                                  \\'remote\\')\\n        if os.path.isdir(road_cache):\\n            for road in os.listdir(road_cache):\\n                root, ext = os.path.splitext(road)\\n                if ext not in [\\'.json\\', \\'.msgpack\\']:\\n                    continue\\n                prefix, sep, name = root.partition(\\'.\\')\\n                if not name or prefix != \\'estate\\':\\n                    continue\\n                path = os.path.join(road_cache, road)\\n                with salt.utils.fopen(path, \\'rb\\') as fp_:\\n                    if ext == \\'.json\\':\\n                        data = json.load(fp_)\\n                    elif ext == \\'.msgpack\\':\\n                        data = msgpack.load(fp_)\\n                    if data[\\'role\\'] not in minions:\\n                        os.remove(path)', \"def check_master(self):\\n        '''\\n        Log if the master is not running\\n        NOT YET IMPLEMENTED\\n        '''\\n        return True\", 'def status(self, minion_id, pub, verify):\\n        \\'\\'\\'\\n        Accepts the minion id, device id, curve public and verify keys.\\n        If the key is not present, put it in pending and return \"pending\",\\n        If the key has been accepted return \"accepted\"\\n        if the key should be rejected, return \"rejected\"\\n        \\'\\'\\'\\n        acc, pre, rej = self._check_minions_directories()  # pylint: disable=W0632\\n        acc_path = os.path.join(acc, minion_id)\\n        pre_path = os.path.join(pre, minion_id)\\n        rej_path = os.path.join(rej, minion_id)\\n        # open mode is turned on, force accept the key\\n        keydata = {\\n                \\'minion_id\\': minion_id,\\n                \\'pub\\': pub,\\n                \\'verify\\': verify}\\n        if self.opts[\\'open_mode\\']:  # always accept and overwrite\\n            with salt.utils.fopen(acc_path, \\'w+b\\') as fp_:\\n                fp_.write(self.serial.dumps(keydata))\\n                return self.ACC\\n        if os.path.isfile(rej_path):\\n            log.debug(\"Rejection Reason: Keys already rejected.\\\\n\")\\n            return self.REJ\\n        elif os.path.isfile(acc_path):\\n            # The minion id has been accepted, verify the key strings\\n            with salt.utils.fopen(acc_path, \\'rb\\') as fp_:\\n                keydata = self.serial.loads(fp_.read())\\n            if keydata[\\'pub\\'] == pub and keydata[\\'verify\\'] == verify:\\n                return self.ACC\\n            else:\\n                log.debug(\"Rejection Reason: Keys not match prior accepted.\\\\n\")\\n                return self.REJ\\n        elif os.path.isfile(pre_path):\\n            auto_reject = self.auto_key.check_autoreject(minion_id)\\n            auto_sign = self.auto_key.check_autosign(minion_id)\\n            with salt.utils.fopen(pre_path, \\'rb\\') as fp_:\\n                keydata = self.serial.loads(fp_.read())\\n            if keydata[\\'pub\\'] == pub and keydata[\\'verify\\'] == verify:\\n                if auto_reject:\\n                    self.reject(minion_id)\\n                    log.debug(\"Rejection Reason: Auto reject pended.\\\\n\")\\n                    return self.REJ\\n                elif auto_sign:\\n                    self.accept(minion_id)\\n                    return self.ACC\\n                return self.PEND\\n            else:\\n                log.debug(\"Rejection Reason: Keys not match prior pended.\\\\n\")\\n                return self.REJ\\n        # This is a new key, evaluate auto accept/reject files and place\\n        # accordingly\\n        auto_reject = self.auto_key.check_autoreject(minion_id)\\n        auto_sign = self.auto_key.check_autosign(minion_id)\\n        if self.opts[\\'auto_accept\\']:\\n            w_path = acc_path\\n            ret = self.ACC\\n        elif auto_sign:\\n            w_path = acc_path\\n            ret = self.ACC\\n        elif auto_reject:\\n            w_path = rej_path\\n            log.debug(\"Rejection Reason: Auto reject new.\\\\n\")\\n            ret = self.REJ\\n        else:\\n            w_path = pre_path\\n            ret = self.PEND\\n        with salt.utils.fopen(w_path, \\'w+b\\') as fp_:\\n            fp_.write(self.serial.dumps(keydata))\\n            return ret', \"def _get_key_finger(self, path):\\n        '''\\n        Return a sha256 kingerprint for the key\\n        '''\\n        with salt.utils.fopen(path, 'r') as fp_:\\n            keydata = self.serial.loads(fp_.read())\\n            key = 'pub: {0}\\\\nverify: {1}'.format(\\n                    keydata['pub'],\\n                    keydata['verify'])\\n        return hashlib.sha256(key).hexdigest()\", \"def key_str_all(self):\\n        '''\\n        Return all managed key strings\\n        '''\\n        ret = {}\\n        for status, keys in six.iteritems(self.list_keys()):\\n            ret[status] = {}\\n            for key in salt.utils.isorted(keys):\\n                ret[status][key] = self._get_key_str(key, status)\\n        return ret\", \"def accept_all(self):\\n        '''\\n        Accept all keys in pre\\n        '''\\n        keys = self.list_keys()\\n        for key in keys[self.PEND]:\\n            try:\\n                shutil.move(\\n                        os.path.join(\\n                            self.opts['pki_dir'],\\n                            self.PEND,\\n                            key),\\n                        os.path.join(\\n                            self.opts['pki_dir'],\\n                            self.ACC,\\n                            key)\\n                        )\\n            except (IOError, OSError):\\n                pass\\n        return self.list_keys()\", \"def delete_all(self):\\n        '''\\n        Delete all keys\\n        '''\\n        for status, keys in six.iteritems(self.list_keys()):\\n            for key in keys:\\n                try:\\n                    os.remove(os.path.join(self.opts['pki_dir'], status, key))\\n                except (OSError, IOError):\\n                    pass\\n        self.check_minion_cache()\\n        return self.list_keys()\", \"def reject_all(self):\\n        '''\\n        Reject all keys in pre\\n        '''\\n        keys = self.list_keys()\\n        for key in keys[self.PEND]:\\n            try:\\n                shutil.move(\\n                        os.path.join(\\n                            self.opts['pki_dir'],\\n                            self.PEND,\\n                            key),\\n                        os.path.join(\\n                            self.opts['pki_dir'],\\n                            self.REJ,\\n                            key)\\n                        )\\n            except (IOError, OSError):\\n                pass\\n        self.check_minion_cache()\\n        return self.list_keys()\", \"def finger_all(self):\\n        '''\\n        Return fingerprints for all keys\\n        '''\\n        ret = {}\\n        for status, keys in six.iteritems(self.list_keys()):\\n            ret[status] = {}\\n            for key in keys:\\n                if status == 'local':\\n                    path = os.path.join(self.opts['pki_dir'], key)\\n                else:\\n                    path = os.path.join(self.opts['pki_dir'], status, key)\\n                ret[status][key] = self._get_key_finger(path)\\n        return ret\", \"def read_remote(self, minion_id, status=ACC):\\n        '''\\n        Read in a remote key of status\\n        '''\\n        path = os.path.join(self.opts['pki_dir'], status, minion_id)\\n        if not os.path.isfile(path):\\n            return {}\\n        with salt.utils.fopen(path, 'rb') as fp_:\\n            return self.serial.loads(fp_.read())\", \"def write_local(self, priv, sign):\\n        '''\\n        Write the private key and the signing key to a file on disk\\n        '''\\n        keydata = {'priv': priv,\\n                   'sign': sign}\\n        path = os.path.join(self.opts['pki_dir'], 'local.key')\\n        c_umask = os.umask(191)\\n        if os.path.exists(path):\\n            #mode = os.stat(path).st_mode\\n            os.chmod(path, stat.S_IWUSR | stat.S_IRUSR)\\n        with salt.utils.fopen(path, 'w+') as fp_:\\n            fp_.write(self.serial.dumps(keydata))\\n            os.chmod(path, stat.S_IRUSR)\\n        os.umask(c_umask)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def callback(publisher, coord_publisher, cascade, imagemsg):\\n    #\\n    #  Convert the ROS imagemsg to an opencv image.\\n    #\\n    image = cv_bridge.imgmsg_to_cv(imagemsg, 'mono8')\\n\\n    #\\n    #  Blur the image.\\n    #\\n    cv.Smooth(image, image, cv.CV_GAUSSIAN)\\n\\n    #\\n    #  Allocate some storage for the haar detect operation.\\n    #\\n    storage = cv.CreateMemStorage(0)\\n\\n    #\\n    #  Call the face detector function.\\n    #\\n    faces = cv.HaarDetectObjects(image, cascade, storage, 1.2, 2, \\n                                 cv.CV_HAAR_DO_CANNY_PRUNING, (100,100))\", 'def listener(publisher, coord_publisher):\\n    rospy.init_node(\\'face_detector\\', anonymous=True)\\n    #\\n    #  Load the haar cascade.  Note we get the \\n    #  filename from the \"classifier\" parameter\\n    #  that is configured in the launch script.\\n    #\\n    cascadeFileName = rospy.get_param(\"~classifier\")\\n    cascade = cv.Load(cascadeFileName)\\n    rospy.Subscriber(\"/stereo/left/image_rect\", \\n                     Image, \\n                     lambda image: callback(publisher, coord_publisher, cascade, image))\\n    rospy.spin()']}, {'features': [], 'snippets': ['def __init__(self):\\n\\t\\t\"\"\"ドラゴンのモデルのコンストラクタ。\"\"\"\\n\\t\\tif TRACE: print(__name__), self.__init__.__doc__\\n\\n\\t\\tsuper(DragonModel, self).__init__()\\n\\t\\tself._eye_point = [-5.5852450791872 , 3.07847342734 , 15.794105252496]\\n\\t\\tself._sight_point = [0.27455347776413 , 0.20096999406815 , -0.11261999607086]\\n\\t\\tself._up_vector = [0.1018574904194 , 0.98480906061847 , -0.14062775604137]\\n\\t\\tself._fovy = self._default_fovy = 12.642721790235\\n\\n\\t\\tfilename = os.path.join(os.getcwd(), \\'dragon.txt\\')\\n\\t\\tif os.path.exists(filename) and os.path.isfile(filename):\\n\\t\\t\\tpass\\n\\t\\telse:\\n\\t\\t\\turl = \\'http://www.cc.kyoto-su.ac.jp/~atsushi/Programs/Dragon/dragon.txt\\'\\n\\t\\t\\turllib.urlretrieve(url, filename)\\n\\n\\t\\twith open(filename, \"rU\") as a_file:\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\ta_string = a_file.readline()\\n\\t\\t\\t\\tif len(a_string) == 0: break\\n\\t\\t\\t\\ta_list = a_string.split()\\n\\t\\t\\t\\tif len(a_list) == 0: continue\\n\\t\\t\\t\\tfirst_string = a_list[0]\\n\\t\\t\\t\\tif first_string == \"number_of_vertexes\":\\n\\t\\t\\t\\t\\tnumber_of_vertexes = int(a_list[1])\\n\\t\\t\\t\\tif first_string == \"number_of_triangles\":\\n\\t\\t\\t\\t\\tnumber_of_triangles = int(a_list[1])\\n\\t\\t\\t\\tif first_string == \"end_header\":\\n\\t\\t\\t\\t\\tget_tokens = (lambda file: file.readline().split())\\n\\t\\t\\t\\t\\tcollection_of_vertexes = []\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_vertexes):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\ta_vertex = map(float, a_list[0:3])\\n\\t\\t\\t\\t\\t\\tcollection_of_vertexes.append(a_vertex)\\n\\t\\t\\t\\t\\tindex_to_vertex = (lambda index: collection_of_vertexes[index-1])\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_triangles):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\tindexes = map(int, a_list[0:3])\\n\\t\\t\\t\\t\\t\\tvertexes = map(index_to_vertex, indexes)\\n\\t\\t\\t\\t\\t\\ta_tringle = OpenGLTriangle(*vertexes)\\n\\t\\t\\t\\t\\t\\tself._display_object.append(a_tringle)\\n\\n\\t\\treturn', 'def __init__(self):\\n\\t\\t\"\"\"スズメバチのモデルのコンストラクタ。\"\"\"\\n\\t\\tif TRACE: print(__name__), self.__init__.__doc__\\n\\n\\t\\tsuper(WaspModel, self).__init__()\\n\\t\\tself._eye_point = [-5.5852450791872 , 3.07847342734 , 15.794105252496]\\n\\t\\tself._sight_point = [0.19825005531311 , 1.8530999422073 , -0.63795006275177]\\n\\t\\tself._up_vector = [0.070077999093727 , 0.99630606032682 , -0.049631725731267]\\n\\t\\tself._fovy = self._default_fovy = 41.480099231656\\n\\n\\t\\tfilename = os.path.join(os.getcwd(), \\'wasp.txt\\')\\n\\t\\tif os.path.exists(filename) and os.path.isfile(filename):\\n\\t\\t\\tpass\\n\\t\\telse:\\n\\t\\t\\turl = \\'http://www.cc.kyoto-su.ac.jp/~atsushi/Programs/Wasp/wasp.txt\\'\\n\\t\\t\\turllib.urlretrieve(url, filename)\\n\\n\\t\\twith open(filename, \"rU\") as a_file:\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\ta_string = a_file.readline()\\n\\t\\t\\t\\tif len(a_string) == 0: break\\n\\t\\t\\t\\ta_list = a_string.split()\\n\\t\\t\\t\\tif len(a_list) == 0: continue\\n\\t\\t\\t\\tfirst_string = a_list[0]\\n\\t\\t\\t\\tif first_string == \"number_of_vertexes\":\\n\\t\\t\\t\\t\\tnumber_of_vertexes = int(a_list[1])\\n\\t\\t\\t\\tif first_string == \"number_of_polygons\":\\n\\t\\t\\t\\t\\tnumber_of_polygons = int(a_list[1])\\n\\t\\t\\t\\tif first_string == \"end_header\":\\n\\t\\t\\t\\t\\tget_tokens = (lambda file: file.readline().split())\\n\\t\\t\\t\\t\\tcollection_of_vertexes = []\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_vertexes):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\ta_vertex = map(float, a_list[0:3])\\n\\t\\t\\t\\t\\t\\tcollection_of_vertexes.append(a_vertex)\\n\\t\\t\\t\\t\\tindex_to_vertex = (lambda index: collection_of_vertexes[index-1])\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_polygons):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\tnumber_of_indexes = int(a_list[0])\\n\\t\\t\\t\\t\\t\\tindex = number_of_indexes + 1\\n\\t\\t\\t\\t\\t\\tindexes = map(int, a_list[1:index])\\n\\t\\t\\t\\t\\t\\tvertexes = map(index_to_vertex, indexes)\\n\\t\\t\\t\\t\\t\\trgb_color = map(float, a_list[index:index+3])\\n\\t\\t\\t\\t\\t\\ta_polygon = OpenGLPolygon(vertexes, rgb_color)\\n\\t\\t\\t\\t\\t\\tself._display_object.append(a_polygon)\\n\\n\\t\\treturn', 'def default_window_title(self):\\n\\t\\t\"\"\"スズメバチのウィンドウのタイトル(ラベル)を応答する。\"\"\"\\n\\t\\tif TRACE: print(__name__), self.default_window_title.__doc__\\n\\n\\t\\treturn \"Wasp\"', 'def __init__(self):\\n\\t\\t\"\"\"うさぎのモデルのコンストラクタ。\"\"\"\\n\\t\\tif TRACE: print(__name__), self.__init__.__doc__\\n\\n\\t\\tsuper(BunnyModel, self).__init__()\\n\\n\\t\\tfilename = os.path.join(os.getcwd(), \\'bunny.ply\\')\\n\\t\\tif os.path.exists(filename) and os.path.isfile(filename):\\n\\t\\t\\tpass\\n\\t\\telse:\\n\\t\\t\\turl = \\'http://www.cc.kyoto-su.ac.jp/~atsushi/Programs/Bunny/bunny.ply\\'\\n\\t\\t\\turllib.urlretrieve(url, filename)\\n\\n\\t\\twith open(filename, \"rU\") as a_file:\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\ta_string = a_file.readline()\\n\\t\\t\\t\\tif len(a_string) == 0: break\\n\\t\\t\\t\\ta_list = a_string.split()\\n\\t\\t\\t\\tif len(a_list) == 0: continue\\n\\t\\t\\t\\tfirst_string = a_list[0]\\n\\t\\t\\t\\tif first_string == \"element\":\\n\\t\\t\\t\\t\\tsecond_string = a_list[1]\\n\\t\\t\\t\\t\\tif second_string == \"vertex\":\\n\\t\\t\\t\\t\\t\\tnumber_of_vertexes = int(a_list[2])\\n\\t\\t\\t\\t\\tif second_string == \"face\":\\n\\t\\t\\t\\t\\t\\tnumber_of_faces = int(a_list[2])\\n\\t\\t\\t\\tif first_string == \"end_header\":\\n\\t\\t\\t\\t\\tget_tokens = (lambda file: file.readline().split())\\n\\t\\t\\t\\t\\tcollection_of_vertexes = []\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_vertexes):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\ta_vertex = map(float, a_list[0:3])\\n\\t\\t\\t\\t\\t\\tcollection_of_vertexes.append(a_vertex)\\n\\t\\t\\t\\t\\tindex_to_vertex = (lambda index: collection_of_vertexes[index])\\n\\t\\t\\t\\t\\tfor n_th in range(number_of_faces):\\n\\t\\t\\t\\t\\t\\ta_list = get_tokens(a_file)\\n\\t\\t\\t\\t\\t\\tindexes = map(int, a_list[1:4])\\n\\t\\t\\t\\t\\t\\tvertexes = map(index_to_vertex, indexes)\\n\\t\\t\\t\\t\\t\\ta_tringle = OpenGLTriangle(*vertexes)\\n\\t\\t\\t\\t\\t\\tself._display_object.append(a_tringle)\\n\\t\\t\\t\\tif first_string == \"comment\":\\n\\t\\t\\t\\t\\tsecond_string = a_list[1]\\n\\t\\t\\t\\t\\tif second_string == \"eye_point_xyz\":\\n\\t\\t\\t\\t\\t\\tself._eye_point = map(float, a_list[2:5])\\n\\t\\t\\t\\t\\tif second_string == \"sight_point_xyz\":\\n\\t\\t\\t\\t\\t\\tself._sight_point = map(float, a_list[2:5])\\n\\t\\t\\t\\t\\tif second_string == \"up_vector_xyz\":\\n\\t\\t\\t\\t\\t\\tself._up_vector = map(float, a_list[2:5])\\n\\t\\t\\t\\t\\tif second_string == \"zoom_height\" and a_list[3] == \"fovy\":\\n\\t\\t\\t\\t\\t\\tself._fovy = self._default_fovy = float(a_list[4])\\n\\n\\t\\treturn', 'def default_window_title(self):\\n\\t\\t\"\"\"うさぎのウィンドウのタイトル(ラベル)を応答する。\"\"\"\\n\\t\\tif TRACE: print(__name__), self.default_window_title.__doc__\\n\\n\\t\\treturn \"Stanford Bunny\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def as_sql(self, compiler, connection):\\n        lhs, params = compiler.compile(self.lhs)\\n        return 'TIME({})'.format(lhs), params\", 'def output_field(self):\\n        return TimeField()']}, {'features': [], 'snippets': ['def __init__(self, target=None, listeners=None, name=None, service=None, cleanup_method=None,\\n                 heartbeat_secs=10, **kwargs):\\n        \"\"\"\\n        Constructs the control part of an ION process.\\n        Used by the container\\'s IonProcessThreadManager, as part of spawn_process.\\n\\n        @param  target          A callable to run in the PyonThread. If None (typical), will use the target method\\n                                defined in this class.\\n        @param  listeners       A list of listening endpoints attached to this thread.\\n        @param  name            The name of this ION process.\\n        @param  service         An instance of the BaseService derived class which contains the business logic for\\n                                the ION process.\\n        @param  cleanup_method  An optional callable to run when the process is stopping. Runs after all other\\n                                notify_stop calls have run. Should take one param, this instance.\\n        @param  heartbeat_secs  Number of seconds to wait in between heartbeats.\\n        \"\"\"\\n        self._startup_listeners = listeners or []\\n        self.listeners          = []\\n        self._listener_map      = {}\\n        self.name               = name\\n        self.service            = service\\n        self._cleanup_method    = cleanup_method\\n\\n        self.thread_manager     = ThreadManager(failure_notify_callback=self._child_failed)  # bubbles up to main thread manager\\n        self._dead_children     = []        # save any dead children for forensics\\n        self._ctrl_thread       = None\\n        self._ctrl_queue        = Queue()\\n        self._ready_control     = Event()\\n        self._errors            = []\\n        self._ctrl_current      = None      # set to the AR generated by _routing_call when in the context of a call\\n\\n        # processing vs idle time (ms)\\n        self._start_time        = None\\n        self._proc_time         = 0   # busy time since start\\n        self._proc_time_prior   = 0   # busy time at the beginning of the prior interval\\n        self._proc_time_prior2  = 0   # busy time at the beginning of 2 interval\\'s ago\\n        self._proc_interval_num = 0   # interval num of last record\\n\\n        # for heartbeats, used to detect stuck processes\\n        self._heartbeat_secs    = heartbeat_secs    # amount of time to wait between heartbeats\\n        self._heartbeat_stack   = None              # stacktrace of last heartbeat\\n        self._heartbeat_time    = None              # timestamp of heart beat last matching the current op\\n        self._heartbeat_op      = None              # last operation (by AR)\\n        self._heartbeat_count   = 0                 # number of times this operation has been seen consecutively\\n\\n        self._log_call_exception = CFG.get_safe(\"container.process.log_exceptions\", False)\\n        self._log_call_dbstats = CFG.get_safe(\"container.process.log_dbstats\", False)\\n        self._warn_call_dbstmt_threshold = CFG.get_safe(\"container.process.warn_dbstmt_threshold\", 0)\\n\\n        PyonThread.__init__(self, target=target, **kwargs)', 'def time_stats(self):\\n        \"\"\"\\n        Returns a 5-tuple of (total time, idle time, processing time, time since prior interval start,\\n        busy since prior interval start), all in ms (int).\\n        \"\"\"\\n        now = get_ion_ts_millis()\\n        running_time = now - self._start_time\\n        idle_time = running_time - self._proc_time\\n\\n        cur_interval = now / STAT_INTERVAL_LENGTH\\n        now_since_prior = now - (cur_interval - 1) * STAT_INTERVAL_LENGTH\\n\\n        if cur_interval == self._proc_interval_num:\\n            proc_time_since_prior = self._proc_time-self._proc_time_prior2\\n        elif cur_interval-1 == self._proc_interval_num:\\n            proc_time_since_prior = self._proc_time-self._proc_time_prior\\n        else:\\n            proc_time_since_prior = 0\\n\\n        return (running_time, idle_time, self._proc_time, now_since_prior, proc_time_since_prior)', 'def add_endpoint(self, listener, activate=True):\\n        \"\"\"\\n        Adds a listening endpoint to be managed by this ION process.\\n\\n        Spawns the listen loop and sets the routing call to synchronize incoming messages\\n        here. If this process hasn\\'t been started yet, adds it to the list of listeners\\n        to start on startup.\\n        @param activate  If True (default), start consuming from listener\\n        \"\"\"\\n        if self.proc:\\n            listener.routing_call = self._routing_call\\n\\n            if self.name:\\n                svc_name = \"unnamed-service\"\\n                if self.service is not None and hasattr(self.service, \\'name\\'):\\n                    svc_name = self.service.name\\n\\n                listen_thread_name = \"%s-%s-listen-%s\" % (svc_name, self.name, len(self.listeners)+1)\\n            else:\\n                listen_thread_name = \"unknown-listener-%s\" % (len(self.listeners)+1)\\n\\n            listen_thread = self.thread_manager.spawn(listener.listen, thread_name=listen_thread_name, activate=activate)\\n            listen_thread.proc._glname = \"ION Proc listener %s\" % listen_thread_name\\n            self._listener_map[listener] = listen_thread\\n            self.listeners.append(listener)\\n        else:\\n            self._startup_listeners.append(listener)', 'def target(self, *args, **kwargs):\\n        \"\"\"\\n        Entry point for the main process greenlet.\\n        Setup the base properties for this process (mainly the control thread).\\n        \"\"\"\\n        if self.name:\\n            threading.current_thread().name = \"%s-target\" % self.name\\n\\n        # start time\\n        self._start_time = get_ion_ts_millis()\\n        self._proc_interval_num = self._start_time / STAT_INTERVAL_LENGTH\\n\\n        # spawn control flow loop\\n        self._ctrl_thread = self.thread_manager.spawn(self._control_flow)\\n        self._ctrl_thread.proc._glname = \"ION Proc CL %s\" % self.name\\n\\n        # wait on control flow loop, heartbeating as appropriate\\n        while not self._ctrl_thread.ev_exit.wait(timeout=self._heartbeat_secs):\\n            hbst = self.heartbeat()\\n\\n            if not all(hbst):\\n                log.warn(\"Heartbeat status for process %s returned %s\", self, hbst)\\n                if self._heartbeat_stack is not None:\\n                    stack_out = \"\".join(traceback.format_list(self._heartbeat_stack))\\n                else:\\n                    stack_out = \"N/A\"\\n\\n                #raise PyonHeartbeatError(\"Heartbeat failed: %s, stacktrace:\\\\n%s\" % (hbst, stack_out))\\n                log.warn(\"Heartbeat failed: %s, stacktrace:\\\\n%s\", hbst, stack_out)\\n\\n        # this is almost a no-op as we don\\'t fall out of the above loop without\\n        # exiting the ctrl_thread, but having this line here makes testing much easier.\\n        self._ctrl_thread.join()', 'def has_pending_call(self, ar):\\n        \"\"\"\\n        Returns true if the call (keyed by the AsyncResult returned by _routing_call) is still pending.\\n        \"\"\"\\n        for _, qar, _, _, _, _ in self._ctrl_queue.queue:\\n            if qar == ar:\\n                return True\\n\\n        return False', 'def _interrupt_control_thread(self):\\n        \"\"\"\\n        Signal the control flow thread that it needs to abort processing, likely due to a timeout.\\n        \"\"\"\\n        self._ctrl_thread.proc.kill(exception=OperationInterruptedException, block=False)', 'def _control_flow(self):\\n        \"\"\"\\n        Entry point for process control thread of execution.\\n\\n        This method is run by the control greenlet for each ION process. Listeners attached\\n        to the process, either RPC Servers or Subscribers, synchronize calls to the process\\n        by placing call requests into the queue by calling _routing_call.\\n\\n        This method blocks until there are calls to be made in the synchronized queue, and\\n        then calls from within this greenlet.  Any exception raised is caught and re-raised\\n        in the greenlet that originally scheduled the call.  If successful, the AsyncResult\\n        created at scheduling time is set with the result of the call.\\n        \"\"\"\\n        svc_name = getattr(self.service, \"name\", \"unnamed-service\") if self.service else \"unnamed-service\"\\n        proc_id = getattr(self.service, \"id\", \"unknown-pid\") if self.service else \"unknown-pid\"\\n        if self.name:\\n            threading.current_thread().name = \"%s-%s\" % (svc_name, self.name)\\n        thread_base_name = threading.current_thread().name\\n\\n        self._ready_control.set()\\n\\n        for calltuple in self._ctrl_queue:\\n            calling_gl, ar, call, callargs, callkwargs, context = calltuple\\n            request_id = (context or {}).get(\"request-id\", None)\\n            if request_id:\\n                threading.current_thread().name = thread_base_name + \"-\" + str(request_id)\\n            #log.debug(\"control_flow making call: %s %s %s (has context: %s)\", call, callargs, callkwargs, context is not None)\\n\\n            res = None\\n            start_proc_time = get_ion_ts_millis()\\n            self._record_proc_time(start_proc_time)\\n\\n            # check context for expiration\\n            if context is not None and \\'reply-by\\' in context:\\n                if start_proc_time >= int(context[\\'reply-by\\']):\\n                    log.info(\"control_flow: attempting to process message already exceeding reply-by, ignore\")\\n\\n                    # raise a timeout in the calling thread to allow endpoints to continue processing\\n                    e = IonTimeout(\"Reply-by time has already occurred (reply-by: %s, op start time: %s)\" % (context[\\'reply-by\\'], start_proc_time))\\n                    calling_gl.kill(exception=e, block=False)\\n\\n                    continue\\n\\n            # If ar is set, means it is cancelled\\n            if ar.ready():\\n                log.info(\"control_flow: attempting to process message that has been cancelled, ignore\")\\n                continue\\n\\n            init_db_stats()\\n            try:\\n                # ******************************************************************\\n                # ****** THIS IS WHERE THE RPC OPERATION/SERVICE CALL IS MADE ******\\n\\n                with self.service.push_context(context), \\\\\\n                     self.service.container.context.push_context(context):\\n                    self._ctrl_current = ar\\n                    res = call(*callargs, **callkwargs)\\n\\n                # ****** END CALL, EXCEPTION HANDLING FOLLOWS                 ******\\n                # ******************************************************************\\n\\n            except OperationInterruptedException:\\n                # endpoint layer takes care of response as it\\'s the one that caused this\\n                log.debug(\"Operation interrupted\")\\n                pass\\n\\n            except Exception as e:\\n                if self._log_call_exception:\\n                    log.exception(\"PROCESS exception: %s\" % e.message)\\n\\n                # Raise the exception in the calling greenlet.\\n                # Try decorating the args of the exception with the true traceback -\\n                # this should be reported by ThreadManager._child_failed\\n                exc = PyonThreadTraceback(\"IonProcessThread _control_flow caught an exception \"\\n                                          \"(call: %s, *args %s, **kwargs %s, context %s)\\\\n\"\\n                                          \"True traceback captured by IonProcessThread\\' _control_flow:\\\\n\\\\n%s\" % (\\n                                          call, callargs, callkwargs, context, traceback.format_exc()))\\n                e.args = e.args + (exc,)\\n\\n                if isinstance(e, (TypeError, IonException)):\\n                    # Pass through known process exceptions, in particular IonException\\n                    calling_gl.kill(exception=e, block=False)\\n                else:\\n                    # Otherwise, wrap unknown, forward and hopefully we can continue on our way\\n                    self._errors.append((call, callargs, callkwargs, context, e, exc))\\n\\n                    log.warn(exc)\\n                    log.warn(\"Attempting to continue...\")\\n\\n                    # Note: Too large exception string will crash the container (when passed on as msg header).\\n                    exception_str = str(exc)\\n                    if len(exception_str) > 10000:\\n                        exception_str = (\\n                            \"Exception string representation too large. \"\\n                            \"Begin and end of the exception:\\\\n\"\\n                            + exception_str[:2000] + \"\\\\n...\\\\n\" + exception_str[-2000:]\\n                        )\\n                    calling_gl.kill(exception=ContainerError(exception_str), block=False)\\n            finally:\\n                try:\\n                    # Compute statistics\\n                    self._compute_proc_stats(start_proc_time)\\n\\n                    db_stats = get_db_stats()\\n                    if db_stats:\\n                        if self._warn_call_dbstmt_threshold > 0 and db_stats.get(\"count.all\", 0) >= self._warn_call_dbstmt_threshold:\\n                            stats_str = \", \".join(\"{}={}\".format(k, db_stats[k]) for k in sorted(db_stats.keys()))\\n                            log.warn(\"PROC_OP \\'%s.%s\\' EXCEEDED DB THRESHOLD. stats=%s\", svc_name, call.__name__, stats_str)\\n                        elif self._log_call_dbstats:\\n                            stats_str = \", \".join(\"{}={}\".format(k, db_stats[k]) for k in sorted(db_stats.keys()))\\n                            log.info(\"PROC_OP \\'%s.%s\\' DB STATS: %s\", svc_name, call.__name__, stats_str)\\n                    clear_db_stats()\\n\\n                    if stats_callback:\\n                        stats_callback(proc_id=proc_id, proc_name=self.name, svc=svc_name, op=call.__name__,\\n                                       request_id=request_id, context=context,\\n                                       db_stats=db_stats, proc_stats=self.time_stats, result=res, exc=None)\\n                except Exception:\\n                    log.exception(\"Error computing process call stats\")\\n\\n                self._ctrl_current = None\\n                threading.current_thread().name = thread_base_name\\n\\n            # Set response in AsyncEvent of caller (endpoint greenlet)\\n            ar.set(res)', 'def _compute_proc_stats(self, start_proc_time):\\n        cur_time = get_ion_ts_millis()\\n        self._record_proc_time(cur_time)\\n        proc_time = cur_time - start_proc_time\\n        self._proc_time += proc_time', 'def _notify_stop(self):\\n        \"\"\"\\n        Called when the process is about to be shut down.\\n\\n        Instructs all listeners to close, puts a StopIteration into the synchronized queue,\\n        and waits for the listeners to close and for the control queue to exit.\\n        \"\"\"\\n        for listener in self.listeners:\\n            try:\\n                listener.close()\\n            except Exception as ex:\\n                tb = traceback.format_exc()\\n                log.warn(\"Could not close listener, attempting to ignore: %s\\\\nTraceback:\\\\n%s\", ex, tb)\\n\\n        self._ctrl_queue.put(StopIteration)\\n\\n        # wait_children will join them and then get() them, which may raise an exception if any of them\\n        # died with an exception.\\n        self.thread_manager.wait_children(30)\\n\\n        PyonThread._notify_stop(self)\\n\\n        # run the cleanup method if we have one\\n        if self._cleanup_method is not None:\\n            try:\\n                self._cleanup_method(self)\\n            except Exception as ex:\\n                log.warn(\"Cleanup method error, attempting to ignore: %s\\\\nTraceback: %s\", ex, traceback.format_exc())', 'def _create_thread(self, target=None, **kwargs):\\n        return IonProcessThread(target=target, heartbeat_secs=self.heartbeat_secs, **kwargs)', 'def call_process(self, message, stream_route, stream_id):\\n        \"\"\"\\n        Handles pre-processing of packet and process work\\n        \"\"\"\\n        self.process(message)', 'def get_ion_actor_id(process):\\n    \"\"\"Given an ION process, return the ion-actor-id from the context, if set and present\"\"\"\\n    ion_actor_id = None\\n    if process:\\n        ctx = process.get_context()\\n        ion_actor_id = ctx.get(MSG_HEADER_ACTOR, None) if ctx else None\\n    return ion_actor_id']}, {'features': [], 'snippets': ['def cmd_list(self, argument):\\n        \"\"\"List commands\"\"\"\\n        arg = argument.lower()\\n        index = self.bot.help_index\\n        public = \"public commands  -- %s\" % \" \".join(index[\\'public\\'])\\n        private = \"private commands -- %s\" % \" \".join(index[\\'private\\'])\\n        if \\'all\\' in arg or \\'both\\' in arg:\\n            output = \"\\\\n\".join((public, private))\\n        elif \\'pub\\' in arg or self.target.startswith(\\'#\\'):\\n            output = public\\n        elif \\'priv\\' in arg or not self.target.startswith(\\'#\\'):\\n            output = private\\n        else:\\n            # we shouldn\\'t be here\\n            self.logger.error(\"cmd_list\")\\n            return\\n        self.send(self.target, output)', 'def cmd_help(self, argument):\\n        \"\"\"Get help on a command or module\"\"\"\\n        arg = argument.lower()\\n        index = self.bot.help_index\\n        target = self.target\\n        args = arg.split()\\n        if not args:\\n            s = \"usage: help <command> [public|private] / help module <module>\"\\n            self.send(target, s)\\n        elif args[0] == \\'module\\':\\n            args.pop(0)\\n            if not args:\\n                self.send(target, \"usage: help module <module>\")\\n            else:\\n                help_item = index[\\'modules\\'].get(args[0])\\n                if help_item:\\n                    self.send(target, help_item[\\'summary\\'])\\n                else:\\n                    self.send(target, _(\"No help for %s\"), args[0])\\n        else:\\n            args.append(\"\")\\n            cmd = args.pop(0)\\n            cmd_type = args.pop(0)\\n            if \\'pu\\' in cmd_type or self.target.startswith(\\'#\\'):\\n                cmd_type = \\'public\\'\\n            elif \\'pr\\' in cmd_type or not self.target.startswith(\\'#\\'):\\n                cmd_type = \\'private\\'\\n            else:\\n                # we shouldn\\'t be here\\n                self.logger.error(\"cmd_list\")\\n                return\\n            help_item = index[cmd_type].get(cmd)\\n            if help_item:\\n                self.send(target, index[cmd_type][cmd][\\'summary\\'])\\n            else:\\n                self.send(target, _(\"No help for %s\"), cmd)']}, {'features': [], 'snippets': ['def main(): \\n    app = QtGui.QApplication(sys.argv)\\n    window=PreprocessDialog()']}, {'features': [], 'snippets': [\"def test_correct_output(self):\\n        hfmt = HtmlFormatter(nowrap=True)\\n        houtfile = StringIO.StringIO()\\n        hfmt.format(tokensource, houtfile)\\n\\n        nfmt = NullFormatter()\\n        noutfile = StringIO.StringIO()\\n        nfmt.format(tokensource, noutfile)\\n\\n        stripped_html = re.sub('<.*?>', '', houtfile.getvalue())\\n        escaped_text = escape_html(noutfile.getvalue())\\n        self.assertEquals(stripped_html, escaped_text)\", 'def test_all_options(self):\\n        for optdict in [dict(nowrap=True),\\n                        dict(linenos=True),\\n                        dict(linenos=True, full=True),\\n                        dict(linenos=True, full=True, noclasses=True)]:\\n\\n            outfile = StringIO.StringIO()\\n            fmt = HtmlFormatter(**optdict)\\n            fmt.format(tokensource, outfile)', \"def test_get_style_defs(self):\\n        fmt = HtmlFormatter()\\n        sd = fmt.get_style_defs()\\n        self.assert_(sd.startswith('.'))\\n\\n        fmt = HtmlFormatter(cssclass='foo')\\n        sd = fmt.get_style_defs()\\n        self.assert_(sd.startswith('.foo'))\\n        sd = fmt.get_style_defs('.bar')\\n        self.assert_(sd.startswith('.bar'))\\n        sd = fmt.get_style_defs(['.bar', '.baz'])\\n        fl = sd.splitlines()[0]\\n        self.assert_('.bar' in fl and '.baz' in fl)\"]}, {'features': [], 'snippets': [\"def test_column_list_select2(self):\\n        # make sure SDC copies the columns like Pandas does\\n        def test_impl(df):\\n            df2 = df[['A']]\\n            df2['A'] += 10\\n            return df2.A, df.A\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        df = pd.DataFrame(\\n            {'A': np.arange(n), 'B': np.ones(n), 'C': np.random.ranf(n)})\\n        np.testing.assert_array_equal(hpat_func(df.copy())[1], test_impl(df)[1])\", \"def test_pd_DataFrame_from_series_par(self):\\n        def test_impl(n):\\n            S1 = pd.Series(np.ones(n))\\n            S2 = pd.Series(np.random.ranf(n))\\n            df = pd.DataFrame({'A': S1, 'B': S2})\\n            return df.A.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        self.assertEqual(count_parfor_OneDs(), 1)\", \"def test_getitem_bool_series(self):\\n        def test_impl(df):\\n            return df['A'][df['B']].values\\n\\n        hpat_func = self.jit(test_impl)\\n        df = pd.DataFrame({'A': [1, 2, 3], 'B': [True, False, True]})\\n        np.testing.assert_array_equal(test_impl(df), hpat_func(df))\", \"def test_fillna(self):\\n        def test_impl():\\n            A = np.array([1., 2., 3.])\\n            A[0] = np.nan\\n            df = pd.DataFrame({'A': A})\\n            B = df.A.fillna(5.0)\\n            return B.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\", \"def test_fillna_inplace(self):\\n        def test_impl():\\n            A = np.array([1., 2., 3.])\\n            A[0] = np.nan\\n            df = pd.DataFrame({'A': A})\\n            df.A.fillna(5.0, inplace=True)\\n            return df.A.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\", \"def test_column_mean(self):\\n        def test_impl():\\n            A = np.array([1., 2., 3.])\\n            A[0] = np.nan\\n            df = pd.DataFrame({'A': A})\\n            return df.A.mean()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\", \"def test_column_var(self):\\n        def test_impl():\\n            A = np.array([1., 2., 3.])\\n            A[0] = 4.0\\n            df = pd.DataFrame({'A': A})\\n            return df.A.var()\\n\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\", \"def test_column_std(self):\\n        def test_impl():\\n            A = np.array([1., 2., 3.])\\n            A[0] = 4.0\\n            df = pd.DataFrame({'A': A})\\n            return df.A.std()\\n\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\", \"def test_column_map(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n)})\\n            df['B'] = df.A.map(lambda a: 2 * a)\\n            return df.B.sum()\\n\\n        n = 121\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\", \"def test_column_map_arg(self):\\n        def test_impl(df):\\n            df['B'] = df.A.map(lambda a: 2 * a)\\n            return\\n\\n        n = 121\\n        df1 = pd.DataFrame({'A': np.arange(n)})\\n        df2 = pd.DataFrame({'A': np.arange(n)})\\n        hpat_func = self.jit(test_impl)\\n        hpat_func(df1)\\n        self.assertTrue(hasattr(df1, 'B'))\\n        test_impl(df2)\\n        np.testing.assert_equal(df1.B.values, df2.B.values)\", \"def test_cumsum(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})\\n            Ac = df.A.cumsum()\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_array_OneDs(), 2)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        self.assertEqual(count_parfor_OneDs(), 2)\\n        self.assertTrue(dist_IR_contains('dist_cumsum'))\", \"def test_column_distribution(self):\\n        # make sure all column calls are distributed\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})\\n            df.A.fillna(5.0, inplace=True)\\n            DF = df.A.fillna(5.0)\\n            s = DF.sum()\\n            m = df.A.mean()\\n            v = df.A.var()\\n            t = df.A.std()\\n            Ac = df.A.cumsum()\\n            return Ac.sum() + s + m + v + t\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        self.assertTrue(dist_IR_contains('dist_cumsum'))\", \"def test_quantile_parallel(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})\\n            return df.A.quantile(.25)\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_quantile_parallel_float_nan(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float32)})\\n            df.A[0:100] = np.nan\\n            df.A[200:331] = np.nan\\n            return df.A.quantile(.25)\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_quantile_parallel_int(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(0, n, 1, np.int32)})\\n            return df.A.quantile(.25)\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_quantile_sequential(self):\\n        def test_impl(A):\\n            df = pd.DataFrame({'A': A})\\n            return df.A.quantile(.25)\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        A = np.arange(0, n, 1, np.float64)\\n        np.testing.assert_almost_equal(hpat_func(A), test_impl(A))\", \"def test_nunique(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n)})\\n            df.A[2] = 0\\n            return df.A.nunique()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        # test compile again for overload related issues\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\", \"def test_nunique_parallel(self):\\n        # TODO: test without file\\n        def test_impl():\\n            df = pq.read_table('example.parquet').to_pandas()\\n            return df.four.nunique()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\\n        # test compile again for overload related issues\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\", \"def test_nunique_str(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': ['aa', 'bb', 'aa', 'cc', 'cc']})\\n            return df.A.nunique()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        # test compile again for overload related issues\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\", \"def test_nunique_str_parallel(self):\\n        # TODO: test without file\\n        def test_impl():\\n            df = pq.read_table('example.parquet').to_pandas()\\n            return df.two.nunique()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\\n        # test compile again for overload related issues\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\", \"def test_unique_parallel(self):\\n        # TODO: test without file\\n        def test_impl():\\n            df = pq.read_table('example.parquet').to_pandas()\\n            return (df.four.unique() == 3.0).sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\", \"def test_unique_str_parallel(self):\\n        # TODO: test without file\\n        def test_impl():\\n            df = pq.read_table('example.parquet').to_pandas()\\n            return (df.two.unique() == 'foo').sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\", \"def test_describe(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(0, n, 1, np.float64)})\\n            return df.A.describe()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 1001\\n        hpat_func(n)\\n        # XXX: test actual output\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_str_contains_regex(self):\\n        def test_impl():\\n            A = StringArray(['ABC', 'BB', 'ADEF'])\\n            df = pd.DataFrame({'A': A})\\n            B = df.A.str.contains('AB*', regex=True)\\n            return B.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), 2)\", \"def test_str_contains_noregex(self):\\n        def test_impl():\\n            A = StringArray(['ABC', 'BB', 'ADEF'])\\n            df = pd.DataFrame({'A': A})\\n            B = df.A.str.contains('BB', regex=False)\\n            return B.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), 1)\", \"def test_str_replace_regex(self):\\n        def test_impl(df):\\n            return df.A.str.replace('AB*', 'EE', regex=True)\\n\\n        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_replace_noregex(self):\\n        def test_impl(df):\\n            return df.A.str.replace('AB', 'EE', regex=False)\\n\\n        df = pd.DataFrame({'A': ['ABCC', 'CABBD']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_replace_regex_parallel(self):\\n        def test_impl(df):\\n            B = df.A.str.replace('AB*', 'EE', regex=True)\\n            return B\\n\\n        n = 5\\n        A = ['ABCC', 'CABBD', 'CCD', 'CCDAABB', 'ED']\\n        start, end = get_start_end(n)\\n        df = pd.DataFrame({'A': A[start:end]})\\n        hpat_func = self.jit(distributed={'df', 'B'})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\\n        self.assertEqual(count_array_REPs(), 3)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_str_split(self):\\n        def test_impl(df):\\n            return df.A.str.split(',')\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_split_default(self):\\n        def test_impl(df):\\n            return df.A.str.split()\\n\\n        df = pd.DataFrame({'A': ['AB CC', 'C ABB D', 'G ', ' ', 'g\\\\t f']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_split_filter(self):\\n        def test_impl(df):\\n            B = df.A.str.split(',')\\n            df2 = pd.DataFrame({'B': B})\\n            return df2[df2.B.str.len() > 1]\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D', 'G', '', 'g,f']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_frame_equal(\\n            hpat_func(df), test_impl(df).reset_index(drop=True))\", \"def test_str_split_box_df(self):\\n        def test_impl(df):\\n            return pd.DataFrame({'B': df.A.str.split(',')})\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df).B, test_impl(df).B, check_names=False)\", \"def test_str_split_unbox_df(self):\\n        def test_impl(df):\\n            return df.A.iloc[0]\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})\\n        df2 = pd.DataFrame({'A': df.A.str.split(',')})\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(df2), test_impl(df2))\", \"def test_str_split_bool_index(self):\\n        def test_impl(df):\\n            C = df.A.str.split(',')\\n            return C[df.B == 'aa']\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D'], 'B': ['aa', 'bb']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_split_parallel(self):\\n        def test_impl(df):\\n            B = df.A.str.split(',')\\n            return B\\n\\n        n = 5\\n        start, end = get_start_end(n)\\n        A = ['AB,CC', 'C,ABB,D', 'CAD', 'CA,D', 'AA,,D']\\n        df = pd.DataFrame({'A': A[start:end]})\\n        hpat_func = self.jit(distributed={'df', 'B'})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\\n        self.assertEqual(count_array_REPs(), 3)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_str_get(self):\\n        def test_impl(df):\\n            B = df.A.str.split(',')\\n            return B.str.get(1)\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_split(self):\\n        def test_impl(df):\\n            return df.A.str.split(',')\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_get_parallel(self):\\n        def test_impl(df):\\n            A = df.A.str.split(',')\\n            B = A.str.get(1)\\n            return B\\n\\n        n = 5\\n        start, end = get_start_end(n)\\n        A = ['AB,CC', 'C,ABB,D', 'CAD,F', 'CA,D', 'AA,,D']\\n        df = pd.DataFrame({'A': A[start:end]})\\n        hpat_func = self.jit(distributed={'df', 'B'})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\\n        self.assertEqual(count_array_REPs(), 3)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_str_get_to_numeric(self):\\n        def test_impl(df):\\n            B = df.A.str.split(',')\\n            C = pd.to_numeric(B.str.get(1), errors='coerce')\\n            return C\\n\\n        df = pd.DataFrame({'A': ['AB,12', 'C,321,D']})\\n        hpat_func = self.jit(locals={'C': types.int64[:]})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_flatten(self):\\n        def test_impl(df):\\n            A = df.A.str.split(',')\\n            return pd.Series(list(itertools.chain(*A)))\\n\\n        df = pd.DataFrame({'A': ['AB,CC', 'C,ABB,D']})\\n        hpat_func = self.jit(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_str_flatten_parallel(self):\\n        def test_impl(df):\\n            A = df.A.str.split(',')\\n            B = pd.Series(list(itertools.chain(*A)))\\n            return B\\n\\n        n = 5\\n        start, end = get_start_end(n)\\n        A = ['AB,CC', 'C,ABB,D', 'CAD', 'CA,D', 'AA,,D']\\n        df = pd.DataFrame({'A': A[start:end]})\\n        hpat_func = self.jit(distributed={'df', 'B'})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\\n        self.assertEqual(count_array_REPs(), 3)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_to_numeric(self):\\n        def test_impl(df):\\n            B = pd.to_numeric(df.A, errors='coerce')\\n            return B\\n\\n        df = pd.DataFrame({'A': ['123.1', '331.2']})\\n        hpat_func = self.jit(locals={'B': types.float64[:]})(test_impl)\\n        pd.testing.assert_series_equal(\\n            hpat_func(df), test_impl(df), check_names=False)\", \"def test_1D_Var_len(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n), 'B': np.arange(n) + 1.0})\\n            df1 = df[df.A > 5]\\n            return len(df1.B)\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_rolling1(self):\\n        # size 3 without unroll\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n), 'B': np.random.ranf(n)})\\n            Ac = df.A.rolling(3).sum()\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 121\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        # size 7 with unroll\\n\\n        def test_impl_2(n):\\n            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})\\n            Ac = df.A.rolling(7).sum()\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 121\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_rolling2(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})\\n            df['moving average'] = df.A.rolling(window=5, center=True).mean()\\n            return df['moving average'].sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 121\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_rolling3(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})\\n            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 121\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_shift1(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})\\n            Ac = df.A.shift(1)\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_shift2(self):\\n        def test_impl(n):\\n            df = pd.DataFrame({'A': np.arange(n) + 1.0, 'B': np.random.ranf(n)})\\n            Ac = df.A.pct_change(1)\\n            return Ac.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_df_input(self):\\n        def test_impl(df):\\n            return df.B.sum()\\n\\n        n = 121\\n        df = pd.DataFrame({'A': np.ones(n), 'B': np.random.ranf(n)})\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))\", \"def test_df_input2(self):\\n        def test_impl(df):\\n            C = df.B == 'two'\\n            return C.sum()\\n\\n        n = 11\\n        df = pd.DataFrame({'A': np.random.ranf(3 * n), 'B': ['one', 'two', 'three'] * n})\\n        hpat_func = self.jit(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))\", \"def test_df_input_dist1(self):\\n        def test_impl(df):\\n            return df.B.sum()\\n\\n        n = 121\\n        A = [3, 4, 5, 6, 1]\\n        B = [5, 6, 2, 1, 3]\\n        n = 5\\n        start, end = get_start_end(n)\\n        df = pd.DataFrame({'A': A, 'B': B})\\n        df_h = pd.DataFrame({'A': A[start:end], 'B': B[start:end]})\\n        hpat_func = self.jit(distributed={'df'})(test_impl)\\n        np.testing.assert_almost_equal(hpat_func(df_h), test_impl(df))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_concat(self):\\n        def test_impl(n):\\n            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})\\n            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})\\n            df3 = pd.concat([df1, df2])\\n            return df3.A.sum() + df3.key2.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        n = 11111\\n        self.assertEqual(hpat_func(n), test_impl(n))\", \"def test_concat_str(self):\\n        def test_impl():\\n            df1 = pq.read_table('example.parquet').to_pandas()\\n            df2 = pq.read_table('example.parquet').to_pandas()\\n            A3 = pd.concat([df1, df2])\\n            return (A3.two == 'foo').sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_concat_series(self):\\n        def test_impl(n):\\n            df1 = pd.DataFrame({'key1': np.arange(n), 'A': np.arange(n) + 1.0})\\n            df2 = pd.DataFrame({'key2': n - np.arange(n), 'A': n + np.arange(n) + 1.0})\\n            A3 = pd.concat([df1.A, df2.A])\\n            return A3.sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\\n        n = 11111\\n        self.assertEqual(hpat_func(n), test_impl(n))\", \"def test_concat_series_str(self):\\n        def test_impl():\\n            df1 = pq.read_table('example.parquet').to_pandas()\\n            df2 = pq.read_table('example.parquet').to_pandas()\\n            A3 = pd.concat([df1.two, df2.two])\\n            return (A3 == 'foo').sum()\\n\\n        hpat_func = self.jit(test_impl)\\n        self.assertEqual(hpat_func(), test_impl())\\n        self.assertEqual(count_array_REPs(), 0)\\n        self.assertEqual(count_parfor_REPs(), 0)\", \"def test_intraday(self):\\n        def test_impl(nsyms):\\n            max_num_days = 100\\n            all_res = 0.0\\n            for i in sdc.prange(nsyms):\\n                s_open = 20 * np.ones(max_num_days)\\n                s_low = 28 * np.ones(max_num_days)\\n                s_close = 19 * np.ones(max_num_days)\\n                df = pd.DataFrame({'Open': s_open, 'Low': s_low, 'Close': s_close})\\n                df['Stdev'] = df['Close'].rolling(window=90).std()\\n                df['Moving Average'] = df['Close'].rolling(window=20).mean()\\n                df['Criteria1'] = (df['Open'] - df['Low'].shift(1)) < -df['Stdev']\\n                df['Criteria2'] = df['Open'] > df['Moving Average']\\n                df['BUY'] = df['Criteria1'] & df['Criteria2']\\n                df['Pct Change'] = (df['Close'] - df['Open']) / df['Open']\\n                df['Rets'] = df['Pct Change'][df['BUY']]\\n                all_res += df['Rets'].mean()\\n            return all_res\\n\\n        hpat_func = self.jit(test_impl)\\n        n = 11\\n        self.assertEqual(hpat_func(n), test_impl(n))\\n        self.assertEqual(count_array_OneDs(), 0)\\n        self.assertEqual(count_parfor_OneDs(), 1)\", \"def test_var_dist1(self):\\n        def test_impl(A, B):\\n            df = pd.DataFrame({'A': A, 'B': B})\\n            df2 = df.groupby('A', as_index=False)['B'].sum()\\n            # TODO: fix handling of df setitem to force match of array dists\\n            # probably with a new node that is appended to the end of basic block\\n            # df2['C'] = np.full(len(df2.B), 3, np.int8)\\n            # TODO: full_like for Series\\n            df2['C'] = np.full_like(df2.B.values, 3, np.int8)\\n            return df2\\n\\n        A = np.array([1, 1, 2, 3])\\n        B = np.array([3, 4, 5, 6])\\n        hpat_func = self.jit(locals={'A:input': 'distributed',\\n                                     'B:input': 'distributed', 'df2:return': 'distributed'})(test_impl)\\n        start, end = get_start_end(len(A))\\n        df2 = hpat_func(A[start:end], B[start:end])\\n        # TODO:\\n        # pd.testing.assert_frame_equal(\\n        #     hpat_func(A[start:end], B[start:end]), test_impl(A, B))\"]}, {'features': [], 'snippets': [\"def test_scheme():\\n\\n    # does not raise NotImplementedError\\n    UrlPath('/dev/null').touch()\", \"def test_scheme_not_listed():\\n\\n    with pytest.raises(NotImplementedError):\\n        UrlPath('test:///tmp/test').touch()\"]}, {'features': [], 'snippets': [\"def set_status_and_headers():\\n    headers = {'Location': 'http://example.com/456'}\\n    return {'example': 'content'}, status.HTTP_201_CREATED, headers\", \"def set_headers():\\n    headers = {'Location': 'http://example.com/456'}\\n    return {'example': 'content'}, headers\", \"def make_response_view():\\n    response = make_response({'example': 'content'})\\n    response.headers['Location'] = 'http://example.com/456'\\n    return response\", 'def api_exception():\\n    raise exceptions.PermissionDenied()', 'def abort_view():\\n    abort(status.HTTP_403_FORBIDDEN)', 'def options_view():\\n    return {}', \"def accepted_media_type():\\n    return {'accepted_media_type': str(request.accepted_media_type)}\", 'def test_set_status_and_headers(self):\\n        with app.test_client() as client:\\n            response = client.get(\\'/set_status_and_headers/\\')\\n            self.assertEqual(response.status_code, status.HTTP_201_CREATED)\\n            self.assertEqual(response.headers[\\'Location\\'], \\'http://example.com/456\\')\\n            self.assertEqual(response.content_type, \\'application/json\\')\\n            expected = \\'{\"example\": \"content\"}\\'\\n            self.assertEqual(response.get_data().decode(\\'utf8\\'), expected)', 'def test_make_response(self):\\n        with app.test_client() as client:\\n            response = client.get(\\'/make_response_view/\\')\\n            self.assertEqual(response.content_type, \\'application/json\\')\\n            self.assertEqual(response.headers[\\'Location\\'], \\'http://example.com/456\\')\\n            self.assertEqual(response.content_type, \\'application/json\\')\\n            expected = \\'{\"example\": \"content\"}\\'\\n            self.assertEqual(response.get_data().decode(\\'utf8\\'), expected)', \"def test_abort_view(self):\\n        with app.test_client() as client:\\n            response = client.get('/abort_view/')\\n            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_was_published_recently_with_future_question(self):\\n        \"\"\"\\n        was_published_recently() should return False for questions whose\\n        pub_date is in the future.\\n        \"\"\"\\n        time = timezone.now() + datetime.timedelta(days=30)\\n        future_question = Question(pub_date=time)\\n        self.assertIs(future_question.was_published_recently(), False)', 'def test_was_published_recently_with_recent_question(self):\\n        \"\"\"\\n        was_published_recently() should return True for questions whose\\n        pub_date is within the last day.\\n        \"\"\"\\n        time = timezone.now() - datetime.timedelta(hours=1)\\n        recent_question = Question(pub_date=time)\\n        self.assertIs(recent_question.was_published_recently(), True)', 'def test_index_view_with_no_questions(self):\\n        \"\"\"\\n        If no questions exist, an appropriate message should be displayed.\\n        \"\"\"\\n        response = self.client.get(reverse(\\'polls:index\\'))\\n        self.assertEqual(response.status_code, 200)\\n        self.assertContains(response, \"No polls are available.\")\\n        self.assertQuerysetEqual(response.context[\\'latest_question_list\\'], [])', 'def test_index_view_with_a_future_question(self):\\n        \"\"\"\\n        Questions with a pub_date in the future should not be displayed on\\n        the index page.\\n        \"\"\"\\n        create_question(question_text=\"Future question.\", days=30)\\n        response = self.client.get(reverse(\\'polls:index\\'))\\n        self.assertContains(response, \"No polls are available.\")\\n        self.assertQuerysetEqual(response.context[\\'latest_question_list\\'], [])', 'def test_index_view_with_two_past_questions(self):\\n        \"\"\"\\n        The questions index page may display multiple questions.\\n        \"\"\"\\n        create_question(question_text=\"Past question 1.\", days=-30)\\n        create_question(question_text=\"Past question 2.\", days=-5)\\n        response = self.client.get(reverse(\\'polls:index\\'))\\n        self.assertQuerysetEqual(\\n            response.context[\\'latest_question_list\\'],\\n            [\\'<Question: Past question 2.>\\', \\'<Question: Past question 1.>\\']\\n        )', 'def test_detail_view_with_a_future_question(self):\\n        \"\"\"\\n        The detail view of a question with a pub_date in the future should\\n        return a 404 not found.\\n        \"\"\"\\n        future_question = create_question(question_text=\\'Future question.\\', days=5)\\n        url = reverse(\\'polls:detail\\', args=(future_question.id,))\\n        response = self.client.get(url)\\n        self.assertEqual(response.status_code, 404)']}, {'features': [], 'snippets': ['def update_point(move, point):\\n    \"\"\"Returns new point representing position after move\"\"\"\\n    moves = {\\n        \\'^\\': (0, -1),\\n        \\'<\\': (-1, 0),\\n        \\'v\\': (0, 1),\\n        \\'>\\': (1, 0),\\n    }\\n    return (point[0]+moves.get(move, (0, 0))[0],\\n            point[1]+moves.get(move, (0, 0))[1])', 'def number_of_houses_covered(text, robo_santa=False):\\n    return len(map_single_delivery(text)) if not robo_santa else \\\\\\n        len(map_multiple_deliveries(text))', 'def map_multiple_deliveries(text):\\n    directions = split_directions(text)\\n    points = map_single_delivery(directions[0])\\n    return points.union(map_single_delivery(directions[1]))', 'def calculate_solution_2(text):\\n    return number_of_houses_covered(text, robo_santa=True)', 'def main(source_file):\\n    \"\"\"Simple solution to adventofcode problem 3.\"\"\"\\n    data = \\'\\'\\n    with open(source_file) as source:\\n        data = source.read()\\n    print(\\'Santa gave at least one present to {} houses.\\'.format(\\n        number_of_houses_covered(data)))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def _create_3d_axis():\\n    \"\"\"creates a subplot with 3d projection if one does not already exist\"\"\"\\n    from matplotlib.projections import get_projection_class\\n    from matplotlib import _pylab_helpers\\n\\n    create_axis = True\\n    if _pylab_helpers.Gcf.get_active() is not None:\\n        if isinstance(plt.gca(), get_projection_class(\\'3d\\')):\\n            create_axis = False\\n    if create_axis:\\n        plt.figure()\\n        plt.subplot(111, projection=\\'3d\\')', 'def __phasor_plot(ax, up, idq, uxdq):\\n    uref = max(up, uxdq[0])\\n    uxd = uxdq[0]/uref\\n    uxq = uxdq[1]/uref\\n    u1d, u1q = (uxd, 1+uxq)\\n    u1 = np.sqrt(u1d**2 + u1q**2)*uref\\n    i1 = np.linalg.norm(idq)\\n    i1d, i1q = (idq[0]/i1, idq[1]/i1)\\n\\n    qhw = 6   # width arrow head\\n    qhl = 15  # length arrow head\\n    qlw = 2   # line width\\n    qts = 10  # textsize\\n    # Length of the Current adjust to Ud: Initally 0.9, Maier(Oswald) = 0.5\\n    curfac = max(0.9, 1.5*i1q/up)\\n\\n    def label_line(ax, X, Y, U, V, label, color=\\'k\\', size=8):\\n        \"\"\"Add a label to a line, at the proper angle.\\n\\n        Arguments\\n        ---------\\n        line : matplotlib.lines.Line2D object,\\n        label : str\\n        x : float\\n        x-position to place center of text (in data coordinated\\n        y : float\\n        y-position to place center of text (in data coordinates)\\n        color : str\\n        size : float\\n        \"\"\"\\n\\n        x1, x2 = X, X + U\\n        y1, y2 = Y, Y + V\\n\\n        if y2 == 0:\\n            y2 = y1\\n        if x2 == 0:\\n            x2 = x1\\n\\n        x = (x1 + x2) / 2\\n        y = (y1 + y2) / 2\\n\\n        slope_degrees = np.rad2deg(np.angle(U + V * 1j))\\n        if slope_degrees < 0:\\n            slope_degrees += 180\\n        if 90 < slope_degrees <= 270:\\n            slope_degrees += 180\\n\\n        x_offset = np.sin(np.deg2rad(slope_degrees))\\n        y_offset = np.cos(np.deg2rad(slope_degrees))\\n        bbox_props = dict(boxstyle=\"Round4, pad=0.1\", fc=\"white\", lw=0)\\n        text = ax.annotate(label, xy=(x, y), xytext=(x_offset * 10, y_offset * 8),\\n                           textcoords=\\'offset points\\',\\n                           size=size, color=color,\\n                           horizontalalignment=\\'center\\',\\n                           verticalalignment=\\'center\\',\\n                           fontfamily=\\'monospace\\', fontweight=\\'bold\\', bbox=bbox_props)\\n\\n        text.set_rotation(slope_degrees)\\n        return text\\n\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.axes.xaxis.set_ticklabels([])\\n    ax.axes.yaxis.set_ticklabels([])\\n    # ax.set_aspect(\\'equal\\')\\n\\n    ax.set_title(\\n        r\\'$U_1$={0} V, $I_1$={1} A, $U_p$={2} V\\'.format(\\n            round(u1, 1), round(i1, 1), round(up, 1)), fontsize=14)\\n\\n    up /= uref\\n    ax.quiver(0, 0, 0, up, angles=\\'xy\\', scale_units=\\'xy\\', scale=1, units=\\'dots\\',\\n              headwidth=qhw/2, headlength=qhl/2, headaxislength=qhl/2, width=qlw*2, color=\\'k\\')\\n    label_line(ax, 0, 0, 0, up, \\'$U_p$\\', \\'k\\', qts)\\n\\n    ax.quiver(0, 0, u1d, u1q, angles=\\'xy\\', scale_units=\\'xy\\', scale=1, units=\\'dots\\',\\n              headwidth=qhw, headlength=qhl, headaxislength=qhl, width=qlw, color=\\'r\\')\\n    label_line(ax, 0, 0, u1d, u1q, \\'$U_1$\\', \\'r\\', qts)\\n\\n    ax.quiver(0, 1, uxd, 0, angles=\\'xy\\', scale_units=\\'xy\\', scale=1, units=\\'dots\\',\\n              headwidth=qhw, headlength=qhl, headaxislength=qhl, width=qlw, color=\\'g\\')\\n    label_line(ax, 0, 1, uxd, 0, \\'$U_d$\\', \\'g\\', qts)\\n\\n    ax.quiver(uxd, 1, 0, uxq, angles=\\'xy\\', scale_units=\\'xy\\', scale=1, units=\\'dots\\',\\n              headwidth=qhw, headlength=qhl, headaxislength=qhl, width=qlw, color=\\'g\\')\\n    label_line(ax, uxd, 1, 0, uxq, \\'$U_q$\\', \\'g\\', qts)\\n\\n    ax.quiver(0, 0, curfac*i1d, curfac*i1q, angles=\\'xy\\', scale_units=\\'xy\\', scale=1,\\n              units=\\'dots\\', headwidth=qhw, headlength=qhl, headaxislength=qhl, width=qlw, color=\\'b\\')\\n    label_line(ax, 0, 0, curfac*i1d, curfac*i1q, \\'$I_1$\\', \\'b\\', qts)\\n\\n    xmin, xmax = (min(0, uxd, i1d), max(0, i1d, uxd))\\n    ymin, ymax = (min(0, i1q, 1-uxq), max(1, i1q, 1+uxq))\\n\\n    ax.set_xlim([xmin-0.1, xmax+0.1])\\n    ax.set_ylim([ymin-0.1, ymax+0.1])\\n    ax.grid(True)', 'def iqd_phasor(up, iqd, uqd, ax=0):\\n    \"\"\"creates a phasor plot\\n    up: internal voltage\\n    iqd: current\\n    uqd: terminal voltage\"\"\"\\n\\n    uxdq = (uqd[1]/np.sqrt(2), (uqd[0]/np.sqrt(2)-up))\\n    __phasor_plot(ax, up, (iqd[1]/np.sqrt(2), iqd[0]/np.sqrt(2)), uxdq)', 'def airgap(airgap, ax=0):\\n    \"\"\"creates plot of flux density in airgap\"\"\"\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(\\'Airgap Flux Density [T]\\')\\n    ax.plot(airgap[\\'pos\\'], airgap[\\'B\\'],\\n            label=\\'Max {:4.2f} T\\'.format(max(airgap[\\'B\\'])))\\n    ax.plot(airgap[\\'pos\\'], airgap[\\'B_fft\\'],\\n            label=\\'Base Ampl {:4.2f} T\\'.format(airgap[\\'Bamp\\']))\\n    ax.set_xlabel(\\'Position/°\\')\\n    ax.legend()\\n    ax.grid(True)', 'def torque(pos, torque, ax=0):\\n    \"\"\"creates plot from torque vs position\"\"\"\\n    k = 20\\n    alpha = np.linspace(pos[0], pos[-1],\\n                        k*len(torque))\\n    f = ip.interp1d(pos, torque, kind=\\'quadratic\\')\\n    unit = \\'Nm\\'\\n    scale = 1\\n    if np.min(torque) < -9.9e3 or np.max(torque) > 9.9e3:\\n        scale = 1e-3\\n        unit = \\'kNm\\'\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(\\'Torque / {}\\'.format(unit))\\n    ax.grid(True)\\n    ax.plot(pos, [scale*t for t in torque], \\'go\\')\\n    ax.plot(alpha, scale*f(alpha))\\n    if np.min(torque) > 0 and np.max(torque) > 0:\\n        ax.set_ylim(bottom=0)\\n    elif np.min(torque) < 0 and np.max(torque) < 0:\\n        ax.set_ylim(top=0)', 'def force(title, pos, force, xlabel=\\'\\', ax=0):\\n    \"\"\"plot force vs position\"\"\"\\n    unit = \\'N\\'\\n    scale = 1\\n    if min(force) < -9.9e3 or max(force) > 9.9e3:\\n        scale = 1e-3\\n        unit = \\'kN\\'\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(\\'{} / {}\\'.format(title, unit))\\n    ax.grid(True)\\n    ax.plot(pos, [scale*f for f in force])\\n    if xlabel:\\n        ax.set_xlabel(xlabel)\\n    if min(force) > 0:\\n        ax.set_ylim(bottom=0)', 'def forcedens(title, pos, fdens, ax=0):\\n    \"\"\"plot force densities\"\"\"\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(title)\\n    ax.grid(True)\\n\\n    ax.plot(pos, [1e-3*ft for ft in fdens[0]], label=\\'F tang\\')\\n    ax.plot(pos, [1e-3*fn for fn in fdens[1]], label=\\'F norm\\')\\n    ax.legend()\\n    ax.set_xlabel(\\'Pos / deg\\')\\n    ax.set_ylabel(\\'Force Density / kN/m²\\')', 'def forcedens_fft(title, fdens, ax=0):\\n    \"\"\"plot force densities FFT\\n    Args:\\n      title: plot title\\n      fdens: force density object\\n    \"\"\"\\n    if ax == 0:\\n        ax = plt.axes(projection=\"3d\")\\n\\n    F = 1e-3*fdens.fft()\\n    fmin = 0.2\\n    num_bars = F.shape[0] + 1\\n    _xx, _yy = np.meshgrid(np.arange(1, num_bars),\\n                           np.arange(1, num_bars))\\n    z_size = F[F > fmin]\\n    x_pos, y_pos = _xx[F > fmin], _yy[F > fmin]\\n    z_pos = np.zeros_like(z_size)\\n    x_size = 2\\n    y_size = 2\\n\\n    ax.bar3d(x_pos, y_pos, z_pos, x_size, y_size, z_size)\\n    ax.view_init(azim=120)\\n    ax.set_xlim(0, num_bars+1)\\n    ax.set_ylim(0, num_bars+1)\\n    ax.set_title(title)\\n    ax.set_xlabel(\\'M\\')\\n    ax.set_ylabel(\\'N\\')\\n    ax.set_zlabel(\\'kN/m²\\')', 'def winding_current(pos, current, ax=0):\\n    \"\"\"plot winding currents\"\"\"\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(\\'Winding Currents / A\\')\\n    ax.grid(True)\\n    for p, i in zip(pos, current):\\n        ax.plot(p, i)', 'def voltage_fft(title, order, voltage, ax=0):\\n    \"\"\"plot FFT harmonics of voltage\"\"\"\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_title(\\'{} / V\\'.format(title))\\n    ax.grid(True)\\n    if max(order) < 5:\\n        order += [5]\\n        voltage += [0]\\n    try:\\n        bw = 2.5E-2*max(order)\\n        ax.bar(order, voltage, width=bw, align=\\'center\\')\\n    except ValueError:  # empty sequence\\n        pass', 'def mcv_muer(mcv, ax=0):\\n    \"\"\"plot rel. permeability vs. B of mcv dict\"\"\"\\n    MUE0 = 4e-7*np.pi\\n    bi, ur = zip(*[(bx, bx/hx/MUE0)\\n                   for bx, hx in zip(mcv[\\'curve\\'][0][\\'bi\\'],\\n                                     mcv[\\'curve\\'][0][\\'hi\\']) if not hx == 0])\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.plot(bi, ur)\\n    ax.set_xlabel(\\'B / T\\')\\n    ax.set_title(\\'rel. Permeability\\')\\n    ax.grid()', 'def mtpv(pmrel, u1max, i1max, title=\\'\\', projection=\\'\\', ax=0):\\n    \"\"\"create a line or surface plot with voltage and mtpv curve\"\"\"\\n    w1 = pmrel.w2_imax_umax(i1max, u1max)\\n    nsamples = 20\\n    if projection == \\'3d\\':\\n        nsamples = 50\\n\\n    iqmax, idmax = pmrel.iqdmax(i1max)\\n    iqmin, idmin = pmrel.iqdmin(i1max)\\n    id = np.linspace(idmin, idmax, nsamples)\\n    iq = np.linspace(iqmin, iqmax, nsamples)\\n    u1_iqd = np.array(\\n        [[np.linalg.norm(pmrel.uqd(w1, iqx, idx))/np.sqrt(2)\\n          for idx in id] for iqx in iq])\\n    u1 = np.mean(u1_iqd)\\n    imtpv = np.array([pmrel.mtpv(wx, u1, i1max)\\n                      for wx in np.linspace(w1, 20*w1, nsamples)]).T\\n\\n    if projection == \\'3d\\':\\n        torque_iqd = np.array(\\n            [[pmrel.torque_iqd(x, y)\\n              for y in id] for x in iq])\\n        ax = idq_torque(id, iq, torque_iqd, ax)\\n        ax.plot(imtpv[1], imtpv[0], imtpv[2],\\n                color=\\'red\\', linewidth=2)\\n    else:\\n        if ax == 0:\\n            ax = plt.gca()\\n        ax.set_aspect(\\'equal\\')\\n        x, y = np.meshgrid(id, iq)\\n        CS = ax.contour(x, y, u1_iqd, 4, colors=\\'b\\')  # linestyles=\\'dashed\\')\\n        ax.clabel(CS, fmt=\\'%d\\', inline=1)\\n\\n        ax.plot(imtpv[1], imtpv[0],\\n                color=\\'red\\', linewidth=2,\\n                label=\\'MTPV: {0:5.0f} Nm\\'.format(np.max(imtpv[2])))\\n        # beta = np.arctan2(imtpv[1][0], imtpv[0][0])\\n        # b = np.linspace(beta, 0)\\n        # ax.plot(np.sqrt(2)*i1max*np.sin(b), np.sqrt(2)*i1max*np.cos(b), \\'r-\\')\\n\\n        ax.grid()\\n        ax.legend()\\n    ax.set_xlabel(\\'Id/A\\')\\n    ax.set_ylabel(\\'Iq/A\\')\\n    if title:\\n        ax.set_title(title)', 'def pmrelsim(bch, title=\\'\\'):\\n    \"\"\"creates a plot of a PM/Rel motor simulation\"\"\"\\n    cols = 2\\n    rows = 4\\n    if len(bch.flux[\\'1\\']) > 1:\\n        rows += 1\\n    htitle = 1.5 if title else 0\\n    fig, ax = plt.subplots(nrows=rows, ncols=cols,\\n                           figsize=(10, 3*rows + htitle))\\n    if title:\\n        fig.suptitle(title, fontsize=16)\\n\\n    row = 1\\n    plt.subplot(rows, cols, row)\\n    if bch.torque:\\n        torque(bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'torque\\'])\\n        plt.subplot(rows, cols, row+1)\\n        tq = list(bch.torque_fft[-1][\\'torque\\'])\\n        order = list(bch.torque_fft[-1][\\'order\\'])\\n        if order and max(order) < 5:\\n            order += [15]\\n            tq += [0]\\n        torque_fft(order, tq)\\n        plt.subplot(rows, cols, row+2)\\n        force(\\'Force Fx\\',\\n              bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'force_x\\'])\\n        plt.subplot(rows, cols, row+3)\\n        force(\\'Force Fy\\',\\n              bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'force_y\\'])\\n        row += 3\\n    elif bch.linearForce:\\n        title, keys = __get_linearForce_title_keys(bch.linearForce[-1])\\n        force(title[0], bch.linearForce[-1][\\'displ\\'],\\n              bch.linearForce[-1][keys[0]], \\'Displt. / mm\\')\\n        plt.subplot(rows, cols, row+1)\\n        force_fft(bch.linearForce_fft[-2][\\'order\\'],\\n                  bch.linearForce_fft[-2][\\'force\\'])\\n        plt.subplot(rows, cols, row+2)\\n        force(title[1], bch.linearForce[-1][\\'displ\\'],\\n              bch.linearForce[-1][keys[1]], \\'Displt. / mm\\')\\n        plt.subplot(rows, cols, row+3)\\n        force_fft(bch.linearForce_fft[-1][\\'order\\'],\\n                  bch.linearForce_fft[-1][\\'force\\'])\\n        row += 3\\n\\n    plt.subplot(rows, cols, row+1)\\n    flux = [bch.flux[k][-1] for k in bch.flux]\\n    pos = [f[\\'displ\\'] for f in flux]\\n    winding_flux(pos,\\n                 [f[\\'flux_k\\'] for f in flux])\\n    plt.subplot(rows, cols, row+2)\\n    winding_current(pos,\\n                    [f[\\'current_k\\'] for f in flux])\\n    plt.subplot(rows, cols, row+3)\\n    voltage(\\'Internal Voltage\\',\\n            bch.flux[\\'1\\'][-1][\\'displ\\'],\\n            bch.flux[\\'1\\'][-1][\\'voltage_dpsi\\'])\\n    plt.subplot(rows, cols, row+4)\\n    try:\\n        voltage_fft(\\'Internal Voltage Harmonics\\',\\n                    bch.flux_fft[\\'1\\'][-1][\\'order\\'],\\n                    bch.flux_fft[\\'1\\'][-1][\\'voltage\\'])\\n    except:\\n        pass\\n    if len(bch.flux[\\'1\\']) > 1:\\n        plt.subplot(rows, cols, row+5)\\n        voltage(\\'No Load Voltage\\',\\n                bch.flux[\\'1\\'][0][\\'displ\\'],\\n                bch.flux[\\'1\\'][0][\\'voltage_dpsi\\'])\\n        plt.subplot(rows, cols, row+6)\\n        try:\\n            voltage_fft(\\'No Load Voltage Harmonics\\',\\n                        bch.flux_fft[\\'1\\'][0][\\'order\\'],\\n                        bch.flux_fft[\\'1\\'][0][\\'voltage\\'])\\n        except:\\n            pass\\n    fig.tight_layout(h_pad=3.5)\\n    if title:\\n        fig.subplots_adjust(top=0.92)', 'def fasttorque(bch, title=\\'\\'):\\n    \"\"\"creates a plot of a Fast Torque simulation\"\"\"\\n    cols = 2\\n    rows = 4\\n    if len(bch.flux[\\'1\\']) > 1:\\n        rows += 1\\n    htitle = 1.5 if title else 0\\n    fig, ax = plt.subplots(nrows=rows, ncols=cols,\\n                           figsize=(10, 3*rows + htitle))\\n    if title:\\n        fig.suptitle(title, fontsize=16)\\n\\n    row = 1\\n    plt.subplot(rows, cols, row)\\n    if bch.torque:\\n        torque(bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'torque\\'])\\n        plt.subplot(rows, cols, row+1)\\n        torque_fft(bch.torque_fft[-1][\\'order\\'], bch.torque_fft[-1][\\'torque\\'])\\n        plt.subplot(rows, cols, row+2)\\n        force(\\'Force Fx\\',\\n              bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'force_x\\'])\\n        plt.subplot(rows, cols, row+3)\\n        force(\\'Force Fy\\',\\n              bch.torque[-1][\\'angle\\'], bch.torque[-1][\\'force_y\\'])\\n        row += 3\\n    elif bch.linearForce:\\n        title, keys = __get_linearForce_title_keys(bch.linearForce[-1])\\n        force(title[0], bch.linearForce[-1][\\'displ\\'],\\n              bch.linearForce[-1][keys[0]], \\'Displt. / mm\\')\\n        plt.subplot(rows, cols, row+1)\\n        force_fft(bch.linearForce_fft[-2][\\'order\\'],\\n                  bch.linearForce_fft[-2][\\'force\\'])\\n        plt.subplot(rows, cols, row+2)\\n        force(title[1], bch.linearForce[-1][\\'displ\\'],\\n              bch.linearForce[-1][keys[1]], \\'Displt. / mm\\')\\n        plt.subplot(rows, cols, row+3)\\n        force_fft(bch.linearForce_fft[-1][\\'order\\'],\\n                  bch.linearForce_fft[-1][\\'force\\'])\\n        row += 3\\n\\n    plt.subplot(rows, cols, row+1)\\n    flux = [bch.flux[k][-1] for k in bch.flux]\\n    pos = [f[\\'displ\\'] for f in flux]\\n    winding_flux(pos, [f[\\'flux_k\\'] for f in flux])\\n    plt.subplot(rows, cols, row+2)\\n    winding_current(pos, [f[\\'current_k\\'] for f in flux])\\n    plt.subplot(rows, cols, row+3)\\n    voltage(\\'Internal Voltage\\',\\n            bch.flux[\\'1\\'][-1][\\'displ\\'],\\n            bch.flux[\\'1\\'][-1][\\'voltage_dpsi\\'])\\n    plt.subplot(rows, cols, row+4)\\n    try:\\n        voltage_fft(\\'Internal Voltage Harmonics\\',\\n                    bch.flux_fft[\\'1\\'][-1][\\'order\\'],\\n                    bch.flux_fft[\\'1\\'][-1][\\'voltage\\'])\\n    except:\\n        pass\\n    if len(bch.flux[\\'1\\']) > 1:\\n        plt.subplot(rows, cols, row+5)\\n        voltage(\\'No Load Voltage\\',\\n                bch.flux[\\'1\\'][0][\\'displ\\'],\\n                bch.flux[\\'1\\'][0][\\'voltage_dpsi\\'])\\n        plt.subplot(rows, cols, row+6)\\n        try:\\n            voltage_fft(\\'No Load Voltage Harmonics\\',\\n                        bch.flux_fft[\\'1\\'][0][\\'order\\'],\\n                        bch.flux_fft[\\'1\\'][0][\\'voltage\\'])\\n        except:\\n            pass\\n    fig.tight_layout(h_pad=3.5)\\n    if title:\\n        fig.subplots_adjust(top=0.92)', 'def transientsc(bch, title=\\'\\'):\\n    \"\"\"creates a transient short circuit plot\"\"\"\\n    cols = 1\\n    rows = 2\\n    htitle = 1.5 if title else 0\\n    fig, ax = plt.subplots(nrows=rows, ncols=cols,\\n                           figsize=(10, 3*rows + htitle))\\n    if title:\\n        fig.suptitle(title, fontsize=16)\\n\\n    row = 1\\n    plt.subplot(rows, cols, row)\\n    ax = plt.gca()\\n    ax.set_title(\\'Currents / A\\')\\n    ax.grid(True)\\n    for i in (\\'ia\\', \\'ib\\', \\'ic\\'):\\n        ax.plot(bch.scData[\\'time\\'], bch.scData[i], label=i)\\n    ax.set_xlabel(\\'Time / s\\')\\n    ax.legend()\\n\\n    row = 2\\n    plt.subplot(rows, cols, row)\\n    ax = plt.gca()\\n    ax.set_title(\\'Torque / Nm\\')\\n    ax.grid(True)\\n    ax.plot(bch.scData[\\'time\\'], bch.scData[\\'torque\\'])\\n    ax.set_xlabel(\\'Time / s\\')\\n\\n    fig.tight_layout(h_pad=2)\\n    if title:\\n        fig.subplots_adjust(top=0.92)', 'def i1beta_ld(i1, beta, ld, ax=0):\\n    \"\"\"creates a surface plot of ld vs i1, beta\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    _plot_surface(ax, i1, beta, np.asarray(ld)*1e3,\\n                  (u\\'I1/A\\', u\\'Beta/°\\', u\\'Ld/mH\\'),\\n                  azim=60)', 'def i1beta_psim(i1, beta, psim, ax=0):\\n    \"\"\"creates a surface plot of psim vs i1, beta\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    _plot_surface(ax, i1, beta, psim,\\n                  (u\\'I1/A\\', u\\'Beta/°\\', u\\'Psi m/Vs\\'),\\n                  azim=60)', 'def i1beta_psid(i1, beta, psid, ax=0):\\n    \"\"\"creates a surface plot of psid vs i1, beta\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    azim = -60\\n    if 0 < np.mean(beta) or -90 > np.mean(beta):\\n        azim = 60\\n    _plot_surface(ax, i1, beta, psid,\\n                  (u\\'I1/A\\', u\\'Beta/°\\', u\\'Psi d/Vs\\'),\\n                  azim=azim)', 'def idq_torque(id, iq, torque, ax=0):\\n    \"\"\"creates a surface plot of torque vs id, iq\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    unit = \\'Nm\\'\\n    scale = 1\\n    if np.min(torque) < -9.9e3 or np.max(torque) > 9.9e3:\\n        scale = 1e-3\\n        unit = \\'kNm\\'\\n    _plot_surface(ax, id, iq, scale*np.asarray(torque),\\n                  (u\\'Id/A\\', u\\'Iq/A\\', u\\'Torque/{}\\'.format(unit)),\\n                  azim=-60)\\n    return ax', 'def idq_psiq(id, iq, psiq, ax=0):\\n    \"\"\"creates a surface plot of psiq vs id, iq\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    _plot_surface(ax, id, iq, psiq,\\n                  (u\\'Id/A\\', u\\'Iq/A\\', u\\'Psi q/Vs\\'),\\n                  azim=210)', 'def idq_ld(id, iq, ld, ax=0):\\n    \"\"\"creates a surface plot of ld vs. id, iq\"\"\"\\n    if ax == 0:\\n        _create_3d_axis()\\n        ax = plt.gca()\\n    _plot_surface(ax, id, iq, np.asarray(ld)*1e3,\\n                  (u\\'Id/A\\', u\\'Iq/A\\', u\\'L d/mH\\'),\\n                  azim=120)', 'def ldlq(bch):\\n    \"\"\"creates the surface plots of a BCH reader object\\n    with a ld-lq identification\"\"\"\\n    beta = bch.ldq[\\'beta\\']\\n    i1 = bch.ldq[\\'i1\\']\\n    torque = bch.ldq[\\'torque\\']\\n    ld = np.array(bch.ldq[\\'ld\\'])\\n    lq = np.array(bch.ldq[\\'lq\\'])\\n    psid = bch.ldq[\\'psid\\']\\n    psiq = bch.ldq[\\'psiq\\']\\n\\n    rows = 3\\n    fig = plt.figure(figsize=(10, 4*rows))\\n    fig.suptitle(\\'Ld-Lq Identification {}\\'.format(bch.filename), fontsize=16)\\n    fig.add_subplot(rows, 2, 1, projection=\\'3d\\')\\n    i1beta_torque(i1, beta, torque)\\n\\n    fig.add_subplot(rows, 2, 2, projection=\\'3d\\')\\n    i1beta_psid(i1, beta, psid)\\n\\n    fig.add_subplot(rows, 2, 3, projection=\\'3d\\')\\n    i1beta_psiq(i1, beta, psiq)\\n\\n    fig.add_subplot(rows, 2, 4, projection=\\'3d\\')\\n    try:\\n        i1beta_psim(i1, beta, bch.ldq[\\'psim\\'])\\n    except:\\n        i1beta_up(i1, beta, bch.ldq[\\'up\\'])\\n\\n    fig.add_subplot(rows, 2, 5, projection=\\'3d\\')\\n    i1beta_ld(i1, beta, ld)\\n\\n    fig.add_subplot(rows, 2, 6, projection=\\'3d\\')\\n    i1beta_lq(i1, beta, lq)', 'def felosses(losses, coeffs, title=\\'\\', log=True, ax=0):\\n    \"\"\"plot iron losses with steinmetz or jordan approximation\\n    Args:\\n      losses: dict with f, B, pfe values\\n      coeffs: list with steinmetz (cw, alpha, beta) or\\n              jordan (cw, alpha, ch, beta, gamma) coeffs\\n      title: title string\\n      log: log scale for x and y axes if True\\n\\n    \"\"\"\\n    import femagtools.losscoeffs as lc\\n    if ax == 0:\\n        ax = plt.gca()\\n\\n    fo = losses[\\'fo\\']\\n    Bo = losses[\\'Bo\\']\\n    B = plt.np.linspace(0.9*np.min(losses[\\'B\\']),\\n                        1.1*0.9*np.max(losses[\\'B\\']))\\n\\n    for i, f in enumerate(losses[\\'f\\']):\\n        pfe = [p for p in np.array(losses[\\'pfe\\'])[i] if p]\\n        if f > 0:\\n            if len(coeffs) == 5:\\n                ax.plot(B, lc.pfe_jordan(f, B, *coeffs, fo=fo, Bo=Bo))\\n            elif len(coeffs) == 3:\\n                ax.plot(B, lc.pfe_steinmetz(f, B, *coeffs, fo=fo, Bo=Bo))\\n        plt.plot(losses[\\'B\\'][:len(pfe)], pfe,\\n                 marker=\\'o\\', label=\"{} Hz\".format(f))\\n\\n    ax.set_title(\"Fe Losses/(W/kg) \" + title)\\n    if log:\\n        ax.set_yscale(\\'log\\')\\n        ax.set_xscale(\\'log\\')\\n    ax.set_xlabel(\"Flux Density [T]\")\\n    # plt.ylabel(\"Pfe [W/kg]\")\\n    ax.legend()\\n    ax.grid(True)', 'def mesh(isa, with_axis=False, ax=0):\\n    \"\"\"plot mesh of I7/ISA7 model\\n    Args:\\n      isa: Isa7 object\\n    \"\"\"\\n    from matplotlib.lines import Line2D\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.set_aspect(\\'equal\\')\\n    for el in isa.elements:\\n        pts = [list(i) for i in zip(*[v.xy for v in el.vertices])]\\n        ax.add_line(Line2D(pts[0], pts[1], color=\\'b\\', ls=\\'-\\', lw=0.25))\\n\\n    # for nc in isa.nodechains:\\n    #    pts = [list(i) for i in zip(*[(n.x, n.y) for n in nc.nodes])]\\n    #    ax.add_line(Line2D(pts[0], pts[1], color=\"b\", ls=\"-\", lw=0.25,\\n    #                       marker=\".\", ms=\"2\", mec=\"None\"))\\n\\n    # for nc in isa.nodechains:\\n    #    if nc.nodemid is not None:\\n    #        plt.plot(*nc.nodemid.xy, \"rx\")\\n\\n    ax.autoscale(enable=True)\\n    if not with_axis:\\n        ax.axis(\\'off\\')', 'def demag(isa, ax=0):\\n    \"\"\"plot demag of NC/I7/ISA7 model\\n    Args:\\n      isa: Isa7/NC object\\n    \"\"\"\\n    emag = [e for e in isa.elements if e.is_magnet()]\\n    demag = np.array([e.demagnetization(isa.MAGN_TEMPERATURE) for e in emag])\\n    _contour(ax, f\\'Demagnetization at {isa.MAGN_TEMPERATURE} °C\\',\\n             emag, demag, \\'-H / kA/m\\', isa)\\n    logger.info(\"Max demagnetization %f\", np.max(demag))', 'def flux_density(isa, subreg=[], ax=0):\\n    \"\"\"plot flux density of NC/I7/ISA7 model\\n    Args:\\n      isa: Isa7/NC object\\n    \"\"\"\\n    if subreg:\\n        if isinstance(subreg, list):\\n            sr = subreg\\n        else:\\n            sr = [subreg]\\n        elements = [e for s in sr for se in isa.get_subregion(s).elements()\\n                    for e in se]\\n    else:\\n        elements = [e for e in isa.elements]\\n\\n    fluxd = np.array([np.linalg.norm(e.flux_density()) for e in elements])\\n    _contour(ax, f\\'Flux Density T\\', elements, fluxd)\\n    logger.info(\"Max flux dens %f\", np.max(fluxd))', 'def mmf(f, title=\\'\\', ax=0):\\n    \"\"\"plot magnetomotive force (mmf) of winding\"\"\"\\n    if ax == 0:\\n        ax = plt.gca()\\n    if title:\\n        ax.set_title(title)\\n    ax.plot(np.array(f[\\'pos\\'])/np.pi*180, f[\\'mmf\\'])\\n    ax.plot(np.array(f[\\'pos_fft\\'])/np.pi*180, f[\\'mmf_fft\\'])\\n    ax.set_xlabel(\\'Position / Deg\\')\\n\\n    phi = [f[\\'alfa0\\']/np.pi*180, f[\\'alfa0\\']/np.pi*180]\\n    y = [min(f[\\'mmf_fft\\']), 1.1*max(f[\\'mmf_fft\\'])]\\n    ax.plot(phi, y, \\'--\\')\\n    alfa0 = round(f[\\'alfa0\\']/np.pi*180, 3)\\n    ax.text(phi[0]/2, y[0]+0.05, f\"{alfa0}°\",\\n            ha=\"center\", va=\"bottom\")\\n    ax.annotate(f\"\", xy=(phi[0], y[0]),\\n                xytext=(0, y[0]), arrowprops=dict(arrowstyle=\"->\"))\\n    ax.grid()', 'def zoneplan(wdg, ax=0):\\n    \"\"\"plot zone plan of winding wdg\"\"\"\\n    from matplotlib.patches import Rectangle\\n    upper, lower = wdg.zoneplan()\\n    Qb = len([n for l in upper for n in l])\\n    from femagtools.windings import coil_color\\n    rh = 0.5\\n    if lower:\\n        yl = rh\\n        ymax = 2*rh + 0.2\\n    else:\\n        yl = 0\\n        ymax = rh + 0.2\\n    if ax == 0:\\n        ax = plt.gca()\\n    ax.axis(\\'off\\')\\n    ax.set_xlim([-0.5, Qb-0.5])\\n    ax.set_ylim([0, ymax])\\n    ax.set_aspect(Qb/6+0.3)\\n\\n    for i, p in enumerate(upper):\\n        for x in p:\\n            ax.add_patch(Rectangle((abs(x)-1.5, yl), 1, rh,\\n                                   facecolor=coil_color[i],\\n                                   edgecolor=\\'white\\', fill=True))\\n            s = f\\'+{i+1}\\' if x > 0 else f\\'-{i+1}\\'\\n            ax.text(abs(x)-1, yl+rh/2, s, color=\\'black\\',\\n                    ha=\"center\", va=\"center\")\\n    for i, p in enumerate(lower):\\n        for x in p:\\n            ax.add_patch(Rectangle((abs(x)-1.5, yl-rh), 1, rh,\\n                                   facecolor=coil_color[i],\\n                                   edgecolor=\\'white\\', fill=True))\\n            s = f\\'+{i+1}\\' if x > 0 else f\\'-{i+1}\\'\\n            ax.text(abs(x)-1, yl-rh/2, s, color=\\'black\\',\\n                    ha=\"center\", va=\"center\")\\n\\n    yu = yl+rh\\n    step = 1 if Qb < 25 else 2\\n    if lower:\\n        yl -= rh\\n    margin = 0.05\\n    ax.text(-0.5, yu+margin, f\\'Q={wdg.Q}, p={wdg.p}, q={round(wdg.q,4)}\\',\\n            ha=\\'left\\', va=\\'bottom\\', size=15)\\n    for i in range(0, Qb, step):\\n        ax.text(i, yl-margin, f\\'{i+1}\\', ha=\"center\", va=\"top\")', 'def winding(wdg, ax=0):\\n    \"\"\"plot coils of windings wdg\"\"\"\\n    from matplotlib.patches import Rectangle\\n    from matplotlib.lines import Line2D\\n    from femagtools.windings import coil_color\\n\\n    coil_len = 25\\n    coil_height = 4\\n    dslot = 8\\n    arrow_head_length = 2\\n    arrow_head_width = 2\\n\\n    if ax == 0:\\n        ax = plt.gca()\\n    z = wdg.zoneplan()\\n    xoff = 0\\n    if z[-1]:\\n        xoff = 0.75\\n    yd = dslot*wdg.yd\\n    mh = 2*coil_height/yd\\n    slots = sorted([abs(n) for m in z[0] for n in m])\\n    smax = slots[-1]*dslot\\n    for n in slots:\\n        x = n*dslot\\n        ax.add_patch(Rectangle((x + dslot/4, 1), dslot /\\n                     2, coil_len - 2, fc=\"lightblue\"))\\n        ax.text(x, coil_len / 2,\\n                str(n),\\n                horizontalalignment=\"center\",\\n                verticalalignment=\"center\",\\n                backgroundcolor=\"white\",\\n                bbox=dict(boxstyle=\\'circle,pad=0\\', fc=\"white\", lw=0))\\n    line_thickness = [0.6, 1.2]\\n    for i, layer in enumerate(z):\\n        b = -xoff if i else xoff\\n        lw = line_thickness[i]\\n        for m, mslots in enumerate(layer):\\n            for k in mslots:\\n                x = abs(k) * dslot + b\\n                xpoints = []\\n                ypoints = []\\n                if (i == 0 and (k > 0 or (k < 0 and wdg.l > 1))):\\n                    # first layer, positive dir or neg. dir and 2-layers:\\n                    #   from right bottom\\n                    if x + yd > smax+b:\\n                        dx = dslot if yd > dslot else yd/4\\n                        xpoints = [x + yd//2 + dx - xoff]\\n                        ypoints = [-coil_height + mh*dx]\\n                    xpoints += [x + yd//2 - xoff, x, x, x + yd//2-xoff]\\n                    ypoints += [-coil_height, 0, coil_len,\\n                                coil_len+coil_height]\\n                    if x + yd > smax+b:\\n                        xpoints += [x + yd//2 + dx - xoff]\\n                        ypoints += [coil_len+coil_height - mh*dx]\\n                else:\\n                    # from left bottom\\n                    if x - yd < 0:  # and x - yd/2 > -3*dslot:\\n                        dx = dslot if yd > dslot else yd/4\\n                        xpoints = [x - yd//2 - dx + xoff]\\n                        ypoints = [- coil_height + mh*dx]\\n                    xpoints += [x - yd//2+xoff, x, x, x - yd/2+xoff]\\n                    ypoints += [-coil_height, 0, coil_len,\\n                                coil_len+coil_height]\\n                    if x - yd < 0:  # and x - yd > -3*dslot:\\n                        xpoints += [x - yd//2 - dx + xoff]\\n                        ypoints += [coil_len + coil_height - mh*dx]\\n\\n                ax.add_line(Line2D(xpoints, ypoints,\\n                            color=coil_color[m], lw=lw))\\n\\n                if k > 0:\\n                    h = arrow_head_length\\n                    y = coil_len * 0.8\\n                else:\\n                    h = -arrow_head_length\\n                    y = coil_len * 0.2\\n                ax.arrow(x, y, 0, h,\\n                         length_includes_head=True,\\n                         head_starts_at_zero=False,\\n                         head_length=arrow_head_length,\\n                         head_width=arrow_head_width,\\n                         fc=coil_color[m], lw=0)\\n    if False:  # TODO show winding connections\\n        m = 0\\n        for k in [n*wdg.Q/wdg.p/wdg.m + 1 for n in range(wdg.m)]:\\n            if k < len(slots):\\n                x = k * dslot + b + yd/2 - xoff\\n                ax.add_line(Line2D([x, x],\\n                                   [-2*coil_height, -coil_height],\\n                                   color=coil_color[m], lw=lw))\\n                ax.text(x, -2*coil_height+0.5, str(m+1), color=coil_color[m])\\n            m += 1\\n    ax.autoscale(enable=True)\\n    ax.set_axis_off()', 'def characteristics(char, title=\\'\\'):\\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\\n    if title:\\n        fig.suptitle(title)\\n\\n    n = np.array(char[\\'n\\'])*60\\n    pmech = np.array(char[\\'pmech\\'])*1e-3\\n\\n    axs[0, 0].plot(n, np.array(char[\\'T\\']), \\'C0-\\', label=\\'Torque\\')\\n    axs[0, 0].set_ylabel(\"Torque / Nm\")\\n    axs[0, 0].grid()\\n    axs[0, 0].legend(loc=\\'center left\\')\\n    ax1 = axs[0, 0].twinx()\\n    ax1.plot(n, pmech, \\'C1-\\', label=\\'P mech\\')\\n    ax1.set_ylabel(\"Power / kW\")\\n    ax1.legend(loc=\\'lower center\\')\\n\\n    axs[0, 1].plot(n[1:], np.array(char[\\'u1\\'][1:]), \\'C0-\\', label=\\'Voltage\\')\\n    axs[0, 1].set_ylabel(\"Voltage / V\",)\\n    axs[0, 1].grid()\\n    axs[0, 1].legend(loc=\\'center left\\')\\n    ax2 = axs[0, 1].twinx()\\n    ax2.plot(n[1:], char[\\'cosphi\\'][1:], \\'C1-\\', label=\\'Cos Phi\\')\\n    ax2.set_ylabel(\"Cos Phi\")\\n    ax2.legend(loc=\\'lower right\\')\\n\\n    if \\'id\\' in char:\\n        axs[1, 0].plot(n, np.array(char[\\'id\\']), label=\\'Id\\')\\n    if \\'iq\\' in char:\\n        axs[1, 0].plot(n, np.array(char[\\'iq\\']), label=\\'Iq\\')\\n    axs[1, 0].plot(n, np.array(char[\\'i1\\']), label=\\'I1\\')\\n    axs[1, 0].set_xlabel(\"Speed / rpm\")\\n    axs[1, 0].set_ylabel(\"Current / A\")\\n    axs[1, 0].legend(loc=\\'center left\\')\\n    if \\'beta\\' in char:\\n        ax3 = axs[1, 0].twinx()\\n        ax3.plot(n, char[\\'beta\\'], \\'C3-\\', label=\\'Beta\\')\\n        ax3.set_ylabel(\"Beta / °\")\\n        ax3.legend(loc=\\'center right\\')\\n    axs[1, 0].grid()\\n\\n    plfe = np.array(char[\\'plfe\\'])*1e-3\\n    plcu = np.array(char[\\'plcu\\'])*1e-3\\n    pl = np.array(char[\\'losses\\'])*1e-3\\n    axs[1, 1].plot(n, plcu, \\'C0-\\', label=\\'Cu Losses\\')\\n    axs[1, 1].plot(n, plfe, \\'C1-\\', label=\\'Fe Losses\\')\\n    axs[1, 1].set_ylabel(\"Losses / kW\")\\n    axs[1, 1].legend(loc=\\'center left\\')\\n    axs[1, 1].grid()\\n    axs[1, 1].set_xlabel(\"Speed / rpm\")\\n    ax4 = axs[1, 1].twinx()\\n    ax4.plot(n[1:-1], char[\\'eta\\'][1:-1], \\'C3-\\', label=\"Eta\")\\n    ax4.legend(loc=\\'upper center\\')\\n    ax4.set_ylabel(\"Efficiency\")\\n\\n    fig.tight_layout()']}, {'features': [], 'snippets': ['def symbols(s):\\n    \"\"\" mimics sympy.symbols \"\"\"\\n    tup = tuple(map(Symbol, s.replace(\\',\\', \\' \\').split()))\\n    if len(tup) == 1:\\n        return tup[0]\\n    else:\\n        return tup', 'def lambdify(args, exprs):\\n    \"\"\"\\n    lambdify mimics sympy.lambdify\\n    \"\"\"\\n    try:\\n        nargs = len(args)\\n    except TypeError:\\n        args = (args,)\\n        nargs = 1\\n    try:\\n        nexprs = len(exprs)\\n    except TypeError:\\n        exprs = (exprs,)\\n        nexprs = 1\\n\\n    @_wrap_numbers\\n    def f(*inp):\\n        if len(inp) != nargs:\\n            raise TypeError(\"Incorrect number of arguments\")\\n        try:\\n            len(inp)\\n        except TypeError:\\n            inp = (inp,)\\n        subsd = dict(zip(args, inp))\\n        return [expr.subs(subsd).evalf() for expr in exprs][\\n            0 if nexprs == 1 else slice(None)]\\n    return f', 'def __init__(self, syms, exprs):\\n        self.syms = syms\\n        self.exprs = exprs', 'def _eval(expr_iter):\\n            return [expr.subs(subsd).evalf() for expr in expr_iter]']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self, out='noise', realization=0, component=0, noise='noise',\\n                 rate=None, altFFT=False):\\n\\n        # We call the parent class constructor, which currently does nothing\\n        super().__init__()\\n\\n        self._out = out\\n        self._oversample = 2\\n        self._realization = realization\\n        self._component = component\\n        self._noisekey = noise\\n        self._rate = rate\\n        self._altfft = altFFT\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def test_scaffold(self, mock_cookiecutter):\\n        from molo.core.scripts import cli\\n        package = pkg_resources.get_distribution('molo.core')\\n\\n        runner = CliRunner()\\n        runner.invoke(cli.scaffold, ['foo'])\\n        [call] = mock_cookiecutter.call_args_list\\n        args, kwargs = call\\n        self.assertTrue(kwargs['extra_context'].pop('secret_key'))\\n        self.assertEqual(kwargs, {\\n            'no_input': True,\\n            'extra_context': {\\n                'app_name': 'foo',\\n                'directory': 'foo',\\n                'author': 'Praekelt Foundation',\\n                'author_email': 'dev@praekelt.com',\\n                'url': None,\\n                'license': 'BSD',\\n                'molo_version': package.version,\\n                'require': (),\\n                'include': (),\\n            }\\n        })\", \"def test_scaffold_with_custom_dir(self, mock_cookiecutter):\\n        from molo.core.scripts import cli\\n        package = pkg_resources.get_distribution('molo.core')\\n\\n        runner = CliRunner()\\n        runner.invoke(cli.scaffold, ['foo', 'bar'])\\n        [call] = mock_cookiecutter.call_args_list\\n        args, kwargs = call\\n        self.assertTrue(kwargs['extra_context'].pop('secret_key'))\\n        self.assertEqual(kwargs, {\\n            'no_input': True,\\n            'extra_context': {\\n                'app_name': 'foo',\\n                'directory': 'bar',\\n                'author': 'Praekelt Foundation',\\n                'author_email': 'dev@praekelt.com',\\n                'url': None,\\n                'license': 'BSD',\\n                'molo_version': package.version,\\n                'require': (),\\n                'include': (),\\n            }\\n        })\", \"def test_scaffold_with_requirements(self, mock_cookiecutter):\\n        from molo.core.scripts import cli\\n        package = pkg_resources.get_distribution('molo.core')\\n\\n        runner = CliRunner()\\n        runner.invoke(cli.scaffold, ['foo', '--require', 'bar'])\\n        [call] = mock_cookiecutter.call_args_list\\n        args, kwargs = call\\n        self.assertTrue(kwargs['extra_context'].pop('secret_key'))\\n        self.assertEqual(kwargs, {\\n            'no_input': True,\\n            'extra_context': {\\n                'app_name': 'foo',\\n                'directory': 'foo',\\n                'author': 'Praekelt Foundation',\\n                'author_email': 'dev@praekelt.com',\\n                'url': None,\\n                'license': 'BSD',\\n                'molo_version': package.version,\\n                'require': ('bar',),\\n                'include': (),\\n            }\\n        })\", \"def test_scaffold_with_includes(self, mock_cookiecutter):\\n        from molo.core.scripts import cli\\n        package = pkg_resources.get_distribution('molo.core')\\n\\n        runner = CliRunner()\\n        runner.invoke(cli.scaffold, ['foo', '--include', 'bar', 'baz'])\\n        [call] = mock_cookiecutter.call_args_list\\n        args, kwargs = call\\n        self.assertTrue(kwargs['extra_context'].pop('secret_key'))\\n        self.assertEqual(kwargs, {\\n            'no_input': True,\\n            'extra_context': {\\n                'app_name': 'foo',\\n                'directory': 'foo',\\n                'author': 'Praekelt Foundation',\\n                'author_email': 'dev@praekelt.com',\\n                'url': None,\\n                'license': 'BSD',\\n                'molo_version': package.version,\\n                'require': (),\\n                'include': (('bar', 'baz'),),\\n            }\\n        })\", \"def test_unpack(self, mock_copytree, mock_get_template_dirs,\\n                    mock_get_package):\\n        package = pkg_resources.get_distribution('molo.core')\\n        mock_get_package.return_value = package\\n        mock_get_template_dirs.return_value = ['foo']\\n        mock_copytree.return_value = True\\n\\n        from molo.core.scripts import cli\\n        runner = CliRunner()\\n        runner.invoke(cli.unpack_templates, ['app1', 'app2'])\\n\\n        mock_copytree.assert_called_with(\\n            pkg_resources.resource_filename('molo.core', 'templates/foo'),\\n            pkg_resources.resource_filename('molo.core', 'templates/foo'))\"]}, {'features': [], 'snippets': ['def __new__(cls, array, mapping):\\n        obj = np.asarray(array).view(cls)\\n        obj.mapping = {SplitName(k): v for k, v in mapping.items()}\\n        return obj', 'def _mapped_eq(self, other):\\n        if other in self.mapping:\\n            return super().__eq__(self.mapping[other])\\n        else:\\n            result = np.zeros(len(self), dtype=np.bool)\\n            for k, v in self.mapping.items():\\n                if k == other:\\n                    result = np.logical_or(result, super().__eq__(v))\\n            return result', 'def __ne__(self, other):\\n        if isinstance(other, str):\\n            return np.logical_not(self._mapped_eq(other))\\n        else:\\n            return super().__ne__(other)', \"def __init__(self, *args, **kwargs):\\n        mapping = kwargs.pop('mapping', {})\\n        if not mapping:\\n            mapping = getattr(args[0], 'mapping', {})\\n\\n        super().__init__(*args, **kwargs)\\n        self.data = AliasArray(self.data, mapping)\", \"def format(self):\\n        return 'csr'\", 'def format(self, _):\\n        pass', 'def mapping(self):\\n        return self.data.mapping', \"def __getitem__(self, item):\\n        result = super().__getitem__(item)\\n        if getattr(result, 'format', '') == 'csr':\\n            return AliasCSRMatrix(result, mapping=self.mapping)\\n        else:\\n            return result\", 'def __init__(self, value, shape):\\n            self.value = value\\n            self.shape = shape', 'def __array__(self):\\n            return np.full(self.shape, self.value)', 'def __str__(self):\\n        return self.name', 'def __ne__(self, other):\\n        return self.LazyArray(self.name != other, self.shape)', 'def eye(self):\\n        return np.eye(*self.orbs)', 'def first(self):\\n        return self.split(\"|\")[0]', 'def __ne__(self, other):\\n        return super().__ne__(other) and self.first != other']}, {'features': [], 'snippets': ['def test_coverage(self):\\n        all_np_ufuncs = set([ufunc for ufunc in np.core.umath.__dict__.values()\\n                             if type(ufunc) == np.ufunc])\\n\\n        from .. import quantity_helper as qh\\n\\n        all_q_ufuncs = (qh.UNSUPPORTED_UFUNCS |\\n                        set(qh.UFUNC_HELPERS.keys()))\\n\\n        assert all_np_ufuncs - all_q_ufuncs == set([])\\n        assert all_q_ufuncs - all_np_ufuncs == set([])', 'def test_sin_scalar(self):\\n        q = np.sin(30. * u.degree)\\n        assert q.unit == u.dimensionless_unscaled\\n        assert_allclose(q.value, 0.5)', 'def test_arcsin_scalar(self):\\n        q1 = 30. * u.degree\\n        q2 = np.arcsin(np.sin(q1)).to(q1.unit)\\n        assert_allclose(q1.value, q2.value)', 'def test_sin_invalid_units(self):\\n        with pytest.raises(TypeError) as exc:\\n            np.sin(3. * u.m)\\n        assert exc.value.args[0] == (\"Can only apply \\'sin\\' function \"\\n                                     \"to quantities with angle units\")', \"def test_arcsin_no_warning_on_unscaled_quantity(self):\\n        a = 15 * u.kpc\\n        b = 27 * u.pc\\n\\n        with warnings.catch_warnings():\\n            warnings.filterwarnings('error')\\n            np.arcsin(b/a)\", 'def test_cos_array(self):\\n        q = np.cos(np.array([0., np.pi / 4., np.pi / 2.]) * u.radian)\\n        assert q.unit == u.dimensionless_unscaled\\n        assert_allclose(q.value,\\n                        np.array([1., 1. / np.sqrt(2.), 0.]), atol=1.e-15)', 'def test_arccos_array(self):\\n        q1 = np.array([0., np.pi / 4., np.pi / 2.]) * u.radian\\n        q2 = np.arccos(np.cos(q1)).to(q1.unit)\\n        assert_allclose(q1.value, q2.value)', 'def test_arccos_invalid_units(self):\\n        with pytest.raises(TypeError) as exc:\\n            np.arccos(3. * u.s)\\n        assert exc.value.args[0] == (\"Can only apply \\'arccos\\' function to \"\\n                                     \"dimensionless quantities\")', 'def test_tan_array(self):\\n        q = np.tan(np.array([0., 45., 135., 180.]) * u.degree)\\n        assert q.unit == u.dimensionless_unscaled\\n        assert_allclose(q.value,\\n                        np.array([0., 1., -1., 0.]), atol=1.e-15)', 'def test_arctan_array(self):\\n        q = np.array([10., 30., 70., 80.]) * u.degree\\n        assert_allclose(np.arctan(np.tan(q)).to_value(q.unit), q.value)', 'def test_arctan_invalid_units(self):\\n        with pytest.raises(TypeError) as exc:\\n            np.arctan(np.array([1, 2, 3]) * u.N)\\n        assert exc.value.args[0] == (\"Can only apply \\'arctan\\' function to \"\\n                                     \"dimensionless quantities\")', 'def test_arctan2_invalid(self):\\n        with pytest.raises(u.UnitsError) as exc:\\n            np.arctan2(np.array([1, 2, 3]) * u.N, 1. * u.s)\\n        assert \"compatible dimensions\" in exc.value.args[0]\\n        with pytest.raises(u.UnitsError) as exc:\\n            np.arctan2(np.array([1, 2, 3]) * u.N, 1.)\\n        assert \"dimensionless quantities when other arg\" in exc.value.args[0]', \"def test_degrees(self):\\n\\n        # the following doesn't make much sense in terms of the name of the\\n        # routine, but we check it gives the correct result.\\n        q1 = np.rad2deg(60. * u.degree)\\n        assert_allclose(q1.value, 60.)\\n        assert q1.unit == u.degree\\n\\n        q2 = np.degrees(60. * u.degree)\\n        assert_allclose(q2.value, 60.)\\n        assert q2.unit == u.degree\\n\\n        q3 = np.rad2deg(np.pi * u.radian)\\n        assert_allclose(q3.value, 180.)\\n        assert q3.unit == u.degree\\n\\n        q4 = np.degrees(np.pi * u.radian)\\n        assert_allclose(q4.value, 180.)\\n        assert q4.unit == u.degree\\n\\n        with pytest.raises(TypeError):\\n            np.rad2deg(3. * u.m)\\n\\n        with pytest.raises(TypeError):\\n            np.degrees(3. * u.m)\", 'def test_multiply_scalar(self):\\n        assert np.multiply(4. * u.m, 2. / u.s) == 8. * u.m / u.s\\n        assert np.multiply(4. * u.m, 2.) == 8. * u.m\\n        assert np.multiply(4., 2. / u.s) == 8. / u.s', 'def test_divide_scalar(self, function):\\n        assert function(4. * u.m, 2. * u.s) == function(4., 2.) * u.m / u.s\\n        assert function(4. * u.m, 2.) == function(4., 2.) * u.m\\n        assert function(4., 2. * u.s) == function(4., 2.) / u.s', 'def test_divide_array(self, function):\\n        assert np.all(function(np.arange(3.) * u.m, 2. * u.s) ==\\n                      function(np.arange(3.), 2.) * u.m / u.s)', 'def test_sqrt_scalar(self):\\n        assert np.sqrt(4. * u.m) == 2. * u.m ** 0.5', 'def test_square_scalar(self):\\n        assert np.square(4. * u.m) == 16. * u.m ** 2', 'def test_reciprocal_scalar(self):\\n        assert np.reciprocal(4. * u.m) == 0.25 / u.m', 'def test_heaviside_scalar(self):\\n        assert np.heaviside(0. * u.m, 0.5) == 0.5 * u.dimensionless_unscaled\\n        assert np.heaviside(0. * u.s,\\n                            25 * u.percent) == 0.25 * u.dimensionless_unscaled\\n        assert np.heaviside(2. * u.J, 0.25) == 1. * u.dimensionless_unscaled', 'def test_heaviside_array(self):\\n        values = np.array([-1., 0., 0., +1.])\\n        halfway = np.array([0.75, 0.25, 0.75, 0.25]) * u.dimensionless_unscaled\\n        assert np.all(np.heaviside(values * u.m,\\n                                   halfway * u.dimensionless_unscaled) ==\\n                      [0, 0.25, 0.75, +1.] * u.dimensionless_unscaled)', 'def test_cbrt_scalar(self):\\n        assert np.cbrt(8. * u.m**3) == 2. * u.m', 'def test_cbrt_array(self):\\n        # Calculate cbrt on both sides since on Windows the cube root of 64\\n        # does not exactly equal 4.  See 4388.\\n        values = np.array([1., 8., 64.])\\n        assert np.all(np.cbrt(values * u.m**3) ==\\n                      np.cbrt(values) * u.m)', 'def test_power_array(self):\\n        assert np.all(np.power(np.array([1., 2., 3.]) * u.m, 3.)\\n                      == np.array([1., 8., 27.]) * u.m ** 3)\\n        # regression check on #1696\\n        assert np.all(np.power(np.arange(4.) * u.m, 0.) ==\\n                      1. * u.dimensionless_unscaled)', 'def test_float_power_array(self):\\n        assert np.all(np.float_power(np.array([1., 2., 3.]) * u.m, 3.)\\n                      == np.array([1., 8., 27.]) * u.m ** 3)\\n        # regression check on #1696\\n        assert np.all(np.float_power(np.arange(4.) * u.m, 0.) ==\\n                      1. * u.dimensionless_unscaled)', 'def test_power_array_array(self):\\n        np.power(4. * u.m, [2., 4.])', 'def test_power_array_array2(self):\\n        np.power([2., 4.] * u.m, [2., 4.])', 'def test_power_invalid(self):\\n        with pytest.raises(TypeError) as exc:\\n            np.power(3., 4. * u.m)\\n        assert \"raise something to a dimensionless\" in exc.value.args[0]', 'def test_copysign_array(self):\\n        assert np.all(np.copysign(np.array([1., 2., 3.]) * u.s, -1.) == -np.array([1., 2., 3.]) * u.s)\\n        assert np.all(np.copysign(np.array([1., 2., 3.]) * u.s, -1. * u.m) == -np.array([1., 2., 3.]) * u.s)\\n        assert np.all(np.copysign(np.array([1., 2., 3.]) * u.s, np.array([-2., 2., -4.]) * u.m) == np.array([-1., 2., -3.]) * u.s)\\n\\n        q = np.copysign(np.array([1., 2., 3.]), -3 * u.m)\\n        assert np.all(q == np.array([-1., -2., -3.]))\\n        assert not isinstance(q, u.Quantity)', 'def test_ldexp_array(self):\\n        assert np.all(np.ldexp(np.array([1., 2., 3.]) * u.m, [3, 2, 1])\\n                      == np.array([8., 8., 6.]) * u.m)', 'def test_exp_scalar(self, function):\\n        q = function(3. * u.m / (6. * u.m))\\n        assert q.unit == u.dimensionless_unscaled\\n        assert q.value == function(0.5)', 'def test_exp_array(self, function):\\n        q = function(np.array([2., 3., 6.]) * u.m / (6. * u.m))\\n        assert q.unit == u.dimensionless_unscaled\\n        assert np.all(q.value\\n                      == function(np.array([1. / 3., 1. / 2., 1.])))\\n        # should also work on quantities that can be made dimensionless\\n        q2 = function(np.array([2., 3., 6.]) * u.m / (6. * u.cm))\\n        assert q2.unit == u.dimensionless_unscaled\\n        assert_allclose(q2.value,\\n                        function(np.array([100. / 3., 100. / 2., 100.])))', 'def test_exp_invalid_units(self, function):\\n        # Can\\'t use exp() with non-dimensionless quantities\\n        with pytest.raises(TypeError) as exc:\\n            function(3. * u.m / u.s)\\n        assert exc.value.args[0] == (\"Can only apply \\'{0}\\' function to \"\\n                                     \"dimensionless quantities\"\\n                                     .format(function.__name__))', 'def test_modf_array(self):\\n        v = np.arange(10.) * u.m / (500. * u.cm)\\n        q = np.modf(v)\\n        n = np.modf(v.to_value(u.dimensionless_unscaled))\\n        assert q[0].unit == u.dimensionless_unscaled\\n        assert q[1].unit == u.dimensionless_unscaled\\n        assert all(q[0].value == n[0])\\n        assert all(q[1].value == n[1])', 'def test_frexp_array(self):\\n        q = np.frexp(np.array([2., 3., 6.]) * u.m / (6. * u.m))\\n        assert all((_q0, _q1) == np.frexp(_d) for _q0, _q1, _d\\n                   in zip(q[0], q[1], [1. / 3., 1. / 2., 1.]))', 'def test_dimensionless_twoarg_array(self, function):\\n        q = function(np.array([2., 3., 6.]) * u.m / (6. * u.cm), 1.)\\n        assert q.unit == u.dimensionless_unscaled\\n        assert_allclose(q.value,\\n                        function(np.array([100. / 3., 100. / 2., 100.]), 1.))', 'def test_dimensionless_twoarg_invalid_units(self, function):\\n\\n        with pytest.raises(TypeError) as exc:\\n            function(1. * u.km / u.s, 3. * u.m / u.s)\\n        assert exc.value.args[0] == (\"Can only apply \\'{0}\\' function to \"\\n                                     \"dimensionless quantities\"\\n                                     .format(function.__name__))', 'def test_invariant_scalar(self, ufunc):\\n\\n        q_i = 4.7 * u.m\\n        q_o = ufunc(q_i)\\n        assert isinstance(q_o, u.Quantity)\\n        assert q_o.unit == q_i.unit\\n        assert q_o.value == ufunc(q_i.value)', 'def test_invariant_array(self, ufunc):\\n\\n        q_i = np.array([-3.3, 2.1, 10.2]) * u.kg / u.s\\n        q_o = ufunc(q_i)\\n        assert isinstance(q_o, u.Quantity)\\n        assert q_o.unit == q_i.unit\\n        assert np.all(q_o.value == ufunc(q_i.value))', 'def test_invariant_twoarg_scalar(self, ufunc):\\n\\n        q_i1 = 4.7 * u.m\\n        q_i2 = 9.4 * u.km\\n        q_o = ufunc(q_i1, q_i2)\\n        assert isinstance(q_o, u.Quantity)\\n        assert q_o.unit == q_i1.unit\\n        assert_allclose(q_o.value, ufunc(q_i1.value, q_i2.to_value(q_i1.unit)))', 'def test_invariant_twoarg_array(self, ufunc):\\n\\n        q_i1 = np.array([-3.3, 2.1, 10.2]) * u.kg / u.s\\n        q_i2 = np.array([10., -5., 1.e6]) * u.g / u.us\\n        q_o = ufunc(q_i1, q_i2)\\n        assert isinstance(q_o, u.Quantity)\\n        assert q_o.unit == q_i1.unit\\n        assert_allclose(q_o.value, ufunc(q_i1.value, q_i2.to_value(q_i1.unit)))', 'def test_invariant_twoarg_one_arbitrary(self, ufunc):\\n\\n        q_i1 = np.array([-3.3, 2.1, 10.2]) * u.kg / u.s\\n        arbitrary_unit_value = np.array([0.])\\n        q_o = ufunc(q_i1, arbitrary_unit_value)\\n        assert isinstance(q_o, u.Quantity)\\n        assert q_o.unit == q_i1.unit\\n        assert_allclose(q_o.value, ufunc(q_i1.value, arbitrary_unit_value))', 'def test_invariant_twoarg_invalid_units(self, ufunc):\\n\\n        q_i1 = 4.7 * u.m\\n        q_i2 = 9.4 * u.s\\n        with pytest.raises(u.UnitsError) as exc:\\n            ufunc(q_i1, q_i2)\\n        assert \"compatible dimensions\" in exc.value.args[0]', 'def test_comparison_valid_units(self, ufunc):\\n        q_i1 = np.array([-3.3, 2.1, 10.2]) * u.kg / u.s\\n        q_i2 = np.array([10., -5., 1.e6]) * u.g / u.Ms\\n        q_o = ufunc(q_i1, q_i2)\\n        assert not isinstance(q_o, u.Quantity)\\n        assert q_o.dtype == np.bool\\n        assert np.all(q_o == ufunc(q_i1.value, q_i2.to_value(q_i1.unit)))\\n        q_o2 = ufunc(q_i1 / q_i2, 2.)\\n        assert not isinstance(q_o2, u.Quantity)\\n        assert q_o2.dtype == np.bool\\n        assert np.all(q_o2 == ufunc((q_i1 / q_i2)\\n                                    .to_value(u.dimensionless_unscaled), 2.))\\n        # comparison with 0., inf, nan is OK even for dimensional quantities\\n        for arbitrary_unit_value in (0., np.inf, np.nan):\\n            ufunc(q_i1, arbitrary_unit_value)\\n            ufunc(q_i1, arbitrary_unit_value*np.ones(len(q_i1)))\\n        # and just for completeness\\n        ufunc(q_i1, np.array([0., np.inf, np.nan]))', 'def test_comparison_invalid_units(self, ufunc):\\n        q_i1 = 4.7 * u.m\\n        q_i2 = 9.4 * u.s\\n        with pytest.raises(u.UnitsError) as exc:\\n            ufunc(q_i1, q_i2)\\n        assert \"compatible dimensions\" in exc.value.args[0]', 'def test_one_argument_ufunc_inplace(self, value):\\n        # without scaling\\n        s = value * u.rad\\n        check = s\\n        np.sin(s, out=s)\\n        assert check is s\\n        assert check.unit == u.dimensionless_unscaled\\n        # with scaling\\n        s2 = (value * u.rad).to(u.deg)\\n        check2 = s2\\n        np.sin(s2, out=s2)\\n        assert check2 is s2\\n        assert check2.unit == u.dimensionless_unscaled\\n        assert_allclose(s.value, s2.value)', 'def test_one_argument_ufunc_inplace_2(self, value):\\n        \"\"\"Check inplace works with non-quantity input and quantity output\"\"\"\\n        s = value * u.m\\n        check = s\\n        np.absolute(value, out=s)\\n        assert check is s\\n        assert np.all(check.value == np.absolute(value))\\n        assert check.unit is u.dimensionless_unscaled\\n        np.sqrt(value, out=s)\\n        assert check is s\\n        assert np.all(check.value == np.sqrt(value))\\n        assert check.unit is u.dimensionless_unscaled\\n        np.exp(value, out=s)\\n        assert check is s\\n        assert np.all(check.value == np.exp(value))\\n        assert check.unit is u.dimensionless_unscaled\\n        np.arcsin(value/10., out=s)\\n        assert check is s\\n        assert np.all(check.value == np.arcsin(value/10.))\\n        assert check.unit is u.radian', 'def test_one_argument_two_output_ufunc_inplace(self, value):\\n        v = 100. * value * u.cm / u.m\\n        v_copy = v.copy()\\n        tmp = v.copy()\\n        check = v\\n        np.modf(v, tmp, v)  # cannot use out1,out2 keywords with numpy 1.7\\n        assert check is v\\n        assert check.unit == u.dimensionless_unscaled\\n        v2 = v_copy.to(u.dimensionless_unscaled)\\n        check2 = v2\\n        np.modf(v2, tmp, v2)\\n        assert check2 is v2\\n        assert check2.unit == u.dimensionless_unscaled\\n        # can also replace in last position if no scaling is needed\\n        v3 = v_copy.to(u.dimensionless_unscaled)\\n        check3 = v3\\n        np.modf(v3, v3, tmp)\\n        assert check3 is v3\\n        assert check3.unit == u.dimensionless_unscaled\\n        # in np<1.13, without __array_ufunc__, one cannot replace input with\\n        # first output when scaling\\n        v4 = v_copy.copy()\\n        if NUMPY_LT_1_13:\\n            with pytest.raises(TypeError):\\n                np.modf(v4, v4, tmp)\\n        else:\\n            check4 = v4\\n            np.modf(v4, v4, tmp)\\n            assert check4 is v4\\n            assert check4.unit == u.dimensionless_unscaled', 'def test_two_argument_ufunc_inplace_1(self, value):\\n        s = value * u.cycle\\n        check = s\\n        s /= 2.\\n        assert check is s\\n        assert np.all(check.value == value / 2.)\\n        s /= u.s\\n        assert check is s\\n        assert check.unit == u.cycle / u.s\\n        s *= 2. * u.s\\n        assert check is s\\n        assert np.all(check == value * u.cycle)', 'def test_two_argument_ufunc_inplace_2(self, value):\\n        s = value * u.cycle\\n        check = s\\n        np.arctan2(s, s, out=s)\\n        assert check is s\\n        assert check.unit == u.radian\\n        with pytest.raises(u.UnitsError):\\n            s += 1. * u.m\\n        assert check is s\\n        assert check.unit == u.radian\\n        np.arctan2(1. * u.deg, s, out=s)\\n        assert check is s\\n        assert check.unit == u.radian\\n        np.add(1. * u.deg, s, out=s)\\n        assert check is s\\n        assert check.unit == u.deg\\n        np.multiply(2. / u.s, s, out=s)\\n        assert check is s\\n        assert check.unit == u.deg / u.s', 'def test_two_argument_two_output_ufunc_inplace(self, value):\\n        v = value * u.m\\n        divisor = 70.*u.cm\\n        v1 = v.copy()\\n        tmp = v.copy()\\n        check = np.divmod(v1, divisor, out=(tmp, v1))\\n        assert check[0] is tmp and check[1] is v1\\n        assert tmp.unit == u.dimensionless_unscaled\\n        assert v1.unit == v.unit\\n        v2 = v.copy()\\n        check2 = np.divmod(v2, divisor, out=(v2, tmp))\\n        assert check2[0] is v2 and check2[1] is tmp\\n        assert v2.unit == u.dimensionless_unscaled\\n        assert tmp.unit == v.unit\\n        v3a = v.copy()\\n        v3b = v.copy()\\n        check3 = np.divmod(v3a, divisor, out=(v3a, v3b))\\n        assert check3[0] is v3a and check3[1] is v3b\\n        assert v3a.unit == u.dimensionless_unscaled\\n        assert v3b.unit == v.unit', 'def test_ufunc_inplace_non_standard_dtype(self):\\n        \"\"\"Check that inplace operations check properly for casting.\\n\\n        First two tests that check that float32 is kept close #3976.\\n        \"\"\"\\n        a1 = u.Quantity([1, 2, 3, 4], u.m, dtype=np.float32)\\n        a1 *= np.float32(10)\\n        assert a1.unit is u.m\\n        assert a1.dtype == np.float32\\n        a2 = u.Quantity([1, 2, 3, 4], u.m, dtype=np.float32)\\n        a2 += (20.*u.km)\\n        assert a2.unit is u.m\\n        assert a2.dtype == np.float32\\n        # For integer, in-place only works if no conversion is done.\\n        a3 = u.Quantity([1, 2, 3, 4], u.m, dtype=np.int32)\\n        a3 += u.Quantity(10, u.m, dtype=np.int64)\\n        assert a3.unit is u.m\\n        assert a3.dtype == np.int32\\n        a4 = u.Quantity([1, 2, 3, 4], u.m, dtype=np.int32)\\n        with pytest.raises(TypeError):\\n            a4 += u.Quantity(10, u.mm, dtype=np.int64)', \"def test_one_argument_ufunc_at(self):\\n        q = np.arange(10.) * u.m\\n        i = np.array([1, 2])\\n        qv = q.value.copy()\\n        np.negative.at(q, i)\\n        np.negative.at(qv, i)\\n        assert np.all(q.value == qv)\\n        assert q.unit is u.m\\n\\n        # cannot change from quantity to bool array\\n        with pytest.raises(TypeError):\\n            np.isfinite.at(q, i)\\n\\n        # for selective in-place, cannot change the unit\\n        with pytest.raises(u.UnitsError):\\n            np.square.at(q, i)\\n\\n        # except if the unit does not change (i.e., dimensionless)\\n        d = np.arange(10.) * u.dimensionless_unscaled\\n        dv = d.value.copy()\\n        np.square.at(d, i)\\n        np.square.at(dv, i)\\n        assert np.all(d.value == dv)\\n        assert d.unit is u.dimensionless_unscaled\\n\\n        d = np.arange(10.) * u.dimensionless_unscaled\\n        dv = d.value.copy()\\n        np.log.at(d, i)\\n        np.log.at(dv, i)\\n        assert np.all(d.value == dv)\\n        assert d.unit is u.dimensionless_unscaled\\n\\n        # also for sine it doesn't work, even if given an angle\\n        a = np.arange(10.) * u.radian\\n        with pytest.raises(u.UnitsError):\\n            np.sin.at(a, i)\\n\\n        # except, for consistency, if we have made radian equivalent to\\n        # dimensionless (though hopefully it will never be needed)\\n        av = a.value.copy()\\n        with u.add_enabled_equivalencies(u.dimensionless_angles()):\\n            np.sin.at(a, i)\\n            np.sin.at(av, i)\\n            assert_allclose(a.value, av)\\n\\n            # but we won't do double conversion\\n            ad = np.arange(10.) * u.degree\\n            with pytest.raises(u.UnitsError):\\n                np.sin.at(ad, i)\", 'def test_one_argument_ufunc_reduce_accumulate(self):\\n        # one argument cannot be used\\n        s = np.arange(10.) * u.radian\\n        i = np.array([0, 5, 1, 6])\\n        with pytest.raises(ValueError):\\n            np.sin.reduce(s)\\n        with pytest.raises(ValueError):\\n            np.sin.accumulate(s)\\n        with pytest.raises(ValueError):\\n            np.sin.reduceat(s, i)', 'def test_one_argument_ufunc_outer(self):\\n        # one argument cannot be used\\n        s = np.arange(10.) * u.radian\\n        with pytest.raises(ValueError):\\n            np.sin.outer(s)']}, {'features': [], 'snippets': ['def __init__(self):\\n        # Retrieve ROS parameters and configuration and cosntruct various objects.\\n        self.robot_name = rospy.get_param(\"robot_name\")\\n        self.planner_config_name = rospy.get_param(\"planner_config_name\")\\n        self.current_joint_names = []\\n        self.current_group_name = \"\"\\n        self.plan_trajectory_wrapper = PlanTrajectoryWrapper(\"rr\", int(rospy.get_param(\"~num_rr_planners\")))\\n        self.invalid_section_wrapper = InvalidSectionWrapper()\\n        self.path_library = PathLibrary(rospy.get_param(\"~path_library_dir\"), rospy.get_param(\"step_size\"), node_size=int(rospy.get_param(\"~path_library_path_node_size\")), sg_node_size=int(rospy.get_param(\"~path_library_sg_node_size\")), dtw_dist=float(rospy.get_param(\"~dtw_distance\")))\\n        self.num_paths_checked = int(rospy.get_param(\"~num_paths_to_collision_check\"))\\n        self.stop_lock = threading.Lock()\\n        self.stop = True\\n        self.rr_server = actionlib.SimpleActionServer(RR_NODE_NAME, RRAction, execute_cb=self._retrieve_repair, auto_start=False)\\n        self.rr_server.start()\\n        self.stop_rr_subscriber = rospy.Subscriber(STOP_RR_NAME, StopPlanning, self._stop_rr_planner)\\n        self.stop_rr_planner_publisher = rospy.Publisher(STOP_PLANNER_NAME, StopPlanning, queue_size=10)\\n        self.manage_library_service = rospy.Service(MANAGE_LIBRARY, ManagePathLibrary, self._do_manage_action)\\n        self.stats_pub = rospy.Publisher(\"rr_stats\", RRStats, queue_size=10)\\n        self.repaired_sections_lock = threading.Lock()\\n        self.repaired_sections = []\\n        self.working_lock = threading.Lock() #to ensure that node is not doing RR and doing a library management action at the same time\\n\\n        #if draw_points is True, then display points in rviz\\n        self.draw_points = rospy.get_param(\"draw_points\")\\n        if self.draw_points:\\n            self.draw_points_wrapper = DrawPointsWrapper()', 'def _call_planner(self, start, goal, planning_time):\\n        \"\"\"\\n          Calls a standard planner to plan between two points with an allowed\\n            planning time.\\n\\n          Args:\\n            start (list of float): A joint configuration corresponding to the\\n              start position of the path.\\n            goal (list of float): The jount configuration corresponding to the\\n              goal position for the path.\\n\\n          Returns:\\n            path: A list of joint configurations corresponding to the planned\\n              path.\\n        \"\"\"\\n        ret = None\\n        planner_number = self.plan_trajectory_wrapper.acquire_planner()\\n        if not self._need_to_stop():\\n            ret = self.plan_trajectory_wrapper.plan_trajectory(start, goal, planner_number, self.current_joint_names, self.current_group_name, planning_time, self.planner_config_name)\\n        self.plan_trajectory_wrapper.release_planner(planner_number)\\n        return ret', 'def _need_to_stop(self):\\n        self.stop_lock.acquire();\\n        ret = self.stop;\\n        self.stop_lock.release();\\n        return ret;', 'def do_retrieved_path_drawing(self, projected, retrieved, invalid):\\n        \"\"\"\\n          Draws the points from the various paths involved in the planning\\n            in different colors in different namespaces.\\n          All of the arguments are lists of joint configurations, where each\\n            joint configuration is a list of joint angles.\\n          The only distinction between the different arguments being passed in\\n            are which color the points in question are being drawn in.\\n          Uses the DrawPointsWrapper to draw the points.\\n\\n          Args:\\n            projected (list of list of float): List of points to draw as\\n              projected between the library path and the actual start/goal\\n              position. Will be drawn in blue.\\n            retrieved (list of list of float): The path retrieved straight\\n              from the path library. Will be drawn in white.\\n            invalid (list of list of float): List of points which were invalid.\\n              Will be drawn in red.\\n        \"\"\"\\n        if len(projected) > 0:\\n            if self.draw_points:\\n                self.draw_points_wrapper.draw_points(retrieved, self.current_group_name, \"retrieved\", DrawPointsWrapper.ANGLES, DrawPointsWrapper.WHITE, 0.1)\\n                projectionDisplay = projected[:projected.index(retrieved[0])]+projected[projected.index(retrieved[-1])+1:]\\n                self.draw_points_wrapper.draw_points(projectionDisplay, self.current_group_name, \"projection\", DrawPointsWrapper.ANGLES, DrawPointsWrapper.BLUE, 0.2)\\n                invalidDisplay = []\\n                for invSec in invalid:\\n                    invalidDisplay += projected[invSec[0]+1:invSec[-1]]\\n                self.draw_points_wrapper.draw_points(invalidDisplay, self.current_group_name, \"invalid\", DrawPointsWrapper.ANGLES, DrawPointsWrapper.RED, 0.2)', 'def _path_repair(self, original_path, planning_time, invalid_sections=None, use_parallel_repairing=True):\\n        \"\"\"\\n          Goes through each invalid section in a path and calls a planner to\\n            repair it, with the potential for multi-threading. Returns the\\n            repaired path.\\n\\n          Args:\\n            original_path (path): The original path which needs repairing.\\n            planning_time (float): The maximum allowed planning time for\\n              each repair, in seconds.\\n            invalid_sections (list of pairs of indicies): The pairs of indicies\\n              describing the invalid sections. If None, then the invalid\\n              sections will be computed by this function.\\n            use_parallel_repairing (bool): Whether or not to use multi-threading.\\n\\n          Returns:\\n            path: The repaired path.\\n        \"\"\"\\n        zeros_tuple = tuple([0 for i in xrange(len(self.current_joint_names))])\\n        rospy.loginfo(\"RR action server: got path with %d points\" % len(original_path))\\n\\n        if invalid_sections is None:\\n            invalid_sections = self.invalid_section_wrapper.getInvalidSectionsForPath(original_path, self.current_group_name)\\n        rospy.loginfo(\"RR action server: invalid sections: %s\" % (str(invalid_sections)))\\n        if len(invalid_sections) > 0:\\n            if invalid_sections[0][0] == -1:\\n                rospy.loginfo(\"RR action server: Start is not a valid state...nothing can be done\")\\n                return None\\n            if invalid_sections[-1][1] == len(original_path):\\n                rospy.loginfo(\"RR action server: Goal is not a valid state...nothing can be done\")\\n                return None\\n\\n            if use_parallel_repairing:\\n                #multi-threaded repairing\\n                self.repaired_sections = [None for i in xrange(len(invalid_sections))]\\n                #each thread replans an invalid section\\n                threadList = []\\n                for i, sec in enumerate(invalid_sections):\\n                    th = threading.Thread(target=self._repair_thread, args=(i, original_path[sec[0]], original_path[sec[-1]], sec[0], sec[-1], planning_time))\\n                    threadList.append(th)\\n                    th.start()\\n                for th in threadList:\\n                    th.join()\\n                #once all threads return, then the repaired sections can be combined\\n                for item in self.repaired_sections:\\n                    if item is None:\\n                        rospy.loginfo(\"RR action server: RR node was stopped during repair or repair failed\")\\n                        return None\\n                #replace invalid sections with replanned sections\\n                new_path = original_path[0:invalid_sections[0][0]]\\n                for i in xrange(len(invalid_sections)):\\n                    new_path += self.repaired_sections[i]\\n                    if i+1 < len(invalid_sections):\\n                        new_path += original_path[invalid_sections[i][1]+1:invalid_sections[i+1][0]]\\n                new_path += original_path[invalid_sections[-1][1]+1:]\\n                self.repaired_sections = [] #reset repaired_sections\\n            else:\\n                #single-threaded repairing\\n                rospy.loginfo(\"RR action server: Got invalid sections: %s\" % str(invalid_sections))\\n                new_path = original_path[0:invalid_sections[0][0]]\\n                for i in xrange(len(invalid_sections)):\\n                    if not self._need_to_stop():\\n                        #start_invalid and end_invalid must correspond to valid states when passed to the planner\\n                        start_invalid, end_invalid = invalid_sections[i]\\n                        rospy.loginfo(\"RR action server: Requesting path to replace from %d to %d\" % (start_invalid, end_invalid))\\n                        repairedSection = self._call_planner(original_path[start_invalid], original_path[end_invalid])\\n                        if repairedSection is None:\\n                            rospy.loginfo(\"RR action server: RR section repair was stopped or failed\")\\n                            return None\\n                        rospy.loginfo(\"RR action server: Planner returned a trajectory of %d points for %d to %d\" % (len(repairedSection), start_invalid, end_invalid))\\n                        new_path += repairedSection\\n                        if i+1 < len(invalid_sections):\\n                            new_path += original_path[end_invalid+1:invalid_sections[i+1][0]]\\n                    else:\\n                        rospy.loginfo(\"RR action server: RR was stopped while it was repairing the retrieved path\")\\n                        return None\\n                new_path += original_path[invalid_sections[-1][1]+1:]\\n            rospy.loginfo(\"RR action server: Trajectory after replan has %d points\" % len(new_path))\\n        else:\\n            new_path = original_path\\n\\n        rospy.loginfo(\"RR action server: new trajectory has %i points\" % (len(new_path)))\\n        return new_path', 'def _do_manage_action(self, request):\\n        \"\"\"\\n          Processes a ManagePathLibraryRequest as part of the ManagePathLibrary\\n            service. Basically, either stores a path in the library or deletes it.\\n        \"\"\"\\n        response = ManagePathLibraryResponse()\\n        response.result = response.FAILURE\\n        if request.robot_name == \"\" or len(request.joint_names) == 0:\\n            rospy.logerr(\"RR action server: robot name or joint names were not provided\")\\n            return response\\n\\n        self.working_lock.acquire()\\n        if request.action == request.ACTION_STORE:\\n            rospy.loginfo(\"RR action server: got a path to store in path library\")\\n            if len(request.path_to_store) > 0:\\n                new_path = [p.positions for p in request.path_to_store]\\n\\n                if len(request.retrieved_path) == 0:\\n                    #PFS won so just store the path\\n                    store_path_result = self.path_library.store_path(new_path, request.robot_name, request.joint_names)\\n                else:\\n                    store_path_result = self.path_library.store_path(new_path, request.robot_name, request.joint_names, [p.positions for p in request.retrieved_path])\\n                response.result = response.SUCCESS\\n                response.path_stored, response.num_library_paths = store_path_result\\n            else:\\n                response.message = \"Path to store had no points\"\\n        elif request.action == request.ACTION_DELETE_PATH:\\n            rospy.loginfo(\"RR action server: got a request to delete path %i in the path library\" % (request.delete_id))\\n            if self.path_library.delete_path_by_id(request.delete_id, request.robot_name, request.joint_names):\\n                response.result = response.SUCCESS\\n            else:\\n                response.message = \"No path in the library had id %i\" % (request.delete_id)\\n        elif request.action == request.ACTION_DELETE_LIBRARY:\\n            rospy.loginfo(\"RR action server: got a request to delete library corresponding to robot %s and joints %s\" % (request.robot_name, request.joint_names))\\n            if self.path_library.delete_library(request.robot_name, request.joint_names):\\n                response.result = response.SUCCESS\\n            else:\\n                response.message = \"No library corresponding to robot %s and joint names %s exists\"\\n        else:\\n            rospy.logerr(\"RR action server: manage path library request did not have a valid action set\")\\n        self.working_lock.release()\\n        return response']}, {'features': [], 'snippets': ['def send(msg):\\n   irc.send(msg + \"\\\\r\\\\n\")\\n   print \"{SENT} \" + msg\\n   return', \"def processline(line):\\n   parts = line.split(' :',1)\\n   args = parts[0].split(' ')\\n   if (len(parts) > 1):\\n      args.append(parts[1])\"]}, {'features': [], 'snippets': ['def func(self):\\n        session = self.caller\\n        player = PlayerDB.objects.player_search(self.lhs)\\n        if len(player) != 1:\\n            player = None\\n        else:\\n            player = player[0]\\n            if player.name.lower() != self.lhs.lower():\\n                player=None\\n        pswd = None\\n        if player:\\n            pswd = self.rhs == player.db.magic_cookie\\n\\n        if not (player and pswd):\\n        # No playername or password match\\n            session.msg(\"Could not verify Magic Cookie. Please email the server administrator for assistance.\")\\n            return\\n\\n        # Check IP and/or name bans\\n        bans = ServerConfig.objects.conf(\"server_bans\") \\n        if bans and (any(tup[0]==player.name for tup in bans)\\n                     or\\n                     any(tup[2].match(session.address[0]) for tup in bans if tup[2])):\\n            # this is a banned IP or name!\\n            string = \"{rYou have been banned and cannot continue from here.\"\\n            string += \"\\\\nIf you feel this ban is in error, please email an admin.{x\"\\n            session.msg(string)\\n            session.execute_cmd(\"quit\")\\n            return\\n\\n        session.sessionhandler.login(session, player)', 'def func(self):\\n        \"\"\"\\n        Uses the Django admin api. Note that unlogged-in commands\\n        have a unique position in that their func() receives\\n        a session object instead of a source_object like all\\n        other types of logged-in commands (this is because\\n        there is no object yet before the player has logged in)\\n        \"\"\"\\n\\n        session = self.caller\\n        args = self.args\\n        # extract quoted parts\\n        parts = [part.strip() for part in re.split(r\"\\\\\"|\\\\\\'\", args) if part.strip()]\\n        if len(parts) == 1:\\n            # this was (hopefully) due to no quotes being found\\n            parts = parts[0].split(None, 1)\\n        if len(parts) != 2:\\n            session.msg(\"\\\\n\\\\r Usage (without <>): connect <name> <password>\")\\n            return\\n        playername, password = parts\\n\\n        # Match account name and check password\\n        player = PlayerDB.objects.player_search(playername)\\n        if len(player) != 1:\\n            player = None\\n        else:\\n            player = player[0]\\n            if player.name.lower() != playername.lower():\\n                player=None\\n        pswd = None\\n        if player:\\n            pswd = player.check_password(password)\\n\\n        if not (player and pswd):\\n        # No playername or password match\\n            string = \"Wrong login information given.\\\\nIf you have spaces in your name or \"\\n            string += \"password, don\\'t forget to enclose it in quotes. Also capitalization matters.\"\\n            string += \"\\\\nIf you are new you should first create a new account \"\\n            string += \"using the \\'create\\' command.\"\\n            session.msg(string)\\n            return\\n\\n        # Check IP and/or name bans\\n        bans = ServerConfig.objects.conf(\"server_bans\")\\n        if bans and (any(tup[0]==player.name for tup in bans)\\n                     or\\n                     any(tup[2].match(session.address[0]) for tup in bans if tup[2])):\\n            # this is a banned IP or name!\\n            string = \"{rYou have been banned and cannot continue from here.\"\\n            string += \"\\\\nIf you feel this ban is in error, please email an admin.{x\"\\n            session.msg(string)\\n            session.execute_cmd(\"quit\")\\n            return\\n\\n        # actually do the login. This will call all other hooks:\\n        #   session.at_init()\\n        #   if character:\\n        #      at_first_login()  # only once\\n        #      at_pre_login()\\n        #   player.at_post_login()     - calls look if no character is set\\n        #   character.at_post_login()  - this calls look command by default\\n        session.sessionhandler.login(session, player)', 'def func(self):\\n        \"Do checks and create account\"\\n\\n        session = self.caller\\n        args = self.args.strip()\\n\\n        # extract quoted parts\\n        parts = [part.strip() for part in re.split(r\"\\\\\"|\\\\\\'\", args) if part.strip()]\\n        if len(parts) == 1:\\n            # this was (hopefully) due to no quotes being found\\n            parts = parts[0].split(None, 1)\\n        if len(parts) != 2:\\n            string = \"\\\\n Usage (without <>): create <name> <password>\"\\n            string += \"\\\\nIf <name> or <password> contains spaces, enclose it in quotes.\"\\n            session.msg(string)\\n            return\\n        playername, password = parts\\n        print \"playername \\'%s\\', password: \\'%s\\'\" % (playername, password)\\n\\n        # sanity checks\\n        if not re.findall(\\'^[\\\\w. @+-]+$\\', playername) or not (0 < len(playername) <= 30):\\n            # this echoes the restrictions made by django\\'s auth module (except not\\n            # allowing spaces, for convenience of logging in).\\n            string = \"\\\\n\\\\r Playername can max be 30 characters or fewer. Letters, spaces, digits and @/./+/-/_ only.\"\\n            session.msg(string)\\n            return\\n        # strip excessive spaces in playername\\n        playername = re.sub(r\"\\\\s+\", \" \", playername).strip()\\n        if PlayerDB.objects.filter(user__username__iexact=playername) or PlayerDB.objects.filter(username__iexact=playername):\\n            # player already exists (we also ignore capitalization here)\\n            session.msg(\"Sorry, there is already a player with the name \\'%s\\'.\" % playername)\\n            return\\n        if not re.findall(\\'^[\\\\w. @+-]+$\\', password) or not (3 < len(password)):\\n            string = \"\\\\n\\\\r Password should be longer than 3 characers. Letters, spaces, digits and @\\\\.\\\\+\\\\-\\\\_ only.\"\\n            string += \"\\\\nFor best security, make it longer than 8 characters. You can also use a phrase of\"\\n            string += \"\\\\nmany words if you enclose the password in quotes.\"\\n            session.msg(string)\\n            return\\n\\n        # everything\\'s ok. Create the new player account.\\n        try:\\n            default_home = ObjectDB.objects.get_id(settings.CHARACTER_DEFAULT_HOME)\\n\\n            typeclass = settings.BASE_CHARACTER_TYPECLASS\\n            permissions = settings.PERMISSION_PLAYER_DEFAULT\\n\\n            try:\\n                new_character = create.create_player(playername, None, password,\\n                                                     permissions=permissions,\\n                                                     character_typeclass=typeclass,\\n                                                     character_location=default_home,\\n                                                     character_home=default_home)\\n            except Exception:\\n                session.msg(\"There was an error creating the default Character/Player:\\\\n%s\\\\n If this problem persists, contact an admin.\")\\n                return\\n            new_player = new_character.player\\n\\n            # This needs to be called so the engine knows this player is logging in for the first time.\\n            # (so it knows to call the right hooks during login later)\\n            utils.init_new_player(new_player)\\n\\n            # join the new player to the public channel\\n            pchanneldef = settings.CHANNEL_PUBLIC\\n            if pchanneldef:\\n                pchannel = Channel.objects.get_channel(pchanneldef[0])\\n                if not pchannel.connect_to(new_player):\\n                    string = \"New player \\'%s\\' could not connect to public channel!\" % new_player.key\\n                    logger.log_errmsg(string)\\n\\n            # allow only the character itself and the player to puppet this character (and Immortals).\\n            new_character.locks.add(\"puppet:id(%i) or pid(%i) or perm(Immortals) or pperm(Immortals)\" %\\n                                    (new_character.id, new_player.id))\\n\\n\\n            # If no description is set, set a default description\\n            if not new_character.db.desc:\\n                new_character.db.desc = \"This is a Player.\"\\n\\n            # tell the caller everything went well.\\n            string = \"A new account \\'%s\\' was created. Welcome!\"\\n            if \" \" in playername:\\n                string += \"\\\\n\\\\nYou can now log in with the command \\'connect \\\\\"%s\\\\\" <your password>\\'.\"\\n            else:\\n                string += \"\\\\n\\\\nYou can now log with the command \\'connect %s <your password>\\'.\"\\n            session.msg(string % (playername, playername))\\n\\n        except Exception:\\n            # We are in the middle between logged in and -not, so we have to handle tracebacks\\n            # ourselves at this point. If we don\\'t, we won\\'t see any errors at all.\\n            string = \"%s\\\\nThis is a bug. Please e-mail an admin if the problem persists.\"\\n            session.msg(string % (traceback.format_exc()))\\n            logger.log_errmsg(traceback.format_exc())', 'def func(self):\\n        \"Simply close the connection.\"\\n        session = self.caller\\n        session.msg(\"Good bye! Disconnecting ...\")\\n        session.session_disconnect()', 'def func(self):\\n        \"Show the connect screen.\"\\n        self.caller.msg(CONNECTION_SCREEN)', 'def func(self):\\n        \"Shows help\"\\n\\n        string = \\\\\\n            \"\"\"']}, {'features': [], 'snippets': ['def EscapeJson(data):\\n  return \\'\"\\' + json.dumps(data).replace(\\'\"\\', r\\'\\\\\"\\') + \\'\"\\'']}, {'features': [], 'snippets': [\"def __init__(self, **kw):\\n        super(EnhancedDummyRequest, self).__init__(**kw)\\n        self.GET = MultiDict(self.GET)\\n        # Make sure content_type attr exists is not passed in via **kw\\n        self.content_type = getattr(self, 'content_type', None)\", 'def validation_context(request, response=None):\\n    try:\\n        yield\\n    except Exception:\\n        raise CustomResponseValidationException', \"def get_registry(settings):\\n    registry = Registry('testing')\\n    config = Configurator(registry=registry)\\n    if getattr(registry, 'settings', None) is None:\\n        config._set_settings(settings)\\n    registry.registerUtility(RoutesMapper(), IRoutesMapper)\\n    config.commit()\\n    return registry\", 'def _validate_against_tween(request, response=None, **overrides):\\n    \"\"\"\\n    Acceptance testing helper for testing the validation tween with Swagger 1.2\\n    responses.\\n\\n    :param request: pytest fixture\\n    :param response: standard fixture by default\\n    \"\"\"\\n    def handler(request):\\n        return response or Response()\\n\\n    settings = dict({\\n        \\'pyramid_swagger.swagger_versions\\': [\\'1.2\\'],\\n        \\'pyramid_swagger.enable_swagger_spec_validation\\': False,\\n        \\'pyramid_swagger.schema_directory\\': \\'tests/sample_schemas/good_app/\\'},\\n        **overrides\\n    )\\n    settings[\\'pyramid_swagger.schema12\\'] = get_swagger_schema()\\n    settings[\\'pyramid_swagger.schema20\\'] = None\\n    registry = get_registry(settings)\\n\\n    # Let\\'s make request validation a no-op so we can focus our tests.\\n    with mock.patch.object(pyramid_swagger.tween, \\'validate_request\\'):\\n        validation_tween_factory(handler, registry)(request)', 'def test_500_when_response_is_missing_required_field():\\n    request = EnhancedDummyRequest(\\n        method=\\'GET\\',\\n        path=\\'/sample/path_arg1/resource\\',\\n        params={\\'required_arg\\': \\'test\\'},\\n        matchdict={\\'path_arg\\': \\'path_arg1\\'},\\n    )\\n    # Omit the logging_info key from the response.\\n    response = Response(\\n        body=simplejson.dumps({\\'raw_response\\': \\'foo\\'}),\\n        headers={\\'Content-Type\\': \\'application/json; charset=UTF-8\\'},\\n    )\\n    with pytest.raises(ResponseValidationError) as excinfo:\\n        _validate_against_tween(request, response=response)\\n    assert \"\\'logging_info\\' is a required property\" in str(excinfo.value)', \"def test_200_when_response_is_void_with_empty_response():\\n    request = EnhancedDummyRequest(\\n        method='GET',\\n        path='/sample/nonstring/{int_arg}/{float_arg}/{boolean_arg}',\\n        params={'required_arg': 'test'},\\n        matchdict={'int_arg': '1', 'float_arg': '2.0', 'boolean_arg': 'true'},\\n    )\\n    response = Response(body='{}')\\n    _validate_against_tween(request, response=response)\", 'def test_500_for_bad_validated_array_response():\\n    request = EnhancedDummyRequest(\\n        method=\\'GET\\',\\n        path=\\'/sample_array_response\\',\\n    )\\n    response = Response(\\n        body=simplejson.dumps([{\"enum_value\": \"bad_enum_value\"}]),\\n        headers={\\'Content-Type\\': \\'application/json; charset=UTF-8\\'},\\n    )\\n    with pytest.raises(ResponseValidationError) as excinfo:\\n        _validate_against_tween(request, response=response)\\n    assert \"is not one of [\\'good_enum_value\\']\" in str(excinfo.value)', \"def test_200_for_normal_response_validation():\\n    app = test_app(\\n        request=Mock(spec=FixtureRequest, param=['1.2']),\\n        **{'pyramid_swagger.enable_response_validation': True}\\n    )\\n    response = app.post_json('/sample', {'foo': 'test', 'bar': 'test'})\\n    assert response.status_code == 200\", 'def test_app_error_if_path_not_in_spec_and_path_validation_disabled():\\n    \"\"\"If path missing and validation is disabled we want to let something else\\n    handle the error. TestApp throws an AppError, but Pyramid would throw a\\n    HTTPNotFound exception.\\n    \"\"\"\\n    with pytest.raises(AppError):\\n        app = test_app(\\n            request=Mock(spec=FixtureRequest, param=[\\'1.2\\']),\\n            **{\\'pyramid_swagger.enable_path_validation\\': False}\\n        )\\n        assert app.get(\\'/this/path/doesnt/exist\\')']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, app=None):\\n        \"\"\"\\n        :param app: the flask application (eve itself). This can be used by\\n        the class to access, amongst other things, the app.config object to\\n        retrieve class-specific settings.\\n        \"\"\"\\n        self.app = app', 'def put(self, content, filename=None, content_type=None):\\n        \"\"\" Saves a new file using the storage system, preferably with the name\\n        specified. If there already exists a file with this name name, the\\n        storage system may modify the filename as necessary to get a unique\\n        name. Depending on the storage system, a unique id or the actual name\\n        of the stored file will be returned. The content type argument is used\\n        to appropriately identify the file when it is retrieved.\\n\\n        .. versionchanged:: 0.5\\n           Allow filename to be optional (#414).\\n        \"\"\"\\n        raise NotImplementedError']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def squeeze_image(img):\\n    ''' Return image, remove axes length 1 at end of image shape\\n\\n    For example, an image may have shape (10,20,30,1,1).  In this case\\n    squeeze will result in an image with shape (10,20,30).  See doctests\\n    for further description of behavior.\\n\\n    Parameters\\n    ----------\\n    img : ``SpatialImage``\\n\\n    Returns\\n    -------\\n    squeezed_img : ``SpatialImage``\\n       Copy of img, such that data, and data shape have been squeezed,\\n       for dimensions > 3rd, and at the end of the shape list\\n\\n    Examples\\n    --------\\n    >>> import nipype.externals.pynifti as nf\\n    >>> shape = (10,20,30,1,1)\\n    >>> data = np.arange(np.prod(shape)).reshape(shape)\\n    >>> affine = np.eye(4)\\n    >>> img = nf.Nifti1Image(data, affine)\\n    >>> img.get_shape()\\n    (10, 20, 30, 1, 1)\\n    >>> img2 = squeeze_image(img)\\n    >>> img2.get_shape()\\n    (10, 20, 30)\\n\\n    If the data are 3D then last dimensions of 1 are ignored\\n\\n    >>> shape = (10,1,1)\\n    >>> data = np.arange(np.prod(shape)).reshape(shape)\\n    >>> img = nf.ni1.Nifti1Image(data, affine)\\n    >>> img.get_shape()\\n    (10, 1, 1)\\n    >>> img2 = squeeze_image(img)\\n    >>> img2.get_shape()\\n    (10, 1, 1)\\n\\n    Only *final* dimensions of 1 are squeezed\\n\\n    >>> shape = (1, 1, 5, 1, 2, 1, 1)\\n    >>> data = data.reshape(shape)\\n    >>> img = nf.ni1.Nifti1Image(data, affine)\\n    >>> img.get_shape()\\n    (1, 1, 5, 1, 2, 1, 1)\\n    >>> img2 = squeeze_image(img)\\n    >>> img2.get_shape()\\n    (1, 1, 5, 1, 2)\\n    '''\\n    klass = img.__class__\\n    shape = img.get_shape()\\n    slen = len(shape)\\n    if slen < 4:\\n        return klass.from_image(img)\\n    for bdim in shape[3::][::-1]:\\n        if bdim == 1:\\n           slen-=1\\n        else:\\n            break\\n    if slen == len(shape):\\n        return klass.from_image(img)\\n    shape = shape[:slen]\\n    data = img.get_data()\\n    data = data.reshape(shape)\\n    return klass(data,\\n                 img.get_affine(),\\n                 img.get_header(),\\n                 img.extra)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_version_constants(self, space):\\n        w_res = space.execute(\"return Marshal::MAJOR_VERSION\")\\n        assert space.int_w(w_res) == 4\\n\\n        w_res = space.execute(\"return Marshal::MINOR_VERSION\")\\n        assert space.int_w(w_res) == 8\\n\\n        w_res = space.execute(\"return Marshal.dump(\\'test\\')[0].ord\")\\n        assert space.int_w(w_res) == 4\\n\\n        w_res = space.execute(\"return Marshal.dump(\\'test\\')[1].ord\")\\n        assert space.int_w(w_res) == 8', 'def test_load_constants(self, space):\\n        w_res = space.execute(\"return Marshal.load(\\'\\\\x04\\\\b0\\')\")\\n        assert w_res == space.w_nil\\n\\n        w_res = space.execute(\"return Marshal.load(\\'\\\\x04\\\\bT\\')\")\\n        assert w_res == space.w_true\\n\\n        w_res = space.execute(\"return Marshal.load(\\'\\\\x04\\\\bF\\')\")\\n        assert w_res == space.w_false', 'def test_dump_tiny_integer(self, space):\\n        w_res = space.execute(\"return Marshal.dump(5)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\n\"\\n\\n        w_res = space.execute(\"return Marshal.dump(100)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bii\"\\n\\n        w_res = space.execute(\"return Marshal.dump(0)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x00\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFA\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-123)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x80\"\\n\\n        w_res = space.execute(\"return Marshal.dump(122)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x7F\"', 'def test_dump_array(self, space):\\n        w_res = space.execute(\"return Marshal.dump([])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\x00\"\\n\\n        w_res = space.execute(\"return Marshal.dump([nil])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\x060\"\\n\\n        w_res = space.execute(\"return Marshal.dump([nil, true, false])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\b0TF\"\\n\\n        w_res = space.execute(\"return Marshal.dump([1, 2, 3])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\x08i\\\\x06i\\\\x07i\\\\x08\"\\n\\n        w_res = space.execute(\"return Marshal.dump([1, [2, 3], 4])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\bi\\\\x06[\\\\ai\\\\ai\\\\bi\\\\t\"\\n\\n        w_res = space.execute(\"return Marshal.dump([:foo, :bar])\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b[\\\\a:\\\\bfoo:\\\\bbar\"', 'def test_dump_symbol(self, space):\\n        w_res = space.execute(\"return Marshal.dump(:abc)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b:\\\\babc\"\\n\\n        w_res = space.execute(\"return Marshal.dump((\\'hello\\' * 25).to_sym)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b:\\\\x01}\" + \"hello\" * 25\\n\\n        w_res = space.execute(\"return Marshal.dump((\\'hello\\' * 100).to_sym)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b:\\\\x02\\\\xF4\\\\x01\" + \"hello\" * 100', 'def test_dump_hash(self, space):\\n        w_res = space.execute(\"return Marshal.dump({})\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\b{\\\\x00\"\\n\\n        w_res = space.execute(\"return Marshal.dump({1 => 2, 3 => 4})\")\\n        assert self.unwrap(space, w_res) == \"\\\\x04\\\\b{\\\\ai\\\\x06i\\\\ai\\\\bi\\\\t\"\\n\\n        w_res = space.execute(\"return Marshal.dump({1 => {2 => 3}, 4 => 5})\")\\n        assert self.unwrap(space, w_res) == \"\\\\x04\\\\b{\\\\ai\\\\x06{\\\\x06i\\\\ai\\\\bi\\\\ti\\\\n\"\\n\\n        w_res = space.execute(\"return Marshal.dump({1234 => {23456 => 3456789}, 4 => 5})\")\\n        assert self.unwrap(space, w_res) == \"\\\\x04\\\\b{\\\\ai\\\\x02\\\\xD2\\\\x04{\\\\x06i\\\\x02\\\\xA0[i\\\\x03\\\\x15\\\\xBF4i\\\\ti\\\\n\"', 'def test_dump_integer(self, space):\\n        w_res = space.execute(\"return Marshal.dump(123)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x01{\"\\n\\n        w_res = space.execute(\"return Marshal.dump(255)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x01\\\\xFF\"\\n\\n        w_res = space.execute(\"return Marshal.dump(256)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x02\\\\x00\\\\x01\"\\n\\n        w_res = space.execute(\"return Marshal.dump(2 ** 16 - 2)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x02\\\\xFE\\\\xFF\"\\n\\n        w_res = space.execute(\"return Marshal.dump(2 ** 16 - 1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x02\\\\xFF\\\\xFF\"\\n\\n        w_res = space.execute(\"return Marshal.dump(2 ** 16)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x03\\\\x00\\\\x00\\\\x01\"\\n\\n        w_res = space.execute(\"return Marshal.dump(2 ** 16 + 1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x03\\\\x01\\\\x00\\\\x01\"\\n\\n        w_res = space.execute(\"return Marshal.dump(2 ** 30 - 1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x04\\\\xFF\\\\xFF\\\\xFF?\"\\n\\n        # TODO: test tooo big numbers (they give a warning and inf)', 'def test_dump_negative_integer(self, space):\\n        w_res = space.execute(\"return Marshal.dump(-1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFA\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-123)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\x80\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-124)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFF\\\\x84\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-256)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFF\\\\x00\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-257)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFE\\\\xFF\\\\xFE\"\\n\\n        w_res = space.execute(\"return Marshal.dump(-(2 ** 30))\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bi\\\\xFC\\\\x00\\\\x00\\\\x00\\\\xC0\"', 'def test_dump_float(self, space):\\n        w_res = space.execute(\"return Marshal.dump(0.0)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\x060\"\\n\\n        w_res = space.execute(\"return Marshal.dump(0.1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\b0.1\"\\n\\n        w_res = space.execute(\"return Marshal.dump(1.0)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\x061\"\\n\\n        w_res = space.execute(\"return Marshal.dump(1.1)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\b1.1\"\\n\\n        w_res = space.execute(\"return Marshal.dump(1.001)\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\n1.001\"\\n\\n        #w_res = space.execute(\"return Marshal.dump(123456789.123456789)\")\\n        #assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\x17123456789.12345679\"\\n\\n        #w_res = space.execute(\"return Marshal.dump(-123456789.123456789)\")\\n        #assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\x18-123456789.12345679\"\\n\\n        #w_res = space.execute(\"return Marshal.dump(-0.0)\")\\n        #assert space.str_w(w_res) == \"\\\\x04\\\\bf\\\\a-0\"', 'def test_dump_string(self, space):\\n        w_res = space.execute(\"return Marshal.dump(\\'\\')\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bI\\\\\"\\\\x00\\\\x06:\\\\x06ET\"\\n\\n        w_res = space.execute(\"return Marshal.dump(\\'abc\\')\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bI\\\\\"\\\\babc\\\\x06:\\\\x06ET\"\\n\\n        w_res = space.execute(\"return Marshal.dump(\\'i am a longer string\\')\")\\n        assert space.str_w(w_res) == \"\\\\x04\\\\bI\\\\\"\\\\x19i am a longer string\\\\x06:\\\\x06ET\"', 'def test_array(self, space):\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([1, 2, 3]))\")\\n        assert self.unwrap(space, w_res) == [1, 2, 3]\\n\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([1, [2, 3], 4]))\")\\n        assert self.unwrap(space, w_res) == [1, [2, 3], 4]\\n\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([130, [2, 3], 4]))\")\\n        assert self.unwrap(space, w_res) == [130, [2, 3], 4]\\n\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([-10000, [2, 123456], -9000]))\")\\n        assert self.unwrap(space, w_res) == [-10000, [2, 123456], -9000]\\n\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([:foo, :bar]))\")\\n        assert self.unwrap(space, w_res) == [\"foo\", \"bar\"]\\n\\n        w_res = space.execute(\"return Marshal.load(Marshal.dump([\\'foo\\', \\'bar\\']))\")\\n        assert self.unwrap(space, w_res) == [\"foo\", \"bar\"]', 'def test_short_data(self, space):\\n        with self.raises(space, \"ArgumentError\", \"marshal data too short\"):\\n            space.execute(\"Marshal.load(\\'\\')\")']}, {'features': [], 'snippets': ['def test( self ) :\\n\\n\\t\\ti = IECore.Reader.create( self.fileName ).read()\\n\\n\\t\\tn = GafferImage.ObjectToImage()\\n\\t\\tn[\"object\"].setValue( i )\\n\\n\\t\\tself.assertEqual( n[\"out\"].image(), i )', 'def testHashVariesPerTileAndChannel( self ) :\\n\\n\\t\\tn = GafferImage.ObjectToImage()\\n\\t\\tn[\"object\"].setValue( IECore.Reader.create( self.fileName ).read() )\\n\\n\\t\\tself.assertNotEqual(\\n\\t\\t\\tn[\"out\"].channelDataHash( \"R\", IECore.V2i( 0 ) ),\\n\\t\\t\\tn[\"out\"].channelDataHash( \"G\", IECore.V2i( 0 ) )\\n\\t\\t)\\n\\n\\t\\tself.assertNotEqual(\\n\\t\\t\\tn[\"out\"].channelDataHash( \"R\", IECore.V2i( 0 ) ),\\n\\t\\t\\tn[\"out\"].channelDataHash( \"R\", IECore.V2i( GafferImage.ImagePlug.tileSize() ) )\\n\\t\\t)']}, {'features': [], 'snippets': [\"def warn(*msgs):\\n\\tfor x in msgs: print('[WARNING]:', x, file=sys.stderr)\", \"def __init__(self, filename):\\n\\t\\t#self.tree = ET.parse(filename)\\n\\t\\t#self.root = self.tree.getroot()\\n\\t\\tdef strsum(l):\\n\\t\\t\\ts = ''\\n\\t\\t\\tfor x in l: s += x.rstrip() + '\\\\n'\\n\\t\\t\\treturn s\\n\\t\\tf = open(filename)\\n\\t\\ts = []\\n\\t\\tfor l in f: s.append(l)\\n\\t\\t#s = strsum(s[1:-1]).strip()\\n\\t\\ts = strsum(s).strip()\\n\\n\\t\\tself.root = ET.fromstring(s)\\n\\t\\tprint(root)\", \"def build_database(fn, prefix):\\n\\tprint('Unpacking database...', file=sys.stderr)\\n\\tf = open(fn)\\n\\tdb = f.read()\\n\\tf.close()\\n\\tfirstline = 1\\n\\theader = ''\\n\\tentries = []\\n\\tpdbids = []\\n\\tfor l in db.split('\\\\n'):\\n\\t\\tif firstline: \\n\\t\\t\\theader += l\\n\\t\\t\\tfirstline -= 1\\n\\t\\t\\tcontinue\\n\\t\\tif 'PDBTM>' in l: continue\\n\\t\\tif l.startswith('<?'): continue\\n\\t\\tif l.startswith('<pdbtm'):\\n\\t\\t\\ta = l.find('ID=') + 4\\n\\t\\t\\tb = a + 4\\n\\t\\t\\tpdbids.append(l[a:b])\\n\\t\\t\\tentries.append(header)\\n\\t\\tentries[-1] += '\\\\n' + l\\n\\tif not prefix.endswith('/'): prefix += '/'\\n\\tif not os.path.isdir(prefix): os.mkdir(prefix)\\n\\tfor entry in zip(pdbids, entries):\\n\\t\\tf = open(prefix + entry[0] + '.xml', 'w')\\n\\t\\tf.write(entry[1])\\n\\t\\tf.close()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, url, app=None, highlight=None):\\n        self.url = url\\n        self.app = app\\n        self.highlight = highlight or []\\n\\n        self.data = []\\n        self.created = None', 'def summary(self):\\n        \"\"\"Generates summary data of today\\'s state\"\"\"\\n        self.get_data()\\n\\n        highlight = self.highlight\\n        last_item = self.data[-1]\\n\\n        output = {}\\n        output[\\'app\\'] = self.app or \\'ALL\\'\\n\\n        data = last_item[\\'locales\\']\\n\\n        if self.app:\\n            get_item = lambda x: x[\\'apps\\'][self.app]\\n        else:\\n            get_item = lambda x: x\\n\\n        apps = data.items()[0][1][\\'apps\\'].keys()\\n        apps.sort()\\n        output[\\'apps\\'] = apps\\n\\n        items = [item for item in data.items() if item[0] not in highlight]\\n        hitems = [item for item in data.items() if item[0] in highlight]\\n\\n        highlighted = []\\n        if hitems:\\n            for loc, loc_data in sorted(hitems, key=lambda x: -x[1][\\'percent\\']):\\n                if loc in self.SKIP_LOCALES:\\n                    continue\\n                item = get_item(loc_data)\\n                total = item.get(\\'total\\', -1)\\n                translated = item.get(\\'translated\\', -1)\\n                percent = item.get(\\'percent\\', -1)\\n                untranslated_words = item.get(\\'untranslated_words\\', -1)\\n\\n                highlighted.append({\\n                    \\'locale\\': loc,\\n                    \\'percent\\': percent,\\n                    \\'total\\': total,\\n                    \\'translated\\': translated,\\n                    \\'untranslated\\': total - translated,\\n                    \\'untranslated_words\\': untranslated_words\\n                })\\n        output[\\'highlighted\\'] = highlighted\\n\\n        locales = []\\n        for loc, loc_data in sorted(items, key=lambda x: -x[1][\\'percent\\']):\\n            if loc in self.SKIP_LOCALES:\\n                continue\\n            item = get_item(loc_data)\\n            total = item.get(\\'total\\', -1)\\n            translated = item.get(\\'translated\\', -1)\\n            percent = item.get(\\'percent\\', -1)\\n            untranslated_words = item.get(\\'untranslated_words\\', -1)\\n\\n            locales.append({\\n                \\'locale\\': loc,\\n                \\'percent\\': percent,\\n                \\'total\\': total,\\n                \\'translated\\': translated,\\n                \\'untranslated\\': total - translated,\\n                \\'untranslated_words\\': untranslated_words\\n            })\\n\\n        output[\\'locales\\'] = locales\\n\\n        output[\\'created\\'] = self.created\\n\\n        return output']}, {'features': [], 'snippets': ['def __init__(self):\\n        \"\"\"Constructeur de la commande\"\"\"\\n        Commande.__init__(self, \"debarquer\", \"debark\")\\n        self.nom_categorie = \"navire\"\\n        self.aide_courte = \"débarque du navire\"\\n        self.aide_longue = \\\\\\n            \"Cette commande permet de débarquer du navire sur lequel \" \\\\\\n            \"on se trouve. On doit se trouver assez prêt d\\'une côte \" \\\\\\n            \"pour débarquer dessus.\"']}, {'features': [], 'snippets': [\"def __init__(self, queue=None, logger=None, ev_quit=None):\\n        # You can pass in a queue if you prefer to do so\\n        if not queue:\\n            queue = que.Queue()\\n        self.gui_queue = queue\\n        # You can pass in a logger if you prefer to do so\\n        if logger == None:\\n            logger = logging.getLogger('GtkHelper')\\n        self.logger = logger\\n        if not ev_quit:\\n            ev_quit = threading.Event()\\n        self.ev_quit = ev_quit\", 'def update_pending(self, timeout=0.0):\\n        \"\"\"Process all pending GTK events and return.  _timeout_ is a tuning\\n        parameter for performance.\\n        \"\"\"\\n        # Process \"out-of-band\" GTK events\\n        try:\\n            while gtk.events_pending():\\n                #gtk.main_iteration(False)\\n                gtk.main_iteration()\\n        finally:\\n            pass\\n\\n        done = False\\n        while not done:\\n            # Process \"in-band\" GTK events\\n            try:\\n                future = self.gui_queue.get(block=True, \\n                                            timeout=timeout)\\n\\n                # Execute the GUI method\\n                try:\\n                    try:\\n                        res = future.thaw(suppress_exception=False)\\n\\n                    except Exception, e:\\n                        future.resolve(e)\\n\\n                        self.logger.error(\"gui error: %s\" % str(e))\\n                        try:\\n                            (type, value, tb) = sys.exc_info()\\n                            tb_str = \"\".join(traceback.format_tb(tb))\\n                            self.logger.error(\"Traceback:\\\\n%s\" % (tb_str))\\n\\n                        except Exception, e:\\n                            self.logger.error(\"Traceback information unavailable.\")\\n\\n                finally:\\n                    pass', 'def gui_do(self, method, *args, **kwdargs):\\n        \"\"\"General method for asynchronously calling into the GUI.\\n        It makes a future to call the given (method) with the given (args)\\n        and (kwdargs) inside the gui thread.  If the calling thread is a\\n        non-gui thread the future is returned.\\n        \"\"\"\\n        future = Future.Future()\\n        future.freeze(method, *args, **kwdargs)\\n        self.gui_queue.put(future)\\n\\n        my_id = thread.get_ident() \\n        if my_id != self.gui_thread_id:\\n            return future', 'def gui_call(self, method, *args, **kwdargs):\\n        \"\"\"General method for synchronously calling into the GUI.\\n        This waits until the method has completed before returning.\\n        \"\"\"\\n        my_id = thread.get_ident() \\n        if my_id == self.gui_thread_id:\\n            return method(*args, **kwdargs)\\n        else:\\n            future = self.gui_do(method, *args, **kwdargs)\\n            return future.wait()', 'def gui_do_future(self, future):\\n        self.gui_queue.put(future)\\n        return future', 'def nongui_do_cb(self, tup, method, *args, **kwdargs):\\n        task = Task.FuncTask(method, args, kwdargs, logger=self.logger)\\n        task.register_callback(tup[0], args=tup[1:])\\n        return self.nongui_do_task(task)', 'def nongui_do_future(self, future):\\n        task = Task.FuncTask(future.thaw, (), {}, logger=self.logger)\\n        return self.nongui_do_task(task)', 'def nongui_do_task(self, task):\\n        try:\\n            task.init_and_start(self)\\n            return task\\n        except Exception, e:\\n            self.logger.error(\"Error starting task: %s\" % (str(e)))\\n            raise(e)', 'def assert_nongui_thread(self):\\n        my_id = thread.get_ident() \\n        assert my_id != self.gui_thread_id, \\\\\\n               Exception(\"GUI thread (%d) is executing non-GUI code!\" % (\\n            my_id))', 'def mainloop(self, timeout=0.001):\\n        # Mark our thread id\\n        self.gui_thread_id = thread.get_ident()\\n\\n        while not self.ev_quit.isSet():\\n            self.update_pending(timeout=timeout)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, list, title, flags=0, deflt=1, icon=37):\\n    self.list = list\\n    self.title = title\\n\\n    self.flags = flags\\n    self.x0 = -1\\n    self.x1 = -1\\n    self.y0 = -1\\n    self.y1 = -1\\n\\n    self.width = -1\\n    self.deflt = deflt\\n    self.icon = icon\\n\\n    # HACK: Add a circular reference for non-modal choosers. This prevents the GC\\n    # from collecting the class object the callbacks need. Unfortunately this means\\n    # that the class will never be collected, unless refhack is set to None explicitly.\\n    if (flags & Choose2.CH_MODAL) == 0:\\n      self.refhack = self', 'def getl(self, n):\\n    \"\"\"\\n    Callback: getl - get one item from the list\\n    \"\"\"\\n    if n == 0:\\n       return self.title\\n    if n <= self.sizer():\\n      return str(self.list[n-1])\\n    else:\\n      return \"<Empty>\"', 'def update(self, n):\\n    pass', 'def enter(self, n):\\n    print \"enter(%d) called\" % n', 'def get_icon(self, n):\\n    pass']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, n_iter=100, init_stdev=0.1, rank=8, random_state=123,\\n                 l2_reg_w=0.1, l2_reg_V=0.1, l2_reg=0, step_size=0.1):\\n        super(FMRecommender, self).\\\\\\n            __init__(n_iter=n_iter, init_stdev=init_stdev, rank=rank,\\n                     random_state=random_state)\\n        if (l2_reg != 0):\\n            self.l2_reg_V = l2_reg\\n            self.l2_reg_w = l2_reg\\n        else:\\n            self.l2_reg_w = l2_reg_w\\n            self.l2_reg_V = l2_reg_V\\n        self.step_size = step_size\\n        self.task = \"ranking\"']}, {'features': [], 'snippets': ['def __init__(self, parent, **traits):\\n        super(PythonEditor, self).__init__(**traits)\\n        self.control = self._create_control(parent)', 'def load(self, path=None):\\n        \"\"\" Loads the contents of the editor.\\n        \"\"\"\\n        if path is None:\\n            path = self.path\\n\\n        # We will have no path for a new script.\\n        if len(path) > 0:\\n            f = open(self.path, \\'r\\')\\n            text = f.read()\\n            f.close()\\n        else:\\n            text = \\'\\'\\n\\n        self.control.code.setPlainText(text)\\n        self.dirty = False', 'def select_line(self, lineno):\\n        \"\"\" Selects the specified line.\\n        \"\"\"\\n        self.control.code.set_line_column(lineno, 0)\\n        self.control.code.moveCursor(QtGui.QTextCursor.EndOfLine,\\n                                     QtGui.QTextCursor.KeepAnchor)', 'def _path_changed(self):\\n        self._changed_path()', 'def _create_control(self, parent):\\n        \"\"\" Creates the toolkit-specific control for the widget.\\n        \"\"\"\\n        self.control = control = AdvancedCodeWidget(parent)\\n        self._show_line_numbers_changed()\\n\\n        # Install event filter to trap key presses.\\n        event_filter = PythonEditorEventFilter(self, self.control)\\n        self.control.installEventFilter(event_filter)\\n        self.control.code.installEventFilter(event_filter)\\n\\n        # Connect signals for text changes.\\n        control.code.modificationChanged.connect(self._on_dirty_changed)\\n        control.code.textChanged.connect(self._on_text_changed)\\n\\n        # Load the editor\\'s contents.\\n        self.load()\\n\\n        return control', 'def _on_text_changed(self):\\n        \"\"\" Called whenever a change is made to the text of the document.\\n        \"\"\"\\n        self.changed = True', 'def __init__(self, editor, parent):\\n        super(PythonEditorEventFilter, self).__init__(parent)\\n        self.__editor = editor']}, {'features': [], 'snippets': ['def md5(file_path):\\n    \"\"\"Get md5 hash of a file.\\n    Parameters\\n    ----------\\n    file_path: str\\n        File path.\\n    Returns\\n    -------\\n    md5_hash: str\\n        md5 hash of data in file_path\\n    \"\"\"\\n    hash_md5 = hashlib.md5()\\n    with open(file_path, \\'rb\\') as fhandle:\\n        for chunk in iter(lambda: fhandle.read(4096), b\\'\\'):\\n            hash_md5.update(chunk)\\n    return hash_md5.hexdigest()', \"def make_irmas_index(irmas_data_path):\\n    count = 0\\n    irmas_dict = dict()\\n    for root, dirs, files in os.walk(irmas_data_path):\\n        for directory in dirs:\\n            if 'Train' in directory:\\n                for root_, dirs_, files_ in os.walk(\\n                    os.path.join(irmas_data_path, directory)\\n                ):\\n                    for directory_ in dirs_:\\n                        for root__, dirs__, files__ in os.walk(\\n                            os.path.join(irmas_data_path, directory, directory_)\\n                        ):\\n                            for file in files__:\\n                                if file.endswith('.wav'):\\n                                    if 'dru' in file:\\n                                        irmas_id_dru = file.split(']')[3]  # Obtain id\\n                                        irmas_id_dru_no_wav = irmas_id_dru.split('.')[\\n                                            0\\n                                        ]  # Obtain id without '.wav'\\n                                        irmas_dict[irmas_id_dru_no_wav] = os.path.join(\\n                                            directory, directory_, file\\n                                        )\\n                                    if 'nod' in file:\\n                                        irmas_id_nod = file.split(']')[3]  # Obtain id\\n                                        irmas_id_nod_no_wav = irmas_id_nod.split('.')[\\n                                            0\\n                                        ]  # Obtain id without '.wav'\\n                                        irmas_dict[irmas_id_nod_no_wav] = os.path.join(\\n                                            directory, directory_, file\\n                                        )\\n                                    else:\\n                                        irmas_id = file.split(']')[2]  # Obtain id\\n                                        irmas_id_no_wav = irmas_id.split('.')[\\n                                            0\\n                                        ]  # Obtain id without '.wav'\\n                                        irmas_dict[irmas_id_no_wav] = os.path.join(\\n                                            directory, directory_, file\\n                                        )\\n\\n    irmas_test_dict = dict()\\n    for root, dirs, files in os.walk(irmas_data_path):\\n        for directory in dirs:\\n            if 'Test' in directory:\\n                for root_, dirs_, files_ in os.walk(\\n                    os.path.join(irmas_data_path, directory)\\n                ):\\n                    for directory_ in dirs_:\\n                        for root__, dirs__, files__ in os.walk(\\n                            os.path.join(irmas_data_path, directory, directory_)\\n                        ):\\n                            for file in files__:\\n                                if file.endswith('.wav'):\\n                                    file_name = os.path.join(\\n                                        directory, directory_, file\\n                                    )\\n                                    track_name = str(file_name.split('.wa')[0]) + '.txt'\\n                                    irmas_test_dict[count] = [file_name, track_name]\\n                                    count += 1\\n\\n    irmas_id_list = sorted(irmas_dict.items())  # Sort strokes by id\\n\\n    irmas_index = {}\\n    for inst in irmas_id_list:\\n        print(inst[1])\\n        audio_checksum = md5(os.path.join(irmas_data_path, inst[1]))\\n\\n        irmas_index[inst[0]] = {\\n            'audio': (inst[1], audio_checksum),\\n            'annotation': (inst[1], audio_checksum),\\n        }\\n\\n    index = 1\\n    for inst in irmas_test_dict.values():\\n        audio_checksum = md5(os.path.join(irmas_data_path, inst[0]))\\n        annotation_checksum = md5(os.path.join(irmas_data_path, inst[1]))\\n\\n        irmas_index[index] = {\\n            'audio': (inst[0], audio_checksum),\\n            'annotation': (inst[1], annotation_checksum),\\n        }\\n        index += 1\\n\\n    with open(IRMAS_INDEX_PATH, 'w') as fhandle:\\n        json.dump(irmas_index, fhandle, indent=2)\", 'def main(args):\\n    make_irmas_index(args.irmas_data_path)\\n    # make_irmas_test_index(args.irmas_data_path)']}, {'features': [], 'snippets': ['def __init__(self, *args, **kwargs):\\n\\t\\tlogSetup.initLogging()\\n\\t\\tsuper().__init__(*args, **kwargs)', 'def dist_check(self, distance, dbid, phash):\\n\\n\\t\\tqtime1 = time.time()\\n\\t\\thave1 = self.tree.getWithinDistance_db(phash, distance=distance)\\n\\t\\tqtime2 = time.time()\\n\\t\\tqtime3 = time.time()\\n\\t\\thave2 = self.tree.getIdsWithinDistance(phash, distance=distance)\\n\\t\\tqtime4 = time.time()\\n\\n\\n\\t\\t# print(dbid, have1)\\n\\t\\tif have1 != have2:\\n\\t\\t\\tself.log.error(\"Mismatch!\")\\n\\t\\t\\tfor line in pprint.pformat(have1).split(\"\\\\n\"):\\n\\t\\t\\t\\tself.log.error(line)\\n\\t\\t\\tfor line in pprint.pformat(have2).split(\"\\\\n\"):\\n\\t\\t\\t\\tself.log.error(line)\\n\\n\\t\\tself.assertTrue(dbid in have1)\\n\\t\\tself.assertTrue(dbid in have2)\\n\\t\\tself.assertEqual(have1, have2)\\n\\n\\t\\tself.log.info(\\'Dist %s %s, %s\\', distance, qtime2-qtime1, qtime4-qtime3)']}, {'features': [], 'snippets': [\"def min_wave(A, omega, x, tol=1e-5, maxiter=25):\\n    '''\\n\\n    parameters\\n    ----------\\n    A {matrix}\\n        1D Helmholtz Operator\\n    omega {scalar}\\n        Wavenumber used to discretize Helmholtz problem\\n    x {array}\\n        1D mesh for the problem\\n    tol {scalar}\\n        minimization tolerance\\n    maxit {integer}\\n        maximum iters for minimization algorithm\\n\\n    returns\\n    -------\\n    Applies minimization algorithm to find numerically lowest energy wavenumber\\n    for the matrix A, i.e., the omega shift that minimizes <Ac, c> / <c, c>, \\n    for c = cosine((omega+shift)x)\", 'def obj_fcn(alpha):\\n        c = cos((omega+alpha)*x)\\n        Ac = (A*c)[1:-1]\\n        return norm(Ac)/norm(c[1:-1])', \"def one_D_helmholtz(h, omega=1.0, nplane_waves=2):\\n    '''\\n\\n    parameters\\n    ----------\\n    h {int}\\n        Number of grid spacings for 1-D Helmholtz\\n    omega {float}\\n        Defines Helmholtz wave number\\n    nplane_waves {int}\\n        Defines the number of planewaves used for the near null-space modes, B.\\n        1: B = [ exp(ikx) ]\\n        2: B = [ real(exp(ikx)), complex(exp(ikx)) ]\\n\\n    returns\\n    -------\\n    dictionary containing:\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        \"\"\"Constructeur du paramètre.\"\"\"\\n        Parametre.__init__(self, \"info\", \"info\")\\n        self.schema = \"<nombre>\"\\n        self.aide_courte = \"affiche des informations sur l\\'alerte\"\\n        self.aide_longue = \\\\\\n            \"Affiche des informations sur l\\'alerte permettant de la corriger.\"', 'def interpreter(self, personnage, dic_masques):\\n        \"\"\"Méthode d\\'interprétation de commande\"\"\"\\n        nombre = dic_masques[\"nombre\"].nombre\\n        try:\\n            alerte = type(self).importeur.scripting.alertes[nombre]\\n        except KeyError:\\n            personnage << \"|err|Ce numéro d\\'alerte est invalide.|ff|\"\\n        else:\\n            msg = \"Informations sur l\\'alerte {} :\".format(alerte.no)\\n            msg += \"\\\\n  S\\'est produit sur {} {}\".format(alerte.type,\\n                    alerte.objet) + \" \" + get_date(alerte.date.timetuple())\\n            msg += \"\\\\n  Evenement {}, test {}, ligne {}\".format(\\n                    alerte.evenement, echapper_accolades(alerte.test),\\n                    alerte.no_ligne)\\n            msg += \"\\\\n      {}\\\\n\".format(echapper_accolades(alerte.ligne))\\n            msg += \"\\\\n  Message d\\'erreur : |err|{}|ff|\".format(\\n                    echapper_accolades(alerte.message))\\n            if personnage.nom_groupe == \"administrateur\":\\n                msg += \"\\\\n  Traceback Python :\\\\n  {}\".format(\\n                        echapper_accolades(alerte.traceback))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_tps():\\n    \"\"\"Test TPS warping.\"\"\"\\n    az = np.linspace(0., 2 * np.pi, 20, endpoint=False)\\n    pol = np.linspace(0, np.pi, 12)[1:-1]\\n    sph = np.array(np.meshgrid(1, az, pol, indexing=\\'ij\\'))\\n    sph.shape = (3, -1)\\n    assert_equal(sph.shape[1], 200)\\n    source = _sph_to_cart(sph.T)\\n    destination = source.copy()\\n    destination *= 2\\n    destination[:, 0] += 1\\n    # fit with 100 points\\n    warp = SphericalSurfaceWarp()\\n    assert \\'no \\' in repr(warp)\\n    warp.fit(source[::3], destination[::2])\\n    assert \\'oct5\\' in repr(warp)\\n    destination_est = warp.transform(source)\\n    assert_allclose(destination_est, destination, atol=1e-3)', 'def test_get_trans():\\n    \"\"\"Test converting \\'-trans.txt\\' to \\'-trans.fif\\'.\"\"\"\\n    trans = read_trans(fname)\\n    trans = invert_transform(trans)  # starts out as head->MRI, so invert\\n    trans_2 = _get_trans(fname_trans)[0]\\n    assert trans.__eq__(trans_2, atol=1e-5)', 'def test_io_trans(tmpdir):\\n    \"\"\"Test reading and writing of trans files.\"\"\"\\n    tempdir = str(tmpdir)\\n    os.mkdir(op.join(tempdir, \\'sample\\'))\\n    pytest.raises(RuntimeError, _find_trans, \\'sample\\', subjects_dir=tempdir)\\n    trans0 = read_trans(fname)\\n    fname1 = op.join(tempdir, \\'sample\\', \\'test-trans.fif\\')\\n    trans0.save(fname1)\\n    assert fname1 == _find_trans(\\'sample\\', subjects_dir=tempdir)\\n    trans1 = read_trans(fname1)\\n\\n    # check all properties\\n    assert trans0 == trans1\\n\\n    # check reading non -trans.fif files\\n    pytest.raises(IOError, read_trans, fname_eve)\\n\\n    # check warning on bad filenames\\n    fname2 = op.join(tempdir, \\'trans-test-bad-name.fif\\')\\n    with pytest.warns(RuntimeWarning, match=\\'-trans.fif\\'):\\n        write_trans(fname2, trans0)', 'def _cartesian_to_sphere(x, y, z):\\n    \"\"\"Convert using old function.\"\"\"\\n    hypotxy = np.hypot(x, y)\\n    r = np.hypot(hypotxy, z)\\n    elev = np.arctan2(z, hypotxy)\\n    az = np.arctan2(y, x)\\n    return az, elev, r', 'def test_sph_to_cart():\\n    \"\"\"Test conversion between sphere and cartesian.\"\"\"\\n    # Simple test, expected value (11, 0, 0)\\n    r, theta, phi = 11., 0., np.pi / 2.\\n    z = r * np.cos(phi)\\n    rsin_phi = r * np.sin(phi)\\n    x = rsin_phi * np.cos(theta)\\n    y = rsin_phi * np.sin(theta)\\n    coord = _sph_to_cart(np.array([[r, theta, phi]]))[0]\\n    assert_allclose(coord, (x, y, z), atol=1e-7)\\n    assert_allclose(coord, (r, 0, 0), atol=1e-7)\\n    rng = np.random.RandomState(0)\\n    # round-trip test\\n    coords = rng.randn(10, 3)\\n    assert_allclose(_sph_to_cart(_cart_to_sph(coords)), coords, atol=1e-5)\\n    # equivalence tests to old versions\\n    for coord in coords:\\n        sph = _cart_to_sph(coord[np.newaxis])\\n        cart = _sph_to_cart(sph)\\n        sph_old = np.array(_cartesian_to_sphere(*coord))\\n        cart_old = _sphere_to_cartesian(*sph_old)\\n        sph_old[1] = np.pi / 2. - sph_old[1]  # new convention\\n        assert_allclose(sph[0], sph_old[[2, 0, 1]], atol=1e-7)\\n        assert_allclose(cart[0], cart_old, atol=1e-7)\\n        assert_allclose(cart[0], coord, atol=1e-7)', 'def test_polar_to_cartesian():\\n    \"\"\"Test helper transform function from polar to cartesian.\"\"\"\\n    r = 1\\n    theta = np.pi\\n    # expected values are (-1, 0)\\n    x = r * np.cos(theta)\\n    y = r * np.sin(theta)\\n    coord = _pol_to_cart(np.array([[r, theta]]))[0]\\n    # np.pi is an approx since pi is irrational\\n    assert_allclose(coord, (x, y), atol=1e-7)\\n    assert_allclose(coord, (-1, 0), atol=1e-7)\\n    assert_allclose(coord, _polar_to_cartesian(theta, r), atol=1e-7)\\n    rng = np.random.RandomState(0)\\n    r = rng.randn(10)\\n    theta = rng.rand(10) * (2 * np.pi)\\n    polar = np.array((r, theta)).T\\n    assert_allclose([_polar_to_cartesian(p[1], p[0]) for p in polar],\\n                    _pol_to_cart(polar), atol=1e-7)', 'def test_topo_to_sph():\\n    \"\"\"Test topo to sphere conversion.\"\"\"\\n    rng = np.random.RandomState(0)\\n    angles = rng.rand(10) * 360\\n    radii = rng.rand(10)\\n    angles[0] = 30\\n    radii[0] = 0.25\\n    # new way\\n    sph = _topo_to_sph(np.array([angles, radii]).T)\\n    new = _sph_to_cart(sph)\\n    new[:, [0, 1]] = new[:, [1, 0]] * [-1, 1]\\n    # old way\\n    for ii, (angle, radius) in enumerate(zip(angles, radii)):\\n        sph_phi, sph_theta = _topo_to_phi_theta(angle, radius)\\n        if ii == 0:\\n            assert_allclose(_topo_to_phi_theta(angle, radius), [45, -30])\\n        azimuth = sph_theta / 180.0 * np.pi\\n        elevation = sph_phi / 180.0 * np.pi\\n        assert_allclose(sph[ii], [1., azimuth, np.pi / 2. - elevation],\\n                        atol=1e-7)\\n        r = np.ones_like(radius)\\n        x, y, z = _sphere_to_cartesian(azimuth, elevation, r)\\n        pos = [-y, x, z]\\n        if ii == 0:\\n            expected = np.array([1. / 2., np.sqrt(3) / 2., 1.])\\n            expected /= np.sqrt(2)\\n            assert_allclose(pos, expected, atol=1e-7)\\n        assert_allclose(pos, new[ii], atol=1e-7)', 'def test_rotation3d_align_z_axis():\\n    \"\"\"Test rotation3d_align_z_axis.\"\"\"\\n    # The more complex z axis fails the assert presumably due to tolerance\\n    #\\n    inp_zs = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, -1],\\n              [-0.75071668, -0.62183808, 0.22302888]]\\n\\n    exp_res = [[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\\n               [[1., 0., 0.], [0., 0., 1.], [0., -1., 0.]],\\n               [[0., 0., 1.], [0., 1., 0.], [-1., 0., 0.]],\\n               [[1., 0., 0.], [0., -1., 0.], [0., 0., -1.]],\\n               [[0.53919688, -0.38169517, -0.75071668],\\n                [-0.38169517, 0.683832, -0.62183808],\\n                [0.75071668, 0.62183808, 0.22302888]]]\\n\\n    for res, z in zip(exp_res, inp_zs):\\n        assert_allclose(res, rotation3d_align_z_axis(z), atol=1e-7)', 'def test_combine():\\n    \"\"\"Test combining transforms.\"\"\"\\n    trans = read_trans(fname)\\n    inv = invert_transform(trans)\\n    combine_transforms(trans, inv, trans[\\'from\\'], trans[\\'from\\'])\\n    pytest.raises(RuntimeError, combine_transforms, trans, inv,\\n                  trans[\\'to\\'], trans[\\'from\\'])\\n    pytest.raises(RuntimeError, combine_transforms, trans, inv,\\n                  trans[\\'from\\'], trans[\\'to\\'])\\n    pytest.raises(RuntimeError, combine_transforms, trans, trans,\\n                  trans[\\'from\\'], trans[\\'to\\'])', 'def test_vector_rotation():\\n    \"\"\"Test basic rotation matrix math.\"\"\"\\n    x = np.array([1., 0., 0.])\\n    y = np.array([0., 1., 0.])\\n    rot = _find_vector_rotation(x, y)\\n    assert_array_equal(rot,\\n                       [[0, -1, 0], [1, 0, 0], [0, 0, 1]])\\n    quat_1 = rot_to_quat(rot)\\n    quat_2 = rot_to_quat(np.eye(3))\\n    assert_allclose(_angle_between_quats(quat_1, quat_2), np.pi / 2.)', 'def test_fs_xfm(subject, tmpdir):\\n    \"\"\"Test reading and writing of Freesurfer transforms.\"\"\"\\n    fname = op.join(data_path, \\'subjects\\', subject, \\'mri\\', \\'transforms\\',\\n                    \\'talairach.xfm\\')\\n    xfm, kind = _read_fs_xfm(fname)\\n    if subject == \\'fsaverage\\':\\n        assert_allclose(xfm, np.eye(4), atol=1e-5)  # fsaverage is in MNI\\n    assert kind == \\'MNI Transform File\\'\\n    tempdir = str(tmpdir)\\n    fname_out = op.join(tempdir, \\'out.xfm\\')\\n    _write_fs_xfm(fname_out, xfm, kind)\\n    xfm_read, kind_read = _read_fs_xfm(fname_out)\\n    assert kind_read == kind\\n    assert_allclose(xfm, xfm_read, rtol=1e-5, atol=1e-5)\\n    # Some wacky one\\n    xfm[:3] = np.random.RandomState(0).randn(3, 4)\\n    _write_fs_xfm(fname_out, xfm, \\'foo\\')\\n    xfm_read, kind_read = _read_fs_xfm(fname_out)\\n    assert kind_read == \\'foo\\'\\n    assert_allclose(xfm, xfm_read, rtol=1e-5, atol=1e-5)\\n    # degenerate conditions\\n    with open(fname_out, \\'w\\') as fid:\\n        fid.write(\\'foo\\')\\n    with pytest.raises(ValueError, match=\\'Failed to find\\'):\\n        _read_fs_xfm(fname_out)\\n    _write_fs_xfm(fname_out, xfm[:2], \\'foo\\')\\n    with pytest.raises(ValueError, match=\\'Could not find\\'):\\n        _read_fs_xfm(fname_out)', 'def quats():\\n    \"\"\"Make some unit quats.\"\"\"\\n    quats = np.random.RandomState(0).randn(5, 3)\\n    quats[:, 0] = 0  # identity\\n    quats /= 2 * np.linalg.norm(quats, axis=1, keepdims=True)  # some real part\\n    return quats', 'def test_fit_matched_points(quats, scaling, do_scale):\\n    \"\"\"Test analytical least-squares matched point fitting.\"\"\"\\n    if scaling != 1 and not do_scale:\\n        return  # no need to test this, it will not be good\\n    rng = np.random.RandomState(0)\\n    fro = rng.randn(10, 3)\\n    translation = rng.randn(3)\\n    for qi, quat in enumerate(quats):\\n        to = scaling * np.dot(quat_to_rot(quat), fro.T).T + translation\\n        for corrupted in (False, True):\\n            # mess up a point\\n            if corrupted:\\n                to[0, 2] += 100\\n                weights = np.ones(len(to))\\n                weights[0] = 0\\n            else:\\n                weights = None\\n            est, scale_est = _check_fit_matched_points(\\n                fro, to, weights=weights, do_scale=do_scale)\\n            assert_allclose(scale_est, scaling, rtol=1e-5)\\n            assert_allclose(est[:3], quat, atol=1e-14)\\n            assert_allclose(est[3:], translation, atol=1e-14)\\n        # if we don\\'t adjust for the corruption above, it should get worse\\n        angle = dist = None\\n        for weighted in (False, True):\\n            if not weighted:\\n                weights = None\\n                dist_bounds = (5, 20)\\n                if scaling == 1:\\n                    angle_bounds = (5, 95)\\n                    angtol, dtol, stol = 1, 15, 3\\n                else:\\n                    angle_bounds = (5, 105)\\n                    angtol, dtol, stol = 20, 15, 3\\n            else:\\n                weights = np.ones(len(to))\\n                weights[0] = 10  # weighted=True here means \"make it worse\"\\n                angle_bounds = (angle, 180)  # unweighted values as new min\\n                dist_bounds = (dist, 100)\\n                if scaling == 1:\\n                    # XXX this angtol is not great but there is a hard to\\n                    # identify linalg/angle calculation bug on Travis...\\n                    angtol, dtol, stol = 180, 70, 3\\n                else:\\n                    angtol, dtol, stol = 50, 70, 3\\n            est, scale_est = _check_fit_matched_points(\\n                fro, to, weights=weights, do_scale=do_scale,\\n                angtol=angtol, dtol=dtol, stol=stol)\\n            assert not np.allclose(est[:3], quat, atol=1e-5)\\n            assert not np.allclose(est[3:], translation, atol=1e-5)\\n            angle = np.rad2deg(_angle_between_quats(est[:3], quat))\\n            assert_array_less(angle_bounds[0], angle)\\n            assert_array_less(angle, angle_bounds[1])\\n            dist = np.linalg.norm(est[3:] - translation)\\n            assert_array_less(dist_bounds[0], dist)\\n            assert_array_less(dist, dist_bounds[1])']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def setUp(self):\\n        self.app_helper = self.add_helper(AppWorkerHelper(StreamingHTTPWorker))\\n\\n        self.config = {\\n            'health_path': '/health/',\\n            'web_path': '/foo',\\n            'web_port': 0,\\n            'metrics_prefix': 'metrics_prefix.',\\n            'conversation_cache_ttl': 0,\\n        }\\n        self.app = yield self.app_helper.get_app_worker(self.config)\\n        self.addr = self.app.webserver.getHost()\\n        self.url = 'http://%s:%s%s' % (\\n            self.addr.host, self.addr.port, self.config['web_path'])\\n\\n        conv_config = {\\n            'http_api': {\\n                'api_tokens': [\\n                    'token-1',\\n                    'token-2',\\n                    'token-3',\\n                ],\\n                'metric_store': 'metric_store',\\n            }\\n        }\\n        conversation = yield self.app_helper.create_conversation(\\n            config=conv_config)\\n        yield self.app_helper.start_conversation(conversation)\\n        self.conversation = yield self.app_helper.get_conversation(\\n            conversation.key)\\n\\n        self.auth_headers = {\\n            'Authorization': ['Basic ' + base64.b64encode('%s:%s' % (\\n                conversation.user_account.key, 'token-1'))],\\n        }\\n\\n        self.client = StreamingClient()\\n\\n        # Mock server to test HTTP posting of inbound messages & events\\n        self.mock_push_server = MockHttpServer(self.handle_request)\\n        yield self.mock_push_server.start()\\n        self.add_cleanup(self.mock_push_server.stop)\\n        self.push_calls = DeferredQueue()\\n        self._setup_wait_for_request()\\n        self.add_cleanup(self._wait_for_requests)\", \"def track_wrapper(*args, **kw):\\n            self._req_state['expected'] += 1\\n            return orig_track(*args, **kw)\", \"def _wait_for_requests(self):\\n        while self._req_state['expected'] > 0:\\n            yield self._req_state['queue'].get()\\n            self._req_state['expected'] -= 1\", \"def pull_message(self, count=1):\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n\\n        messages = DeferredQueue()\\n        errors = DeferredQueue()\\n        receiver = self.client.stream(\\n            TransportUserMessage, messages.put, errors.put, url,\\n            Headers(self.auth_headers))\\n\\n        received_messages = []\\n        for msg_id in range(count):\\n            yield self.app_helper.make_dispatch_inbound(\\n                'in %s' % (msg_id,), message_id=str(msg_id),\\n                conv=self.conversation)\\n            recv_msg = yield messages.get()\\n            received_messages.append(recv_msg)\\n\\n        receiver.disconnect()\\n        returnValue((receiver, received_messages))\", \"def test_proxy_buffering_headers_off(self):\\n        # This is the default, but we patch it anyway to make sure we're\\n        # testing the right thing should the default change.\\n        self.patch(StreamResourceMixin, 'proxy_buffering', False)\\n        receiver, received_messages = yield self.pull_message()\\n        headers = receiver._response.headers\\n        self.assertEqual(headers.getRawHeaders('x-accel-buffering'), ['no'])\", \"def test_proxy_buffering_headers_on(self):\\n        self.patch(StreamResourceMixin, 'proxy_buffering', True)\\n        receiver, received_messages = yield self.pull_message()\\n        headers = receiver._response.headers\\n        self.assertEqual(headers.getRawHeaders('x-accel-buffering'), ['yes'])\", \"def test_content_type(self):\\n        receiver, received_messages = yield self.pull_message()\\n        headers = receiver._response.headers\\n        self.assertEqual(\\n            headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\", \"def test_messages_stream(self):\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n\\n        messages = DeferredQueue()\\n        errors = DeferredQueue()\\n        receiver = self.client.stream(\\n            TransportUserMessage, messages.put, errors.put, url,\\n            Headers(self.auth_headers))\\n\\n        msg1 = yield self.app_helper.make_dispatch_inbound(\\n            'in 1', message_id='1', conv=self.conversation)\\n\\n        msg2 = yield self.app_helper.make_dispatch_inbound(\\n            'in 2', message_id='2', conv=self.conversation)\\n\\n        rm1 = yield messages.get()\\n        rm2 = yield messages.get()\\n\\n        receiver.disconnect()\\n\\n        # Sometimes messages arrive out of order if we're hitting real redis.\\n        rm1, rm2 = sorted([rm1, rm2], key=lambda m: m['message_id'])\\n\\n        self.assertEqual(msg1['message_id'], rm1['message_id'])\\n        self.assertEqual(msg2['message_id'], rm2['message_id'])\\n        self.assertEqual(errors.size, None)\", \"def test_events_stream(self):\\n        url = '%s/%s/events.json' % (self.url, self.conversation.key)\\n\\n        events = DeferredQueue()\\n        errors = DeferredQueue()\\n        receiver = yield self.client.stream(TransportEvent, events.put,\\n                                            events.put, url,\\n                                            Headers(self.auth_headers))\\n\\n        msg1 = yield self.app_helper.make_stored_outbound(\\n            self.conversation, 'out 1', message_id='1')\\n        ack1 = yield self.app_helper.make_dispatch_ack(\\n            msg1, conv=self.conversation)\\n\\n        msg2 = yield self.app_helper.make_stored_outbound(\\n            self.conversation, 'out 2', message_id='2')\\n        ack2 = yield self.app_helper.make_dispatch_ack(\\n            msg2, conv=self.conversation)\\n\\n        ra1 = yield events.get()\\n        ra2 = yield events.get()\\n\\n        receiver.disconnect()\\n\\n        # Sometimes messages arrive out of order if we're hitting real redis.\\n        if ra1['event_id'] != ack1['event_id']:\\n            ra1, ra2 = ra2, ra1\\n\\n        self.assertEqual(ack1['event_id'], ra1['event_id'])\\n        self.assertEqual(ack2['event_id'], ra2['event_id'])\\n        self.assertEqual(errors.size, None)\", 'def test_missing_auth(self):\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n\\n        queue = DeferredQueue()\\n        receiver = self.client.stream(\\n            TransportUserMessage, queue.put, queue.put, url)\\n        response = yield receiver.get_response()\\n        self.assertEqual(response.code, http.UNAUTHORIZED)\\n        self.assertEqual(response.headers.getRawHeaders(\\'www-authenticate\\'), [\\n            \\'basic realm=\"Conversation Realm\"\\'])', 'def test_invalid_auth(self):\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n\\n        queue = DeferredQueue()\\n\\n        headers = Headers({\\n            \\'Authorization\\': [\\'Basic %s\\' % (base64.b64encode(\\'foo:bar\\'),)],\\n        })\\n\\n        receiver = self.client.stream(\\n            TransportUserMessage, queue.put, queue.put, url, headers)\\n        response = yield receiver.get_response()\\n        self.assertEqual(response.code, http.UNAUTHORIZED)\\n        self.assertEqual(response.headers.getRawHeaders(\\'www-authenticate\\'), [\\n            \\'basic realm=\"Conversation Realm\"\\'])', \"def test_send_to(self):\\n        msg = {\\n            'to_addr': '+2345',\\n            'content': 'foo',\\n            'message_id': 'evil_id',\\n        }\\n\\n        # TaggingMiddleware.add_tag_to_msg(msg, self.tag)\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n\\n        self.assertEqual(\\n            response.headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\\n        self.assertEqual(response.code, http.OK)\\n        put_msg = json.loads(response.delivered_body)\\n\\n        [sent_msg] = self.app_helper.get_dispatched_outbound()\\n        self.assertEqual(sent_msg['to_addr'], sent_msg['to_addr'])\\n        self.assertEqual(sent_msg['helper_metadata'], {\\n            'go': {\\n                'conversation_key': self.conversation.key,\\n                'conversation_type': 'http_api',\\n                'user_account': self.conversation.user_account.key,\\n            },\\n        })\\n        # We do not respect the message_id that's been given.\\n        self.assertNotEqual(sent_msg['message_id'], msg['message_id'])\\n        self.assertEqual(sent_msg['message_id'], put_msg['message_id'])\\n        self.assertEqual(sent_msg['to_addr'], msg['to_addr'])\\n        self.assertEqual(sent_msg['from_addr'], None)\", \"def test_send_to_within_content_length_limit(self):\\n        self.conversation.config['http_api'].update({\\n            'content_length_limit': 182,\\n        })\\n        yield self.conversation.save()\\n\\n        msg = {\\n            'content': 'foo',\\n            'to_addr': '+1234',\\n        }\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n        self.assertEqual(\\n            response.headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\\n        put_msg = json.loads(response.delivered_body)\\n        self.assertEqual(response.code, http.OK)\\n\\n        [sent_msg] = self.app_helper.get_dispatched_outbound()\\n        self.assertEqual(sent_msg['to_addr'], put_msg['to_addr'])\\n        self.assertEqual(sent_msg['helper_metadata'], {\\n            'go': {\\n                'conversation_key': self.conversation.key,\\n                'conversation_type': 'http_api',\\n                'user_account': self.conversation.user_account.key,\\n            },\\n        })\\n        self.assertEqual(sent_msg['message_id'], put_msg['message_id'])\\n        self.assertEqual(sent_msg['session_event'], None)\\n        self.assertEqual(sent_msg['to_addr'], '+1234')\\n        self.assertEqual(sent_msg['from_addr'], None)\", 'def test_send_to_content_too_long(self):\\n        self.conversation.config[\\'http_api\\'].update({\\n            \\'content_length_limit\\': 10,\\n        })\\n        yield self.conversation.save()\\n\\n        msg = {\\n            \\'content\\': \"This message is longer than 10 characters.\",\\n            \\'to_addr\\': \\'+1234\\',\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(\\n            url, json.dumps(msg), self.auth_headers, method=\\'PUT\\')\\n        self.assert_bad_request(\\n            response, \"Payload content too long: 42 > 10\")', 'def test_send_to_with_evil_content(self):\\n        msg = {\\n            \\'content\\': 0xBAD,\\n            \\'to_addr\\': \\'+1234\\',\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method=\\'PUT\\')\\n        self.assert_bad_request(\\n            response, \"Invalid or missing value for payload key \\'content\\'\")', 'def test_send_to_with_evil_to_addr(self):\\n        msg = {\\n            \\'content\\': \\'good\\',\\n            \\'to_addr\\': 1234,\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method=\\'PUT\\')\\n        self.assert_bad_request(\\n            response, \"Invalid or missing value for payload key \\'to_addr\\'\")', \"def test_in_reply_to(self):\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, 'in 1', message_id='1')\\n\\n        msg = {\\n            'content': 'foo',\\n            'in_reply_to': inbound_msg['message_id'],\\n        }\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n\\n        self.assertEqual(\\n            response.headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\\n        put_msg = json.loads(response.delivered_body)\\n        self.assertEqual(response.code, http.OK)\\n\\n        [sent_msg] = self.app_helper.get_dispatched_outbound()\\n        self.assertEqual(sent_msg['to_addr'], put_msg['to_addr'])\\n        self.assertEqual(sent_msg['helper_metadata'], {\\n            'go': {\\n                'conversation_key': self.conversation.key,\\n                'conversation_type': 'http_api',\\n                'user_account': self.conversation.user_account.key,\\n            },\\n        })\\n        self.assertEqual(sent_msg['message_id'], put_msg['message_id'])\\n        self.assertEqual(sent_msg['session_event'], None)\\n        self.assertEqual(sent_msg['to_addr'], inbound_msg['from_addr'])\\n        self.assertEqual(sent_msg['from_addr'], '9292')\", \"def test_in_reply_to_within_content_length_limit(self):\\n        self.conversation.config['http_api'].update({\\n            'content_length_limit': 182,\\n        })\\n        yield self.conversation.save()\\n\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, 'in 1', message_id='1')\\n\\n        msg = {\\n            'content': 'foo',\\n            'in_reply_to': inbound_msg['message_id'],\\n        }\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n        self.assertEqual(\\n            response.headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\\n        put_msg = json.loads(response.delivered_body)\\n        self.assertEqual(response.code, http.OK)\\n\\n        [sent_msg] = self.app_helper.get_dispatched_outbound()\\n        self.assertEqual(sent_msg['to_addr'], put_msg['to_addr'])\\n        self.assertEqual(sent_msg['helper_metadata'], {\\n            'go': {\\n                'conversation_key': self.conversation.key,\\n                'conversation_type': 'http_api',\\n                'user_account': self.conversation.user_account.key,\\n            },\\n        })\\n        self.assertEqual(sent_msg['message_id'], put_msg['message_id'])\\n        self.assertEqual(sent_msg['session_event'], None)\\n        self.assertEqual(sent_msg['to_addr'], inbound_msg['from_addr'])\\n        self.assertEqual(sent_msg['from_addr'], '9292')\", 'def test_in_reply_to_content_too_long(self):\\n        self.conversation.config[\\'http_api\\'].update({\\n            \\'content_length_limit\\': 10,\\n        })\\n        yield self.conversation.save()\\n\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, \\'in 1\\', message_id=\\'1\\')\\n\\n        msg = {\\n            \\'content\\': \"This message is longer than 10 characters.\",\\n            \\'in_reply_to\\': inbound_msg[\\'message_id\\'],\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(\\n            url, json.dumps(msg), self.auth_headers, method=\\'PUT\\')\\n        self.assert_bad_request(\\n            response, \"Payload content too long: 42 > 10\")', 'def test_in_reply_to_with_evil_content(self):\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, \\'in 1\\', message_id=\\'1\\')\\n\\n        msg = {\\n            \\'content\\': 0xBAD,\\n            \\'in_reply_to\\': inbound_msg[\\'message_id\\'],\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method=\\'PUT\\')\\n        self.assert_bad_request(\\n            response, \"Invalid or missing value for payload key \\'content\\'\")', \"def test_invalid_in_reply_to(self):\\n        msg = {\\n            'content': 'foo',\\n            'in_reply_to': '1',  # this doesn't exist\\n        }\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n        self.assert_bad_request(response, 'Invalid in_reply_to value')\", 'def test_invalid_in_reply_to_with_missing_conversation_key(self):\\n        # create a message with no conversation\\n        inbound_msg = self.app_helper.make_inbound(\\'in 1\\', message_id=\\'msg-1\\')\\n        vumi_api = self.app_helper.vumi_helper.get_vumi_api()\\n        yield vumi_api.mdb.add_inbound_message(inbound_msg)\\n\\n        msg = {\\n            \\'content\\': \\'foo\\',\\n            \\'in_reply_to\\': inbound_msg[\\'message_id\\'],\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        with LogCatcher(message=\\'Invalid reply to message <Message .*>\\'\\n                        \\' which has no conversation key\\') as lc:\\n            response = yield http_request_full(url, json.dumps(msg),\\n                                               self.auth_headers, method=\\'PUT\\')\\n            [error_log] = lc.messages()\\n\\n        self.assert_bad_request(response, \"Invalid in_reply_to value\")\\n        self.assertTrue(inbound_msg[\\'message_id\\'] in error_log)', 'def test_in_reply_to_with_evil_session_event(self):\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, \\'in 1\\', message_id=\\'1\\')\\n\\n        msg = {\\n            \\'content\\': \\'foo\\',\\n            \\'in_reply_to\\': inbound_msg[\\'message_id\\'],\\n            \\'session_event\\': 0xBAD5E55104,\\n        }\\n\\n        url = \\'%s/%s/messages.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method=\\'PUT\\')\\n\\n        self.assert_bad_request(\\n            response,\\n            \"Invalid or missing value for payload key \\'session_event\\'\")\\n        self.assertEqual(self.app_helper.get_dispatched_outbound(), [])', \"def test_in_reply_to_with_evil_message_id(self):\\n        inbound_msg = yield self.app_helper.make_stored_inbound(\\n            self.conversation, 'in 1', message_id='1')\\n\\n        msg = {\\n            'content': 'foo',\\n            'in_reply_to': inbound_msg['message_id'],\\n            'message_id': 'evil_id'\\n        }\\n\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        response = yield http_request_full(url, json.dumps(msg),\\n                                           self.auth_headers, method='PUT')\\n\\n        self.assertEqual(response.code, http.OK)\\n        self.assertEqual(\\n            response.headers.getRawHeaders('content-type'),\\n            ['application/json; charset=utf-8'])\\n        put_msg = json.loads(response.delivered_body)\\n        [sent_msg] = self.app_helper.get_dispatched_outbound()\\n\\n        # We do not respect the message_id that's been given.\\n        self.assertNotEqual(sent_msg['message_id'], msg['message_id'])\\n        self.assertEqual(sent_msg['message_id'], put_msg['message_id'])\\n        self.assertEqual(sent_msg['to_addr'], inbound_msg['from_addr'])\\n        self.assertEqual(sent_msg['from_addr'], '9292')\", 'def test_metric_publishing(self):\\n\\n        metric_data = [\\n            (\"vumi.test.v1\", 1234, \\'SUM\\'),\\n            (\"vumi.test.v2\", 3456, \\'AVG\\'),\\n        ]\\n\\n        url = \\'%s/%s/metrics.json\\' % (self.url, self.conversation.key)\\n        response = yield http_request_full(\\n            url, json.dumps(metric_data), self.auth_headers, method=\\'PUT\\')\\n\\n        self.assertEqual(response.code, http.OK)\\n        self.assertEqual(\\n            response.headers.getRawHeaders(\\'content-type\\'),\\n            [\\'application/json; charset=utf-8\\'])\\n\\n        prefix = \"go.campaigns.test-0-user.stores.metric_store\"\\n\\n        self.assertEqual(\\n            self.app_helper.get_published_metrics(self.app),\\n            [(\"%s.vumi.test.v1\" % prefix, 1234),\\n             (\"%s.vumi.test.v2\" % prefix, 3456)])', \"def test_concurrency_limits(self):\\n        config = yield self.app.get_config(None)\\n        concurrency = config.concurrency_limit\\n        queue = DeferredQueue()\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        max_receivers = [self.client.stream(\\n            TransportUserMessage, queue.put, queue.put, url,\\n            Headers(self.auth_headers)) for _ in range(concurrency)]\\n\\n        for i in range(concurrency):\\n            msg = yield self.app_helper.make_dispatch_inbound(\\n                'in %s' % (i,), message_id=str(i), conv=self.conversation)\\n            received = yield queue.get()\\n            self.assertEqual(msg['message_id'], received['message_id'])\\n\\n        maxed_out_resp = yield http_request_full(\\n            url, method='GET', headers=self.auth_headers)\\n\\n        self.assertEqual(maxed_out_resp.code, 403)\\n        self.assertTrue(\\n            'Too many concurrent connections' in maxed_out_resp.delivered_body)\\n\\n        [r.disconnect() for r in max_receivers]\", 'def test_disabling_concurrency_limit(self):\\n        conv_resource = StreamingConversationResource(\\n            self.app, self.conversation.key)\\n        # negative concurrency limit disables it\\n        ctxt = ConfigContext(user_account=self.conversation.user_account.key,\\n                             concurrency_limit=-1)\\n        config = yield self.app.get_config(msg=None, ctxt=ctxt)\\n        self.assertTrue(\\n            (yield conv_resource.is_allowed(\\n                config, self.conversation.user_account.key)))', \"def test_backlog_on_connect(self):\\n        for i in range(10):\\n            yield self.app_helper.make_dispatch_inbound(\\n                'in %s' % (i,), message_id=str(i), conv=self.conversation)\\n\\n        queue = DeferredQueue()\\n        url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        receiver = self.client.stream(\\n            TransportUserMessage, queue.put, queue.put, url,\\n            Headers(self.auth_headers))\\n\\n        for i in range(10):\\n            received = yield queue.get()\\n            self.assertEqual(received['message_id'], str(i))\\n\\n        receiver.disconnect()\", \"def test_health_response(self):\\n        health_url = 'http://%s:%s%s' % (\\n            self.addr.host, self.addr.port, self.config['health_path'])\\n\\n        response = yield http_request_full(health_url, method='GET')\\n        self.assertEqual(response.delivered_body, '0')\\n\\n        yield self.app_helper.make_dispatch_inbound(\\n            'in 1', message_id='1', conv=self.conversation)\\n\\n        queue = DeferredQueue()\\n        stream_url = '%s/%s/messages.json' % (self.url, self.conversation.key)\\n        stream_receiver = self.client.stream(\\n            TransportUserMessage, queue.put, queue.put, stream_url,\\n            Headers(self.auth_headers))\\n\\n        yield queue.get()\\n\\n        response = yield http_request_full(health_url, method='GET')\\n        self.assertEqual(response.delivered_body, '1')\\n\\n        stream_receiver.disconnect()\\n\\n        response = yield http_request_full(health_url, method='GET')\\n        self.assertEqual(response.delivered_body, '0')\\n\\n        self.assertEqual(self.app.client_manager.clients, {\\n            'sphex.stream.message.%s' % (self.conversation.key,): []\\n        })\", \"def test_post_inbound_message(self):\\n        # Set the URL so stuff is HTTP Posted instead of streamed.\\n        self.conversation.config['http_api'].update({\\n            'push_message_url': self.mock_push_server.url,\\n        })\\n        yield self.conversation.save()\\n\\n        msg_d = self.app_helper.make_dispatch_inbound(\\n            'in 1', message_id='1', conv=self.conversation)\\n\\n        req = yield self.push_calls.get()\\n        posted_json_data = req.content.read()\\n        req.finish()\\n        msg = yield msg_d\\n\\n        posted_msg = TransportUserMessage.from_json(posted_json_data)\\n        self.assertEqual(posted_msg['message_id'], msg['message_id'])\", \"def test_post_inbound_message_201_response(self):\\n        # Set the URL so stuff is HTTP Posted instead of streamed.\\n        self.conversation.config['http_api'].update({\\n            'push_message_url': self.mock_push_server.url,\\n        })\\n        yield self.conversation.save()\\n\\n        with LogCatcher(message='Got unexpected response code') as lc:\\n            msg_d = self.app_helper.make_dispatch_inbound(\\n                'in 1', message_id='1', conv=self.conversation)\\n            req = yield self.push_calls.get()\\n            req.setResponseCode(201)\\n            req.finish()\\n            yield msg_d\\n        self.assertEqual(lc.messages(), [])\", \"def test_post_inbound_message_500_response(self):\\n        # Set the URL so stuff is HTTP Posted instead of streamed.\\n        self.conversation.config['http_api'].update({\\n            'push_message_url': self.mock_push_server.url,\\n        })\\n        yield self.conversation.save()\\n\\n        with LogCatcher(message='Got unexpected response code') as lc:\\n            msg_d = self.app_helper.make_dispatch_inbound(\\n                'in 1', message_id='1', conv=self.conversation)\\n            req = yield self.push_calls.get()\\n            req.setResponseCode(500)\\n            req.finish()\\n            yield msg_d\\n        [warning_log] = lc.messages()\\n        self.assertTrue(self.mock_push_server.url in warning_log)\\n        self.assertTrue('500' in warning_log)\", \"def test_post_inbound_event(self):\\n        # Set the URL so stuff is HTTP Posted instead of streamed.\\n        self.conversation.config['http_api'].update({\\n            'push_event_url': self.mock_push_server.url,\\n        })\\n        yield self.conversation.save()\\n\\n        msg = yield self.app_helper.make_stored_outbound(\\n            self.conversation, 'out 1', message_id='1')\\n        event_d = self.app_helper.make_dispatch_ack(\\n            msg, conv=self.conversation)\\n\\n        req = yield self.push_calls.get()\\n        posted_json_data = req.content.read()\\n        req.finish()\\n        ack = yield event_d\\n\\n        self.assertEqual(TransportEvent.from_json(posted_json_data), ack)\", \"def test_bad_urls(self):\\n        def assert_not_found(url, headers={}):\\n            d = http_request_full(self.url, method='GET', headers=headers)\\n            d.addCallback(lambda r: self.assertEqual(r.code, http.NOT_FOUND))\\n            return d\\n\\n        yield assert_not_found(self.url)\\n        yield assert_not_found(self.url + '/')\\n        yield assert_not_found('%s/%s' % (self.url, self.conversation.key),\\n                               headers=self.auth_headers)\\n        yield assert_not_found('%s/%s/' % (self.url, self.conversation.key),\\n                               headers=self.auth_headers)\\n        yield assert_not_found('%s/%s/foo' % (self.url, self.conversation.key),\\n                               headers=self.auth_headers)\", 'def test_send_message_command(self):\\n        yield self.app_helper.dispatch_command(\\n            \\'send_message\\',\\n            user_account_key=self.conversation.user_account.key,\\n            conversation_key=self.conversation.key,\\n            command_data={\\n                u\\'batch_id\\': u\\'batch-id\\',\\n                u\\'content\\': u\\'foo\\',\\n                u\\'to_addr\\': u\\'to_addr\\',\\n                u\\'msg_options\\': {\\n                    u\\'helper_metadata\\': {\\n                        u\\'tag\\': {\\n                            u\\'tag\\': [u\\'longcode\\', u\\'default10080\\']\\n                        }\\n                    },\\n                    u\\'from_addr\\': u\\'default10080\\',\\n                }\\n            })\\n\\n        [msg] = self.app_helper.get_dispatched_outbound()\\n        self.assertEqual(msg.payload[\\'to_addr\\'], \"to_addr\")\\n        self.assertEqual(msg.payload[\\'from_addr\\'], \"default10080\")\\n        self.assertEqual(msg.payload[\\'content\\'], \"foo\")\\n        self.assertEqual(msg.payload[\\'message_type\\'], \"user_message\")\\n        self.assertEqual(\\n            msg.payload[\\'helper_metadata\\'][\\'go\\'][\\'user_account\\'],\\n            self.conversation.user_account.key)\\n        self.assertEqual(\\n            msg.payload[\\'helper_metadata\\'][\\'tag\\'][\\'tag\\'],\\n            [\\'longcode\\', \\'default10080\\'])']}, {'features': [], 'snippets': ['def get_abusehelper():\\n    \"\"\"Return a list of available abusehelper\\n\\n    **Example request**:\\n\\n    .. sourcecode:: http\\n\\n        GET /api/1.0/abusehelper HTTP/1.1\\n        Host: do.cert.europa.eu\\n        Accept: application/json\\n\\n    **Example response**:\\n\\n    .. sourcecode:: http\\n\\n        HTTP/1.0 200 OK\\n        Content-Type: application/json\\n\\n        {\\n          \"abusehelper\": [\\n            {\\n              \"name\": \"ShadowServerBot\",\\n              \"url\": \"http://sample.com/path.html\",\\n              \"id\": 1\\n            }\\n          ]\\n        }\\n\\n    :reqheader Accept: Content type(s) accepted by the client\\n    :resheader Content-Type: this depends on `Accept` header or request\\n\\n    :>json array abusehelper: List of available bots\\n    :>jsonobj integer id: Bot ID\\n    :>jsonobj integer name: Bot name\\n\\n    :status 200: Deliverable endpoint found, response may be empty\\n    :status 404: Not found\\n    \"\"\"\\n    bots = Bot.query.filter().all()\\n    return {\\'abusehelper\\': [a.serialize() for a in bots]}', 'def get_got(bot_id):\\n    \"\"\"Get bot from database\\n\\n    **Example request**:\\n\\n    .. sourcecode:: http\\n\\n        GET /api/1.0/abusehelper/1 HTTP/1.1\\n        Host: do.cert.europa.eu\\n        Accept: application/json\\n\\n    **Example response**:\\n\\n    .. sourcecode:: http\\n\\n        HTTP/1.0 200 OK\\n        Content-Type: application/json\\n\\n        {\\n          \"name\": \"ShadowServerBot\",\\n          \"url\": \"http://sample.com/path.html\",\\n          \"id\": 1\\n        }\\n\\n    :param bot_id: Bot unique ID\\n\\n    :reqheader Accept: Content type(s) accepted by the client\\n    :resheader Content-Type: this depends on `Accept` header or request\\n\\n    :>json integer id: Bot unique ID\\n    :>json integer name: Bot name\\n\\n    :status 200: ASN found\\n    :status 404: Resource not found\\n    \"\"\"\\n    a = Bot.query.get_or_404(bot_id)\\n    return a.serialize()', 'def add_bot():\\n    \"\"\"Add new bot entry\\n\\n    **Example request**:\\n\\n    .. sourcecode:: http\\n\\n        POST /api/1.0/abusehelper HTTP/1.1\\n        Host: do.cert.europa.eu\\n        Accept: application/json\\n        Content-Type: application/json\\n\\n        {\\n          \"name\": \"ShadowServerBot\",\\n          \"url\": \"http://sample.com/path.html\"\\n        }\\n\\n    **Example response**:\\n\\n    .. sourcecode:: http\\n\\n        HTTP/1.0 201 CREATED\\n        Content-Type: application/json\\n\\n        {\\n          \"bot\": {\\n            \"name\": \"ShadowServerBot\",\\n            \"url\": \"http://sample.com/path.html\",\\n            \"id\": 1\\n          },\\n          \\'message\\': \"Bot added\"\\n        }\\n\\n    :reqheader Accept: Content type(s) accepted by the client\\n    :resheader Content-Type: this depends on `Accept` header or request\\n\\n    :<json integer name: Bot name\\n    :>jsonobj integer id: Unique ID of new bot\\n    :>jsonobj integer name: bot name\\n    :>json string message: Status message\\n\\n    :status 201: ASN successfully saved\\n    :status 400: Bad request\\n    \"\"\"\\n    a = Bot.fromdict(request.json)\\n    db.session.add(a)\\n    db.session.commit()\\n    return {\\'bot\\': a.serialize(), \\'message\\': \\'Bot added\\'}, 201, \\\\\\n           {\\'Location\\': url_for(\\'api.get_bot\\', bot_id=a.id)}', 'def update_bot(bot_id):\\n    return NotImplemented', 'def delete_bot(bot_id):\\n    \"\"\"Delete bot\\n\\n    **Example request**:\\n\\n    .. sourcecode:: http\\n\\n        DELETE /api/1.0/abusehelper/1 HTTP/1.1\\n        Host: do.cert.europa.eu\\n        Accept: application/json\\n\\n    **Example response**:\\n\\n    .. sourcecode:: http\\n\\n        HTTP/1.0 200 OK\\n        Content-Type: application/json\\n\\n        {\\n          \"message\": \"Bot deleted\"\\n        }\\n\\n    :param bot_id: Bot unique ID.\\n\\n    :reqheader Accept: Content type(s) accepted by the client\\n    :resheader Content-Type: this depends on `Accept` header or request\\n\\n    :>json string message: Action status status\\n\\n    :status 200: Bot was deleted\\n    :status 404: Bot was not found\\n    \"\"\"\\n    a = Bot.query.filter_by(id == bot_id).delete()\\n    if not a:\\n        return {\\'message\\': \\'No such bot\\'}, 404\\n    db.session.commit()\\n    return {\\'message\\': \\'Bot deleted\\'}']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        values = numpy.arange(10)\\n        self.plot = create_line_plot((values, values))\\n        self.plot.bounds = [100, 100]\\n        self.plot._window = self.create_mock_window()\\n        self.tool = BetterZoom(component=self.plot)\\n        self.plot.active_tool = self.tool\\n        self.plot.do_layout()']}, {'features': [], 'snippets': ['def test_connect(gateway):\\n    url = \"jdbc:derby:memory:testdb;create=true\"\\n    conn = connect(url, gateway=gateway)\\n    cur = conn.cursor()\\n    rs = cur.execute(\"select * from SYS.SYSTABLES\")\\n    assert isinstance(rs, ResultSet)', 'def test_execute_with_params(derby):\\n    derby.autocommit = False\\n    cur = derby.cursor()\\n    cur.execute(\"create schema x_with_params\")\\n    cur.execute(\"create table x_with_params.cowtest(a int, b char(1))\")\\n    # Verify table is empty.\\n    rows = cur.execute(\"select * from x_with_params.cowtest as r\").fetchall()\\n    assert len(rows) == 0\\n    # Insert one with parameter binding..\\n    sql = \"insert into x_with_params.cowtest (a, b) values (?, ?)\"\\n    cur.execute(sql, (12, \"m\"))\\n    # Verify there\\'s 1 row.\\n    rows = cur.execute(\"select * from x_with_params.cowtest as r\").fetchall()\\n    assert len(rows) == 1\\n    # Insert a bunch.\\n    params = list(enumerate(\"thecowsaremooing\"))\\n    cur.executemany(sql, params)\\n    rows = cur.execute(\"select * from x_with_params.cowtest as r\").fetchall()\\n    assert len(rows) == len(\"thecowsaremooing\") + 1\\n    derby.rollback()\\n    derby.autocommit = True', 'def test_fetchmany(derby):\\n    \\'\\'\\'Assert all rows of result set have the correct class.\\n    \\'\\'\\'\\n    cur = derby.cursor()\\n    rs = cur.execute(\"select * from SYS.SYSTABLES\")\\n    assert all({isinstance(row, rs.Row) for row in rs.fetchmany(5)})', 'def test_fetchall(derby):\\n    \\'\\'\\'Assert all rows of result set have the correct class.\\n    \\'\\'\\'\\n    cur = derby.cursor()\\n    rs = cur.execute(\"select * from SYS.SYSTABLES\")\\n    assert all({isinstance(row, rs.Row) for row in rs.fetchall()})', 'def test_Cursor__iter__(derby):\\n    cur = derby.cursor()\\n    rs = cur.execute(\"select * from SYS.SYSTABLES\")\\n    # Exhaust all rows.\\n    list(rs)\\n    assert rs.fetchone() == None', 'def test_close_and_fetchone(derby):\\n    cur = derby.cursor()\\n    cur.execute(\"select * from SYS.SYSTABLES\")\\n    cur.close()\\n    with pytest.raises(Error):\\n        cur.fetchone()']}, {'features': [], 'snippets': ['def __init__(self):\\n        super().__init__()\\n        self.visited = []', 'def visit_BasicNodeReplacement(self, node):\\n        self.visited.append(\"visited Copy!\")', 'def __init__(self):\\n        super().__init__()\\n        self.recorded_access_path = None', 'def __init__(self):\\n        super().__init__()\\n        self.visited = []', 'def visit_BasicNodeReplacement(self, node):\\n        self.visited.append(\"visited Copy!\")\\n        return node', 'def __init__(self):\\n        super().__init__()\\n        self.recorded_access_path = None', 'def visit_BasicNode(self, node):\\n        return BasicNodeReplacement()', 'def visit_BasicNodeReplacement(self, node):\\n        return self.NONE_DEPUTY  # Replace this node with None', 'def test_visit_order(self, node, order):\\n        to_visit = self.load(node)\\n\\n        # The main stuff\\n        visitor = VisitOrderCheckingVisitor()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(retval, None)\\n        assert_equal(visitor.visited, order)', 'def test_access_path(self, node, access):\\n        to_visit = self.load(node)\\n        access_path = self.load(access)\\n\\n        # The main stuff\\n        visitor = AccessPathCheckingVisitor()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(retval, None)\\n        assert_equal(visitor.recorded_access_path, access_path)', 'def test_empty_transformer(self, node, order):\\n        to_visit = self.load(node)\\n\\n        # The main stuff\\n        visitor = EmptyTransformer()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(to_visit, retval)', 'def test_visit_order(self, node, order):\\n        to_visit = self.load(node)\\n\\n        # The main stuff\\n        visitor = VisitOrderCheckingTransformer()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(to_visit, retval)\\n        assert_equal(visitor.visited, order)', 'def test_access_path(self, node, access):\\n        to_visit = self.load(node)\\n        access_path = self.load(access)\\n\\n        # The main stuff\\n        visitor = AccessPathCheckingTransformer()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(retval, to_visit)\\n        assert_equal(visitor.recorded_access_path, access_path)', 'def test_transformation(self, node, expected):\\n        to_visit = self.load(node)\\n        expected_node = self.load(expected)\\n\\n        # The main stuff\\n        visitor = TransformationCheckingTransformer()\\n        retval = visitor.visit(to_visit)\\n\\n        assert_equal(retval, expected_node)']}, {'features': [], 'snippets': [\"def attribs(name):\\n    mod = import_module(name)\\n    print name\\n    print 'Has __all__?', hasattr(mod, '__all__')    \\n    print 'Has __doc__?', hasattr(mod, '__doc__')\\n    print 'doc: ', getdoc(mod)\"]}, {'features': [], 'snippets': ['def __init__(self, required=True, ensure=None, depends=None, post=None):\\n        self.required = required\\n        self.ensure = ensure or \"exists\"\\n        self.depends = depends or ()\\n        if self.ensure == \"removed\":\\n            self.required = False\\n        self.post = post or ()\\n\\n        # Increase the creation counter, and save our local copy.\\n        self.creation_counter = Requirement.creation_counter\\n        Requirement.creation_counter += 1']}, {'features': [], 'snippets': ['def run(self, func, args):\\n        return interpret(func, args)', 'def main(p):\\n            return p[0]', 'def expr_op(p):\\n            return BoxInt(p[0].getint() + p[2].getint())', 'def expr_num(p):\\n            return BoxInt(int(p[0].getstr()))', 'def f(n):\\n            return parser.parse(FakeLexer([\\n                Token(\"NUMBER\", str(n)),\\n                Token(\"PLUS\", \"+\"),\\n                Token(\"NUMBER\", str(n))\\n            ])).getint()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def output_file(filename, title=\"Bokeh Plot\", mode=None, root_dir=None):\\n    \\'\\'\\'Configure the default output state to generate output saved\\n    to a file when :func:`show` is called.\\n\\n    Does not change the current ``Document`` from ``curdoc()``. File and notebook\\n    output may be active at the same time, so e.g., this does not clear the\\n    effects of ``output_notebook()``.\\n\\n    Args:\\n        filename (str) : a filename for saving the HTML document\\n\\n        title (str, optional) : a title for the HTML document (default: \"Bokeh Plot\")\\n\\n        mode (str, optional) : how to include BokehJS (default: ``\\'cdn\\'``)\\n            One of: ``\\'inline\\'``, ``\\'cdn\\'``, ``\\'relative(-dev)\\'`` or\\n            ``\\'absolute(-dev)\\'``. See :class:`bokeh.resources.Resources` for more details.\\n\\n        root_dir (str, optional) : root directory to use for \\'absolute\\' resources. (default: None)\\n            This value is ignored for other resource types, e.g. ``INLINE`` or\\n            ``CDN``.\\n\\n    Returns:\\n        None\\n\\n    .. note::\\n        Generally, this should be called at the beginning of an interactive\\n        session or the top of a script.\\n\\n    .. warning::\\n        This output file will be overwritten on every save, e.g., each time\\n        show() or save() is invoked.\\n\\n    \\'\\'\\'\\n    curstate().output_file(\\n        filename,\\n        title=title,\\n        mode=mode,\\n        root_dir=root_dir\\n    )', \"def reset_output(state=None):\\n    ''' Clear the default state of all output modes.\\n\\n    Returns:\\n        None\\n\\n    '''\\n    curstate().reset()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_mongo_collection():\\n    \"Open a connection to MongoDB and return the collection to use.\"\\n    if settings.RIGHT_MONGODB_HOST:\\n        connection = Connection.paired(\\n                left=(settings.MONGODB_HOST, settings.MONGODB_PORT),\\n                right=(settings.RIGHT_MONGODB_HOST, settings.RIGHT_MONGODB_PORT)\\n            )\\n    else:\\n        connection = Connection(host=settings.MONGODB_HOST, port=settings.MONGODB_PORT)\\n    return connection[settings.MONGODB_DB][settings.MONGODB_COLLECTION]', 'def save_event(collection, event, timestamp, params):\\n    \"Save the event in MongoDB collection\"\\n    collection.insert({\\n        \\'event\\': event,\\n        \\'timestamp\\': datetime.fromtimestamp(timestamp),\\n        \\'params\\': params\\n    })']}, {'features': [], 'snippets': ['def urlopen(*args, **kwargs):\\n    # Only parse one arg: the url\\n    return Urls[args[0]]', \"def __init__(self, content):\\n        super(MockUrlContent, self).__init__(content)\\n        self.headers = {\\n            'last-modified': time()\\n        }\", 'def __setitem__(self, name, content):\\n        super(MockUrlCache, self).__setitem__(name, MockUrlContent(content))', 'def clear_configs():\\n    pass', 'def testImportContent():\\n    \"Cannot import content from a file\"\\n    from xmlconfig import getConfig\\n    Urls.clear()\\n    Urls[\"file:file.txt\"] = \"Content embedded in a file\"\\n    Urls[\"config.xml\"] = \\\\\\n    u\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n    <config>\\n        <constants>\\n            <string key=\"import\" src=\"file:file.txt\"/>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    conf=getConfig()\\n    conf.load(\"config.xml\")\\n    assert conf.get(\"import\") == \"Content embedded in a file\"', 'def testImportConfig():\\n    \"Cannot import another config file\"\\n    from xmlconfig import getConfig\\n    Urls.clear()\\n    Urls[\"config2.xml\"] = \\\\\\n    \"\"\"<?xml version=\"1.0\"?>\\n    <config>\\n        <constants>\\n            <string key=\"key22\">This was imported from config2.xml</string>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    Urls[\"config.xml\"] = \\\\\\n    u\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n    <config>\\n        <constants namespace=\"import\" src=\"file:config2.xml\"/>\\n        <constants>\\n            <string key=\"imported\">%(import:key22)</string>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    conf=getConfig()\\n    conf.load(\"config.xml\")\\n    assert conf.get(\"imported\") == \"This was imported from config2.xml\"', 'def testCircularImport():\\n    \"Property detect circluar importing\"\\n    from xmlconfig import getConfig\\n    Urls.clear()\\n    Urls[\"config2.xml\"] = \\\\\\n    \"\"\"<?xml version=\"1.0\"?>\\n    <config>\\n        <constants namespace=\"circular\" src=\"file:config.xml\"/>        \\n        <constants>\\n            <string key=\"key22\">This was imported from config2.xml</string>        \\n            <string key=\"foreign\">\\n                Namespace changed in %(circular:key4.import)\\n            </string>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    Urls[\"config.xml\"] = \\\\\\n    \"\"\"<?xml version=\"1.0\"?>\\n    <config>\\n        <constants namespace=\"import\" src=\"file:config2.xml\"/>\\n        <constants>\\n            <section key=\"key4\">\\n                <string key=\"key5\">value2</string>\\n                <string key=\"import\">%(import:key22)</string>\\n            </section>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    conf=getConfig()\\n    conf.load(\"config.xml\")\\n    assert conf.get(\"import:foreign\") == \\\\\\n        \"Namespace changed in This was imported from config2.xml\"', 'def testRelativeImport():\\n    \"\"\"Transfer leading absolute or relative path to the location of \\n    documents imported\"\"\"\\n    from xmlconfig import getConfig\\n    Urls[\"../config/config2.xml\"] = \\\\\\n    \"\"\"<?xml version=\"1.0\"?>\\n    <config>\\n        <constants>\\n            <string key=\"key22\">This was imported from config2.xml</string>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    Urls[\"../config/config.xml\"] = \\\\\\n    \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n    <config>\\n        <constants namespace=\"import\" src=\"file:config2.xml\"/>\\n        <constants>\\n            <string key=\"imported\">%(import:key22)</string>\\n        </constants>\\n    </config>\\n    \"\"\"\\n    conf=getConfig()\\n    conf.load(\"../config/config.xml\")\\n    assert conf.get(\"imported\") == \"This was imported from config2.xml\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_keywords():\\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\\n    # these strings will be replaced by git during git-archive.\\n    # setup.py/versioneer.py will grep for the variable names, so they must\\n    # each be defined on a line of their own. _version.py will just call\\n    # get_keywords().\\n    git_refnames = \"$Format:%d$\"\\n    git_full = \"$Format:%H$\"\\n    git_date = \"$Format:%ci$\"\\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\\n    return keywords', 'def get_config():\\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\\n    # these strings are filled in when \\'setup.py versioneer\\' creates\\n    # _version.py\\n    cfg = VersioneerConfig()\\n    cfg.VCS = \"git\"\\n    cfg.style = \"pep440\"\\n    cfg.tag_prefix = \"v\"\\n    cfg.parentdir_prefix = \"\"\\n    cfg.versionfile_source = \"jxl2txt/_version.py\"\\n    cfg.verbose = False\\n    return cfg', 'def register_vcs_handler(vcs, method):  # decorator\\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\\n    def decorate(f):\\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\\n        if vcs not in HANDLERS:\\n            HANDLERS[vcs] = {}\\n        HANDLERS[vcs][method] = f\\n        return f\\n    return decorate', 'def versions_from_parentdir(parentdir_prefix, root, verbose):\\n    \"\"\"Try to determine the version from the parent directory name.\\n\\n    Source tarballs conventionally unpack into a directory that includes both\\n    the project name and a version string. We will also support searching up\\n    two directory levels for an appropriately named parent directory\\n    \"\"\"\\n    rootdirs = []\\n\\n    for i in range(3):\\n        dirname = os.path.basename(root)\\n        if dirname.startswith(parentdir_prefix):\\n            return {\"version\": dirname[len(parentdir_prefix):],\\n                    \"full-revisionid\": None,\\n                    \"dirty\": False, \"error\": None, \"date\": None}\\n        else:\\n            rootdirs.append(root)\\n            root = os.path.dirname(root)  # up a level\\n\\n    if verbose:\\n        print(\"Tried directories %s but none started with prefix %s\" %\\n              (str(rootdirs), parentdir_prefix))\\n    raise NotThisMethod(\"rootdir doesn\\'t start with parentdir_prefix\")', 'def git_get_keywords(versionfile_abs):\\n    \"\"\"Extract version information from the given file.\"\"\"\\n    # the code embedded in _version.py can just fetch the value of these\\n    # keywords. When used from setup.py, we don\\'t want to import _version.py,\\n    # so we do it with a regexp instead. This function is not used from\\n    # _version.py.\\n    keywords = {}\\n    try:\\n        f = open(versionfile_abs, \"r\")\\n        for line in f.readlines():\\n            if line.strip().startswith(\"git_refnames =\"):\\n                mo = re.search(r\\'=\\\\s*\"(.*)\"\\', line)\\n                if mo:\\n                    keywords[\"refnames\"] = mo.group(1)\\n            if line.strip().startswith(\"git_full =\"):\\n                mo = re.search(r\\'=\\\\s*\"(.*)\"\\', line)\\n                if mo:\\n                    keywords[\"full\"] = mo.group(1)\\n            if line.strip().startswith(\"git_date =\"):\\n                mo = re.search(r\\'=\\\\s*\"(.*)\"\\', line)\\n                if mo:\\n                    keywords[\"date\"] = mo.group(1)\\n        f.close()\\n    except EnvironmentError:\\n        pass\\n    return keywords', 'def git_versions_from_keywords(keywords, tag_prefix, verbose):\\n    \"\"\"Get version information from git keywords.\"\"\"\\n    if not keywords:\\n        raise NotThisMethod(\"no keywords at all, weird\")\\n    date = keywords.get(\"date\")\\n    if date is not None:\\n        # Use only the last line.  Previous lines may contain GPG signature\\n        # information.\\n        date = date.splitlines()[-1]\\n\\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\\n        # -like\" string, which we must then edit to make compliant), because\\n        # it\\'s been around since git-1.5.3, and it\\'s too difficult to\\n        # discover which version we\\'re using, or to work around using an\\n        # older one.\\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\\n    refnames = keywords[\"refnames\"].strip()\\n    if refnames.startswith(\"$Format\"):\\n        if verbose:\\n            print(\"keywords are unexpanded, not using\")\\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\\n    TAG = \"tag: \"\\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\\n    if not tags:\\n        # Either we\\'re using git < 1.8.3, or there really are no tags. We use\\n        # a heuristic: assume all version tags have a digit. The old git %d\\n        # expansion behaves like git log --decorate=short and strips out the\\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\\n        # between branches and tags. By ignoring refnames without digits, we\\n        # filter out many common branch names like \"release\" and\\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\\n        tags = set([r for r in refs if re.search(r\\'\\\\d\\', r)])\\n        if verbose:\\n            print(\"discarding \\'%s\\', no digits\" % \",\".join(refs - tags))\\n    if verbose:\\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\\n    for ref in sorted(tags):\\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\\n        if ref.startswith(tag_prefix):\\n            r = ref[len(tag_prefix):]\\n            if verbose:\\n                print(\"picking %s\" % r)\\n            return {\"version\": r,\\n                    \"full-revisionid\": keywords[\"full\"].strip(),\\n                    \"dirty\": False, \"error\": None,\\n                    \"date\": date}\\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\\n    if verbose:\\n        print(\"no suitable tags, using unknown + full revision id\")\\n    return {\"version\": \"0+unknown\",\\n            \"full-revisionid\": keywords[\"full\"].strip(),\\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}', 'def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\\n    \"\"\"Get version from \\'git describe\\' in the root of the source tree.\\n\\n    This only gets called if the git-archive \\'subst\\' keywords were *not*\\n    expanded, and _version.py hasn\\'t already been rewritten with a short\\n    version string, meaning we\\'re inside a checked out source tree.\\n    \"\"\"\\n    GITS = [\"git\"]\\n    if sys.platform == \"win32\":\\n        GITS = [\"git.cmd\", \"git.exe\"]\\n\\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\\n                          hide_stderr=True)\\n    if rc != 0:\\n        if verbose:\\n            print(\"Directory %s not under git control\" % root)\\n        raise NotThisMethod(\"\\'git rev-parse --git-dir\\' returned error\")\\n\\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\\n    # if there isn\\'t one, this yields HEX[-dirty] (no NUM)\\n    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\\n                                          \"--always\", \"--long\",\\n                                          \"--match\", \"%s*\" % tag_prefix],\\n                                   cwd=root)\\n    # --long was added in git-1.5.5\\n    if describe_out is None:\\n        raise NotThisMethod(\"\\'git describe\\' failed\")\\n    describe_out = describe_out.strip()\\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\\n    if full_out is None:\\n        raise NotThisMethod(\"\\'git rev-parse\\' failed\")\\n    full_out = full_out.strip()\\n\\n    pieces = {}\\n    pieces[\"long\"] = full_out\\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\\n    pieces[\"error\"] = None\\n\\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\\n    # TAG might have hyphens.\\n    git_describe = describe_out\\n\\n    # look for -dirty suffix\\n    dirty = git_describe.endswith(\"-dirty\")\\n    pieces[\"dirty\"] = dirty\\n    if dirty:\\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\\n\\n    # now we have TAG-NUM-gHEX or HEX\\n\\n    if \"-\" in git_describe:\\n        # TAG-NUM-gHEX\\n        mo = re.search(r\\'^(.+)-(\\\\d+)-g([0-9a-f]+)$\\', git_describe)\\n        if not mo:\\n            # unparseable. Maybe git-describe is misbehaving?\\n            pieces[\"error\"] = (\"unable to parse git-describe output: \\'%s\\'\"\\n                               % describe_out)\\n            return pieces\\n\\n        # tag\\n        full_tag = mo.group(1)\\n        if not full_tag.startswith(tag_prefix):\\n            if verbose:\\n                fmt = \"tag \\'%s\\' doesn\\'t start with prefix \\'%s\\'\"\\n                print(fmt % (full_tag, tag_prefix))\\n            pieces[\"error\"] = (\"tag \\'%s\\' doesn\\'t start with prefix \\'%s\\'\"\\n                               % (full_tag, tag_prefix))\\n            return pieces\\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\\n\\n        # distance: number of commits since tag\\n        pieces[\"distance\"] = int(mo.group(2))\\n\\n        # commit: short hex revision ID\\n        pieces[\"short\"] = mo.group(3)\\n\\n    else:\\n        # HEX: no tags\\n        pieces[\"closest-tag\"] = None\\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\\n                                    cwd=root)\\n        pieces[\"distance\"] = int(count_out)  # total number of commits\\n\\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\\n                       cwd=root)[0].strip()\\n    # Use only the last line.  Previous lines may contain GPG signature\\n    # information.\\n    date = date.splitlines()[-1]\\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\\n\\n    return pieces', 'def render_pep440(pieces):\\n    \"\"\"Build up version string, with post-release \"local version identifier\".\\n\\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\\n    get a tagged build and then dirty it, you\\'ll get TAG+0.gHEX.dirty\\n\\n    Exceptions:\\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\\n    \"\"\"\\n    if pieces[\"closest-tag\"]:\\n        rendered = pieces[\"closest-tag\"]\\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\\n            rendered += plus_or_dot(pieces)\\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\\n            if pieces[\"dirty\"]:\\n                rendered += \".dirty\"\\n    else:\\n        # exception #1\\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\\n                                          pieces[\"short\"])\\n        if pieces[\"dirty\"]:\\n            rendered += \".dirty\"\\n    return rendered', 'def render_pep440_post(pieces):\\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\\n\\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\\n    (a dirty tree will appear \"older\" than the corresponding clean one),\\n    but you shouldn\\'t be releasing software with -dirty anyways.\\n\\n    Exceptions:\\n    1: no tags. 0.postDISTANCE[.dev0]\\n    \"\"\"\\n    if pieces[\"closest-tag\"]:\\n        rendered = pieces[\"closest-tag\"]\\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\\n            rendered += \".post%d\" % pieces[\"distance\"]\\n            if pieces[\"dirty\"]:\\n                rendered += \".dev0\"\\n            rendered += plus_or_dot(pieces)\\n            rendered += \"g%s\" % pieces[\"short\"]\\n    else:\\n        # exception #1\\n        rendered = \"0.post%d\" % pieces[\"distance\"]\\n        if pieces[\"dirty\"]:\\n            rendered += \".dev0\"\\n        rendered += \"+g%s\" % pieces[\"short\"]\\n    return rendered', 'def render_git_describe(pieces):\\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\\n\\n    Like \\'git describe --tags --dirty --always\\'.\\n\\n    Exceptions:\\n    1: no tags. HEX[-dirty]  (note: no \\'g\\' prefix)\\n    \"\"\"\\n    if pieces[\"closest-tag\"]:\\n        rendered = pieces[\"closest-tag\"]\\n        if pieces[\"distance\"]:\\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\\n    else:\\n        # exception #1\\n        rendered = pieces[\"short\"]\\n    if pieces[\"dirty\"]:\\n        rendered += \"-dirty\"\\n    return rendered', 'def render(pieces, style):\\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\\n    if pieces[\"error\"]:\\n        return {\"version\": \"unknown\",\\n                \"full-revisionid\": pieces.get(\"long\"),\\n                \"dirty\": None,\\n                \"error\": pieces[\"error\"],\\n                \"date\": None}\\n\\n    if not style or style == \"default\":\\n        style = \"pep440\"  # the default\\n\\n    if style == \"pep440\":\\n        rendered = render_pep440(pieces)\\n    elif style == \"pep440-pre\":\\n        rendered = render_pep440_pre(pieces)\\n    elif style == \"pep440-post\":\\n        rendered = render_pep440_post(pieces)\\n    elif style == \"pep440-old\":\\n        rendered = render_pep440_old(pieces)\\n    elif style == \"git-describe\":\\n        rendered = render_git_describe(pieces)\\n    elif style == \"git-describe-long\":\\n        rendered = render_git_describe_long(pieces)\\n    else:\\n        raise ValueError(\"unknown style \\'%s\\'\" % style)\\n\\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\\n            \"date\": pieces.get(\"date\")}']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def test_property_delete(self):\\n        e = E()\\n        with self.assertRaises(TraitError):\\n            del e.a\\n        with self.assertRaises(TraitError):\\n            del e.b']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, **kwargs):\\n        \"\"\" Initializes a Avatar instance\\n\\n            Notes:\\n                You can specify all parameters while calling this methods.\\n                A special argument named `data` will enable you to load the\\n                object from a Python dictionary\\n\\n            Examples:\\n                >>> avatar = NUAvatar(id=u\\'xxxx-xxx-xxx-xxx\\', name=u\\'Avatar\\')\\n                >>> avatar = NUAvatar(data=my_dict)\\n        \"\"\"\\n\\n        super(NUAvatar, self).__init__()\\n\\n        # Read/Write Attributes', 'def last_updated_by(self):\\n        \"\"\" Get last_updated_by value.\\n\\n            Notes:\\n                ID of the user who last updated the object.', 'def last_updated_by(self, value):\\n        \"\"\" Set last_updated_by value.\\n\\n            Notes:\\n                ID of the user who last updated the object.', 'def last_updated_date(self):\\n        \"\"\" Get last_updated_date value.\\n\\n            Notes:\\n                Time stamp when this object was last updated.', 'def last_updated_date(self, value):\\n        \"\"\" Set last_updated_date value.\\n\\n            Notes:\\n                Time stamp when this object was last updated.', 'def embedded_metadata(self):\\n        \"\"\" Get embedded_metadata value.\\n\\n            Notes:\\n                Metadata objects associated with this entity. This will contain a list of Metadata objects if the API request is made using the special flag to enable the embedded Metadata feature. Only a maximum of Metadata objects is returned based on the value set in the system configuration.', 'def embedded_metadata(self, value):\\n        \"\"\" Set embedded_metadata value.\\n\\n            Notes:\\n                Metadata objects associated with this entity. This will contain a list of Metadata objects if the API request is made using the special flag to enable the embedded Metadata feature. Only a maximum of Metadata objects is returned based on the value set in the system configuration.', 'def entity_scope(self):\\n        \"\"\" Get entity_scope value.\\n\\n            Notes:\\n                Specify if scope of entity is Data center or Enterprise level', 'def entity_scope(self, value):\\n        \"\"\" Set entity_scope value.\\n\\n            Notes:\\n                Specify if scope of entity is Data center or Enterprise level', 'def creation_date(self):\\n        \"\"\" Get creation_date value.\\n\\n            Notes:\\n                Time stamp when this object was created.', 'def creation_date(self, value):\\n        \"\"\" Set creation_date value.\\n\\n            Notes:\\n                Time stamp when this object was created.', 'def owner(self):\\n        \"\"\" Get owner value.\\n\\n            Notes:\\n                Identifies the user that has created this object.', 'def owner(self, value):\\n        \"\"\" Set owner value.\\n\\n            Notes:\\n                Identifies the user that has created this object.', 'def external_id(self):\\n        \"\"\" Get external_id value.\\n\\n            Notes:\\n                External object ID. Used for integration with third party systems', 'def external_id(self, value):\\n        \"\"\" Set external_id value.\\n\\n            Notes:\\n                External object ID. Used for integration with third party systems', 'def type(self):\\n        \"\"\" Get type value.\\n\\n            Notes:\\n                The image type', 'def type(self, value):\\n        \"\"\" Set type value.\\n\\n            Notes:\\n                The image type']}, {'features': [], 'snippets': [\"def _run_cmd(cmd):\\n        return check_output(['gpmdp-remote', cmd]).decode('utf-8').strip()\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def plot_timeseries(frames, time=None, offset=None, color='k', linestyle='-'):\\n  frames = np.asarray(frames)\\n  if offset == None:\\n    offset = np.max(np.std(frames, axis=0)) * 3\\n  if time == None:\\n    time = np.arange(frames.shape[0])\\n  plt.plot(time, frames - np.mean(frames, axis=0) + \\n    np.arange(frames.shape[1]) * offset, color=color, ls=linestyle)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def setUp(self):\\n        self.nn = FLANN(log_level=\"warning\")', 'def test_nn_2d_10pt(self):\\n        self.__nd_random_test_autotune(2, 2)', 'def test_nn_autotune_2d_1000pt(self):\\n        self.__nd_random_test_autotune(2, 1000)', 'def test_nn_autotune_500d_100pt(self):\\n        self.__nd_random_test_autotune(500, 100)', 'def test_nn_stress_1d_1pt_kmeans_autotune(self):\\n        self.__nd_random_test_autotune(1, 1)', 'def __ensure_list(self,arg):\\n        if type(arg)!=list:\\n            return [arg]\\n        else:\\n            return arg']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def setUp(self):\\n    self.config_instance = roster_core.Config(file_name=CONFIG_FILE)\\n    self.cred_instance = credentials.CredCache(self.config_instance,\\n                                               u'sharrell')\\n    db_instance = self.config_instance.GetDb()\\n\\n    db_instance.CreateRosterDatabase()\\n\\n    data = open(DATA_FILE, 'r').read()\\n    db_instance.StartTransaction()\\n    db_instance.cursor.execute(data)\\n    db_instance.EndTransaction()\\n    db_instance.close()\\n\\n    self.core_instance = roster_core.Core(u'sharrell', self.config_instance)\", \"def testCredentials(self):\\n    self.assertTrue(self.cred_instance.Authenticate(u'sharrell', 'test'))\\n    cred_string = self.cred_instance.GetCredentials(u'sharrell', 'test',\\n                                                    self.core_instance)\\n    self.assertEqual(self.cred_instance.CheckCredential(cred_string,\\n                                                        u'sharrell',\\n                                                       self.core_instance),\\n                     u'')\\n    self.assertEqual(self.cred_instance.CheckCredential(u'test', u'sharrell',\\n                                                        self.core_instance),\\n                     None)\"]}, {'features': [], 'snippets': ['def to_objectset(xs):\\n    return set([frozenset(x.items()) for x in xs])']}, {'features': [], 'snippets': ['def func(self):\\n        \"\"\"\\n        All we do is to scan the current location for an Attribute\\n        called `tutorial_info` and display that.\\n        \"\"\"\\n\\n        caller = self.caller\\n\\n        if not self.args:\\n            target = self.obj  # this is the room the command is defined on\\n        else:\\n            target = caller.search(self.args.strip())\\n            if not target:\\n                return\\n        helptext = target.db.tutorial_info\\n        if helptext:\\n            caller.msg(\"|G%s|n\" % helptext)\\n        else:\\n            caller.msg(\"|RSorry, there is no tutorial help available here.|n\")', 'def func(self):\\n        \"\"\"\\n        All this does is to check if the object has\\n        the set_detail method and uses it.\\n        \"\"\"\\n        if not self.args or not self.rhs:\\n            self.caller.msg(\"Usage: @detail key = description\")\\n            return\\n        if not hasattr(self.obj, \"set_detail\"):\\n            self.caller.msg(\"Details cannot be set on %s.\" % self.obj)\\n            return\\n        for key in self.lhs.split(\";\"):\\n            # loop over all aliases, if any (if not, this will just be\\n            # the one key to loop over)\\n            self.obj.set_detail(key, self.rhs)\\n        self.caller.msg(\"Detail set: \\'%s\\': \\'%s\\'\" % (self.lhs, self.rhs))', 'def func(self):\\n        \"\"\"\\n        Handle the looking. This is a copy of the default look\\n        code except for adding in the details.\\n        \"\"\"\\n        caller = self.caller\\n        args = self.args\\n        if args:\\n            # we use quiet=True to turn off automatic error reporting.\\n            # This tells search that we want to handle error messages\\n            # ourself. This also means the search function will always\\n            # return a list (with 0, 1 or more elements) rather than\\n            # result/None.\\n            looking_at_obj = caller.search(args,\\n                                           # note: excludes room/room aliases\\n                                           candidates=caller.location.contents + caller.contents,\\n                                           use_nicks=True, quiet=True)\\n            if len(looking_at_obj) != 1:\\n                # no target found or more than one target found (multimatch)\\n                # look for a detail that may match\\n                detail = self.obj.return_detail(args)\\n                if detail:\\n                    self.caller.msg(detail)\\n                    return\\n                else:\\n                    # no detail found, delegate our result to the normal\\n                    # error message handler.\\n                    _SEARCH_AT_RESULT(None, caller, args, looking_at_obj)\\n                    return\\n            else:\\n                # we found a match, extract it from the list and carry on\\n                # normally with the look handling.\\n                looking_at_obj = looking_at_obj[0]\\n\\n        else:\\n            looking_at_obj = caller.location\\n            if not looking_at_obj:\\n                caller.msg(\"You have no location to look at!\")\\n                return\\n\\n        if not hasattr(looking_at_obj, \\'return_appearance\\'):\\n            # this is likely due to us having an account instead\\n            looking_at_obj = looking_at_obj.character\\n        if not looking_at_obj.access(caller, \"view\"):\\n            caller.msg(\"Could not find \\'%s\\'.\" % args)\\n            return\\n        # get object\\'s appearance\\n        caller.msg(looking_at_obj.return_appearance(caller))\\n        # the object\\'s at_desc() method.\\n        looking_at_obj.at_desc(looker=caller)\\n        return', 'def at_cmdset_creation(self):\\n        \"\"\"add the tutorial-room commands\"\"\"\\n        self.add(CmdTutorial())\\n        self.add(CmdTutorialSetDetail())\\n        self.add(CmdTutorialLook())', 'def at_object_creation(self):\\n        \"\"\"Called when room is first created\"\"\"\\n        self.db.tutorial_info = \"This is a tutorial room. It allows you to use the \\'tutorial\\' command.\"\\n        self.cmdset.add_default(TutorialRoomCmdSet)', 'def return_detail(self, detailkey):\\n        \"\"\"\\n        This looks for an Attribute \"obj_details\" and possibly\\n        returns the value of it.\\n\\n        Args:\\n            detailkey (str): The detail being looked at. This is\\n                case-insensitive.\\n\\n        \"\"\"\\n        details = self.db.details\\n        if details:\\n            return details.get(detailkey.lower(), None)', 'def at_object_creation(self):\\n        \"\"\"\\n        Called when object is first created.\\n        We set up a ticker to update this room regularly.\\n\\n        Note that we could in principle also use a Script to manage\\n        the ticking of the room; the TickerHandler works fine for\\n        simple things like this though.\\n        \"\"\"\\n        super(WeatherRoom, self).at_object_creation()\\n        # subscribe ourselves to a ticker to repeatedly call the hook\\n        # \"update_weather\" on this object. The interval is randomized\\n        # so as to not have all weather rooms update at the same time.\\n        self.db.interval = random.randint(50, 70)\\n        TICKER_HANDLER.add(interval=self.db.interval, callback=self.update_weather, idstring=\"tutorial\")\\n        # this is parsed by the \\'tutorial\\' command on TutorialRooms.\\n        self.db.tutorial_info = \\\\\\n            \"This room has a Script running that has it echo a weather-related message at irregular intervals.\"', 'def at_object_creation(self):\\n        \"\"\"\\n        Called when the room is first created.\\n        \"\"\"\\n        super(IntroRoom, self).at_object_creation()\\n        self.db.tutorial_info = \"The first room of the tutorial. \" \\\\\\n                                \"This assigns the health Attribute to \"\\\\\\n                                \"the account.\"', 'def func(self):\\n        \"\"\"move one step eastwards\"\"\"\\n        caller = self.caller\\n\\n        bridge_step = min(5, caller.db.tutorial_bridge_position + 1)\\n\\n        if bridge_step > 4:\\n            # we have reached the far east end of the bridge.\\n            # Move to the east room.\\n            eexit = search_object(self.obj.db.east_exit)\\n            if eexit:\\n                caller.move_to(eexit[0])\\n            else:\\n                caller.msg(\"No east exit was found for this room. Contact an admin.\")\\n            return\\n        caller.db.tutorial_bridge_position = bridge_step\\n        # since we are really in one room, we have to notify others\\n        # in the room when we move.\\n        caller.location.msg_contents(\"%s steps eastwards across the bridge.\" % caller.name, exclude=caller)\\n        caller.execute_cmd(\"look\")', 'def func(self):\\n        \"\"\"move one step westwards\"\"\"\\n        caller = self.caller\\n\\n        bridge_step = max(-1, caller.db.tutorial_bridge_position - 1)\\n\\n        if bridge_step < 0:\\n            # we have reached the far west end of the bridge.\\n            # Move to the west room.\\n            wexit = search_object(self.obj.db.west_exit)\\n            if wexit:\\n                caller.move_to(wexit[0])\\n            else:\\n                caller.msg(\"No west exit was found for this room. Contact an admin.\")\\n            return\\n        caller.db.tutorial_bridge_position = bridge_step\\n        # since we are really in one room, we have to notify others\\n        # in the room when we move.\\n        caller.location.msg_contents(\"%s steps westwards across the bridge.\" % caller.name, exclude=caller)\\n        caller.execute_cmd(\"look\")', 'def func(self):\\n        \"\"\"Looking around, including a chance to fall.\"\"\"\\n        caller = self.caller\\n        bridge_position = self.caller.db.tutorial_bridge_position\\n        # this command is defined on the room, so we get it through self.obj\\n        location = self.obj\\n        # randomize the look-echo\\n        message = \"|c%s|n\\\\n%s\\\\n%s\" % (location.key,\\n                                      BRIDGE_POS_MESSAGES[bridge_position],\\n                                      random.choice(BRIDGE_MOODS))\\n\\n        chars = [obj for obj in self.obj.contents_get(exclude=caller) if obj.has_account]\\n        if chars:\\n            # we create the You see: message manually here\\n            message += \"\\\\n You see: %s\" % \", \".join(\"|c%s|n\" % char.key for char in chars)\\n        self.caller.msg(message)\\n\\n        # there is a chance that we fall if we are on the western or central\\n        # part of the bridge.\\n        if bridge_position < 3 and random.random() < 0.05 and not self.caller.is_superuser:\\n            # we fall 5% of time.\\n            fall_exit = search_object(self.obj.db.fall_exit)\\n            if fall_exit:\\n                self.caller.msg(\"|r%s|n\" % FALL_MESSAGE)\\n                self.caller.move_to(fall_exit[0], quiet=True)\\n                # inform others on the bridge\\n                self.obj.msg_contents(\"A plank gives way under %s\\'s feet and \"\\n                                      \"they fall from the bridge!\" % self.caller.key)', 'def func(self):\\n        \"\"\"Implements the command.\"\"\"\\n        string = \"You are trying hard not to fall off the bridge ...\" \\\\\\n                 \"\\\\n\\\\nWhat you can do is trying to cross the bridge |weast|n\" \\\\\\n                 \" or try to get back to the mainland |wwest|n).\"\\n        self.caller.msg(string)', 'def at_cmdset_creation(self):\\n        \"\"\"Called at first cmdset creation\"\"\"\\n        self.add(CmdTutorial())\\n        self.add(CmdEast())\\n        self.add(CmdWest())\\n        self.add(CmdLookBridge())\\n        self.add(CmdBridgeHelp())', 'def at_object_creation(self):\\n        \"\"\"Setups the room\"\"\"\\n        # this will start the weather room\\'s ticker and tell\\n        # it to call update_weather regularly.\\n        super(BridgeRoom, self).at_object_creation()\\n        # this identifies the exits from the room (should be the command\\n        # needed to leave through that exit). These are defaults, but you\\n        # could of course also change them after the room has been created.\\n        self.db.west_exit = \"cliff\"\\n        self.db.east_exit = \"gate\"\\n        self.db.fall_exit = \"cliffledge\"\\n        # add the cmdset on the room.\\n        self.cmdset.add_default(BridgeCmdSet)\\n        # since the default Character\\'s at_look() will access the room\\'s\\n        # return_description (this skips the cmdset) when\\n        # first entering it, we need to explicitly turn off the room\\n        # as a normal view target - once inside, our own look will\\n        # handle all return messages.\\n        self.locks.add(\"view:false()\")', 'def at_object_receive(self, character, source_location):\\n        \"\"\"\\n        This hook is called by the engine whenever the player is moved\\n        into this room.\\n        \"\"\"\\n        if character.has_account:\\n            # we only run this if the entered object is indeed a player object.\\n            # check so our east/west exits are correctly defined.\\n            wexit = search_object(self.db.west_exit)\\n            eexit = search_object(self.db.east_exit)\\n            fexit = search_object(self.db.fall_exit)\\n            if not (wexit and eexit and fexit):\\n                character.msg(\"The bridge\\'s exits are not properly configured. \"\\n                              \"Contact an admin. Forcing west-end placement.\")\\n                character.db.tutorial_bridge_position = 0\\n                return\\n            if source_location == eexit[0]:\\n                # we assume we enter from the same room we will exit to\\n                character.db.tutorial_bridge_position = 4\\n            else:\\n                # if not from the east, then from the west!\\n                character.db.tutorial_bridge_position = 0\\n            character.execute_cmd(\"look\")', 'def func(self):\\n        \"\"\"\\n        Implement the command.\\n\\n        This works both as a look and a search command; there is a\\n        random chance of eventually finding a light source.\\n        \"\"\"\\n        caller = self.caller\\n\\n        if random.random() < 0.8:\\n            # we don\\'t find anything\\n            caller.msg(random.choice(DARK_MESSAGES))\\n        else:\\n            # we could have found something!\\n            if any(obj for obj in caller.contents if utils.inherits_from(obj, LightSource)):\\n                #  we already carry a LightSource object.\\n                caller.msg(ALREADY_LIGHTSOURCE)\\n            else:\\n                # don\\'t have a light source, create a new one.\\n                create_object(LightSource, key=\"splinter\", location=caller)\\n                caller.msg(FOUND_LIGHTSOURCE)', 'def func(self):\\n        \"\"\"\\n        Replace the the help command with a not-so-useful help\\n        \"\"\"\\n        string = \"Can\\'t help you until you find some light! Try looking/feeling around for something to burn. \" \\\\\\n                 \"You shouldn\\'t give up even if you don\\'t find anything right away.\"\\n        self.caller.msg(string)', 'def func(self):\\n        \"\"\"Implements the command.\"\"\"\\n        self.caller.msg(\"Until you find some light, there\\'s not much you can do. Try feeling around.\")', 'def at_cmdset_creation(self):\\n        \"\"\"populate the cmdset.\"\"\"\\n        self.add(CmdTutorial())\\n        self.add(CmdLookDark())\\n        self.add(CmdDarkHelp())\\n        self.add(CmdDarkNoMatch())\\n        self.add(default_cmds.CmdSay)', 'def at_object_creation(self):\\n        \"\"\"\\n        Called when object is first created.\\n        \"\"\"\\n        super(DarkRoom, self).at_object_creation()\\n        self.db.tutorial_info = \"This is a room with custom command sets on itself.\"\\n        # the room starts dark.\\n        self.db.is_lit = False\\n        self.cmdset.add(DarkCmdSet, permanent=True)', 'def _carries_light(self, obj):\\n        \"\"\"\\n        Checks if the given object carries anything that gives light.\\n\\n        Note that we do NOT look for a specific LightSource typeclass,\\n        but for the Attribute is_giving_light - this makes it easy to\\n        later add other types of light-giving items. We also accept\\n        if there is a light-giving object in the room overall (like if\\n        a splinter was dropped in the room)\\n        \"\"\"\\n        return obj.is_superuser or obj.db.is_giving_light or any(o for o in obj.contents if o.db.is_giving_light)', 'def check_light_state(self, exclude=None):\\n        \"\"\"\\n        This method checks if there are any light sources in the room.\\n        If there isn\\'t it makes sure to add the dark cmdset to all\\n        characters in the room. It is called whenever characters enter\\n        the room and also by the Light sources when they turn on.\\n\\n        Args:\\n            exclude (Object): An object to not include in the light check.\\n        \"\"\"\\n        if any(self._carries_light(obj) for obj in self.contents if obj != exclude):\\n            self.locks.add(\"view:all()\")\\n            self.cmdset.remove(DarkCmdSet)\\n            self.db.is_lit = True\\n            for char in (obj for obj in self.contents if obj.has_account):\\n                # this won\\'t do anything if it is already removed\\n                char.msg(\"The room is lit up.\")\\n        else:\\n            # noone is carrying light - darken the room\\n            self.db.is_lit = False\\n            self.locks.add(\"view:false()\")\\n            self.cmdset.add(DarkCmdSet, permanent=True)\\n            for char in (obj for obj in self.contents if obj.has_account):\\n                if char.is_superuser:\\n                    char.msg(\"You are Superuser, so you are not affected by the dark state.\")\\n                else:\\n                    # put players in darkness\\n                    char.msg(\"The room is completely dark.\")', 'def at_object_leave(self, obj, target_location):\\n        \"\"\"\\n        In case people leave with the light, we make sure to clear the\\n        DarkCmdSet if necessary.  This also works if they are\\n        teleported away.\\n        \"\"\"\\n        # since this hook is called while the object is still in the room,\\n        # we exclude it from the light check, to ignore any light sources\\n        # it may be carrying.\\n        self.check_light_state(exclude=obj)', 'def at_object_creation(self):\\n        \"\"\"Called at first creation\"\"\"\\n        super(TeleportRoom, self).at_object_creation()\\n        # what character.db.puzzle_clue must be set to, to avoid teleportation.\\n        self.db.puzzle_value = 1\\n        # target of successful teleportation. Can be a dbref or a\\n        # unique room name.\\n        self.db.success_teleport_msg = \"You are successful!\"\\n        self.db.success_teleport_to = \"treasure room\"\\n        # the target of the failure teleportation.\\n        self.db.failure_teleport_msg = \"You fail!\"\\n        self.db.failure_teleport_to = \"dark cell\"', 'def at_object_creation(self):\\n        \"\"\"\\n        Called when the room is first created.\\n        \"\"\"\\n        super(OutroRoom, self).at_object_creation()\\n        self.db.tutorial_info = \"The last room of the tutorial. \" \\\\\\n                                \"This cleans up all temporary Attributes \" \\\\\\n                                \"the tutorial may have assigned to the \"\\\\\\n                                \"character.\"']}, {'features': [], 'snippets': ['def _complexity_rec(x,y,index,depth,fm):\\n        \"\"\"\\n        Recurrent helper function for complexity()\\n        \"\"\"\\n        global max_value\\n        global global_index\\n        if depth<size(fm.features):\\n            for i in range(size(fm.features[depth].values)):\\n                _complexity_rec(x,y,index + (i,),depth+1,fm)\\n        else:\\n            if max_value < fm.full_matrix[index][x][y]:\\n                global_index = index\\n                max_value = fm.full_matrix[index][x][y]', 'def complexity(full_matrix):\\n    global global_index\\n    global max_value\\n    \"\"\"This function expects as an input a object of type FullMatrix which contains\\n    responses of all neurons in a sheet to stimuly with different varying parameter values.\\n    One of these parameters (features) has to be phase. In such case it computes the classic\\n    modulation ratio (see Hawken et al. for definition) for each neuron and returns them as a matrix.\\n    \"\"\"\\n    rows,cols = full_matrix.matrix_shape\\n    complexity = zeros(full_matrix.matrix_shape)\\n    complex_matrix = zeros(full_matrix.matrix_shape,object_)\\n    fftmeasure = zeros(full_matrix.matrix_shape,Float)\\n    i = 0\\n    for f in full_matrix.features:\\n        if f.name == \"phase\":', 'def compute_ACDC_orientation_tuning_curves(full_matrix,curve_label,sheet):', \"def phase_preference_scatter_plot(sheet_name,diameter=0.39):\\n    r = UniformRandom(seed=1023)\\n    preference_map = topo.sim[sheet_name].sheet_views['PhasePreference']\\n    offset_magnitude = 0.03\\n    datax = []\\n    datay = []\\n    (v,bb) = preference_map.view()\\n    for z in zeros(66):\\n        x = (r() - 0.5)*2*diameter\\n        y = (r() - 0.5)*2*diameter\\n        rand = r()\\n        xoff = sin(rand*2*pi)*offset_magnitude\\n        yoff = cos(rand*2*pi)*offset_magnitude\\n        xx = max(min(x+xoff,diameter),-diameter)\\n        yy = max(min(y+yoff,diameter),-diameter)\\n        x = max(min(x,diameter),-diameter)\\n        y = max(min(y,diameter),-diameter)\\n        [xc1,yc1] = topo.sim[sheet_name].sheet2matrixidx(xx,yy)\\n        [xc2,yc2] = topo.sim[sheet_name].sheet2matrixidx(x,y)\\n        if((xc1==xc2) &  (yc1==yc2)): continue\\n        datax = datax + [v[xc1,yc1]]\\n        datay = datay + [v[xc2,yc2]]\", 'def analyze_complexity(full_matrix,simple_sheet_name,complex_sheet_name,filename=None):\\n    \"\"\"\\n    Compute modulation ratio for each neuron, to distinguish complex from simple cells.\\n\\n    Uses full_matrix data obtained from measure_or_pref().\\n\\n    If there is a sheet named as specified in simple_sheet_name,\\n    also plots its phase preference as a scatter plot.\\n    \"\"\"\\n    import topo\\n    measured_sheets = [s for s in topo.sim.objects(CFSheet).values()\\n                       if hasattr(s,\\'measure_maps\\') and s.measure_maps]\\n\\n    for sheet in measured_sheets:   \\n        # Divide by two to get into 0-1 scale - that means simple/complex boundry is now at 0.5\\n        complx = array(complexity(full_matrix[sheet]))/2.0 \\n        # Should this be renamed to ModulationRatio?\\n        sheet.sheet_views[\\'ComplexSelectivity\\']=SheetView((complx,sheet.bounds), sheet.name , sheet.precedence, topo.sim.time(),sheet.row_precedence)\\n    import topo.command.pylabplots\\n    topo.command.pylabplots.plot_modulation_ratio(full_matrix,simple_sheet_name=simple_sheet_name,complex_sheet_name=complex_sheet_name,filename=filename)\\n\\n    # Avoid error if no simple sheet exists\\n    try:\\n        phase_preference_scatter_plot(simple_sheet_name,diameter=0.24999)\\n    except AttributeError:\\n        print \"Skipping phase preference scatter plot; could not analyze region %s.\" \\\\\\n              % simple_sheet_name', 'def __call__(self,**params):\\n        fm = super(measure_and_analyze_complexity,self).__call__(**params)\\n        #from topo.command.analysis import measure_or_pref\\n        #fm = measure_or_pref()\\n        analyze_complexity(fm,simple_sheet_name=\"V1Simple\",complex_sheet_name=\"V1Complex\",filename=\"ModulationRatio\")']}, {'features': [], 'snippets': ['def new_tool_type(request):\\n    if request.method == \\'POST\\':\\n        tform = ToolTypeForm(request.POST, instance=Tool_Type())\\n        if tform.is_valid():\\n            tform.save()\\n            messages.add_message(request,\\n                                 messages.SUCCESS,\\n                                 \\'Tool Type Configuration Successfully Created.\\',\\n                                 extra_tags=\\'alert-success\\')\\n            return HttpResponseRedirect(reverse(\\'tool_type\\', ))\\n    else:\\n        tform = ToolTypeForm()\\n        add_breadcrumb(title=\"New Tool Type Configuration\", top_level=False, request=request)\\n    return render(request, \\'dojo/new_tool_type.html\\',\\n                  {\\'tform\\': tform})', 'def edit_tool_type(request, ttid):\\n    tool_type = Tool_Type.objects.get(pk=ttid)\\n    if request.method == \\'POST\\':\\n        tform = ToolTypeForm(request.POST, instance=tool_type)\\n        if tform.is_valid():\\n            tform.save()\\n            messages.add_message(request,\\n                                 messages.SUCCESS,\\n                                 \\'Tool Type Configuration Successfully Updated.\\',\\n                                 extra_tags=\\'alert-success\\')\\n            return HttpResponseRedirect(reverse(\\'tool_type\\', ))\\n    else:\\n        tform = ToolTypeForm(instance=tool_type)\\n    add_breadcrumb(title=\"Edit Tool Type Configuration\", top_level=False, request=request)\\n\\n    return render(request,\\n                  \\'dojo/edit_tool_type.html\\',\\n                  {\\n                      \\'tform\\': tform,\\n                  })']}, {'features': [], 'snippets': ['def __init__(self):\\n        \"\"\"Constructeur du paramètre\"\"\"\\n        Parametre.__init__(self, \"voir\", \"view\")\\n        self.schema = \"<cle>\"\\n        self.aide_courte = \"affiche le détail d\\'un chemin\"\\n        self.aide_longue = \\\\\\n            \"Cette commande permet d\\'obtenir plus d\\'informations sur \" \\\\\\n            \"un chemin (ses flags actifs, ses salles et sorties...).\"']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def option_value_str(an_option):\\n    \"\"\"return an instance of Option\\'s value as a string.\\n\\n    The option instance doesn\\'t actually have to be from the Option class. All\\n    it requires is that the passed option instance has a ``value`` attribute.\\n    \"\"\"\\n    if an_option.value is None:\\n        return \\'\\'\\n    try:\\n        converter = to_string_converters[type(an_option.value)]\\n        s = converter(an_option.value)\\n    except KeyError:\\n        if not isinstance(an_option.value, basestring):\\n            s = unicode(an_option.value)\\n        else:\\n            s = an_option.value\\n    if an_option.from_string_converter in converters_requiring_quotes:\\n        s = \"\\'\\'\\'%s\\'\\'\\'\" % s\\n    return s', 'def str_dict_keys(a_dict):\\n    \"\"\"return a modified dict where all the keys that are anything but str get\\n    converted to str.\\n    E.g.\\n\\n      >>> result = str_dict_keys({u\\'name\\': u\\'Peter\\', u\\'age\\': 99, 1: 2})\\n      >>> # can\\'t compare whole dicts in doctests\\n      >>> result[\\'name\\']\\n      u\\'Peter\\'\\n      >>> result[\\'age\\']\\n      99\\n      >>> result[1]\\n      2\\n\\n    The reason for this is that in Python <= 2.6.4 doing\\n    ``MyClass(**{u\\'name\\': u\\'Peter\\'})`` would raise a TypeError\\n\\n    Note that only unicode types are converted to str types.\\n    The reason for that is you might have a class that looks like this::\\n\\n        class Option(object):\\n            def __init__(self, foo=None, bar=None, **kwargs):\\n                ...\\n\\n    And it\\'s being used like this::\\n\\n        Option(**{u\\'foo\\':1, u\\'bar\\':2, 3:4})\\n\\n    Then you don\\'t want to change that {3:4} part which becomes part of\\n    `**kwargs` inside the __init__ method.\\n    Using integers as parameter keys is a silly example but the point is that\\n    due to the python 2.6.4 bug only unicode keys are converted to str.\\n    \"\"\"\\n    new_dict = {}\\n    for key in a_dict:\\n        if isinstance(key, unicode):\\n            new_dict[str(key)] = a_dict[key]\\n        else:\\n            new_dict[key] = a_dict[key]\\n    return new_dict', 'def io_converter(input_str):\\n    \"\"\" a conversion function for to select stdout, stderr or open a file for\\n    writing\"\"\"\\n    if type(input_str) is str:\\n        input_str_lower = input_str.lower()\\n        if input_str_lower == \\'stdout\\':\\n            return sys.stdout\\n        if input_str_lower == \\'stderr\\':\\n            return sys.stderr\\n        return open(input_str, \"w\")\\n    return input_str', 'def timedelta_converter(input_str):\\n    \"\"\"a conversion function for time deltas\"\"\"\\n    if isinstance(input_str, basestring):\\n        days, hours, minutes, seconds = 0, 0, 0, 0\\n        details = input_str.split(\\':\\')\\n        if len(details) >= 4:\\n            days = int(details[-4])\\n        if len(details) >= 3:\\n            hours = int(details[-3])\\n        if len(details) >= 2:\\n            minutes = int(details[-2])\\n        if len(details) >= 1:\\n            seconds = int(details[-1])\\n        return datetime.timedelta(days=days,\\n                                      hours=hours,\\n                                      minutes=minutes,\\n                                      seconds=seconds)\\n    raise ValueError(input_str)', 'def boolean_converter(input_str):\\n    \"\"\" a conversion function for boolean\\n    \"\"\"\\n    return input_str.lower() in (\"true\", \"t\", \"1\", \"y\", \"yes\")', 'def class_converter(input_str):\\n    \"\"\" a conversion that will import a module and class name\\n    \"\"\"\\n    if not input_str:\\n        return None\\n    if \\'.\\' not in input_str and input_str in _all_named_builtins:\\n        return eval(input_str)\\n    parts = [x.strip() for x in input_str.split(\\'.\\') if x.strip()]\\n    try:\\n        # first try as a complete module\\n        package = __import__(input_str)\\n    except ImportError:\\n        # it must be a class from a module\\n        if len(parts) == 1:\\n            # since it has only one part, it must be a class from __main__\\n            parts = (\\'__main__\\', input_str)\\n        package = __import__(\\'.\\'.join(parts[:-1]), globals(), locals(), [])\\n    obj = package\\n    for name in parts[1:]:\\n        obj = getattr(obj, name)\\n    return obj', 'def classes_in_namespaces_converter(template_for_namespace=\"cls%d\",\\n                                    name_of_class_option=\\'cls\\',\\n                                    instantiate_classes=False):\\n    \"\"\"take a comma delimited  list of class names, convert each class name\\n    into an actual class as an option within a numbered namespace.  This\\n    function creates a closure over a new function.  That new function,\\n    in turn creates a class derived from RequiredConfig.  The inner function,\\n    \\'class_list_converter\\', populates the InnerClassList with a Namespace for\\n    each of the classes in the class list.  In addition, it puts the each class\\n    itself into the subordinate Namespace.  The requirement discovery mechanism\\n    of configman then reads the InnerClassList\\'s requried config, pulling in\\n    the namespaces and associated classes within.\\n\\n    For example, if we have a class list like this: \"Alpha, Beta\", then this\\n    converter will add the following Namespaces and options to the\\n    configuration:\\n\\n        \"cls0\" - the subordinate Namespace for Alpha\\n        \"cls0.cls\" - the option containing the class Alpha itself\\n        \"cls1\" - the subordinate Namespace for Beta\\n        \"cls1.cls\" - the option containing the class Beta itself\\n\\n    Optionally, the \\'class_list_converter\\' inner function can embue the\\n    InnerClassList\\'s subordinate namespaces with aggregates that will\\n    instantiate classes from the class list.  This is a convenience to the\\n    programmer who would otherwise have to know ahead of time what the\\n    namespace names were so that the classes could be instantiated within the\\n    context of the correct namespace.  Remember the user could completely\\n    change the list of classes at run time, so prediction could be difficult.\\n\\n        \"cls0\" - the subordinate Namespace for Alpha\\n        \"cls0.cls\" - the option containing the class Alpha itself\\n        \"cls0.cls_instance\" - an instance of the class Alpha\\n        \"cls1\" - the subordinate Namespace for Beta\\n        \"cls1.cls\" - the option containing the class Beta itself\\n        \"cls1.cls_instance\" - an instance of the class Beta\\n\\n    parameters:\\n        template_for_namespace - a template for the names of the namespaces\\n                                 that will contain the classes and their\\n                                 associated required config options.  The\\n                                 namespaces will be numbered sequentially.  By\\n                                 default, they will be \"cls1\", \"cls2\", etc.\\n        class_option_name - the name to be used for the class option within\\n                            the nested namespace.  By default, it will choose:\\n                            \"cls1.cls\", \"cls2.cls\", etc.\\n        instantiate_classes - a boolean to determine if there should be an\\n                              aggregator added to each namespace that\\n                              instantiates each class.  If True, then each\\n                              Namespace will contain elements for the class, as\\n                              well as an aggregator that will instantiate the\\n                              class.\\n                              \"\"\"\\n\\n    #--------------------------------------------------------------------------\\n    def class_list_converter(class_list_str):\\n        \"\"\"This function becomes the actual converter used by configman to\\n        take a string and convert it into the nested sequence of Namespaces,\\n        one for each class in the list.  It does this by creating a proxy\\n        class stuffed with its own \\'required_config\\' that\\'s dynamically\\n        generated.\"\"\"\\n        if isinstance(class_list_str, basestring):\\n            class_list = [x.strip() for x in class_list_str.split(\\',\\')]\\n        else:\\n            raise TypeError(\\'must be derivative of a basestring\\')\\n\\n        #======================================================================\\n        class InnerClassList(RequiredConfig):\\n            \"\"\"This nested class is a proxy list for the classes.  It collects\\n            all the config requirements for the listed classes and places them\\n            each into their own Namespace.\\n            \"\"\"\\n            # we\\'re dynamically creating a class here.  The following block of\\n            # code is actually adding class level attributes to this new class\\n            required_config = Namespace()  # 1st requirement for configman\\n            subordinate_namespace_names = []  # to help the programmer know\\n                                              # what Namespaces we added\\n            namespace_template = template_for_namespace  # save the template\\n                                                         # for future reference\\n            class_option_name = name_of_class_option  # save the class\\'s option\\n                                                      # name for the future\\n            # for each class in the class list\\n            for namespace_index, a_class in enumerate(class_list):\\n                # figure out the Namespace name\\n                namespace_name = template_for_namespace % namespace_index\\n                subordinate_namespace_names.append(namespace_name)\\n                # create the new Namespace\\n                required_config[namespace_name] = Namespace()\\n                # add the option for the class itself\\n                required_config[namespace_name].add_option(\\n                  name_of_class_option,\\n                  #doc=a_class.__doc__  # not helpful if too verbose\\n                  default=a_class,\\n                  from_string_converter=class_converter\\n                )\\n                if instantiate_classes:\\n                    # add an aggregator to instantiate the class\\n                    required_config[namespace_name].add_aggregation(\\n                      \"%s_instance\" % name_of_class_option,\\n                      lambda c, lc, a: lc[name_of_class_option](lc))\\n\\n            @classmethod\\n            def to_str(cls):\\n                \"\"\"this method takes this inner class object and turns it back\\n                into the original string of classnames.  This is used\\n                primarily as for the output of the \\'help\\' option\"\"\"\\n                return \\', \\'.join(\\n                    py_obj_to_str(v[name_of_class_option].value)\\n                        for v in cls.get_required_config().values()\\n                        if isinstance(v, Namespace))\\n\\n        return InnerClassList  # result of class_list_converter\\n    return class_list_converter  # result of classes_in_namespaces_converter', 'def regex_converter(input_str):\\n    return re.compile(input_str)', 'def py_obj_to_str(a_thing):\\n    if a_thing is None:\\n        return \\'\\'\\n    if inspect.ismodule(a_thing):\\n        return a_thing.__name__\\n    if a_thing.__module__ == \\'__builtin__\\':\\n        return a_thing.__name__\\n    if a_thing.__module__ == \"__main__\":\\n        return a_thing.__name__\\n    if hasattr(a_thing, \\'to_str\\'):\\n        return a_thing.to_str()\\n    return \"%s.%s\" % (a_thing.__module__, a_thing.__name__)', \"def list_to_str(a_list):\\n    return ', '.join(to_string_converters[type(x)](x) for x in a_list)\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def inv(s):\\n  if s[0] == '-':\\n    return s[1:]\\n  elif s[0] == '+':\\n    return '-' + s[1:]\\n  else: # plain number\\n    return '-' + s\"]}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self):\\n        script_helper.Script.__init__(self)', 'def add_subparser(self, subparsers):\\n        parser = self._setup_subparser(subparsers)\\n        self._add_arguments(parser)', 'def main():\\n    console, args = script_helper.init_arguments(Console)\\n    return(console.run(args))']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def post_config_hook(self):\\n        body = (\\n            \"[\\\\?color=orange&show <\"\\n            \"[\\\\?color=lightblue&show º]\"\\n            \"[\\\\?color=darkorange&show ,]))\"\\n            \"[\\\\?color=darkorange&show ))>%s]]\"\\n        )\\n        wanda = [body % fin for fin in (\"<\", \">\", \"<\", \"3\")]\\n        self.wanda = [self.py3.safe_format(x) for x in wanda]\\n        self.wanda_length = len(self.wanda)\\n        self.index = 0\\n\\n        self.fortune_command = [\"fortune\", \"-as\"]\\n        self.fortune = self.py3.storage_get(\"fortune\") or None\\n        self.toggled = self.py3.storage_get(\"toggled\") or False\\n        self.motions = {\"motion\": \" \", \"nomotion\": \"\"}\\n\\n        # deal with {new,old} timeout between storage\\n        fortune_timeout = self.py3.storage_get(\"fortune_timeout\")\\n        timeout = None\\n        if self.fortune_timeout != fortune_timeout:\\n            timeout = time() + self.fortune_timeout\\n        self.time = (\\n            timeout or self.py3.storage_get(\"time\") or (time() + self.fortune_timeout)\\n        )', 'def _set_motion(self):\\n        for k in self.motions:\\n            self.motions[k] = \"\" if self.motions[k] else \" \"', 'def wanda_the_fish(self):\\n        self._set_fortune()\\n        self._set_motion()\\n        self._set_wanda()\\n\\n        return {\\n            \"cached_until\": self.py3.time_in(self.cache_timeout),\\n            \"full_text\": self.py3.safe_format(\\n                self.format,\\n                {\\n                    \"fortune\": self.fortune,\\n                    \"motion\": self.motions[\"motion\"],\\n                    \"nomotion\": self.motions[\"nomotion\"],\\n                    \"wanda\": self.wanda[self.index],\\n                },\\n            ),\\n        }', 'def on_click(self, event):\\n        if not self.fortune_command:\\n            return\\n        self._set_fortune(not self.toggled)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, op_map):\\n        processed = {}\\n        for pattern, f in op_map.iteritems():\\n            s = self._build_pattern_groups(pattern.lower())\\n            processed[re.compile(s)] = f\\n\\n        self.operations = processed', \"def _build_pattern_groups(self, pattern):\\n        s = pattern.replace('?', '.')\\n        for id in ['x', 'y', 'z']:\\n            m = re.search('%s+' % id, s)\\n            if m:\\n                s = s[:m.start()] + ('(.{%s})' % (m.end() - m.start())) + s[m.end():] \\n        return '^' + s + '$'\", 'def fill_v0_vx(context, x):\\n    for i in range(x+1):\\n        context.v[i] = context.memory.get_byte(context.index_reg + i)\\n    context.pc += 2', 'def set_bcd_vx(context, x):\\n    val = int(context.v[x])\\n    context.memory.write_byte(context.index_reg, val / 100)\\n    context.memory.write_byte(context.index_reg + 1, val % 100 / 10)\\n    context.memory.write_byte(context.index_reg + 2, val % 100 % 10)\\n    context.pc += 2', 'def set_i_font(context, x):\\n    context.index_reg = context.memory.get_font_address(context.v[x])\\n    context.pc += 2', 'def add_reg_ind(context, x):\\n    context.index_reg += context.v[x]\\n    context.pc += 2', 'def set_delay_timer(context, x):\\n    context.delay_timer = context.v[x]\\n    context.pc += 2', 'def set_vx_key_pressed(context, x):\\n    context.v[x] = context.keypad.wait_for_keypress()\\n    context.pc += 2', 'def skip_key_vx(context, x, result=True):\\n    if context.keypad.is_keypressed(context.v[x]) == result:\\n        context.pc += 2\\n    context.pc += 2', 'def draw_sprite(context, x, y, n):\\n    sprite = []\\n    for cb in range(n):\\n        sprite.append(context.memory.get_byte(context.index_reg + cb))\\n    collision = context.screen.draw(context.v[x], context.v[y], sprite)\\n    context.v[15] = collision\\n    context.pc += 2', 'def jump_nnn_v0(context, nnn):\\n    context.pc = context.v[0] + nnn', 'def jump_noteq(context, x, y):\\n    if context.v[x] != context.v[y]:\\n        context.pc += 2\\n    context.pc += 2', 'def shift_vy_left(context, x, y):\\n    context.v[15] = context.v[15] >> 7 # First value\\n    context.v[x] = (context.v[y] << 1) % 255\\n    context.pc += 2', \"def sub_vx_vy_vf(context, x, y):\\n    logging.info('Setting V[X] = V[X] - V[Y], V[F] = 1 if V[Y] > V[X]')\\n    context.v[15] = 1 if context.v[y] > context.v[x] else 0\\n    context.v[x] = context.v[x] - context.v[y]\\n    context.pc += 2\", \"def add_vx_vy(context, x, y):\\n    logging.info('Setting V[X] = V[X] + V[Y]')\\n    val = context.v[x] + context.v[y]\\n    context.v[15] = 1 if val > 255 else 0\\n    context.v[x] = val % 256\\n    context.pc += 2\", \"def set_vx_or_vy(context, x, y):\\n    logging.info('Setting V[X] = V[X] | V[Y]')\\n    context.v[x] = context.v[x] | context.v[y]\\n    context.pc += 2\", \"def set_vx_xor_vy(context, x, y):\\n    logging.info('Setting V[X] = V[X] ^ V[Y]')\\n    context.v[x] = context.v[x] ^ context.v[y]\\n    context.pc += 2\", \"def set_vx_vy(context, x, y):\\n    logging.info('Setting V[X] = V[Y]')\\n    context.v[x] = context.v[y]\\n    context.pc += 2\", \"def add_reg(context, x, nnn):\\n    logging.info('Adding NNN to V[X]')\\n    context.v[x] = (context.v[x] + nnn) % 256\\n    context.pc += 2\", \"def pop_stack(context):\\n    logging.info('Returning from a subroutine')\\n    context.pc = context.stack.pop()\", \"def clear(context):\\n    logging.info('Clearing screen')\\n    context.screen.clear()\\n    context.pc += 2\", \"def call(context, address):\\n    logging.info('Calling subroutine at 0x%2x address' % address)\\n    context.pc += 2\\n    context.stack.append(context.pc)\\n    context.pc = address\", \"def skip_eq_reg(context, x, y):\\n    logging.info('Skip if V[X] === V[Y]')\\n    if context.v[x] == context.v[y]:\\n        context.pc += 2\\n    context.pc += 2\"]}, {'features': [], 'snippets': ['def getHalfYearEndDates(timestamps):\\r\\n        newTS=[]\\r\\n        tempYear=timestamps[0].year\\r\\n        flag=1', 'def findEvents(symbols, startday,endday, marketSymbol,verbose=False):']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def uniform_random_stream(transact_type, min_delay, max_delay):\\n    \"\"\"Return a new stream of transacts with random delays distributed uniformly.\"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomUniformStream \\' + str(min_delay) + \\' \\' + str(max_delay)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y', 'def triangular_random_stream(transact_type, min_delay, median_delay, max_delay):\\n    \"\"\"Return a new stream of transacts with random delays having the triangular distribution.\"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomTriangularStream \\' + str(min_delay) + \\' \\' +  str(median_delay) + \\' \\' + str(max_delay)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y', 'def lognormal_random_stream(transact_type, normal_mean_delay, normal_delay_deviation):\\n    \"\"\"Return a new stream of transacts with random delays having the lognormal distribution.\\n\\n       The numerical parameters are related to the normal distribution that\\n       this distribution is derived from.\\n    \"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomLogNormalStream \\' + str(normal_mean_delay) + \\' \\' + str(normal_delay_deviation)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y', 'def erlang_random_stream(transact_type, scale, shape):\\n    \"\"\"Return a new stream of transacts with random delays having the Erlang distribution with the specified scale (a reciprocal of the rate) and shape parameters.\"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomErlangStream \\' + str(scale) + \\' \\' + str(shape)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y', 'def binomial_random_stream(transact_type, probability, trials):\\n    \"\"\"Return a new stream of transacts with random delays having the binomial distribution with the specified probability and trials.\"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomBinomialStream \\' + str(probability) + \\' \\' + str(trials)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y', 'def beta_random_stream(transact_type, alpha, beta):\\n    \"\"\"Return a new stream of transacts with random delays having the Beta distribution by the specified shape parameters (alpha and beta).\"\"\"\\n    expect_transact_type(transact_type)\\n    model = transact_type.get_model()\\n    code = \\'return $ mapStream (\\\\\\\\a -> \\' + transact_type.coerce_arrival(\\'a\\') + \\') $ \\'\\n    code += \\'randomBetaStream \\' + str(alpha) + \\' \\' + str(beta)\\n    y = StreamPort(model, transact_type.get_data_type())\\n    y.bind_to_input()\\n    y.write(code)\\n    return y']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def diff2eigenvectors(dx, dy, dz):\\n    \"\"\" numerical derivatives 2 eigenvectors\\n    \"\"\"\\n    u = np.array([dx, dy, dz])\\n    u = u / np.linalg.norm(u)\\n    R = vec2vec_rotmat(basis[:, 0], u)\\n    eig0 = u\\n    eig1 = np.dot(R, basis[:, 1])\\n    eig2 = np.dot(R, basis[:, 2])\\n    eigs = np.zeros((3, 3))\\n    eigs[:, 0] = eig0\\n    eigs[:, 1] = eig1\\n    eigs[:, 2] = eig2\\n    return eigs, R', 'def test_sticks_and_ball():\\n    d = 0.0015\\n    S, sticks = sticks_and_ball(gtab, d=d, S0=1, angles=[(0, 0), ],\\n                                fractions=[100], snr=None)\\n    assert_array_equal(sticks, [[0, 0, 1]])\\n    S_st = SingleTensor(gtab, 1, evals=[d, 0, 0], evecs=[[0, 0, 0],\\n                                                         [0, 0, 0],\\n                                                         [1, 0, 0]])\\n    assert_array_almost_equal(S, S_st)', \"def test_multi_tensor():\\n    sphere = get_sphere('symmetric724')\\n    vertices = sphere.vertices\\n    mevals = np.array(([0.0015, 0.0003, 0.0003],\\n                       [0.0015, 0.0003, 0.0003]))\\n    e0 = np.array([np.sqrt(2) / 2., np.sqrt(2) / 2., 0])\\n    e1 = np.array([0, np.sqrt(2) / 2., np.sqrt(2) / 2.])\\n    mevecs = [all_tensor_evecs(e0), all_tensor_evecs(e1)]\\n    # odf = multi_tensor_odf(vertices, [0.5, 0.5], mevals, mevecs)\\n    # assert_(odf.shape == (len(vertices),))\\n    # assert_(np.all(odf <= 1) & np.all(odf >= 0))\\n\\n    fimg, fbvals, fbvecs = get_data('small_101D')\\n    bvals, bvecs = read_bvals_bvecs(fbvals, fbvecs)\\n    gtab = gradient_table(bvals, bvecs)\\n\\n    s1 = single_tensor(gtab, 100, mevals[0], mevecs[0], snr=None)\\n    s2 = single_tensor(gtab, 100, mevals[1], mevecs[1], snr=None)\\n\\n    Ssingle = 0.5*s1 + 0.5*s2\\n\\n    S, sticks = MultiTensor(gtab, mevals, S0=100, angles=[(90, 45), (45, 90)],\\n                            fractions=[50, 50], snr=None)\\n\\n    assert_array_almost_equal(S, Ssingle)\", 'def test_all_tensor_evecs():\\n    e0 = np.array([1/np.sqrt(2), 1/np.sqrt(2), 0])\\n\\n    desired = np.array([[1/np.sqrt(2), 1/np.sqrt(2), 0],\\n                        [-1/np.sqrt(2), 1/np.sqrt(2), 0],\\n                        [0, 0, 1]]).T\\n\\n    assert_array_almost_equal(all_tensor_evecs(e0), desired)', 'def test_DKI_simulations_aligned_fibers():\\n    \"\"\"\\n    Testing DKI simulations when aligning the same fiber to different axis.\\n\\n    If biological parameters don\\'t change, kt[0] of a fiber aligned to axis x\\n    has to be equal to kt[1] of a fiber aligned to the axis y and equal to\\n    kt[2] of a fiber aligned to axis z. The same is applicable for dt\\n    \"\"\"\\n    # Defining parameters based on Neto Henriques et al., 2015. NeuroImage 111\\n    mevals = np.array([[0.00099, 0, 0],               # Intra-cellular\\n                       [0.00226, 0.00087, 0.00087]])  # Extra-cellular\\n    frac = [49, 51]  # Compartment volume fraction\\n    # axis x\\n    angles = [(90, 0), (90, 0)]\\n    signal_fx, dt_fx, kt_fx = multi_tensor_dki(gtab_2s, mevals, angles=angles,\\n                                               fractions=frac)\\n    # axis y\\n    angles = [(90, 90), (90, 90)]\\n    signal_fy, dt_fy, kt_fy = multi_tensor_dki(gtab_2s, mevals, angles=angles,\\n                                               fractions=frac)\\n    # axis z\\n    angles = [(0, 0), (0, 0)]\\n    signal_fz, dt_fz, kt_fz = multi_tensor_dki(gtab_2s, mevals, angles=angles,\\n                                               fractions=frac)\\n\\n    assert_array_equal([kt_fx[0], kt_fx[1], kt_fx[2]],\\n                       [kt_fy[1], kt_fy[0], kt_fy[2]])\\n    assert_array_equal([kt_fx[0], kt_fx[1], kt_fx[2]],\\n                       [kt_fz[2], kt_fz[0], kt_fz[1]])\\n\\n    assert_array_equal([dt_fx[0], dt_fx[2], dt_fx[5]],\\n                       [dt_fy[2], dt_fy[0], dt_fy[5]])\\n    assert_array_equal([dt_fx[0], dt_fx[2], dt_fx[5]],\\n                       [dt_fz[5], dt_fz[0], dt_fz[2]])\\n\\n    # testing S signal along axis x, y and z\\n    bvals = np.array([0, 0, 0, 1000, 1000, 1000, 2000, 2000, 2000])\\n    bvecs = np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1],\\n                        [1, 0, 0], [0, 1, 0], [0, 0, 1],\\n                        [1, 0, 0], [0, 1, 0], [0, 0, 1]])\\n    gtab_axis = gradient_table(bvals, bvecs)\\n    # axis x\\n    S_fx = DKI_signal(gtab_axis, dt_fx, kt_fx, S0=100)\\n    assert_array_almost_equal(S_fx[0:3], [100, 100, 100])  # test S f0r b=0\\n    # axis y\\n    S_fy = DKI_signal(gtab_axis, dt_fy, kt_fy, S0=100)\\n    assert_array_almost_equal(S_fy[0:3], [100, 100, 100])  # test S f0r b=0\\n    # axis z\\n    S_fz = DKI_signal(gtab_axis, dt_fz, kt_fz, S0=100)\\n    assert_array_almost_equal(S_fz[0:3], [100, 100, 100])  # test S f0r b=0\\n\\n    # test S for b = 1000\\n    assert_array_almost_equal([S_fx[3], S_fx[4], S_fx[5]],\\n                              [S_fy[4], S_fy[3], S_fy[5]])\\n    assert_array_almost_equal([S_fx[3], S_fx[4], S_fx[5]],\\n                              [S_fz[5], S_fz[3], S_fz[4]])\\n    # test S for b = 2000\\n    assert_array_almost_equal([S_fx[6], S_fx[7], S_fx[8]],\\n                              [S_fy[7], S_fy[6], S_fy[8]])\\n    assert_array_almost_equal([S_fx[6], S_fx[7], S_fx[8]],\\n                              [S_fz[8], S_fz[6], S_fz[7]])']}, {'features': [], 'snippets': ['def tuneRF(X_train, Y_train, outputPath):\\n    global results, names, params, bestResults', 'def tuneET(X_train, Y_train, outputPath):\\n    global results, names, params, bestResults', 'def tuneSVM(X_train, Y_train, outputPath):\\n    global results, names, params, bestResults', \"def drawTunedAlgorithmsComparison(results, names, outputPath):\\n    global imageidx\\n    print '\\\\n === Tuned Algorithms Comparison ===\\\\n'\\n\\n    #print bestResults\\n    for x in bestResults:\\n        print x\", 'def set_createImages(value):\\n    global createImages\\n    createImages = value', \"def run(inputFilePath, outputPath, createImagesFlag, dropColumns):\\n    global start\\n\\n    print '####################################################################'\\n    print '############### Running Exploratory Data Analysis #4 ###############'\\n    print '####################################################################'\\n    print ''\"]}, {'features': [], 'snippets': [\"def algorithm(self):\\n        return 'Simultaneous-FA'\", \"def algorithm(self):\\n        return 'Simultaneous-FC'\", 'def _fit(self, lk_fitting, max_iters=20, project=True):\\n        # Initial error > eps\\n        error = self.eps + 1\\n        image = lk_fitting.image\\n        lk_fitting.weights = []\\n        n_iters = 0\\n\\n        # Number of shape weights\\n        n_params = self.transform.n_parameters\\n\\n        # Initial appearance weights\\n        if project:\\n            # Obtained weights by projection\\n            IWxp = image.warp_to(self.template.mask, self.transform,\\n                                 interpolator=self.interpolator)\\n            weights = self.appearance_model.project(IWxp)\\n            # Reset template\\n            self.template = self.appearance_model.instance(weights)\\n        else:\\n            # Set all weights to 0 (yielding the mean)\\n            weights = np.zeros(self.appearance_model.n_active_components)\\n\\n        lk_fitting.weights.append(weights)\\n\\n        # Compute appearance model Jacobian wrt weights\\n        appearance_jacobian = self.appearance_model._jacobian.T\\n\\n        # Forward Additive Algorithm\\n        while n_iters < max_iters and error > self.eps:\\n            # Compute warped image with current weights\\n            IWxp = image.warp_to(self.template.mask, self.transform,\\n                                 interpolator=self.interpolator)\\n\\n            # Compute steepest descent images, VI_dW_dp\\n            J = self.residual.steepest_descent_images(IWxp, self._dW_dp)\\n\\n            # Concatenate VI_dW_dp with appearance model Jacobian\\n            self._J = np.hstack((J, appearance_jacobian))\\n\\n            # Compute Hessian and inverse\\n            self._H = self.residual.calculate_hessian(self._J)\\n\\n            # Compute steepest descent parameter updates\\n            sd_delta_p = self.residual.steepest_descent_update(\\n                self._J, self.template, IWxp)\\n\\n            # Compute gradient descent parameter updates\\n            delta_p = np.real(self._calculate_delta_p(sd_delta_p))\\n\\n            # Update warp weights\\n            self.transform.compose_after_from_vector_inplace(delta_p[:n_params])\\n            lk_fitting.parameters.append(self.transform.as_vector())\\n\\n            # Update appearance weights\\n            weights -= delta_p[n_params:]\\n            self.template = self.appearance_model.instance(weights)\\n            lk_fitting.weights.append(weights)\\n\\n            # Test convergence\\n            error = np.abs(norm(delta_p))\\n            n_iters += 1\\n\\n        lk_fitting.fitted = True\\n        return lk_fitting', \"def algorithm(self):\\n        return 'Simultaneous-IA'\"]}, {'features': [], 'snippets': [\"def __init__(self):\\n    self._network_controller = None\\n    self._tracing_controller = None\\n    self._has_battor = False\\n    self._os_name = 'FakeOS'\\n    self._device_type_name = 'abc'\\n    self._is_svelte = False\\n    self._is_aosp = True\", 'def is_host_platform(self):\\n    raise NotImplementedError', 'def network_controller(self):\\n    if self._network_controller is None:\\n      self._network_controller = _FakeNetworkController()\\n    return  self._network_controller', 'def tracing_controller(self):\\n    if self._tracing_controller is None:\\n      self._tracing_controller = _FakeTracingController()\\n    return  self._tracing_controller', 'def CanMonitorThermalThrottling(self):\\n    return False', 'def HasBeenThermallyThrottled(self):\\n    return False', 'def SetOSName(self, name):\\n    self._os_name = name', 'def GetOSVersionName(self):\\n    raise NotImplementedError', 'def StopAllLocalServers(self):\\n    pass', 'def HasBattOrConnected(self):\\n    return self._has_battor', 'def SetDeviceTypeName(self, name):\\n    self._device_type_name = name', 'def SetIsSvelte(self, b):\\n    assert isinstance(b, bool)\\n    self._is_svelte = b', 'def SetIsAosp(self, b):\\n    assert isinstance(b, bool)\\n    self._is_aosp = b', 'def __init__(self):\\n    super(FakeLinuxPlatform, self).__init__()\\n    self.screenshot_png_data = None\\n    self.http_server_directories = []\\n    self.http_server = FakeHTTPServer()', 'def is_host_platform(self):\\n    return True', \"def GetArchName(self):\\n    return 'x86_64'\", \"def GetOSVersionName(self):\\n    return 'trusty'\", 'def CanTakeScreenshot(self):\\n    return bool(self.screenshot_png_data)', 'def SetHTTPServerDirectories(self, paths):\\n    self.http_server_directories.append(paths)', \"def UrlOf(self, url):\\n    del url  # unused\\n    return 'file:///foo'\", \"def __init__(self, execute_on_startup=None,\\n               execute_after_browser_creation=None):\\n    self._returned_browser = _FakeBrowser(FakeLinuxPlatform())\\n    self.browser_type = 'linux'\\n    self.supports_tab_control = False\\n    self.is_remote = False\\n    self.execute_on_startup = execute_on_startup\\n    self.execute_after_browser_creation = execute_after_browser_creation\", 'def returned_browser(self):\\n    \"\"\"The browser object that will be returned through later API calls.\"\"\"\\n    return self._returned_browser', 'def platform(self):\\n    \"\"\"The platform object from the returned browser.\\n\\n    To change this or set it up, change the returned browser\\'s\\n    platform.\\n    \"\"\"\\n    return self.returned_browser.platform', 'def SetCredentialsPath(self, _):\\n    pass', 'def __init__(self, test, finder_options, story_set):\\n    super(FakeSharedPageState, self).__init__(test, finder_options, story_set)', 'def ConfigurePossibleBrowser(self, possible_browser):\\n    \"\"\"Override this to configure the PossibleBrowser.\\n\\n    Can make changes to the browser\\'s configuration here via e.g.:\\n       possible_browser.returned_browser.returned_system_info = ...\\n    \"\"\"\\n    pass', \"def __init__(self, model_name='', gpu_dict=None, command_line=''):\\n    if gpu_dict == None:\\n      gpu_dict = fake_gpu_info.FAKE_GPU_INFO\\n    super(FakeSystemInfo, self).__init__(model_name, gpu_dict, command_line)\", 'def __init__(self, execute_on_startup=None,\\n               execute_after_browser_creation=None, *args, **kwargs):\\n    browser_options.BrowserFinderOptions.__init__(self, *args, **kwargs)\\n    self.fake_possible_browser = \\\\\\n      FakePossibleBrowser(\\n        execute_on_startup=execute_on_startup,\\n        execute_after_browser_creation=execute_after_browser_creation)', \"def __init__(self, platform):\\n    self._tabs = _FakeTabList(self)\\n    # Fake the creation of the first tab.\\n    self._tabs.New()\\n    self._returned_system_info = FakeSystemInfo()\\n    self._platform = platform\\n    self._browser_type = 'release'\\n    self._is_crashed = False\", 'def platform(self):\\n    return self._platform', 'def platform(self, incoming):\\n    \"\"\"Allows overriding of the fake browser\\'s platform object.\"\"\"\\n    assert isinstance(incoming, FakePlatform)\\n    self._platform = incoming', 'def returned_system_info(self):\\n    \"\"\"The object which will be returned from calls to GetSystemInfo.\"\"\"\\n    return self._returned_system_info', 'def returned_system_info(self, incoming):\\n    \"\"\"Allows overriding of the returned SystemInfo object.\\n\\n    Incoming argument must be an instance of FakeSystemInfo.\"\"\"\\n    assert isinstance(incoming, FakeSystemInfo)\\n    self._returned_system_info = incoming', 'def browser_type(self):\\n    \"\"\"The browser_type this browser claims to be (\\'debug\\', \\'release\\', etc.)\"\"\"\\n    return self._browser_type', 'def browser_type(self, incoming):\\n    \"\"\"Allows setting of the browser_type.\"\"\"\\n    self._browser_type = incoming', 'def credentials(self):\\n    return _FakeCredentials()', 'def supports_system_info(self):\\n    return True', 'def supports_tab_control(self):\\n    return True', 'def tabs(self):\\n    return self._tabs', 'def WarnIfMissingCredentials(self, _):\\n    pass', 'def __init__(self):\\n    self._is_tracing = False', 'def StopTracing(self):\\n    self._is_tracing = False', 'def is_tracing_running(self):\\n    return self._is_tracing', 'def IsChromeTracingSupported(self):\\n    return True', 'def __init__(self):\\n    self.wpr_mode = None\\n    self.extra_wpr_args = None\\n    self.is_initialized = False\\n    self.is_open = False\\n    self.use_live_traffic = None', 'def UpdateTrafficSettings(self, round_trip_latency_ms=None,\\n      download_bandwidth_kbps=None, upload_bandwidth_kbps=None):\\n    pass', 'def Close(self):\\n    self.wpr_mode = None\\n    self.extra_wpr_args = None\\n    self.is_initialized = False\\n    self.is_open = False', 'def StopReplay(self):\\n    self.is_initialized = False', 'def __init__(self, browser, tab_id):\\n    self._browser = browser\\n    self._tab_id = str(tab_id)\\n    self._collect_garbage_count = 0\\n    self.test_png = None', 'def collect_garbage_count(self):\\n    return self._collect_garbage_count', 'def id(self):\\n    return self._tab_id', 'def browser(self):\\n    return self._browser', \"def Navigate(self, url, script_to_evaluate_on_commit=None,\\n               timeout=0):\\n    del script_to_evaluate_on_commit, timeout # unused\\n    if url == 'chrome://crash':\\n      self.browser._is_crashed = True\\n      raise Exception\", 'def WaitForFrameToBeDisplayed(self, timeout=0):\\n    pass', 'def CloseConnections(self):\\n    pass', 'def Close(self):\\n    pass', 'def screenshot_supported(self):\\n    return self.test_png is not None', 'def __init__(self, browser):\\n    self._tabs = []\\n    self._browser = browser', 'def __iter__(self):\\n    return self._tabs.__iter__()', 'def __getitem__(self, index):\\n    if self._tabs[index].browser._is_crashed:\\n      raise Exception\\n    else:\\n      return self._tabs[index]', 'def __init__(self, mock_timer):\\n    self._mock_timer = mock_timer\\n    self._notifications = []\\n    self._response_handlers = {}\\n    self._pending_callbacks = {}\\n    self._handler = None', \"def AddEvent(self, method, params, time):\\n    if self._notifications:\\n      assert self._notifications[-1][1] < time, (\\n          'Current response is scheduled earlier than previous response.')\\n    response = {'method': method, 'params': params}\\n    self._notifications.append((response, time, self._NOTIFICATION_EVENT))\", 'def AddResponseHandler(self, method, handler):\\n    self._response_handlers[method] = handler', \"def AsyncRequest(self, request, callback):\\n    self._pending_callbacks.setdefault(request['method'], []).append(callback)\", 'def Connect(self, _):\\n    pass', 'def __init__(self, module=None):\\n    self._elapsed_time = 0\\n    self._module = module\\n    self._actual_time = None\\n    if module:\\n      assert isinstance(module, ModuleType)\\n      self._actual_time = module.time\\n      self._module.time = self', 'def time(self):\\n    return self._elapsed_time', 'def __del__(self):\\n    self.Restore()']}, {'features': [], 'snippets': ['def fuzzylogic(features, cfg, require=\"all\"):\\n    \"\"\"\\n\\n        FIXME: Think about, should I return 0, or have an assert, and at qc.py\\n          all qc tests are applied with a try, and in case it fails it flag\\n          0s.\\n\\n    \"\"\"\\n    require = cfg.get(\"require\", require)\\n\\n    if (require == \"all\") and not np.all([f in features for f in cfg[\"features\"]]):\\n        module_logger.warning(\\n            \"Not all features (%s) required by fuzzy logic are available\".format(\\n                cfg[\"features\"].keys()\\n            )\\n        )\\n        raise KeyError\\n\\n    uncertainty = fuzzy_uncertainty(\\n        data=features, features=cfg[\"features\"], output=cfg[\"output\"], require=require\\n    )\\n\\n    return uncertainty', 'def set_features(self):\\n        self.features = {}\\n        for v in [f for f in self.cfg[\"features\"] if f not in self.features]:\\n            if v == \"woa_bias\":\\n                woa_comparison = woa_normbias(self.data, self.varname, self.attrs)\\n                self.features[v] = woa_comparison[\"woa_bias\"]\\n            elif v == \"woa_normbias\":\\n                woa_comparison = woa_normbias(self.data, self.varname, self.attrs)\\n                self.features[v] = woa_comparison[\"woa_normbias\"]\\n            elif v == \"spike\":\\n                self.features[v] = spike(self.data[self.varname])\\n            elif v == \"gradient\":\\n                self.features[v] = gradient(self.data[self.varname])\\n\\n        self.features[\"fuzzylogic\"] = fuzzylogic(self.features, self.cfg)']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def loaded(cls):\\n        return 'cudf' in sys.modules\", 'def applies(cls, obj):\\n        if not cls.loaded():\\n            return False\\n        import cudf\\n        return isinstance(obj, (cudf.DataFrame, cudf.Series))', 'def init(cls, eltype, data, kdims, vdims):\\n        import cudf\\n        import pandas as pd\\n\\n        element_params = eltype.param.objects()\\n        kdim_param = element_params[\\'kdims\\']\\n        vdim_param = element_params[\\'vdims\\']\\n\\n        if isinstance(data, (cudf.Series, pd.Series)):\\n            data = data.to_frame()\\n\\n        if not isinstance(data, cudf.DataFrame):\\n            data, _, _ = PandasInterface.init(eltype, data, kdims, vdims)\\n            data = cudf.from_pandas(data)\\n\\n        columns = list(data.columns)\\n        ncols = len(columns)\\n        index_names = [data.index.name]\\n        if index_names == [None]:\\n            index_names = [\\'index\\']\\n        if eltype._auto_indexable_1d and ncols == 1 and kdims is None:\\n            kdims = list(index_names)\\n\\n        if isinstance(kdim_param.bounds[1], int):\\n            ndim = min([kdim_param.bounds[1], len(kdim_param.default)])\\n        else:\\n            ndim = None\\n        nvdim = vdim_param.bounds[1] if isinstance(vdim_param.bounds[1], int) else None\\n        if kdims and vdims is None:\\n            vdims = [c for c in columns if c not in kdims]\\n        elif vdims and kdims is None:\\n            kdims = [c for c in columns if c not in vdims][:ndim]\\n        elif kdims is None:\\n            kdims = list(columns[:ndim])\\n            if vdims is None:\\n                vdims = [d for d in columns[ndim:((ndim+nvdim) if nvdim else None)]\\n                         if d not in kdims]\\n        elif kdims == [] and vdims is None:\\n            vdims = list(columns[:nvdim if nvdim else None])\\n\\n        # Handle reset of index if kdims reference index by name\\n        for kd in kdims:\\n            kd = dimension_name(kd)\\n            if kd in columns:\\n                continue\\n            if any(kd == (\\'index\\' if name is None else name)\\n                   for name in index_names):\\n                data = data.reset_index()\\n                break\\n        if any(isinstance(d, (np.int64, int)) for d in kdims+vdims):\\n            raise DataError(\"cudf DataFrame column names used as dimensions \"\\n                            \"must be strings not integers.\", cls)\\n\\n        if kdims:\\n            kdim = dimension_name(kdims[0])\\n            if eltype._auto_indexable_1d and ncols == 1 and kdim not in columns:\\n                data = data.copy()\\n                data.insert(0, kdim, np.arange(len(data)))\\n\\n        for d in kdims+vdims:\\n            d = dimension_name(d)\\n            if len([c for c in columns if c == d]) > 1:\\n                raise DataError(\\'Dimensions may not reference duplicated DataFrame \\'\\n                                \\'columns (found duplicate %r columns). If you want to plot \\'\\n                                \\'a column against itself simply declare two dimensions \\'\\n                                \\'with the same name. \\'% d, cls)\\n        return data, {\\'kdims\\':kdims, \\'vdims\\':vdims}, {}', \"def range(cls, dataset, dimension):\\n        dimension = dataset.get_dimension(dimension, strict=True)\\n        column = dataset.data[dimension.name]\\n        if dimension.nodata is not None:\\n            column = cls.replace_value(column, dimension.nodata)\\n        if column.dtype.kind == 'O':\\n            return np.NaN, np.NaN\\n        else:\\n            return finite_range(column, column.min(), column.max())\", 'def values(cls, dataset, dim, expanded=True, flat=True, compute=True,\\n               keep_index=False):\\n        dim = dataset.get_dimension(dim, strict=True)\\n        data = dataset.data[dim.name]\\n        if not expanded:\\n            data = data.unique()\\n            return data.values_host if compute else data.values\\n        elif keep_index:\\n            return data\\n        elif compute:\\n            return data.values_host\\n        try:\\n            return data.values\\n        except Exception:\\n            return data.values_host', \"def groupby(cls, dataset, dimensions, container_type, group_type, **kwargs):\\n        # Get dimensions information\\n        dimensions = [dataset.get_dimension(d).name for d in dimensions]\\n        kdims = [kdim for kdim in dataset.kdims if kdim not in dimensions]\\n\\n        # Update the kwargs appropriately for Element group types\\n        group_kwargs = {}\\n        group_type = dict if group_type == 'raw' else group_type\\n        if issubclass(group_type, Element):\\n            group_kwargs.update(util.get_param_values(dataset))\\n            group_kwargs['kdims'] = kdims\\n        group_kwargs.update(kwargs)\\n\\n        # Propagate dataset\\n        group_kwargs['dataset'] = dataset.dataset\\n\\n        # Find all the keys along supplied dimensions\\n        keys = product(*(dataset.data[dimensions[0]].unique().values_host for d in dimensions))\\n\\n        # Iterate over the unique entries applying selection masks\\n        grouped_data = []\\n        for unique_key in util.unique_iterator(keys):\\n            group_data = dataset.select(**dict(zip(dimensions, unique_key)))\\n            if not len(group_data):\\n                continue\\n            group_data = group_type(group_data, **group_kwargs)\\n            grouped_data.append((unique_key, group_data))\\n\\n        if issubclass(container_type, NdMapping):\\n            with item_check(False), sorted_context(False):\\n                kdims = [dataset.get_dimension(d) for d in dimensions]\\n                return container_type(grouped_data, kdims=kdims)\\n        else:\\n            return container_type(grouped_data)\", 'def select_mask(cls, dataset, selection):\\n        \"\"\"\\n        Given a Dataset object and a dictionary with dimension keys and\\n        selection keys (i.e. tuple ranges, slices, sets, lists, or literals)\\n        return a boolean mask over the rows in the Dataset object that\\n        have been selected.\\n        \"\"\"\\n        mask = None\\n        for dim, sel in selection.items():\\n            if isinstance(sel, tuple):\\n                sel = slice(*sel)\\n            arr = cls.values(dataset, dim, keep_index=True)\\n            if util.isdatetime(arr) and util.pd:\\n                try:\\n                    sel = util.parse_datetime_selection(sel)\\n                except:\\n                    pass\\n\\n            new_masks = []\\n            if isinstance(sel, slice):\\n                with warnings.catch_warnings():\\n                    warnings.filterwarnings(\\'ignore\\', r\\'invalid value encountered\\')\\n                    if sel.start is not None:\\n                        new_masks.append(sel.start <= arr)\\n                    if sel.stop is not None:\\n                        new_masks.append(arr < sel.stop)\\n                if not new_masks:\\n                    continue\\n                new_mask = new_masks[0]\\n                for imask in new_masks[1:]:\\n                    new_mask &= imask\\n            elif isinstance(sel, (set, list)):\\n                for v in sel:\\n                    new_masks.append(arr==v)\\n                if not new_masks:\\n                    continue\\n                new_mask = new_masks[0]\\n                for imask in new_masks[1:]:\\n                    new_mask |= imask\\n            elif callable(sel):\\n                new_mask = sel(arr)\\n            else:\\n                new_mask = arr == sel\\n\\n            if mask is None:\\n                mask = new_mask\\n            else:\\n                mask &= new_mask\\n        return mask', 'def select(cls, dataset, selection_mask=None, **selection):\\n        df = dataset.data\\n        if selection_mask is None:\\n            selection_mask = cls.select_mask(dataset, selection)\\n\\n        indexed = cls.indexed(dataset, selection)\\n        if selection_mask is not None:\\n            df = df.loc[selection_mask]\\n        if indexed and len(df) == 1 and len(dataset.vdims) == 1:\\n            return df[dataset.vdims[0].name].iloc[0]\\n        return df', 'def concat_fn(cls, dataframes, **kwargs):\\n        import cudf\\n        return cudf.concat(dataframes, **kwargs)', 'def add_dimension(cls, dataset, dimension, dim_pos, values, vdim):\\n        data = dataset.data.copy()\\n        if dimension.name not in data:\\n            data[dimension.name] = values\\n        return data', \"def aggregate(cls, dataset, dimensions, function, **kwargs):\\n        data = dataset.data\\n        cols = [d.name for d in dataset.kdims if d in dimensions]\\n        vdims = dataset.dimensions('value', label='name')\\n        reindexed = data[cols+vdims]\\n        agg = function.__name__\\n        if len(dimensions):\\n            agg_map = {'amin': 'min', 'amax': 'max'}\\n            agg = agg_map.get(agg, agg)\\n            grouped = reindexed.groupby(cols, sort=False)\\n            if not hasattr(grouped, agg):\\n                raise ValueError('%s aggregation is not supported on cudf DataFrame.' % agg)\\n            df = getattr(grouped, agg)().reset_index()\\n        else:\\n            agg_map = {'amin': 'min', 'amax': 'max', 'size': 'count'}\\n            agg = agg_map.get(agg, agg)\\n            if not hasattr(reindexed, agg):\\n                raise ValueError('%s aggregation is not supported on cudf DataFrame.' % agg)\\n            agg = getattr(reindexed, agg)()\\n            data = dict(((col, [v]) for col, v in zip(agg.index.values_host, agg.to_array())))\\n            df = util.pd.DataFrame(data, columns=list(agg.index.values_host))\\n\\n        dropped = []\\n        for vd in vdims:\\n            if vd not in df.columns:\\n                dropped.append(vd)\\n        return df, dropped\", 'def iloc(cls, dataset, index):\\n        import cudf\\n\\n        rows, cols = index\\n        scalar = False\\n        columns = list(dataset.data.columns)\\n        if isinstance(cols, slice):\\n            cols = [d.name for d in dataset.dimensions()][cols]\\n        elif np.isscalar(cols):\\n            scalar = np.isscalar(rows)\\n            cols = [dataset.get_dimension(cols).name]\\n        else:\\n            cols = [dataset.get_dimension(d).name for d in index[1]]\\n        col_index = [columns.index(c) for c in cols]\\n        if np.isscalar(rows):\\n            rows = [rows]\\n\\n        if scalar:\\n            return dataset.data[cols[0]].iloc[rows[0]]\\n        result = dataset.data.iloc[rows, col_index]\\n\\n        # cuDF does not handle single rows and cols indexing correctly\\n        # as of cudf=0.10.0 so we have to convert Series back to DataFrame\\n        if isinstance(result, cudf.Series):\\n            if len(cols) == 1:\\n                result = result.to_frame(cols[0])\\n            else:\\n                result = result.to_frame().T\\n        return result', 'def sort(cls, dataset, by=[], reverse=False):\\n        cols = [dataset.get_dimension(d, strict=True).name for d in by]\\n        return dataset.data.sort_values(by=cols, ascending=not reverse)', 'def dframe(cls, dataset, dimensions):\\n        if dimensions:\\n            return dataset.data[dimensions].to_pandas()\\n        else:\\n            return dataset.data.to_pandas()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, **kwargs):\\n        \"\"\" Initializes a VMResync instance\\n\\n            Notes:\\n                You can specify all parameters while calling this methods.\\n                A special argument named `data` will enable you to load the\\n                object from a Python dictionary\\n\\n            Examples:\\n                >>> vmresync = NUVMResync(id=u\\'xxxx-xxx-xxx-xxx\\', name=u\\'VMResync\\')\\n                >>> vmresync = NUVMResync(data=my_dict)\\n        \"\"\"\\n\\n        super(NUVMResync, self).__init__()\\n\\n        # Read/Write Attributes', 'def last_request_timestamp(self):\\n        \"\"\" Get last_request_timestamp value.\\n\\n            Notes:\\n                Time of the last timestamp received', 'def last_request_timestamp(self, value):\\n        \"\"\" Set last_request_timestamp value.\\n\\n            Notes:\\n                Time of the last timestamp received', 'def last_time_resync_initiated(self):\\n        \"\"\" Get last_time_resync_initiated value.\\n\\n            Notes:\\n                Time that the resync was initiated', 'def last_time_resync_initiated(self, value):\\n        \"\"\" Set last_time_resync_initiated value.\\n\\n            Notes:\\n                Time that the resync was initiated', 'def last_updated_by(self):\\n        \"\"\" Get last_updated_by value.\\n\\n            Notes:\\n                ID of the user who last updated the object.', 'def last_updated_by(self, value):\\n        \"\"\" Set last_updated_by value.\\n\\n            Notes:\\n                ID of the user who last updated the object.', 'def last_updated_date(self):\\n        \"\"\" Get last_updated_date value.\\n\\n            Notes:\\n                Time stamp when this object was last updated.', 'def last_updated_date(self, value):\\n        \"\"\" Set last_updated_date value.\\n\\n            Notes:\\n                Time stamp when this object was last updated.', 'def embedded_metadata(self):\\n        \"\"\" Get embedded_metadata value.\\n\\n            Notes:\\n                Metadata objects associated with this entity. This will contain a list of Metadata objects if the API request is made using the special flag to enable the embedded Metadata feature. Only a maximum of Metadata objects is returned based on the value set in the system configuration.', 'def embedded_metadata(self, value):\\n        \"\"\" Set embedded_metadata value.\\n\\n            Notes:\\n                Metadata objects associated with this entity. This will contain a list of Metadata objects if the API request is made using the special flag to enable the embedded Metadata feature. Only a maximum of Metadata objects is returned based on the value set in the system configuration.', 'def entity_scope(self):\\n        \"\"\" Get entity_scope value.\\n\\n            Notes:\\n                Specify if scope of entity is Data center or Enterprise level', 'def entity_scope(self, value):\\n        \"\"\" Set entity_scope value.\\n\\n            Notes:\\n                Specify if scope of entity is Data center or Enterprise level', 'def creation_date(self):\\n        \"\"\" Get creation_date value.\\n\\n            Notes:\\n                Time stamp when this object was created.', 'def creation_date(self, value):\\n        \"\"\" Set creation_date value.\\n\\n            Notes:\\n                Time stamp when this object was created.', 'def status(self):\\n        \"\"\" Get status value.\\n\\n            Notes:\\n                Status of the resync', 'def status(self, value):\\n        \"\"\" Set status value.\\n\\n            Notes:\\n                Status of the resync', 'def owner(self):\\n        \"\"\" Get owner value.\\n\\n            Notes:\\n                Identifies the user that has created this object.', 'def owner(self, value):\\n        \"\"\" Set owner value.\\n\\n            Notes:\\n                Identifies the user that has created this object.', 'def external_id(self):\\n        \"\"\" Get external_id value.\\n\\n            Notes:\\n                External object ID. Used for integration with third party systems', 'def external_id(self, value):\\n        \"\"\" Set external_id value.\\n\\n            Notes:\\n                External object ID. Used for integration with third party systems']}, {'features': [], 'snippets': ['def test_package_pass(self):\\n        \"Tests the test_package function with simple data\"\\n\\n        self.setup_err()\\n\\n        name = \"tests/resources/submain/install_rdf.xpi\"\\n        with open(name) as pack:\\n            result = submain.test_package(self.err, pack, name)\\n\\n        self.assert_silent()\\n        eq_(result, \"success\")', 'def test_package_corrupt(self):\\n        \"Tests the test_package function fails with a non-zip\"\\n\\n        self.setup_err()\\n\\n        name = \"tests/resources/junk.xpi\"\\n        with open(name) as pack:\\n            result = submain.test_package(self.err, pack, name)\\n\\n        self.assert_failed()']}, {'features': [], 'snippets': [\"def get_type_name (type_id):\\n    l = get_type (type_id)\\n    if not l:\\n        return None\\n    return l['name']\", 'def get_types ():\\n    q = \"SELECT id, type \"\\\\\\n        \"FROM asset_types;\" % locals()\\n\\n    query = Query(q)\\n\\n    if not len(query):\\n        return None\\n\\n    ret = []\\n    for x in query:\\n        d={\\'id\\':          query[x][\\'id\\'],\\n           \\'name\\':        query[x][\\'type\\']}\\n        ret.append(d)\\n    return ret']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': [\"def __init__(self):\\n        super(MockTestResultsFetcher, self).__init__()\\n        self._canned_results = {}\\n        self._canned_retry_summary_json = {}\\n        self._webdriver_results = {}\\n        self.fetched_builds = []\\n        self.fetched_webdriver_builds = []\\n        self._layout_test_step_name = 'blink_web_tests (with patch)'\", 'def fetch_results(self, build, full=False, step_name=None):\\n        step_name = step_name or self.get_layout_test_step_name(build)\\n        step = BuilderStep(build=build, step_name=step_name)\\n        self.fetched_builds.append(step)\\n        return self._canned_results.get(step)', 'def fetch_results_from_resultdb(self, host, builds, predicate):\\n        rv = []\\n        for build in builds:\\n            results = self._canned_results.get(build.build_id)\\n            if results:\\n                rv.extend(results)\\n        return rv', 'def fetch_webdriver_test_results(self, build, m):\\n        self.fetched_webdriver_builds.append((build, m))\\n        return self._webdriver_results.get((build, m))', 'def fetch_retry_summary_json(self, build):\\n        return self._canned_retry_summary_json.get(build)']}, {'features': [], 'snippets': ['def index():\\n    \"\"\"Produces a list of the feedback obtained for a given venue,\\n    or for all venues.\"\"\"\\n    venue_id = request.args(0)\\n    if venue_id == \\'all\\':\\n        q = (db.submission.user == get_user_email())\\n    else:\\n        q = ((db.submission.user == get_user_email()) \\n            & (db.submission.venue_id == venue_id))\\n    db.submission.id.represent = lambda x, r: A(T(\\'View\\'), _class=\\'btn\\', _href=URL(\\'submission\\', \\'view_own_submission\\', args=[\\'v\\', r.id]))\\n    db.submission.id.label = T(\\'Submission\\')\\n    db.submission.id.readable = True\\n    db.submission.venue_id.readable = True\\n    grid = SQLFORM.grid(q,\\n        fields=[db.submission.id, db.submission.venue_id,\\n                db.submission.date_created, db.submission.date_updated, ],\\n        csv=False, details=False, create=False, editable=False, deletable=False,\\n        args=request.args[:1],\\n        maxtextlength=24,        \\n        )\\n    return dict(grid=grid)', 'def view_feedback():\\n    \"\"\"Shows detailed feedback for a user in a venue.\\n    This controller accepts various types of arguments: \\n    * \\'s\\', submission_id\\n    * \\'u\\', venue_id, username\\n    * \\'v\\', venue_id  (in which case, shows own submission to that venue)\\n    \"\"\"\\n    if len(request.args) == 0:\\n        redirect(URL(\\'default\\', \\'index\\'))\\n    if request.args(0) == \\'s\\':\\n        # submission_id\\n        n_args = 2\\n        subm = db.submission(request.args(1)) or redirect(URL(\\'default\\', \\'index\\'))\\n        c = db.venue(subm.venue_id) or redirect(URL(\\'default\\', \\'index\\'))\\n        username = subm.user\\n    elif request.args(0) == \\'v\\':\\n        # venue_id\\n        n_args = 2\\n        c = db.venue(request.args(1)) or redirect(URL(\\'default\\', \\'index\\'))\\n        username = get_user_email()\\n        subm = db((db.submission.user == username) & (db.submission.venue_id == c.id)).select().first()\\n    else:\\n        # venue_id, username\\n        n_args = 3\\n        c = db.venue(request.args(1)) or redirect(URL(\\'default\\', \\'index\\'))\\n        username = request.args(2) or redirect(URL(\\'default\\', \\'index\\'))\\n        subm = db((db.submission.user == username) & (db.submission.venue_id == c.id)).select().first()\\n\\n    # Checks permissions.\\n    props = db(db.user_properties.user == get_user_email()).select().first()\\n    if props == None:\\n        session.flash = T(\\'Not authorized.\\')\\n        redirect(URL(\\'default\\', \\'index\\'))\\n    is_author = (username == get_user_email())\\n    can_view_feedback = access.can_view_feedback(c, props) or is_author\\n    if (not can_view_feedback):\\n        session.flash = T(\\'Not authorized.\\')\\n        redirect(URL(\\'default\\', \\'index\\'))\\n    if not (access.can_view_feedback(c, props) or datetime.utcnow() > c.rate_close_date):\\n        session.flash = T(\\'The ratings are not yet available.\\')\\n        redirect(URL(\\'feedback\\', \\'index\\', args=[\\'all\\']))\\n\\n    # Produces the link to edit the feedback.\\n    edit_feedback_link = None\\n    if subm is not None and access.can_observe(c, props):\\n        edit_feedback_link = A(T(\\'Edit feedback\\'), _class=\\'btn\\', \\n                               _href=URL(\\'submission\\', \\'edit_feedback\\', args=[subm.id]))\\n    # Produces the download link.\\n    download_link = None\\n    if subm is not None and c.allow_file_upload and subm.content is not None:\\n        if is_author:\\n            download_link = A(T(\\'Download\\'), _class=\\'btn\\', \\n                          _href=URL(\\'submission\\', \\'download_author\\', args=[subm.id, subm.content]))\\n        else:\\n            download_link = A(T(\\'Download\\'), _class=\\'btn\\', \\n                          _href=URL(\\'submission\\', \\'download_manager\\', args=[subm.id, subm.content]))\\n    venue_link = A(c.name, _href=URL(\\'venues\\', \\'view_venue\\', args=[c.id]))\\n\\n    # Submission link.\\n    subm_link = None\\n    if subm is not None and c.allow_link_submission:\\n        subm_link = A(subm.link, _href=subm.link)\\n    # Submission content and feedback.\\n    subm_comment = None\\n    subm_feedback = None\\n    if subm is not None:\\n        raw_subm_comment = keystore_read(subm.comment)\\n        if raw_subm_comment is not None and len(raw_subm_comment) > 0:\\n            subm_comment = MARKMIN(keystore_read(subm.comment))\\n        raw_feedback = keystore_read(subm.feedback)\\n        if raw_feedback is not None and len(raw_feedback) > 0:\\n            subm_feedback = MARKMIN(raw_feedback)\\n    # Display settings.\\n    db.submission.percentile.readable = True\\n    db.submission.comment.readable = True\\n    db.submission.feedback.readable = True\\n    if access.can_observe(c, props):\\n        db.submission.quality.readable = True\\n        db.submission.error.readable = True\\n    # Reads the grade information.\\n    submission_grade = submission_percentile = None\\n    review_grade = review_percentile = user_reputation = None\\n    final_grade = final_percentile = None\\n    assigned_grade = None\\n    if c.grades_released:\\n        grade_info = db((db.grades.user == username) & (db.grades.venue_id == c.id)).select().first()\\n        if grade_info is not None:\\n            submission_grade = represent_quality(grade_info.submission_grade, None)\\n            submission_percentile = represent_percentage(grade_info.submission_percentile, None)\\n            review_grade = represent_quality_10(grade_info.accuracy, None)\\n            review_percentile = represent_percentage(grade_info.accuracy_percentile, None)\\n            user_reputation = represent_01_as_percentage(grade_info.reputation, None)\\n            final_grade = represent_quality(grade_info.grade, None)\\n            final_percentile = represent_percentage(grade_info.percentile, None)\\n            assigned_grade = represent_quality(grade_info.assigned_grade, None)\\n    # Makes a grid of comments.\\n    db.task.submission_name.readable = False\\n    db.task.assigned_date.readable = False\\n    db.task.completed_date.readable = False\\n    db.task.rejected.readable = True\\n    db.task.helpfulness.readable = db.task.helpfulness.writable = True\\n    # Prevent editing the comments; the only thing editable should be the \"is bogus\" field.\\n    db.task.comments.writable = False\\n    db.task.comments.readable = True\\n    ranking_link = None\\n    if access.can_observe(c, props):\\n        db.task.user.readable = True\\n        db.task.completed_date.readable = True\\n        links = [\\n            dict(header=T(\\'Review details\\'), body= lambda r:\\n                 A(T(\\'View\\'), _class=\\'btn\\', _href=URL(\\'ranking\\', \\'view_comparison\\', args=[r.id]))),\\n            ]\\n        details = False\\n        if subm is not None:\\n            ranking_link = A(T(\\'details\\'), _href=URL(\\'ranking\\', \\'view_comparisons_given_submission\\', args=[subm.id]))\\n        reviews_link = A(T(\\'details\\'), _href=URL(\\'ranking\\', \\'view_comparisons_given_user\\', args=[username, c.id]))\\n        db.task.user.represent = lambda v, r: A(v, _href=URL(\\'ranking\\', \\'view_comparisons_given_user\\',\\n                                                                   args=[v, c.id], user_signature=True))\\n    else:\\n        user_reputation = None\\n        links = [\\n            dict(header=T(\\'Review feedback\\'), body = lambda r:\\n                 A(T(\\'Give feedback\\'), _class=\\'btn\\', \\n                   _href=URL(\\'feedback\\', \\'reply_to_review\\', args=[r.id], user_signature=True))),\\n            ]\\n        details = False\\n        ranking_link = None\\n        reviews_link = None\\n    if subm is not None:\\n        q = ((db.task.submission_id == subm.id) & (db.task.is_completed == True))\\n        # q = (db.task.submission_id == subm.id)\\n    else:\\n        q = (db.task.id == -1)\\n    grid = SQLFORM.grid(q,\\n        fields=[db.task.id, db.task.user, db.task.rejected, db.task.comments, db.task.helpfulness, ],\\n        details = details,\\n        csv=False, create=False, editable=False, deletable=False, searchable=False,\\n        links=links,\\n        args=request.args[:n_args],\\n        maxtextlength=24,\\n        )\\n    return dict(subm=subm, download_link=download_link, subm_link=subm_link, username=username,\\n                subm_comment=subm_comment, subm_feedback=subm_feedback,\\n                edit_feedback_link=edit_feedback_link,\\n                is_admin=is_user_admin(), \\n                submission_grade=submission_grade, submission_percentile=submission_percentile, \\n                review_grade=review_grade, review_percentile=review_percentile,\\n                user_reputation=user_reputation,\\n                final_grade=final_grade, final_percentile=final_percentile, \\n                assigned_grade=assigned_grade,\\n                venue_link=venue_link, grid=grid, ranking_link=ranking_link,\\n                reviews_link=reviews_link)', \"def reply_to_review():\\n    t = db.task(request.args(0)) or redirect(URL('default', 'index'))\\n    db.task.submission_name.readable = False\\n    db.task.assigned_date.readable = False\\n    db.task.completed_date.readable = False\\n    db.task.comments.readable = False\\n    db.task.helpfulness.readable = db.task.helpfulness.writable = True\\n    db.task.feedback.readable = db.task.feedback.writable = True\\n    form = SQLFORM(db.task, record=t)\\n    form.vars.feedback = keystore_read(t.feedback)\\n    if form.process(onvalidation=validate_review_feedback(t)).accepted:\\n        session.flash = T('Updated.')\\n        redirect(URL('feedback', 'view_feedback', args=['s', t.submission_id]))\\n    link_to_submission = A(T('View submission'), _href=URL('submission', 'view_own_submission', args=['v', t.submission_id]))\\n    review_comments = MARKMIN(keystore_read(t.comments))\\n    return dict(form=form, link_to_submission=link_to_submission, review_comments=review_comments)\", 'def validate_review_feedback(t):\\n    def f(form):\\n        if not form.errors:\\n            feedback_id = keystore_update(t.feedback, form.vars.feedback)\\n            form.vars.feedback = feedback_id\\n    return f']}, {'features': [], 'snippets': ['def __init__(self, cmd_env):\\n        self.cmd = cmd_env', 'def __call__(self, *args, **kwargs):\\n        raise NotImplementedError()', 'def __call__(self, bundles=None, output=None, directory=None, no_cache=None,\\n              manifest=None, production=None):\\n        \"\"\"Build assets.\\n\\n        ``bundles``\\n            A list of bundle names. If given, only this list of bundles\\n            should be built.\\n\\n        ``output``\\n            List of (bundle, filename) 2-tuples. If given, only these\\n            bundles will be built, using the custom output filenames.\\n            Cannot be used with ``bundles``.\\n\\n        ``directory``\\n            Custom output directory to use for the bundles. The original\\n            basenames defined in the bundle ``output`` attribute will be\\n            used. If the ``output`` of the bundles are pointing to different\\n            directories, they will be offset by their common prefix.\\n            Cannot be used with ``output``.\\n\\n        ``no_cache``\\n            If set, a cache (if one is configured) will not be used.\\n\\n        ``manifest``\\n            If set, the given manifest instance will be used, instead of\\n            any that might have been configured in the Environment. The value\\n            passed will be resolved through ``get_manifest()``. If this fails,\\n            a file-based manifest will be used using the given value as the\\n            filename.\\n\\n        ``production``\\n            If set to ``True``, then :attr:`Environment.debug`` will forcibly\\n            be disabled (set to ``False``) during the build.\\n        \"\"\"\\n\\n        # Validate arguments\\n        if bundles and output:\\n            raise CommandError(\\n                \\'When specifying explicit output filenames you must \\'\\n                \\'do so for all bundles you want to build.\\')\\n        if directory and output:\\n            raise CommandError(\\'A custom output directory cannot be \\'\\n                               \\'combined with explicit output filenames \\'\\n                               \\'for individual bundles.\\')\\n\\n        if production:\\n            # TODO: Reset again (refactor commands to be classes)\\n            self.environment.debug = False\\n\\n        # TODO: Oh how nice it would be to use the future options stack.\\n        if manifest is not None:\\n            try:\\n                manifest = get_manifest(manifest, env=self.environment)\\n            except ValueError:\\n                manifest = get_manifest(\\n                    # abspath() is important, or this will be considered\\n                    # relative to Environment.directory.\\n                    \"file:%s\" % os.path.abspath(manifest),\\n                    env=self.environment)\\n            self.environment.manifest = manifest\\n\\n        # Use output as a dict.\\n        if output:\\n            output = dict(output)\\n\\n        # Validate bundle names\\n        bundle_names = bundles if bundles else (output.keys() if output else [])\\n        for name in bundle_names:\\n            if not name in self.environment:\\n                raise CommandError(\\n                    \\'I do not know a bundle name named \"%s\".\\' % name)\\n\\n        # Make a list of bundles to build, and the filename to write to.\\n        if bundle_names:\\n            # TODO: It\\'s not ok to use an internal property here.\\n            bundles = [(n,b) for n, b in self.environment._named_bundles.items()\\n                             if n in bundle_names]\\n        else:\\n            # Includes unnamed bundles as well.\\n            bundles = [(None, b) for b in self.environment]\\n\\n        # Determine common prefix for use with ``directory`` option.\\n        if directory:\\n            prefix = os.path.commonprefix(\\n                [os.path.normpath(b.resolve_output())\\n                 for _, b in bundles if b.output])\\n            # dirname() gives the right value for a single file.\\n            prefix = os.path.dirname(prefix)\\n\\n        to_build = []\\n        for name, bundle in bundles:\\n            # TODO: We really should support this. This error here\\n            # is just in place of a less understandable error that would\\n            # otherwise occur.\\n            if bundle.is_container and directory:\\n                raise CommandError(\\n                    \\'A custom output directory cannot currently be \\'\\n                    \\'used with container bundles.\\')\\n\\n            # Determine which filename to use, if not the default.\\n            overwrite_filename = None\\n            if output:\\n                overwrite_filename = output[name]\\n            elif directory:\\n                offset = os.path.normpath(\\n                    bundle.resolve_output())[len(prefix)+1:]\\n                overwrite_filename = os.path.join(directory, offset)\\n            to_build.append((bundle, overwrite_filename, name,))\\n\\n        # Build.\\n        built = []\\n        for bundle, overwrite_filename, name in to_build:\\n            if name:\\n                # A name is not necessary available of the bundle was\\n                # registered without one.\\n                self.log.info(\"Building bundle: %s (to %s)\" % (\\n                    name, overwrite_filename or bundle.output))\\n            else:\\n                self.log.info(\"Building bundle: %s\" % bundle.output)\\n\\n            try:\\n                if not overwrite_filename:\\n                    with bundle.bind(self.environment):\\n                        bundle.build(force=True, disable_cache=no_cache)\\n                else:\\n                    # TODO: Rethink how we deal with container bundles here.\\n                    # As it currently stands, we write all child bundles\\n                    # to the target output, merged (which is also why we\\n                    # create and force writing to a StringIO instead of just\\n                    # using the ``Hunk`` objects that build() would return\\n                    # anyway.\\n                    output = StringIO()\\n                    with bundle.bind(self.environment):\\n                        bundle.build(force=True, output=output,\\n                            disable_cache=no_cache)\\n                    if directory:\\n                        # Only auto-create directories in this mode.\\n                        output_dir = os.path.dirname(overwrite_filename)\\n                        if not os.path.exists(output_dir):\\n                            os.makedirs(output_dir)\\n                    MemoryHunk(output.getvalue()).save(overwrite_filename)\\n                built.append(bundle)\\n            except BuildError as e:\\n                self.log.error(\"Failed, error was: %s\" % e)\\n        if len(built):\\n            self.event_handlers[\\'post_build\\']()\\n        if len(built) != len(to_build):\\n            return 2', 'def __call__(self, loop=None):\\n        \"\"\"Watch assets for changes.\\n\\n        ``loop``\\n            A callback, taking no arguments, to be called once every loop\\n            iteration. Can be useful to integrate the command with other code.\\n            If not specified, the loop wil call ``time.sleep()``.\\n        \"\"\"\\n        # TODO: This should probably also restart when the code changes.\\n        mtimes = {}\\n\\n        try:\\n            # Before starting to watch for changes, also recognize changes\\n            # made while we did not run, and apply those immediately.\\n            for bundle in self.environment:\\n                print(\\'Bringing up to date: %s\\' % bundle.output)\\n                bundle.build(force=False)\\n\\n            self.log.info(\"Watching %d bundles for changes...\" %\\n                          len(self.environment))\\n\\n            while True:\\n                changed_bundles = self.check_for_changes(mtimes)\\n\\n                built = []\\n                for bundle in changed_bundles:\\n                    print(\"Building bundle: %s ...\" % bundle.output, end=\\' \\')\\n                    sys.stdout.flush()\\n                    try:\\n                        bundle.build(force=True)\\n                        built.append(bundle)\\n                    except BuildError as e:\\n                        print(\"\")\\n                        print(\"Failed: %s\" % e)\\n                    else:\\n                        print(\"done\")\\n\\n                if len(built):\\n                    self.event_handlers[\\'post_build\\']()\\n\\n                do_end = loop() if loop else time.sleep(0.1)\\n                if do_end:\\n                    break\\n        except KeyboardInterrupt:\\n            pass', 'def yield_files_to_watch(self):\\n        for bundle in self.environment:\\n            for filename in get_all_bundle_files(bundle):\\n                yield filename, set([bundle])', 'def __call__(self):\\n        \"\"\"Delete generated assets.\\n        \"\"\"\\n        self.log.info(\\'Cleaning generated assets...\\')\\n        for bundle in self.environment:\\n            if not bundle.output:\\n                continue\\n            file_path = bundle.resolve_output(self.environment)\\n            if os.path.exists(file_path):\\n                os.unlink(file_path)\\n                self.log.info(\"Deleted asset: %s\" % bundle.output)\\n        if isinstance(self.environment.cache, FilesystemCache):\\n            shutil.rmtree(self.environment.cache.directory)', 'def __call__(self):\\n        \"\"\"Check to see if assets need to be rebuilt.\\n\\n        A non-zero exit status will be returned if any of the input files are\\n        newer (based on mtime) than their output file. This is intended to be\\n        used in pre-commit hooks.\\n        \"\"\"\\n        needsupdate = False\\n        updater = self.environment.updater\\n        if not updater:\\n            self.log.debug(\\'no updater configured, using TimestampUpdater\\')\\n            updater = TimestampUpdater()\\n        for bundle in self.environment:\\n            self.log.info(\\'Checking asset: %s\\', bundle.output)\\n            if updater.needs_rebuild(bundle, self.environment):\\n                self.log.info(\\'  needs update\\')\\n                needsupdate = True\\n        if needsupdate:\\n            sys.exit(-1)', \"def __init__(self, env, log, post_build=None, commands=None):\\n        self.environment = env\\n        self.log = log\\n        self.event_handlers = dict(post_build=lambda: True)\\n        if callable(post_build):\\n            self.event_handlers['post_build'] = post_build\\n\\n        # Instantiate each command\\n        command_def = self.DefaultCommands.copy()\\n        command_def.update(commands or {})\\n        self.commands = {}\\n        for name, construct in command_def.items():\\n            if not construct:\\n                continue\\n            if not isinstance(construct, (list, tuple)):\\n                construct = [construct, (), {}]\\n            self.commands[name] = construct[0](\\n                self, *construct[1], **construct[2])\", 'def invoke(self, command, args):\\n        \"\"\"Invoke ``command``, or throw a CommandError.\\n\\n        This is essentially a simple validation mechanism. Feel free\\n        to call the individual command methods manually.\\n        \"\"\"\\n        try:\\n            function = self.commands[command]\\n        except KeyError as e:\\n            raise CommandError(\\'unknown command: %s\\' % e)\\n        else:\\n            return function(**args)', 'def __init__(self, cmd_env, argparse_ns):\\n            WatchCommand.__init__(self, cmd_env)\\n            self.ns = argparse_ns', 'def reload_config(self):\\n            try:\\n                self.cmd.environment = YAMLLoader(self.ns.config).load_environment()\\n            except Exception as e:\\n                raise EnvironmentError(e)\\n            return True', 'def _construct_parser(self, prog=None, no_global_options=False):\\n        self.parser = parser = self.argparse.ArgumentParser(\\n            description=\"Manage assets.\",\\n            prog=prog)\\n\\n        if not no_global_options:\\n            # Start with the base arguments that are valid for any command.\\n            # XXX: Add those to the subparser?\\n            parser.add_argument(\"-v\", dest=\"verbose\", action=\"store_true\",\\n                help=\"be verbose\")\\n            parser.add_argument(\"-q\", action=\"store_true\", dest=\"quiet\",\\n                help=\"be quiet\")\\n            if self.env is None:\\n                loadenv = parser.add_mutually_exclusive_group()\\n                loadenv.add_argument(\"-c\", \"--config\", dest=\"config\",\\n                    help=\"read environment from a YAML file\")\\n                loadenv.add_argument(\"-m\", \"--module\", dest=\"module\",\\n                    help=\"read environment from a Python module\")\\n\\n        # Add subparsers.\\n        subparsers = parser.add_subparsers(dest=\\'command\\')\\n        for command in CommandLineEnvironment.DefaultCommands.keys():\\n            command_parser = subparsers.add_parser(command)\\n            maker = getattr(self, \\'make_%s_parser\\' % command, False)\\n            if maker:\\n                maker(command_parser)', 'def make_build_parser(parser):\\n        parser.add_argument(\\n            \\'bundles\\', nargs=\\'*\\', metavar=\\'BUNDLE\\',\\n            help=\\'Optional bundle names to process. If none are \\'\\n                 \\'specified, then all known bundles will be built.\\')\\n        parser.add_argument(\\n            \\'--output\\', \\'-o\\', nargs=2, action=\\'append\\',\\n            metavar=(\\'BUNDLE\\', \\'FILE\\'),\\n            help=\\'Build the given bundle, and use a custom output \\'\\n                 \\'file. Can be given multiple times.\\')\\n        parser.add_argument(\\n            \\'--directory\\', \\'-d\\',\\n            help=\\'Write built files to this directory, using the \\'\\n                 \\'basename defined by the bundle. Will offset \\'\\n                 \\'the original bundle output paths on their common \\'\\n                 \\'prefix. Cannot be used with --output.\\')\\n        parser.add_argument(\\n            \\'--no-cache\\', action=\\'store_true\\',\\n            help=\\'Do not use a cache that might be configured.\\')\\n        parser.add_argument(\\n            \\'--manifest\\',\\n            help=\\'Write a manifest to the given file. Also supports \\'\\n                 \\'the id:arg format, if you want to use a different \\'\\n                 \\'manifest implementation.\\')\\n        parser.add_argument(\\n            \\'--production\\', action=\\'store_true\\',\\n            help=\\'Forcably turn off debug mode for the build. This \\'\\n                 \\'only has an effect if debug is set to \"merge\".\\')', 'def _setup_assets_env(self, ns, log):\\n        env = self.env\\n        if env is None:\\n            assert not (ns.module and ns.config)\\n            if ns.module:\\n                env = PythonLoader(ns.module).load_environment()\\n            if ns.config:\\n                env = YAMLLoader(ns.config).load_environment()\\n        return env', 'def _prepare_command_args(self, ns):\\n        # Prepare a dict of arguments cleaned of values that are not\\n        # command-specific, and which the command method would not accept.\\n        args = vars(ns).copy()\\n        for action in self.parser._actions:\\n            dest = action.dest\\n            if dest in args:\\n                del args[dest]\\n        return args', 'def run_with_argv(self, argv):\\n        try:\\n            ns = self.parser.parse_args(argv)\\n        except SystemExit as e:\\n            # We do not want the main() function to exit the program.\\n            # See run() instead.\\n            return e.args[0]\\n\\n        return self.run_with_ns(ns)', 'def main(argv, env=None):\\n    \"\"\"Execute the generic version of the command line interface.\\n\\n    You only need to work directly with ``GenericArgparseImplementation`` if\\n    you desire to customize things.\\n\\n    If no environment is given, additional arguments will be supported to allow\\n    the user to specify/construct the environment on the command line.\\n    \"\"\"\\n    return GenericArgparseImplementation(env).main(argv)']}, {'features': [], 'snippets': ['def __init__(self, contents=None):\\n    if contents is not None: self.MergeFromString(contents)', 'def set_requesting_app_id(self, x):\\n    self.has_requesting_app_id_ = 1\\n    self.requesting_app_id_ = x', 'def has_requesting_app_id(self): return self.has_requesting_app_id_', 'def set_requesting_project_id(self, x):\\n    self.has_requesting_project_id_ = 1\\n    self.requesting_project_id_ = x', 'def has_requesting_project_id(self): return self.has_requesting_project_id_', 'def set_requesting_version_id(self, x):\\n    self.has_requesting_version_id_ = 1\\n    self.requesting_version_id_ = x', 'def has_requesting_version_id(self): return self.has_requesting_version_id_', 'def set_api_settings(self, x):\\n    self.has_api_settings_ = 1\\n    self.api_settings_ = x', 'def has_api_settings(self): return self.has_api_settings_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_requesting_app_id_ != x.has_requesting_app_id_: return 0\\n    if self.has_requesting_app_id_ and self.requesting_app_id_ != x.requesting_app_id_: return 0\\n    if self.has_requesting_project_id_ != x.has_requesting_project_id_: return 0\\n    if self.has_requesting_project_id_ and self.requesting_project_id_ != x.requesting_project_id_: return 0\\n    if self.has_requesting_version_id_ != x.has_requesting_version_id_: return 0\\n    if self.has_requesting_version_id_ and self.requesting_version_id_ != x.requesting_version_id_: return 0\\n    if self.has_api_settings_ != x.has_api_settings_: return 0\\n    if self.has_api_settings_ and self.api_settings_ != x.api_settings_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_requesting_app_id_): n += 1 + self.lengthString(len(self.requesting_app_id_))\\n    if (self.has_requesting_project_id_): n += 1 + self.lengthString(len(self.requesting_project_id_))\\n    if (self.has_requesting_version_id_): n += 1 + self.lengthString(len(self.requesting_version_id_))\\n    if (self.has_api_settings_): n += 1 + self.lengthString(len(self.api_settings_))\\n    return n', 'def Clear(self):\\n    self.clear_requesting_app_id()\\n    self.clear_requesting_project_id()\\n    self.clear_requesting_version_id()\\n    self.clear_api_settings()', 'def OutputPartial(self, out):\\n    if (self.has_requesting_app_id_):\\n      out.putVarInt32(18)\\n      out.putPrefixedString(self.requesting_app_id_)\\n    if (self.has_api_settings_):\\n      out.putVarInt32(26)\\n      out.putPrefixedString(self.api_settings_)\\n    if (self.has_requesting_project_id_):\\n      out.putVarInt32(34)\\n      out.putPrefixedString(self.requesting_project_id_)\\n    if (self.has_requesting_version_id_):\\n      out.putVarInt32(42)\\n      out.putPrefixedString(self.requesting_version_id_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_requesting_app_id_: res+=prefix+(\"requesting_app_id: %s\\\\n\" % self.DebugFormatString(self.requesting_app_id_))\\n    if self.has_requesting_project_id_: res+=prefix+(\"requesting_project_id: %s\\\\n\" % self.DebugFormatString(self.requesting_project_id_))\\n    if self.has_requesting_version_id_: res+=prefix+(\"requesting_version_id: %s\\\\n\" % self.DebugFormatString(self.requesting_version_id_))\\n    if self.has_api_settings_: res+=prefix+(\"api_settings: %s\\\\n\" % self.DebugFormatString(self.api_settings_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def set_handle(self, x):\\n    self.has_handle_ = 1\\n    self.handle_ = x', 'def has_handle(self): return self.has_handle_', 'def set_app(self, x):\\n    self.has_app_ = 1\\n    self.app_ = x', 'def has_app(self): return self.has_app_', 'def set_mark_changes(self, x):\\n    self.has_mark_changes_ = 1\\n    self.mark_changes_ = x', 'def has_mark_changes(self): return self.has_mark_changes_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_handle_ != x.has_handle_: return 0\\n    if self.has_handle_ and self.handle_ != x.handle_: return 0\\n    if self.has_app_ != x.has_app_: return 0\\n    if self.has_app_ and self.app_ != x.app_: return 0\\n    if self.has_mark_changes_ != x.has_mark_changes_: return 0\\n    if self.has_mark_changes_ and self.mark_changes_ != x.mark_changes_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += self.lengthString(len(self.app_))\\n    if (self.has_mark_changes_): n += 2\\n    return n + 10', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_handle()\\n    self.clear_app()\\n    self.clear_mark_changes()', 'def OutputPartial(self, out):\\n    if (self.has_handle_):\\n      out.putVarInt32(9)\\n      out.put64(self.handle_)\\n    if (self.has_app_):\\n      out.putVarInt32(18)\\n      out.putPrefixedString(self.app_)\\n    if (self.has_mark_changes_):\\n      out.putVarInt32(24)\\n      out.putBoolean(self.mark_changes_)\\n    if (self.has_header_):\\n      out.putVarInt32(34)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_handle_: res+=prefix+(\"handle: %s\\\\n\" % self.DebugFormatFixed64(self.handle_))\\n    if self.has_app_: res+=prefix+(\"app: %s\\\\n\" % self.DebugFormatString(self.app_))\\n    if self.has_mark_changes_: res+=prefix+(\"mark_changes: %s\\\\n\" % self.DebugFormatBool(self.mark_changes_))\\n    return res', 'def Operator_Name(cls, x): return cls._Operator_NAMES.get(x, \"\")', 'def __init__(self, contents=None):\\n    self.property_ = []\\n    if contents is not None: self.MergeFromString(contents)', 'def set_op(self, x):\\n    self.has_op_ = 1\\n    self.op_ = x', 'def has_op(self): return self.has_op_', 'def property_list(self): return self.property_', 'def mutable_property(self, i):\\n    return self.property_[i]', 'def clear_property(self):\\n    self.property_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_op_ != x.has_op_: return 0\\n    if self.has_op_ and self.op_ != x.op_: return 0\\n    if len(self.property_) != len(x.property_): return 0\\n    for e1, e2 in zip(self.property_, x.property_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.lengthVarInt64(self.op_)\\n    n += 1 * len(self.property_)\\n    for i in xrange(len(self.property_)): n += self.lengthString(self.property_[i].ByteSize())\\n    return n + 1', 'def Clear(self):\\n    self.clear_op()\\n    self.clear_property()', 'def OutputPartial(self, out):\\n    if (self.has_op_):\\n      out.putVarInt32(48)\\n      out.putVarInt32(self.op_)\\n    for i in xrange(len(self.property_)):\\n      out.putVarInt32(114)\\n      out.putVarInt32(self.property_[i].ByteSizePartial())\\n      self.property_[i].OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_op_: res+=prefix+(\"op: %s\\\\n\" % self.DebugFormatInt32(self.op_))\\n    cnt=0\\n    for e in self.property_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"property%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    return res', 'def Direction_Name(cls, x): return cls._Direction_NAMES.get(x, \"\")', 'def __init__(self, contents=None):\\n    if contents is not None: self.MergeFromString(contents)', 'def set_property(self, x):\\n    self.has_property_ = 1\\n    self.property_ = x', 'def has_property(self): return self.has_property_', 'def set_direction(self, x):\\n    self.has_direction_ = 1\\n    self.direction_ = x', 'def has_direction(self): return self.has_direction_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_property_ != x.has_property_: return 0\\n    if self.has_property_ and self.property_ != x.property_: return 0\\n    if self.has_direction_ != x.has_direction_: return 0\\n    if self.has_direction_ and self.direction_ != x.direction_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.lengthString(len(self.property_))\\n    if (self.has_direction_): n += 1 + self.lengthVarInt64(self.direction_)\\n    return n + 1', 'def Clear(self):\\n    self.clear_property()\\n    self.clear_direction()', 'def OutputPartial(self, out):\\n    if (self.has_property_):\\n      out.putVarInt32(82)\\n      out.putPrefixedString(self.property_)\\n    if (self.has_direction_):\\n      out.putVarInt32(88)\\n      out.putVarInt32(self.direction_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_property_: res+=prefix+(\"property: %s\\\\n\" % self.DebugFormatString(self.property_))\\n    if self.has_direction_: res+=prefix+(\"direction: %s\\\\n\" % self.DebugFormatInt32(self.direction_))\\n    return res', 'def Hint_Name(cls, x): return cls._Hint_NAMES.get(x, \"\")', 'def __init__(self, contents=None):\\n    self.filter_ = []\\n    self.order_ = []\\n    self.composite_index_ = []\\n    self.property_name_ = []\\n    self.group_by_property_name_ = []\\n    self.safe_replica_name_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def set_app(self, x):\\n    self.has_app_ = 1\\n    self.app_ = x', 'def has_app(self): return self.has_app_', 'def set_name_space(self, x):\\n    self.has_name_space_ = 1\\n    self.name_space_ = x', 'def has_name_space(self): return self.has_name_space_', 'def set_kind(self, x):\\n    self.has_kind_ = 1\\n    self.kind_ = x', 'def has_kind(self): return self.has_kind_', 'def mutable_ancestor(self): self.has_ancestor_ = 1; return self.ancestor()', 'def has_ancestor(self): return self.has_ancestor_', 'def filter_list(self): return self.filter_', 'def mutable_filter(self, i):\\n    return self.filter_[i]', 'def clear_filter(self):\\n    self.filter_ = []', 'def set_search_query(self, x):\\n    self.has_search_query_ = 1\\n    self.search_query_ = x', 'def has_search_query(self): return self.has_search_query_', 'def order_list(self): return self.order_', 'def mutable_order(self, i):\\n    return self.order_[i]', 'def clear_order(self):\\n    self.order_ = []', 'def set_hint(self, x):\\n    self.has_hint_ = 1\\n    self.hint_ = x', 'def has_hint(self): return self.has_hint_', 'def set_count(self, x):\\n    self.has_count_ = 1\\n    self.count_ = x', 'def has_count(self): return self.has_count_', 'def set_offset(self, x):\\n    self.has_offset_ = 1\\n    self.offset_ = x', 'def has_offset(self): return self.has_offset_', 'def set_limit(self, x):\\n    self.has_limit_ = 1\\n    self.limit_ = x', 'def has_limit(self): return self.has_limit_', 'def mutable_compiled_cursor(self): self.has_compiled_cursor_ = 1; return self.compiled_cursor()', 'def has_compiled_cursor(self): return self.has_compiled_cursor_', 'def mutable_end_compiled_cursor(self): self.has_end_compiled_cursor_ = 1; return self.end_compiled_cursor()', 'def has_end_compiled_cursor(self): return self.has_end_compiled_cursor_', 'def composite_index_list(self): return self.composite_index_', 'def mutable_composite_index(self, i):\\n    return self.composite_index_[i]', 'def clear_composite_index(self):\\n    self.composite_index_ = []', 'def set_require_perfect_plan(self, x):\\n    self.has_require_perfect_plan_ = 1\\n    self.require_perfect_plan_ = x', 'def has_require_perfect_plan(self): return self.has_require_perfect_plan_', 'def set_keys_only(self, x):\\n    self.has_keys_only_ = 1\\n    self.keys_only_ = x', 'def has_keys_only(self): return self.has_keys_only_', 'def mutable_transaction(self): self.has_transaction_ = 1; return self.transaction()', 'def has_transaction(self): return self.has_transaction_', 'def set_compile(self, x):\\n    self.has_compile_ = 1\\n    self.compile_ = x', 'def has_compile(self): return self.has_compile_', 'def set_failover_ms(self, x):\\n    self.has_failover_ms_ = 1\\n    self.failover_ms_ = x', 'def has_failover_ms(self): return self.has_failover_ms_', 'def set_strong(self, x):\\n    self.has_strong_ = 1\\n    self.strong_ = x', 'def has_strong(self): return self.has_strong_', 'def property_name_list(self): return self.property_name_', 'def set_property_name(self, i, x):\\n    self.property_name_[i] = x', 'def clear_property_name(self):\\n    self.property_name_ = []', 'def group_by_property_name_list(self): return self.group_by_property_name_', 'def set_group_by_property_name(self, i, x):\\n    self.group_by_property_name_[i] = x', 'def clear_group_by_property_name(self):\\n    self.group_by_property_name_ = []', 'def set_distinct(self, x):\\n    self.has_distinct_ = 1\\n    self.distinct_ = x', 'def has_distinct(self): return self.has_distinct_', 'def set_min_safe_time_seconds(self, x):\\n    self.has_min_safe_time_seconds_ = 1\\n    self.min_safe_time_seconds_ = x', 'def has_min_safe_time_seconds(self): return self.has_min_safe_time_seconds_', 'def safe_replica_name_list(self): return self.safe_replica_name_', 'def set_safe_replica_name(self, i, x):\\n    self.safe_replica_name_[i] = x', 'def clear_safe_replica_name(self):\\n    self.safe_replica_name_ = []', 'def set_persist_offset(self, x):\\n    self.has_persist_offset_ = 1\\n    self.persist_offset_ = x', 'def has_persist_offset(self): return self.has_persist_offset_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_app_ != x.has_app_: return 0\\n    if self.has_app_ and self.app_ != x.app_: return 0\\n    if self.has_name_space_ != x.has_name_space_: return 0\\n    if self.has_name_space_ and self.name_space_ != x.name_space_: return 0\\n    if self.has_kind_ != x.has_kind_: return 0\\n    if self.has_kind_ and self.kind_ != x.kind_: return 0\\n    if self.has_ancestor_ != x.has_ancestor_: return 0\\n    if self.has_ancestor_ and self.ancestor_ != x.ancestor_: return 0\\n    if len(self.filter_) != len(x.filter_): return 0\\n    for e1, e2 in zip(self.filter_, x.filter_):\\n      if e1 != e2: return 0\\n    if self.has_search_query_ != x.has_search_query_: return 0\\n    if self.has_search_query_ and self.search_query_ != x.search_query_: return 0\\n    if len(self.order_) != len(x.order_): return 0\\n    for e1, e2 in zip(self.order_, x.order_):\\n      if e1 != e2: return 0\\n    if self.has_hint_ != x.has_hint_: return 0\\n    if self.has_hint_ and self.hint_ != x.hint_: return 0\\n    if self.has_count_ != x.has_count_: return 0\\n    if self.has_count_ and self.count_ != x.count_: return 0\\n    if self.has_offset_ != x.has_offset_: return 0\\n    if self.has_offset_ and self.offset_ != x.offset_: return 0\\n    if self.has_limit_ != x.has_limit_: return 0\\n    if self.has_limit_ and self.limit_ != x.limit_: return 0\\n    if self.has_compiled_cursor_ != x.has_compiled_cursor_: return 0\\n    if self.has_compiled_cursor_ and self.compiled_cursor_ != x.compiled_cursor_: return 0\\n    if self.has_end_compiled_cursor_ != x.has_end_compiled_cursor_: return 0\\n    if self.has_end_compiled_cursor_ and self.end_compiled_cursor_ != x.end_compiled_cursor_: return 0\\n    if len(self.composite_index_) != len(x.composite_index_): return 0\\n    for e1, e2 in zip(self.composite_index_, x.composite_index_):\\n      if e1 != e2: return 0\\n    if self.has_require_perfect_plan_ != x.has_require_perfect_plan_: return 0\\n    if self.has_require_perfect_plan_ and self.require_perfect_plan_ != x.require_perfect_plan_: return 0\\n    if self.has_keys_only_ != x.has_keys_only_: return 0\\n    if self.has_keys_only_ and self.keys_only_ != x.keys_only_: return 0\\n    if self.has_transaction_ != x.has_transaction_: return 0\\n    if self.has_transaction_ and self.transaction_ != x.transaction_: return 0\\n    if self.has_compile_ != x.has_compile_: return 0\\n    if self.has_compile_ and self.compile_ != x.compile_: return 0\\n    if self.has_failover_ms_ != x.has_failover_ms_: return 0\\n    if self.has_failover_ms_ and self.failover_ms_ != x.failover_ms_: return 0\\n    if self.has_strong_ != x.has_strong_: return 0\\n    if self.has_strong_ and self.strong_ != x.strong_: return 0\\n    if len(self.property_name_) != len(x.property_name_): return 0\\n    for e1, e2 in zip(self.property_name_, x.property_name_):\\n      if e1 != e2: return 0\\n    if len(self.group_by_property_name_) != len(x.group_by_property_name_): return 0\\n    for e1, e2 in zip(self.group_by_property_name_, x.group_by_property_name_):\\n      if e1 != e2: return 0\\n    if self.has_distinct_ != x.has_distinct_: return 0\\n    if self.has_distinct_ and self.distinct_ != x.distinct_: return 0\\n    if self.has_min_safe_time_seconds_ != x.has_min_safe_time_seconds_: return 0\\n    if self.has_min_safe_time_seconds_ and self.min_safe_time_seconds_ != x.min_safe_time_seconds_: return 0\\n    if len(self.safe_replica_name_) != len(x.safe_replica_name_): return 0\\n    for e1, e2 in zip(self.safe_replica_name_, x.safe_replica_name_):\\n      if e1 != e2: return 0\\n    if self.has_persist_offset_ != x.has_persist_offset_: return 0\\n    if self.has_persist_offset_ and self.persist_offset_ != x.persist_offset_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 2 + self.lengthString(self.header_.ByteSize())\\n    n += self.lengthString(len(self.app_))\\n    if (self.has_name_space_): n += 2 + self.lengthString(len(self.name_space_))\\n    if (self.has_kind_): n += 1 + self.lengthString(len(self.kind_))\\n    if (self.has_ancestor_): n += 2 + self.lengthString(self.ancestor_.ByteSize())\\n    n += 2 * len(self.filter_)\\n    for i in xrange(len(self.filter_)): n += self.filter_[i].ByteSize()\\n    if (self.has_search_query_): n += 1 + self.lengthString(len(self.search_query_))\\n    n += 2 * len(self.order_)\\n    for i in xrange(len(self.order_)): n += self.order_[i].ByteSize()\\n    if (self.has_hint_): n += 2 + self.lengthVarInt64(self.hint_)\\n    if (self.has_count_): n += 2 + self.lengthVarInt64(self.count_)\\n    if (self.has_offset_): n += 1 + self.lengthVarInt64(self.offset_)\\n    if (self.has_limit_): n += 2 + self.lengthVarInt64(self.limit_)\\n    if (self.has_compiled_cursor_): n += 2 + self.lengthString(self.compiled_cursor_.ByteSize())\\n    if (self.has_end_compiled_cursor_): n += 2 + self.lengthString(self.end_compiled_cursor_.ByteSize())\\n    n += 2 * len(self.composite_index_)\\n    for i in xrange(len(self.composite_index_)): n += self.lengthString(self.composite_index_[i].ByteSize())\\n    if (self.has_require_perfect_plan_): n += 3\\n    if (self.has_keys_only_): n += 3\\n    if (self.has_transaction_): n += 2 + self.lengthString(self.transaction_.ByteSize())\\n    if (self.has_compile_): n += 3\\n    if (self.has_failover_ms_): n += 2 + self.lengthVarInt64(self.failover_ms_)\\n    if (self.has_strong_): n += 3\\n    n += 2 * len(self.property_name_)\\n    for i in xrange(len(self.property_name_)): n += self.lengthString(len(self.property_name_[i]))\\n    n += 2 * len(self.group_by_property_name_)\\n    for i in xrange(len(self.group_by_property_name_)): n += self.lengthString(len(self.group_by_property_name_[i]))\\n    if (self.has_distinct_): n += 3\\n    if (self.has_min_safe_time_seconds_): n += 2 + self.lengthVarInt64(self.min_safe_time_seconds_)\\n    n += 2 * len(self.safe_replica_name_)\\n    for i in xrange(len(self.safe_replica_name_)): n += self.lengthString(len(self.safe_replica_name_[i]))\\n    if (self.has_persist_offset_): n += 3\\n    return n + 1', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_app()\\n    self.clear_name_space()\\n    self.clear_kind()\\n    self.clear_ancestor()\\n    self.clear_filter()\\n    self.clear_search_query()\\n    self.clear_order()\\n    self.clear_hint()\\n    self.clear_count()\\n    self.clear_offset()\\n    self.clear_limit()\\n    self.clear_compiled_cursor()\\n    self.clear_end_compiled_cursor()\\n    self.clear_composite_index()\\n    self.clear_require_perfect_plan()\\n    self.clear_keys_only()\\n    self.clear_transaction()\\n    self.clear_compile()\\n    self.clear_failover_ms()\\n    self.clear_strong()\\n    self.clear_property_name()\\n    self.clear_group_by_property_name()\\n    self.clear_distinct()\\n    self.clear_min_safe_time_seconds()\\n    self.clear_safe_replica_name()\\n    self.clear_persist_offset()', 'def OutputPartial(self, out):\\n    if (self.has_app_):\\n      out.putVarInt32(10)\\n      out.putPrefixedString(self.app_)\\n    if (self.has_kind_):\\n      out.putVarInt32(26)\\n      out.putPrefixedString(self.kind_)\\n    for i in xrange(len(self.filter_)):\\n      out.putVarInt32(35)\\n      self.filter_[i].OutputPartial(out)\\n      out.putVarInt32(36)\\n    if (self.has_search_query_):\\n      out.putVarInt32(66)\\n      out.putPrefixedString(self.search_query_)\\n    for i in xrange(len(self.order_)):\\n      out.putVarInt32(75)\\n      self.order_[i].OutputPartial(out)\\n      out.putVarInt32(76)\\n    if (self.has_offset_):\\n      out.putVarInt32(96)\\n      out.putVarInt32(self.offset_)\\n    if (self.has_limit_):\\n      out.putVarInt32(128)\\n      out.putVarInt32(self.limit_)\\n    if (self.has_ancestor_):\\n      out.putVarInt32(138)\\n      out.putVarInt32(self.ancestor_.ByteSizePartial())\\n      self.ancestor_.OutputPartial(out)\\n    if (self.has_hint_):\\n      out.putVarInt32(144)\\n      out.putVarInt32(self.hint_)\\n    for i in xrange(len(self.composite_index_)):\\n      out.putVarInt32(154)\\n      out.putVarInt32(self.composite_index_[i].ByteSizePartial())\\n      self.composite_index_[i].OutputPartial(out)\\n    if (self.has_require_perfect_plan_):\\n      out.putVarInt32(160)\\n      out.putBoolean(self.require_perfect_plan_)\\n    if (self.has_keys_only_):\\n      out.putVarInt32(168)\\n      out.putBoolean(self.keys_only_)\\n    if (self.has_transaction_):\\n      out.putVarInt32(178)\\n      out.putVarInt32(self.transaction_.ByteSizePartial())\\n      self.transaction_.OutputPartial(out)\\n    if (self.has_count_):\\n      out.putVarInt32(184)\\n      out.putVarInt32(self.count_)\\n    if (self.has_distinct_):\\n      out.putVarInt32(192)\\n      out.putBoolean(self.distinct_)\\n    if (self.has_compile_):\\n      out.putVarInt32(200)\\n      out.putBoolean(self.compile_)\\n    if (self.has_failover_ms_):\\n      out.putVarInt32(208)\\n      out.putVarInt64(self.failover_ms_)\\n    if (self.has_name_space_):\\n      out.putVarInt32(234)\\n      out.putPrefixedString(self.name_space_)\\n    if (self.has_compiled_cursor_):\\n      out.putVarInt32(242)\\n      out.putVarInt32(self.compiled_cursor_.ByteSizePartial())\\n      self.compiled_cursor_.OutputPartial(out)\\n    if (self.has_end_compiled_cursor_):\\n      out.putVarInt32(250)\\n      out.putVarInt32(self.end_compiled_cursor_.ByteSizePartial())\\n      self.end_compiled_cursor_.OutputPartial(out)\\n    if (self.has_strong_):\\n      out.putVarInt32(256)\\n      out.putBoolean(self.strong_)\\n    for i in xrange(len(self.property_name_)):\\n      out.putVarInt32(266)\\n      out.putPrefixedString(self.property_name_[i])\\n    for i in xrange(len(self.group_by_property_name_)):\\n      out.putVarInt32(274)\\n      out.putPrefixedString(self.group_by_property_name_[i])\\n    if (self.has_min_safe_time_seconds_):\\n      out.putVarInt32(280)\\n      out.putVarInt64(self.min_safe_time_seconds_)\\n    for i in xrange(len(self.safe_replica_name_)):\\n      out.putVarInt32(290)\\n      out.putPrefixedString(self.safe_replica_name_[i])\\n    if (self.has_persist_offset_):\\n      out.putVarInt32(296)\\n      out.putBoolean(self.persist_offset_)\\n    if (self.has_header_):\\n      out.putVarInt32(314)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_app_: res+=prefix+(\"app: %s\\\\n\" % self.DebugFormatString(self.app_))\\n    if self.has_name_space_: res+=prefix+(\"name_space: %s\\\\n\" % self.DebugFormatString(self.name_space_))\\n    if self.has_kind_: res+=prefix+(\"kind: %s\\\\n\" % self.DebugFormatString(self.kind_))\\n    if self.has_ancestor_:\\n      res+=prefix+\"ancestor <\\\\n\"\\n      res+=self.ancestor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.filter_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"Filter%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    if self.has_search_query_: res+=prefix+(\"search_query: %s\\\\n\" % self.DebugFormatString(self.search_query_))\\n    cnt=0\\n    for e in self.order_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"Order%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    if self.has_hint_: res+=prefix+(\"hint: %s\\\\n\" % self.DebugFormatInt32(self.hint_))\\n    if self.has_count_: res+=prefix+(\"count: %s\\\\n\" % self.DebugFormatInt32(self.count_))\\n    if self.has_offset_: res+=prefix+(\"offset: %s\\\\n\" % self.DebugFormatInt32(self.offset_))\\n    if self.has_limit_: res+=prefix+(\"limit: %s\\\\n\" % self.DebugFormatInt32(self.limit_))\\n    if self.has_compiled_cursor_:\\n      res+=prefix+\"compiled_cursor <\\\\n\"\\n      res+=self.compiled_cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_end_compiled_cursor_:\\n      res+=prefix+\"end_compiled_cursor <\\\\n\"\\n      res+=self.end_compiled_cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.composite_index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"composite_index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_require_perfect_plan_: res+=prefix+(\"require_perfect_plan: %s\\\\n\" % self.DebugFormatBool(self.require_perfect_plan_))\\n    if self.has_keys_only_: res+=prefix+(\"keys_only: %s\\\\n\" % self.DebugFormatBool(self.keys_only_))\\n    if self.has_transaction_:\\n      res+=prefix+\"transaction <\\\\n\"\\n      res+=self.transaction_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_compile_: res+=prefix+(\"compile: %s\\\\n\" % self.DebugFormatBool(self.compile_))\\n    if self.has_failover_ms_: res+=prefix+(\"failover_ms: %s\\\\n\" % self.DebugFormatInt64(self.failover_ms_))\\n    if self.has_strong_: res+=prefix+(\"strong: %s\\\\n\" % self.DebugFormatBool(self.strong_))\\n    cnt=0\\n    for e in self.property_name_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"property_name%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    cnt=0\\n    for e in self.group_by_property_name_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"group_by_property_name%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    if self.has_distinct_: res+=prefix+(\"distinct: %s\\\\n\" % self.DebugFormatBool(self.distinct_))\\n    if self.has_min_safe_time_seconds_: res+=prefix+(\"min_safe_time_seconds: %s\\\\n\" % self.DebugFormatInt64(self.min_safe_time_seconds_))\\n    cnt=0\\n    for e in self.safe_replica_name_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"safe_replica_name%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    if self.has_persist_offset_: res+=prefix+(\"persist_offset: %s\\\\n\" % self.DebugFormatBool(self.persist_offset_))\\n    return res', 'def __init__(self, contents=None):\\n    self.start_postfix_value_ = []\\n    self.end_postfix_value_ = []\\n    if contents is not None: self.MergeFromString(contents)', 'def set_index_name(self, x):\\n    self.has_index_name_ = 1\\n    self.index_name_ = x', 'def has_index_name(self): return self.has_index_name_', 'def set_start_key(self, x):\\n    self.has_start_key_ = 1\\n    self.start_key_ = x', 'def has_start_key(self): return self.has_start_key_', 'def set_start_inclusive(self, x):\\n    self.has_start_inclusive_ = 1\\n    self.start_inclusive_ = x', 'def has_start_inclusive(self): return self.has_start_inclusive_', 'def set_end_key(self, x):\\n    self.has_end_key_ = 1\\n    self.end_key_ = x', 'def has_end_key(self): return self.has_end_key_', 'def set_end_inclusive(self, x):\\n    self.has_end_inclusive_ = 1\\n    self.end_inclusive_ = x', 'def has_end_inclusive(self): return self.has_end_inclusive_', 'def start_postfix_value_list(self): return self.start_postfix_value_', 'def set_start_postfix_value(self, i, x):\\n    self.start_postfix_value_[i] = x', 'def clear_start_postfix_value(self):\\n    self.start_postfix_value_ = []', 'def end_postfix_value_list(self): return self.end_postfix_value_', 'def set_end_postfix_value(self, i, x):\\n    self.end_postfix_value_[i] = x', 'def clear_end_postfix_value(self):\\n    self.end_postfix_value_ = []', 'def set_end_unapplied_log_timestamp_us(self, x):\\n    self.has_end_unapplied_log_timestamp_us_ = 1\\n    self.end_unapplied_log_timestamp_us_ = x', 'def has_end_unapplied_log_timestamp_us(self): return self.has_end_unapplied_log_timestamp_us_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_index_name_ != x.has_index_name_: return 0\\n    if self.has_index_name_ and self.index_name_ != x.index_name_: return 0\\n    if self.has_start_key_ != x.has_start_key_: return 0\\n    if self.has_start_key_ and self.start_key_ != x.start_key_: return 0\\n    if self.has_start_inclusive_ != x.has_start_inclusive_: return 0\\n    if self.has_start_inclusive_ and self.start_inclusive_ != x.start_inclusive_: return 0\\n    if self.has_end_key_ != x.has_end_key_: return 0\\n    if self.has_end_key_ and self.end_key_ != x.end_key_: return 0\\n    if self.has_end_inclusive_ != x.has_end_inclusive_: return 0\\n    if self.has_end_inclusive_ and self.end_inclusive_ != x.end_inclusive_: return 0\\n    if len(self.start_postfix_value_) != len(x.start_postfix_value_): return 0\\n    for e1, e2 in zip(self.start_postfix_value_, x.start_postfix_value_):\\n      if e1 != e2: return 0\\n    if len(self.end_postfix_value_) != len(x.end_postfix_value_): return 0\\n    for e1, e2 in zip(self.end_postfix_value_, x.end_postfix_value_):\\n      if e1 != e2: return 0\\n    if self.has_end_unapplied_log_timestamp_us_ != x.has_end_unapplied_log_timestamp_us_: return 0\\n    if self.has_end_unapplied_log_timestamp_us_ and self.end_unapplied_log_timestamp_us_ != x.end_unapplied_log_timestamp_us_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_index_name_): n += 1 + self.lengthString(len(self.index_name_))\\n    if (self.has_start_key_): n += 1 + self.lengthString(len(self.start_key_))\\n    if (self.has_start_inclusive_): n += 2\\n    if (self.has_end_key_): n += 1 + self.lengthString(len(self.end_key_))\\n    if (self.has_end_inclusive_): n += 2\\n    n += 2 * len(self.start_postfix_value_)\\n    for i in xrange(len(self.start_postfix_value_)): n += self.lengthString(len(self.start_postfix_value_[i]))\\n    n += 2 * len(self.end_postfix_value_)\\n    for i in xrange(len(self.end_postfix_value_)): n += self.lengthString(len(self.end_postfix_value_[i]))\\n    if (self.has_end_unapplied_log_timestamp_us_): n += 2 + self.lengthVarInt64(self.end_unapplied_log_timestamp_us_)\\n    return n', 'def Clear(self):\\n    self.clear_index_name()\\n    self.clear_start_key()\\n    self.clear_start_inclusive()\\n    self.clear_end_key()\\n    self.clear_end_inclusive()\\n    self.clear_start_postfix_value()\\n    self.clear_end_postfix_value()\\n    self.clear_end_unapplied_log_timestamp_us()', 'def OutputPartial(self, out):\\n    if (self.has_index_name_):\\n      out.putVarInt32(18)\\n      out.putPrefixedString(self.index_name_)\\n    if (self.has_start_key_):\\n      out.putVarInt32(26)\\n      out.putPrefixedString(self.start_key_)\\n    if (self.has_start_inclusive_):\\n      out.putVarInt32(32)\\n      out.putBoolean(self.start_inclusive_)\\n    if (self.has_end_key_):\\n      out.putVarInt32(42)\\n      out.putPrefixedString(self.end_key_)\\n    if (self.has_end_inclusive_):\\n      out.putVarInt32(48)\\n      out.putBoolean(self.end_inclusive_)\\n    if (self.has_end_unapplied_log_timestamp_us_):\\n      out.putVarInt32(152)\\n      out.putVarInt64(self.end_unapplied_log_timestamp_us_)\\n    for i in xrange(len(self.start_postfix_value_)):\\n      out.putVarInt32(178)\\n      out.putPrefixedString(self.start_postfix_value_[i])\\n    for i in xrange(len(self.end_postfix_value_)):\\n      out.putVarInt32(186)\\n      out.putPrefixedString(self.end_postfix_value_[i])', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_index_name_: res+=prefix+(\"index_name: %s\\\\n\" % self.DebugFormatString(self.index_name_))\\n    if self.has_start_key_: res+=prefix+(\"start_key: %s\\\\n\" % self.DebugFormatString(self.start_key_))\\n    if self.has_start_inclusive_: res+=prefix+(\"start_inclusive: %s\\\\n\" % self.DebugFormatBool(self.start_inclusive_))\\n    if self.has_end_key_: res+=prefix+(\"end_key: %s\\\\n\" % self.DebugFormatString(self.end_key_))\\n    if self.has_end_inclusive_: res+=prefix+(\"end_inclusive: %s\\\\n\" % self.DebugFormatBool(self.end_inclusive_))\\n    cnt=0\\n    for e in self.start_postfix_value_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"start_postfix_value%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    cnt=0\\n    for e in self.end_postfix_value_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"end_postfix_value%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    if self.has_end_unapplied_log_timestamp_us_: res+=prefix+(\"end_unapplied_log_timestamp_us: %s\\\\n\" % self.DebugFormatInt64(self.end_unapplied_log_timestamp_us_))\\n    return res', 'def __init__(self, contents=None):\\n    self.prefix_value_ = []\\n    if contents is not None: self.MergeFromString(contents)', 'def set_index_name(self, x):\\n    self.has_index_name_ = 1\\n    self.index_name_ = x', 'def has_index_name(self): return self.has_index_name_', 'def prefix_value_list(self): return self.prefix_value_', 'def set_prefix_value(self, i, x):\\n    self.prefix_value_[i] = x', 'def clear_prefix_value(self):\\n    self.prefix_value_ = []', 'def set_value_prefix(self, x):\\n    self.has_value_prefix_ = 1\\n    self.value_prefix_ = x', 'def has_value_prefix(self): return self.has_value_prefix_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_index_name_ != x.has_index_name_: return 0\\n    if self.has_index_name_ and self.index_name_ != x.index_name_: return 0\\n    if len(self.prefix_value_) != len(x.prefix_value_): return 0\\n    for e1, e2 in zip(self.prefix_value_, x.prefix_value_):\\n      if e1 != e2: return 0\\n    if self.has_value_prefix_ != x.has_value_prefix_: return 0\\n    if self.has_value_prefix_ and self.value_prefix_ != x.value_prefix_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.lengthString(len(self.index_name_))\\n    n += 1 * len(self.prefix_value_)\\n    for i in xrange(len(self.prefix_value_)): n += self.lengthString(len(self.prefix_value_[i]))\\n    if (self.has_value_prefix_): n += 3\\n    return n + 1', 'def Clear(self):\\n    self.clear_index_name()\\n    self.clear_prefix_value()\\n    self.clear_value_prefix()', 'def OutputPartial(self, out):\\n    if (self.has_index_name_):\\n      out.putVarInt32(66)\\n      out.putPrefixedString(self.index_name_)\\n    for i in xrange(len(self.prefix_value_)):\\n      out.putVarInt32(74)\\n      out.putPrefixedString(self.prefix_value_[i])\\n    if (self.has_value_prefix_):\\n      out.putVarInt32(160)\\n      out.putBoolean(self.value_prefix_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_index_name_: res+=prefix+(\"index_name: %s\\\\n\" % self.DebugFormatString(self.index_name_))\\n    cnt=0\\n    for e in self.prefix_value_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"prefix_value%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    if self.has_value_prefix_: res+=prefix+(\"value_prefix: %s\\\\n\" % self.DebugFormatBool(self.value_prefix_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def set_distinct(self, x):\\n    self.has_distinct_ = 1\\n    self.distinct_ = x', 'def has_distinct(self): return self.has_distinct_', 'def set_kind(self, x):\\n    self.has_kind_ = 1\\n    self.kind_ = x', 'def has_kind(self): return self.has_kind_', 'def mutable_ancestor(self): self.has_ancestor_ = 1; return self.ancestor()', 'def has_ancestor(self): return self.has_ancestor_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_distinct_ != x.has_distinct_: return 0\\n    if self.has_distinct_ and self.distinct_ != x.distinct_: return 0\\n    if self.has_kind_ != x.has_kind_: return 0\\n    if self.has_kind_ and self.kind_ != x.kind_: return 0\\n    if self.has_ancestor_ != x.has_ancestor_: return 0\\n    if self.has_ancestor_ and self.ancestor_ != x.ancestor_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_distinct_): n += 2\\n    if (self.has_kind_): n += 2 + self.lengthString(len(self.kind_))\\n    if (self.has_ancestor_): n += 2 + self.lengthString(self.ancestor_.ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_distinct()\\n    self.clear_kind()\\n    self.clear_ancestor()', 'def OutputPartial(self, out):\\n    if (self.has_distinct_):\\n      out.putVarInt32(112)\\n      out.putBoolean(self.distinct_)\\n    if (self.has_kind_):\\n      out.putVarInt32(138)\\n      out.putPrefixedString(self.kind_)\\n    if (self.has_ancestor_):\\n      out.putVarInt32(146)\\n      out.putVarInt32(self.ancestor_.ByteSizePartial())\\n      self.ancestor_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_distinct_: res+=prefix+(\"distinct: %s\\\\n\" % self.DebugFormatBool(self.distinct_))\\n    if self.has_kind_: res+=prefix+(\"kind: %s\\\\n\" % self.DebugFormatString(self.kind_))\\n    if self.has_ancestor_:\\n      res+=prefix+\"ancestor <\\\\n\"\\n      res+=self.ancestor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    self.primaryscan_ = CompiledQuery_PrimaryScan()\\n    self.mergejoinscan_ = []\\n    self.property_name_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_primaryscan(self): self.has_primaryscan_ = 1; return self.primaryscan_', 'def has_primaryscan(self): return self.has_primaryscan_', 'def mergejoinscan_list(self): return self.mergejoinscan_', 'def mutable_mergejoinscan(self, i):\\n    return self.mergejoinscan_[i]', 'def clear_mergejoinscan(self):\\n    self.mergejoinscan_ = []', 'def mutable_index_def(self): self.has_index_def_ = 1; return self.index_def()', 'def has_index_def(self): return self.has_index_def_', 'def set_offset(self, x):\\n    self.has_offset_ = 1\\n    self.offset_ = x', 'def has_offset(self): return self.has_offset_', 'def set_limit(self, x):\\n    self.has_limit_ = 1\\n    self.limit_ = x', 'def has_limit(self): return self.has_limit_', 'def set_keys_only(self, x):\\n    self.has_keys_only_ = 1\\n    self.keys_only_ = x', 'def has_keys_only(self): return self.has_keys_only_', 'def property_name_list(self): return self.property_name_', 'def set_property_name(self, i, x):\\n    self.property_name_[i] = x', 'def clear_property_name(self):\\n    self.property_name_ = []', 'def set_distinct_infix_size(self, x):\\n    self.has_distinct_infix_size_ = 1\\n    self.distinct_infix_size_ = x', 'def has_distinct_infix_size(self): return self.has_distinct_infix_size_', 'def mutable_entityfilter(self): self.has_entityfilter_ = 1; return self.entityfilter()', 'def has_entityfilter(self): return self.has_entityfilter_', 'def set_plan_label(self, x):\\n    self.has_plan_label_ = 1\\n    self.plan_label_ = x', 'def has_plan_label(self): return self.has_plan_label_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_primaryscan_ != x.has_primaryscan_: return 0\\n    if self.has_primaryscan_ and self.primaryscan_ != x.primaryscan_: return 0\\n    if len(self.mergejoinscan_) != len(x.mergejoinscan_): return 0\\n    for e1, e2 in zip(self.mergejoinscan_, x.mergejoinscan_):\\n      if e1 != e2: return 0\\n    if self.has_index_def_ != x.has_index_def_: return 0\\n    if self.has_index_def_ and self.index_def_ != x.index_def_: return 0\\n    if self.has_offset_ != x.has_offset_: return 0\\n    if self.has_offset_ and self.offset_ != x.offset_: return 0\\n    if self.has_limit_ != x.has_limit_: return 0\\n    if self.has_limit_ and self.limit_ != x.limit_: return 0\\n    if self.has_keys_only_ != x.has_keys_only_: return 0\\n    if self.has_keys_only_ and self.keys_only_ != x.keys_only_: return 0\\n    if len(self.property_name_) != len(x.property_name_): return 0\\n    for e1, e2 in zip(self.property_name_, x.property_name_):\\n      if e1 != e2: return 0\\n    if self.has_distinct_infix_size_ != x.has_distinct_infix_size_: return 0\\n    if self.has_distinct_infix_size_ and self.distinct_infix_size_ != x.distinct_infix_size_: return 0\\n    if self.has_entityfilter_ != x.has_entityfilter_: return 0\\n    if self.has_entityfilter_ and self.entityfilter_ != x.entityfilter_: return 0\\n    if self.has_plan_label_ != x.has_plan_label_: return 0\\n    if self.has_plan_label_ and self.plan_label_ != x.plan_label_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.primaryscan_.ByteSize()\\n    n += 2 * len(self.mergejoinscan_)\\n    for i in xrange(len(self.mergejoinscan_)): n += self.mergejoinscan_[i].ByteSize()\\n    if (self.has_index_def_): n += 2 + self.lengthString(self.index_def_.ByteSize())\\n    if (self.has_offset_): n += 1 + self.lengthVarInt64(self.offset_)\\n    if (self.has_limit_): n += 1 + self.lengthVarInt64(self.limit_)\\n    n += 2 * len(self.property_name_)\\n    for i in xrange(len(self.property_name_)): n += self.lengthString(len(self.property_name_[i]))\\n    if (self.has_distinct_infix_size_): n += 2 + self.lengthVarInt64(self.distinct_infix_size_)\\n    if (self.has_entityfilter_): n += 2 + self.entityfilter_.ByteSize()\\n    if (self.has_plan_label_): n += 2 + self.lengthString(len(self.plan_label_))\\n    return n + 4', 'def Clear(self):\\n    self.clear_primaryscan()\\n    self.clear_mergejoinscan()\\n    self.clear_index_def()\\n    self.clear_offset()\\n    self.clear_limit()\\n    self.clear_keys_only()\\n    self.clear_property_name()\\n    self.clear_distinct_infix_size()\\n    self.clear_entityfilter()\\n    self.clear_plan_label()', 'def OutputPartial(self, out):\\n    if (self.has_primaryscan_):\\n      out.putVarInt32(11)\\n      self.primaryscan_.OutputPartial(out)\\n      out.putVarInt32(12)\\n    for i in xrange(len(self.mergejoinscan_)):\\n      out.putVarInt32(59)\\n      self.mergejoinscan_[i].OutputPartial(out)\\n      out.putVarInt32(60)\\n    if (self.has_offset_):\\n      out.putVarInt32(80)\\n      out.putVarInt32(self.offset_)\\n    if (self.has_limit_):\\n      out.putVarInt32(88)\\n      out.putVarInt32(self.limit_)\\n    if (self.has_keys_only_):\\n      out.putVarInt32(96)\\n      out.putBoolean(self.keys_only_)\\n    if (self.has_entityfilter_):\\n      out.putVarInt32(107)\\n      self.entityfilter_.OutputPartial(out)\\n      out.putVarInt32(108)\\n    if (self.has_index_def_):\\n      out.putVarInt32(170)\\n      out.putVarInt32(self.index_def_.ByteSizePartial())\\n      self.index_def_.OutputPartial(out)\\n    for i in xrange(len(self.property_name_)):\\n      out.putVarInt32(194)\\n      out.putPrefixedString(self.property_name_[i])\\n    if (self.has_distinct_infix_size_):\\n      out.putVarInt32(200)\\n      out.putVarInt32(self.distinct_infix_size_)\\n    if (self.has_plan_label_):\\n      out.putVarInt32(210)\\n      out.putPrefixedString(self.plan_label_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_primaryscan_:\\n      res+=prefix+\"PrimaryScan {\\\\n\"\\n      res+=self.primaryscan_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n    cnt=0\\n    for e in self.mergejoinscan_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"MergeJoinScan%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    if self.has_index_def_:\\n      res+=prefix+\"index_def <\\\\n\"\\n      res+=self.index_def_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_offset_: res+=prefix+(\"offset: %s\\\\n\" % self.DebugFormatInt32(self.offset_))\\n    if self.has_limit_: res+=prefix+(\"limit: %s\\\\n\" % self.DebugFormatInt32(self.limit_))\\n    if self.has_keys_only_: res+=prefix+(\"keys_only: %s\\\\n\" % self.DebugFormatBool(self.keys_only_))\\n    cnt=0\\n    for e in self.property_name_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"property_name%s: %s\\\\n\" % (elm, self.DebugFormatString(e)))\\n      cnt+=1\\n    if self.has_distinct_infix_size_: res+=prefix+(\"distinct_infix_size: %s\\\\n\" % self.DebugFormatInt32(self.distinct_infix_size_))\\n    if self.has_entityfilter_:\\n      res+=prefix+\"EntityFilter {\\\\n\"\\n      res+=self.entityfilter_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n    if self.has_plan_label_: res+=prefix+(\"plan_label: %s\\\\n\" % self.DebugFormatString(self.plan_label_))\\n    return res', 'def __init__(self, contents=None):\\n    self.value_ = PropertyValue()\\n    if contents is not None: self.MergeFromString(contents)', 'def set_property(self, x):\\n    self.has_property_ = 1\\n    self.property_ = x', 'def has_property(self): return self.has_property_', 'def mutable_value(self): self.has_value_ = 1; return self.value_', 'def has_value(self): return self.has_value_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_property_ != x.has_property_: return 0\\n    if self.has_property_ and self.property_ != x.property_: return 0\\n    if self.has_value_ != x.has_value_: return 0\\n    if self.has_value_ and self.value_ != x.value_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_property_): n += 2 + self.lengthString(len(self.property_))\\n    n += self.lengthString(self.value_.ByteSize())\\n    return n + 2', 'def Clear(self):\\n    self.clear_property()\\n    self.clear_value()', 'def OutputPartial(self, out):\\n    if (self.has_property_):\\n      out.putVarInt32(242)\\n      out.putPrefixedString(self.property_)\\n    if (self.has_value_):\\n      out.putVarInt32(250)\\n      out.putVarInt32(self.value_.ByteSizePartial())\\n      self.value_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_property_: res+=prefix+(\"property: %s\\\\n\" % self.DebugFormatString(self.property_))\\n    if self.has_value_:\\n      res+=prefix+\"value <\\\\n\"\\n      res+=self.value_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    self.indexvalue_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def set_start_key(self, x):\\n    self.has_start_key_ = 1\\n    self.start_key_ = x', 'def has_start_key(self): return self.has_start_key_', 'def indexvalue_list(self): return self.indexvalue_', 'def mutable_indexvalue(self, i):\\n    return self.indexvalue_[i]', 'def clear_indexvalue(self):\\n    self.indexvalue_ = []', 'def mutable_key(self): self.has_key_ = 1; return self.key()', 'def has_key(self): return self.has_key_', 'def set_start_inclusive(self, x):\\n    self.has_start_inclusive_ = 1\\n    self.start_inclusive_ = x', 'def has_start_inclusive(self): return self.has_start_inclusive_', 'def set_before_ascending(self, x):\\n    self.has_before_ascending_ = 1\\n    self.before_ascending_ = x', 'def has_before_ascending(self): return self.has_before_ascending_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_start_key_ != x.has_start_key_: return 0\\n    if self.has_start_key_ and self.start_key_ != x.start_key_: return 0\\n    if len(self.indexvalue_) != len(x.indexvalue_): return 0\\n    for e1, e2 in zip(self.indexvalue_, x.indexvalue_):\\n      if e1 != e2: return 0\\n    if self.has_key_ != x.has_key_: return 0\\n    if self.has_key_ and self.key_ != x.key_: return 0\\n    if self.has_start_inclusive_ != x.has_start_inclusive_: return 0\\n    if self.has_start_inclusive_ and self.start_inclusive_ != x.start_inclusive_: return 0\\n    if self.has_before_ascending_ != x.has_before_ascending_: return 0\\n    if self.has_before_ascending_ and self.before_ascending_ != x.before_ascending_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_start_key_): n += 2 + self.lengthString(len(self.start_key_))\\n    n += 4 * len(self.indexvalue_)\\n    for i in xrange(len(self.indexvalue_)): n += self.indexvalue_[i].ByteSize()\\n    if (self.has_key_): n += 2 + self.lengthString(self.key_.ByteSize())\\n    if (self.has_start_inclusive_): n += 3\\n    if (self.has_before_ascending_): n += 3\\n    return n', 'def Clear(self):\\n    self.clear_start_key()\\n    self.clear_indexvalue()\\n    self.clear_key()\\n    self.clear_start_inclusive()\\n    self.clear_before_ascending()', 'def OutputPartial(self, out):\\n    if (self.has_start_key_):\\n      out.putVarInt32(218)\\n      out.putPrefixedString(self.start_key_)\\n    if (self.has_start_inclusive_):\\n      out.putVarInt32(224)\\n      out.putBoolean(self.start_inclusive_)\\n    for i in xrange(len(self.indexvalue_)):\\n      out.putVarInt32(235)\\n      self.indexvalue_[i].OutputPartial(out)\\n      out.putVarInt32(236)\\n    if (self.has_key_):\\n      out.putVarInt32(258)\\n      out.putVarInt32(self.key_.ByteSizePartial())\\n      self.key_.OutputPartial(out)\\n    if (self.has_before_ascending_):\\n      out.putVarInt32(264)\\n      out.putBoolean(self.before_ascending_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_start_key_: res+=prefix+(\"start_key: %s\\\\n\" % self.DebugFormatString(self.start_key_))\\n    cnt=0\\n    for e in self.indexvalue_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"IndexValue%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    if self.has_key_:\\n      res+=prefix+\"key <\\\\n\"\\n      res+=self.key_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_start_inclusive_: res+=prefix+(\"start_inclusive: %s\\\\n\" % self.DebugFormatBool(self.start_inclusive_))\\n    if self.has_before_ascending_: res+=prefix+(\"before_ascending: %s\\\\n\" % self.DebugFormatBool(self.before_ascending_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_position(self): self.has_position_ = 1; return self.position()', 'def has_position(self): return self.has_position_', 'def mutable_postfix_position(self): self.has_postfix_position_ = 1; return self.postfix_position()', 'def has_postfix_position(self): return self.has_postfix_position_', 'def mutable_absolute_position(self): self.has_absolute_position_ = 1; return self.absolute_position()', 'def has_absolute_position(self): return self.has_absolute_position_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_position_ != x.has_position_: return 0\\n    if self.has_position_ and self.position_ != x.position_: return 0\\n    if self.has_postfix_position_ != x.has_postfix_position_: return 0\\n    if self.has_postfix_position_ and self.postfix_position_ != x.postfix_position_: return 0\\n    if self.has_absolute_position_ != x.has_absolute_position_: return 0\\n    if self.has_absolute_position_ and self.absolute_position_ != x.absolute_position_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_position_): n += 2 + self.position_.ByteSize()\\n    if (self.has_postfix_position_): n += 1 + self.lengthString(self.postfix_position_.ByteSize())\\n    if (self.has_absolute_position_): n += 1 + self.lengthString(self.absolute_position_.ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_position()\\n    self.clear_postfix_position()\\n    self.clear_absolute_position()', 'def OutputPartial(self, out):\\n    if (self.has_postfix_position_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.postfix_position_.ByteSizePartial())\\n      self.postfix_position_.OutputPartial(out)\\n    if (self.has_position_):\\n      out.putVarInt32(19)\\n      self.position_.OutputPartial(out)\\n      out.putVarInt32(20)\\n    if (self.has_absolute_position_):\\n      out.putVarInt32(26)\\n      out.putVarInt32(self.absolute_position_.ByteSizePartial())\\n      self.absolute_position_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_position_:\\n      res+=prefix+\"Position {\\\\n\"\\n      res+=self.position_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n    if self.has_postfix_position_:\\n      res+=prefix+\"postfix_position <\\\\n\"\\n      res+=self.postfix_position_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_absolute_position_:\\n      res+=prefix+\"absolute_position <\\\\n\"\\n      res+=self.absolute_position_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    if contents is not None: self.MergeFromString(contents)', 'def set_cursor(self, x):\\n    self.has_cursor_ = 1\\n    self.cursor_ = x', 'def has_cursor(self): return self.has_cursor_', 'def set_app(self, x):\\n    self.has_app_ = 1\\n    self.app_ = x', 'def has_app(self): return self.has_app_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_cursor_ != x.has_cursor_: return 0\\n    if self.has_cursor_ and self.cursor_ != x.cursor_: return 0\\n    if self.has_app_ != x.has_app_: return 0\\n    if self.has_app_ and self.app_ != x.app_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_app_): n += 1 + self.lengthString(len(self.app_))\\n    return n + 9', 'def Clear(self):\\n    self.clear_cursor()\\n    self.clear_app()', 'def OutputPartial(self, out):\\n    if (self.has_cursor_):\\n      out.putVarInt32(9)\\n      out.put64(self.cursor_)\\n    if (self.has_app_):\\n      out.putVarInt32(18)\\n      out.putPrefixedString(self.app_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_cursor_: res+=prefix+(\"cursor: %s\\\\n\" % self.DebugFormatFixed64(self.cursor_))\\n    if self.has_app_: res+=prefix+(\"app: %s\\\\n\" % self.DebugFormatString(self.app_))\\n    return res', 'def ErrorCode_Name(cls, x): return cls._ErrorCode_NAMES.get(x, \"\")', 'def __init__(self, contents=None):\\n    pass\\n    if contents is not None: self.MergeFromString(contents)', 'def Equals(self, x):\\n    if x is self: return 1\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    return n', 'def Clear(self):\\n    pass', 'def OutputPartial(self, out):\\n    pass', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    return res', 'def __init__(self, contents=None):\\n    if contents is not None: self.MergeFromString(contents)', 'def set_requested_entity_puts(self, x):\\n    self.has_requested_entity_puts_ = 1\\n    self.requested_entity_puts_ = x', 'def has_requested_entity_puts(self): return self.has_requested_entity_puts_', 'def set_requested_entity_deletes(self, x):\\n    self.has_requested_entity_deletes_ = 1\\n    self.requested_entity_deletes_ = x', 'def has_requested_entity_deletes(self): return self.has_requested_entity_deletes_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_requested_entity_puts_ != x.has_requested_entity_puts_: return 0\\n    if self.has_requested_entity_puts_ and self.requested_entity_puts_ != x.requested_entity_puts_: return 0\\n    if self.has_requested_entity_deletes_ != x.has_requested_entity_deletes_: return 0\\n    if self.has_requested_entity_deletes_ and self.requested_entity_deletes_ != x.requested_entity_deletes_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_requested_entity_puts_): n += 1 + self.lengthVarInt64(self.requested_entity_puts_)\\n    if (self.has_requested_entity_deletes_): n += 1 + self.lengthVarInt64(self.requested_entity_deletes_)\\n    return n', 'def Clear(self):\\n    self.clear_requested_entity_puts()\\n    self.clear_requested_entity_deletes()', 'def OutputPartial(self, out):\\n    if (self.has_requested_entity_puts_):\\n      out.putVarInt32(48)\\n      out.putVarInt32(self.requested_entity_puts_)\\n    if (self.has_requested_entity_deletes_):\\n      out.putVarInt32(56)\\n      out.putVarInt32(self.requested_entity_deletes_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_requested_entity_puts_: res+=prefix+(\"requested_entity_puts: %s\\\\n\" % self.DebugFormatInt32(self.requested_entity_puts_))\\n    if self.has_requested_entity_deletes_: res+=prefix+(\"requested_entity_deletes: %s\\\\n\" % self.DebugFormatInt32(self.requested_entity_deletes_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def set_index_writes(self, x):\\n    self.has_index_writes_ = 1\\n    self.index_writes_ = x', 'def has_index_writes(self): return self.has_index_writes_', 'def set_index_write_bytes(self, x):\\n    self.has_index_write_bytes_ = 1\\n    self.index_write_bytes_ = x', 'def has_index_write_bytes(self): return self.has_index_write_bytes_', 'def set_entity_writes(self, x):\\n    self.has_entity_writes_ = 1\\n    self.entity_writes_ = x', 'def has_entity_writes(self): return self.has_entity_writes_', 'def set_entity_write_bytes(self, x):\\n    self.has_entity_write_bytes_ = 1\\n    self.entity_write_bytes_ = x', 'def has_entity_write_bytes(self): return self.has_entity_write_bytes_', 'def mutable_commitcost(self): self.has_commitcost_ = 1; return self.commitcost()', 'def has_commitcost(self): return self.has_commitcost_', 'def set_approximate_storage_delta(self, x):\\n    self.has_approximate_storage_delta_ = 1\\n    self.approximate_storage_delta_ = x', 'def has_approximate_storage_delta(self): return self.has_approximate_storage_delta_', 'def set_id_sequence_updates(self, x):\\n    self.has_id_sequence_updates_ = 1\\n    self.id_sequence_updates_ = x', 'def has_id_sequence_updates(self): return self.has_id_sequence_updates_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_index_writes_ != x.has_index_writes_: return 0\\n    if self.has_index_writes_ and self.index_writes_ != x.index_writes_: return 0\\n    if self.has_index_write_bytes_ != x.has_index_write_bytes_: return 0\\n    if self.has_index_write_bytes_ and self.index_write_bytes_ != x.index_write_bytes_: return 0\\n    if self.has_entity_writes_ != x.has_entity_writes_: return 0\\n    if self.has_entity_writes_ and self.entity_writes_ != x.entity_writes_: return 0\\n    if self.has_entity_write_bytes_ != x.has_entity_write_bytes_: return 0\\n    if self.has_entity_write_bytes_ and self.entity_write_bytes_ != x.entity_write_bytes_: return 0\\n    if self.has_commitcost_ != x.has_commitcost_: return 0\\n    if self.has_commitcost_ and self.commitcost_ != x.commitcost_: return 0\\n    if self.has_approximate_storage_delta_ != x.has_approximate_storage_delta_: return 0\\n    if self.has_approximate_storage_delta_ and self.approximate_storage_delta_ != x.approximate_storage_delta_: return 0\\n    if self.has_id_sequence_updates_ != x.has_id_sequence_updates_: return 0\\n    if self.has_id_sequence_updates_ and self.id_sequence_updates_ != x.id_sequence_updates_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_index_writes_): n += 1 + self.lengthVarInt64(self.index_writes_)\\n    if (self.has_index_write_bytes_): n += 1 + self.lengthVarInt64(self.index_write_bytes_)\\n    if (self.has_entity_writes_): n += 1 + self.lengthVarInt64(self.entity_writes_)\\n    if (self.has_entity_write_bytes_): n += 1 + self.lengthVarInt64(self.entity_write_bytes_)\\n    if (self.has_commitcost_): n += 2 + self.commitcost_.ByteSize()\\n    if (self.has_approximate_storage_delta_): n += 1 + self.lengthVarInt64(self.approximate_storage_delta_)\\n    if (self.has_id_sequence_updates_): n += 1 + self.lengthVarInt64(self.id_sequence_updates_)\\n    return n', 'def Clear(self):\\n    self.clear_index_writes()\\n    self.clear_index_write_bytes()\\n    self.clear_entity_writes()\\n    self.clear_entity_write_bytes()\\n    self.clear_commitcost()\\n    self.clear_approximate_storage_delta()\\n    self.clear_id_sequence_updates()', 'def OutputPartial(self, out):\\n    if (self.has_index_writes_):\\n      out.putVarInt32(8)\\n      out.putVarInt32(self.index_writes_)\\n    if (self.has_index_write_bytes_):\\n      out.putVarInt32(16)\\n      out.putVarInt32(self.index_write_bytes_)\\n    if (self.has_entity_writes_):\\n      out.putVarInt32(24)\\n      out.putVarInt32(self.entity_writes_)\\n    if (self.has_entity_write_bytes_):\\n      out.putVarInt32(32)\\n      out.putVarInt32(self.entity_write_bytes_)\\n    if (self.has_commitcost_):\\n      out.putVarInt32(43)\\n      self.commitcost_.OutputPartial(out)\\n      out.putVarInt32(44)\\n    if (self.has_approximate_storage_delta_):\\n      out.putVarInt32(64)\\n      out.putVarInt32(self.approximate_storage_delta_)\\n    if (self.has_id_sequence_updates_):\\n      out.putVarInt32(72)\\n      out.putVarInt32(self.id_sequence_updates_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_index_writes_: res+=prefix+(\"index_writes: %s\\\\n\" % self.DebugFormatInt32(self.index_writes_))\\n    if self.has_index_write_bytes_: res+=prefix+(\"index_write_bytes: %s\\\\n\" % self.DebugFormatInt32(self.index_write_bytes_))\\n    if self.has_entity_writes_: res+=prefix+(\"entity_writes: %s\\\\n\" % self.DebugFormatInt32(self.entity_writes_))\\n    if self.has_entity_write_bytes_: res+=prefix+(\"entity_write_bytes: %s\\\\n\" % self.DebugFormatInt32(self.entity_write_bytes_))\\n    if self.has_commitcost_:\\n      res+=prefix+\"CommitCost {\\\\n\"\\n      res+=self.commitcost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n    if self.has_approximate_storage_delta_: res+=prefix+(\"approximate_storage_delta: %s\\\\n\" % self.DebugFormatInt32(self.approximate_storage_delta_))\\n    if self.has_id_sequence_updates_: res+=prefix+(\"id_sequence_updates: %s\\\\n\" % self.DebugFormatInt32(self.id_sequence_updates_))\\n    return res', 'def __init__(self, contents=None):\\n    self.key_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def key_list(self): return self.key_', 'def mutable_key(self, i):\\n    return self.key_[i]', 'def clear_key(self):\\n    self.key_ = []', 'def mutable_transaction(self): self.has_transaction_ = 1; return self.transaction()', 'def has_transaction(self): return self.has_transaction_', 'def set_failover_ms(self, x):\\n    self.has_failover_ms_ = 1\\n    self.failover_ms_ = x', 'def has_failover_ms(self): return self.has_failover_ms_', 'def set_strong(self, x):\\n    self.has_strong_ = 1\\n    self.strong_ = x', 'def has_strong(self): return self.has_strong_', 'def set_allow_deferred(self, x):\\n    self.has_allow_deferred_ = 1\\n    self.allow_deferred_ = x', 'def has_allow_deferred(self): return self.has_allow_deferred_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if len(self.key_) != len(x.key_): return 0\\n    for e1, e2 in zip(self.key_, x.key_):\\n      if e1 != e2: return 0\\n    if self.has_transaction_ != x.has_transaction_: return 0\\n    if self.has_transaction_ and self.transaction_ != x.transaction_: return 0\\n    if self.has_failover_ms_ != x.has_failover_ms_: return 0\\n    if self.has_failover_ms_ and self.failover_ms_ != x.failover_ms_: return 0\\n    if self.has_strong_ != x.has_strong_: return 0\\n    if self.has_strong_ and self.strong_ != x.strong_: return 0\\n    if self.has_allow_deferred_ != x.has_allow_deferred_: return 0\\n    if self.has_allow_deferred_ and self.allow_deferred_ != x.allow_deferred_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += 1 * len(self.key_)\\n    for i in xrange(len(self.key_)): n += self.lengthString(self.key_[i].ByteSize())\\n    if (self.has_transaction_): n += 1 + self.lengthString(self.transaction_.ByteSize())\\n    if (self.has_failover_ms_): n += 1 + self.lengthVarInt64(self.failover_ms_)\\n    if (self.has_strong_): n += 2\\n    if (self.has_allow_deferred_): n += 2\\n    return n', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_key()\\n    self.clear_transaction()\\n    self.clear_failover_ms()\\n    self.clear_strong()\\n    self.clear_allow_deferred()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.key_)):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.key_[i].ByteSizePartial())\\n      self.key_[i].OutputPartial(out)\\n    if (self.has_transaction_):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.transaction_.ByteSizePartial())\\n      self.transaction_.OutputPartial(out)\\n    if (self.has_failover_ms_):\\n      out.putVarInt32(24)\\n      out.putVarInt64(self.failover_ms_)\\n    if (self.has_strong_):\\n      out.putVarInt32(32)\\n      out.putBoolean(self.strong_)\\n    if (self.has_allow_deferred_):\\n      out.putVarInt32(40)\\n      out.putBoolean(self.allow_deferred_)\\n    if (self.has_header_):\\n      out.putVarInt32(50)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.key_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"key%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_transaction_:\\n      res+=prefix+\"transaction <\\\\n\"\\n      res+=self.transaction_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_failover_ms_: res+=prefix+(\"failover_ms: %s\\\\n\" % self.DebugFormatInt64(self.failover_ms_))\\n    if self.has_strong_: res+=prefix+(\"strong: %s\\\\n\" % self.DebugFormatBool(self.strong_))\\n    if self.has_allow_deferred_: res+=prefix+(\"allow_deferred: %s\\\\n\" % self.DebugFormatBool(self.allow_deferred_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_entity(self): self.has_entity_ = 1; return self.entity()', 'def has_entity(self): return self.has_entity_', 'def mutable_key(self): self.has_key_ = 1; return self.key()', 'def has_key(self): return self.has_key_', 'def set_version(self, x):\\n    self.has_version_ = 1\\n    self.version_ = x', 'def has_version(self): return self.has_version_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_entity_ != x.has_entity_: return 0\\n    if self.has_entity_ and self.entity_ != x.entity_: return 0\\n    if self.has_key_ != x.has_key_: return 0\\n    if self.has_key_ and self.key_ != x.key_: return 0\\n    if self.has_version_ != x.has_version_: return 0\\n    if self.has_version_ and self.version_ != x.version_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_entity_): n += 1 + self.lengthString(self.entity_.ByteSize())\\n    if (self.has_key_): n += 1 + self.lengthString(self.key_.ByteSize())\\n    if (self.has_version_): n += 1 + self.lengthVarInt64(self.version_)\\n    return n', 'def Clear(self):\\n    self.clear_entity()\\n    self.clear_key()\\n    self.clear_version()', 'def OutputPartial(self, out):\\n    if (self.has_entity_):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.entity_.ByteSizePartial())\\n      self.entity_.OutputPartial(out)\\n    if (self.has_version_):\\n      out.putVarInt32(24)\\n      out.putVarInt64(self.version_)\\n    if (self.has_key_):\\n      out.putVarInt32(34)\\n      out.putVarInt32(self.key_.ByteSizePartial())\\n      self.key_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_entity_:\\n      res+=prefix+\"entity <\\\\n\"\\n      res+=self.entity_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_key_:\\n      res+=prefix+\"key <\\\\n\"\\n      res+=self.key_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_version_: res+=prefix+(\"version: %s\\\\n\" % self.DebugFormatInt64(self.version_))\\n    return res', 'def __init__(self, contents=None):\\n    self.entity_ = []\\n    self.deferred_ = []\\n    if contents is not None: self.MergeFromString(contents)', 'def entity_list(self): return self.entity_', 'def mutable_entity(self, i):\\n    return self.entity_[i]', 'def clear_entity(self):\\n    self.entity_ = []', 'def deferred_list(self): return self.deferred_', 'def mutable_deferred(self, i):\\n    return self.deferred_[i]', 'def clear_deferred(self):\\n    self.deferred_ = []', 'def set_in_order(self, x):\\n    self.has_in_order_ = 1\\n    self.in_order_ = x', 'def has_in_order(self): return self.has_in_order_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if len(self.entity_) != len(x.entity_): return 0\\n    for e1, e2 in zip(self.entity_, x.entity_):\\n      if e1 != e2: return 0\\n    if len(self.deferred_) != len(x.deferred_): return 0\\n    for e1, e2 in zip(self.deferred_, x.deferred_):\\n      if e1 != e2: return 0\\n    if self.has_in_order_ != x.has_in_order_: return 0\\n    if self.has_in_order_ and self.in_order_ != x.in_order_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += 2 * len(self.entity_)\\n    for i in xrange(len(self.entity_)): n += self.entity_[i].ByteSize()\\n    n += 1 * len(self.deferred_)\\n    for i in xrange(len(self.deferred_)): n += self.lengthString(self.deferred_[i].ByteSize())\\n    if (self.has_in_order_): n += 2\\n    return n', 'def Clear(self):\\n    self.clear_entity()\\n    self.clear_deferred()\\n    self.clear_in_order()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.entity_)):\\n      out.putVarInt32(11)\\n      self.entity_[i].OutputPartial(out)\\n      out.putVarInt32(12)\\n    for i in xrange(len(self.deferred_)):\\n      out.putVarInt32(42)\\n      out.putVarInt32(self.deferred_[i].ByteSizePartial())\\n      self.deferred_[i].OutputPartial(out)\\n    if (self.has_in_order_):\\n      out.putVarInt32(48)\\n      out.putBoolean(self.in_order_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    cnt=0\\n    for e in self.entity_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"Entity%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    cnt=0\\n    for e in self.deferred_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"deferred%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_in_order_: res+=prefix+(\"in_order: %s\\\\n\" % self.DebugFormatBool(self.in_order_))\\n    return res', 'def AutoIdPolicy_Name(cls, x): return cls._AutoIdPolicy_NAMES.get(x, \"\")', 'def __init__(self, contents=None):\\n    self.entity_ = []\\n    self.composite_index_ = []\\n    self.snapshot_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def entity_list(self): return self.entity_', 'def mutable_entity(self, i):\\n    return self.entity_[i]', 'def clear_entity(self):\\n    self.entity_ = []', 'def mutable_transaction(self): self.has_transaction_ = 1; return self.transaction()', 'def has_transaction(self): return self.has_transaction_', 'def composite_index_list(self): return self.composite_index_', 'def mutable_composite_index(self, i):\\n    return self.composite_index_[i]', 'def clear_composite_index(self):\\n    self.composite_index_ = []', 'def set_trusted(self, x):\\n    self.has_trusted_ = 1\\n    self.trusted_ = x', 'def has_trusted(self): return self.has_trusted_', 'def set_force(self, x):\\n    self.has_force_ = 1\\n    self.force_ = x', 'def has_force(self): return self.has_force_', 'def set_mark_changes(self, x):\\n    self.has_mark_changes_ = 1\\n    self.mark_changes_ = x', 'def has_mark_changes(self): return self.has_mark_changes_', 'def snapshot_list(self): return self.snapshot_', 'def mutable_snapshot(self, i):\\n    return self.snapshot_[i]', 'def clear_snapshot(self):\\n    self.snapshot_ = []', 'def set_auto_id_policy(self, x):\\n    self.has_auto_id_policy_ = 1\\n    self.auto_id_policy_ = x', 'def has_auto_id_policy(self): return self.has_auto_id_policy_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if len(self.entity_) != len(x.entity_): return 0\\n    for e1, e2 in zip(self.entity_, x.entity_):\\n      if e1 != e2: return 0\\n    if self.has_transaction_ != x.has_transaction_: return 0\\n    if self.has_transaction_ and self.transaction_ != x.transaction_: return 0\\n    if len(self.composite_index_) != len(x.composite_index_): return 0\\n    for e1, e2 in zip(self.composite_index_, x.composite_index_):\\n      if e1 != e2: return 0\\n    if self.has_trusted_ != x.has_trusted_: return 0\\n    if self.has_trusted_ and self.trusted_ != x.trusted_: return 0\\n    if self.has_force_ != x.has_force_: return 0\\n    if self.has_force_ and self.force_ != x.force_: return 0\\n    if self.has_mark_changes_ != x.has_mark_changes_: return 0\\n    if self.has_mark_changes_ and self.mark_changes_ != x.mark_changes_: return 0\\n    if len(self.snapshot_) != len(x.snapshot_): return 0\\n    for e1, e2 in zip(self.snapshot_, x.snapshot_):\\n      if e1 != e2: return 0\\n    if self.has_auto_id_policy_ != x.has_auto_id_policy_: return 0\\n    if self.has_auto_id_policy_ and self.auto_id_policy_ != x.auto_id_policy_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += 1 * len(self.entity_)\\n    for i in xrange(len(self.entity_)): n += self.lengthString(self.entity_[i].ByteSize())\\n    if (self.has_transaction_): n += 1 + self.lengthString(self.transaction_.ByteSize())\\n    n += 1 * len(self.composite_index_)\\n    for i in xrange(len(self.composite_index_)): n += self.lengthString(self.composite_index_[i].ByteSize())\\n    if (self.has_trusted_): n += 2\\n    if (self.has_force_): n += 2\\n    if (self.has_mark_changes_): n += 2\\n    n += 1 * len(self.snapshot_)\\n    for i in xrange(len(self.snapshot_)): n += self.lengthString(self.snapshot_[i].ByteSize())\\n    if (self.has_auto_id_policy_): n += 1 + self.lengthVarInt64(self.auto_id_policy_)\\n    return n', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_entity()\\n    self.clear_transaction()\\n    self.clear_composite_index()\\n    self.clear_trusted()\\n    self.clear_force()\\n    self.clear_mark_changes()\\n    self.clear_snapshot()\\n    self.clear_auto_id_policy()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.entity_)):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.entity_[i].ByteSizePartial())\\n      self.entity_[i].OutputPartial(out)\\n    if (self.has_transaction_):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.transaction_.ByteSizePartial())\\n      self.transaction_.OutputPartial(out)\\n    for i in xrange(len(self.composite_index_)):\\n      out.putVarInt32(26)\\n      out.putVarInt32(self.composite_index_[i].ByteSizePartial())\\n      self.composite_index_[i].OutputPartial(out)\\n    if (self.has_trusted_):\\n      out.putVarInt32(32)\\n      out.putBoolean(self.trusted_)\\n    if (self.has_force_):\\n      out.putVarInt32(56)\\n      out.putBoolean(self.force_)\\n    if (self.has_mark_changes_):\\n      out.putVarInt32(64)\\n      out.putBoolean(self.mark_changes_)\\n    for i in xrange(len(self.snapshot_)):\\n      out.putVarInt32(74)\\n      out.putVarInt32(self.snapshot_[i].ByteSizePartial())\\n      self.snapshot_[i].OutputPartial(out)\\n    if (self.has_auto_id_policy_):\\n      out.putVarInt32(80)\\n      out.putVarInt32(self.auto_id_policy_)\\n    if (self.has_header_):\\n      out.putVarInt32(90)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.entity_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"entity%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_transaction_:\\n      res+=prefix+\"transaction <\\\\n\"\\n      res+=self.transaction_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.composite_index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"composite_index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_trusted_: res+=prefix+(\"trusted: %s\\\\n\" % self.DebugFormatBool(self.trusted_))\\n    if self.has_force_: res+=prefix+(\"force: %s\\\\n\" % self.DebugFormatBool(self.force_))\\n    if self.has_mark_changes_: res+=prefix+(\"mark_changes: %s\\\\n\" % self.DebugFormatBool(self.mark_changes_))\\n    cnt=0\\n    for e in self.snapshot_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"snapshot%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_auto_id_policy_: res+=prefix+(\"auto_id_policy: %s\\\\n\" % self.DebugFormatInt32(self.auto_id_policy_))\\n    return res', 'def __init__(self, contents=None):\\n    self.key_ = []\\n    self.version_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def key_list(self): return self.key_', 'def mutable_key(self, i):\\n    return self.key_[i]', 'def clear_key(self):\\n    self.key_ = []', 'def mutable_cost(self): self.has_cost_ = 1; return self.cost()', 'def has_cost(self): return self.has_cost_', 'def version_list(self): return self.version_', 'def set_version(self, i, x):\\n    self.version_[i] = x', 'def clear_version(self):\\n    self.version_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if len(self.key_) != len(x.key_): return 0\\n    for e1, e2 in zip(self.key_, x.key_):\\n      if e1 != e2: return 0\\n    if self.has_cost_ != x.has_cost_: return 0\\n    if self.has_cost_ and self.cost_ != x.cost_: return 0\\n    if len(self.version_) != len(x.version_): return 0\\n    for e1, e2 in zip(self.version_, x.version_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += 1 * len(self.key_)\\n    for i in xrange(len(self.key_)): n += self.lengthString(self.key_[i].ByteSize())\\n    if (self.has_cost_): n += 1 + self.lengthString(self.cost_.ByteSize())\\n    n += 1 * len(self.version_)\\n    for i in xrange(len(self.version_)): n += self.lengthVarInt64(self.version_[i])\\n    return n', 'def Clear(self):\\n    self.clear_key()\\n    self.clear_cost()\\n    self.clear_version()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.key_)):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.key_[i].ByteSizePartial())\\n      self.key_[i].OutputPartial(out)\\n    if (self.has_cost_):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.cost_.ByteSizePartial())\\n      self.cost_.OutputPartial(out)\\n    for i in xrange(len(self.version_)):\\n      out.putVarInt32(24)\\n      out.putVarInt64(self.version_[i])', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    cnt=0\\n    for e in self.key_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"key%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_cost_:\\n      res+=prefix+\"cost <\\\\n\"\\n      res+=self.cost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.version_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"version%s: %s\\\\n\" % (elm, self.DebugFormatInt64(e)))\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    self.key_ = []\\n    self.composite_index_ = []\\n    self.snapshot_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def key_list(self): return self.key_', 'def mutable_key(self, i):\\n    return self.key_[i]', 'def clear_key(self):\\n    self.key_ = []', 'def composite_index_list(self): return self.composite_index_', 'def mutable_composite_index(self, i):\\n    return self.composite_index_[i]', 'def clear_composite_index(self):\\n    self.composite_index_ = []', 'def set_force(self, x):\\n    self.has_force_ = 1\\n    self.force_ = x', 'def has_force(self): return self.has_force_', 'def snapshot_list(self): return self.snapshot_', 'def mutable_snapshot(self, i):\\n    return self.snapshot_[i]', 'def clear_snapshot(self):\\n    self.snapshot_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if len(self.key_) != len(x.key_): return 0\\n    for e1, e2 in zip(self.key_, x.key_):\\n      if e1 != e2: return 0\\n    if len(self.composite_index_) != len(x.composite_index_): return 0\\n    for e1, e2 in zip(self.composite_index_, x.composite_index_):\\n      if e1 != e2: return 0\\n    if self.has_force_ != x.has_force_: return 0\\n    if self.has_force_ and self.force_ != x.force_: return 0\\n    if len(self.snapshot_) != len(x.snapshot_): return 0\\n    for e1, e2 in zip(self.snapshot_, x.snapshot_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += 1 * len(self.key_)\\n    for i in xrange(len(self.key_)): n += self.lengthString(self.key_[i].ByteSize())\\n    n += 1 * len(self.composite_index_)\\n    for i in xrange(len(self.composite_index_)): n += self.lengthString(self.composite_index_[i].ByteSize())\\n    if (self.has_force_): n += 2\\n    n += 1 * len(self.snapshot_)\\n    for i in xrange(len(self.snapshot_)): n += self.lengthString(self.snapshot_[i].ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_key()\\n    self.clear_composite_index()\\n    self.clear_force()\\n    self.clear_snapshot()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.key_)):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.key_[i].ByteSizePartial())\\n      self.key_[i].OutputPartial(out)\\n    for i in xrange(len(self.composite_index_)):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.composite_index_[i].ByteSizePartial())\\n      self.composite_index_[i].OutputPartial(out)\\n    if (self.has_force_):\\n      out.putVarInt32(24)\\n      out.putBoolean(self.force_)\\n    for i in xrange(len(self.snapshot_)):\\n      out.putVarInt32(74)\\n      out.putVarInt32(self.snapshot_[i].ByteSizePartial())\\n      self.snapshot_[i].OutputPartial(out)\\n    if (self.has_header_):\\n      out.putVarInt32(82)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.key_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"key%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    cnt=0\\n    for e in self.composite_index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"composite_index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_force_: res+=prefix+(\"force: %s\\\\n\" % self.DebugFormatBool(self.force_))\\n    cnt=0\\n    for e in self.snapshot_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"snapshot%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_cost(self): self.has_cost_ = 1; return self.cost()', 'def has_cost(self): return self.has_cost_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_cost_ != x.has_cost_: return 0\\n    if self.has_cost_ and self.cost_ != x.cost_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_cost_): n += 1 + self.lengthString(self.cost_.ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_cost()', 'def OutputPartial(self, out):\\n    if (self.has_cost_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.cost_.ByteSizePartial())\\n      self.cost_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_cost_:\\n      res+=prefix+\"cost <\\\\n\"\\n      res+=self.cost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    self.key_ = []\\n    self.composite_index_ = []\\n    self.snapshot_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def key_list(self): return self.key_', 'def mutable_key(self, i):\\n    return self.key_[i]', 'def clear_key(self):\\n    self.key_ = []', 'def mutable_transaction(self): self.has_transaction_ = 1; return self.transaction()', 'def has_transaction(self): return self.has_transaction_', 'def composite_index_list(self): return self.composite_index_', 'def mutable_composite_index(self, i):\\n    return self.composite_index_[i]', 'def clear_composite_index(self):\\n    self.composite_index_ = []', 'def set_trusted(self, x):\\n    self.has_trusted_ = 1\\n    self.trusted_ = x', 'def has_trusted(self): return self.has_trusted_', 'def set_force(self, x):\\n    self.has_force_ = 1\\n    self.force_ = x', 'def has_force(self): return self.has_force_', 'def set_mark_changes(self, x):\\n    self.has_mark_changes_ = 1\\n    self.mark_changes_ = x', 'def has_mark_changes(self): return self.has_mark_changes_', 'def snapshot_list(self): return self.snapshot_', 'def mutable_snapshot(self, i):\\n    return self.snapshot_[i]', 'def clear_snapshot(self):\\n    self.snapshot_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if len(self.key_) != len(x.key_): return 0\\n    for e1, e2 in zip(self.key_, x.key_):\\n      if e1 != e2: return 0\\n    if self.has_transaction_ != x.has_transaction_: return 0\\n    if self.has_transaction_ and self.transaction_ != x.transaction_: return 0\\n    if len(self.composite_index_) != len(x.composite_index_): return 0\\n    for e1, e2 in zip(self.composite_index_, x.composite_index_):\\n      if e1 != e2: return 0\\n    if self.has_trusted_ != x.has_trusted_: return 0\\n    if self.has_trusted_ and self.trusted_ != x.trusted_: return 0\\n    if self.has_force_ != x.has_force_: return 0\\n    if self.has_force_ and self.force_ != x.force_: return 0\\n    if self.has_mark_changes_ != x.has_mark_changes_: return 0\\n    if self.has_mark_changes_ and self.mark_changes_ != x.mark_changes_: return 0\\n    if len(self.snapshot_) != len(x.snapshot_): return 0\\n    for e1, e2 in zip(self.snapshot_, x.snapshot_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += 1 * len(self.key_)\\n    for i in xrange(len(self.key_)): n += self.lengthString(self.key_[i].ByteSize())\\n    if (self.has_transaction_): n += 1 + self.lengthString(self.transaction_.ByteSize())\\n    n += 1 * len(self.composite_index_)\\n    for i in xrange(len(self.composite_index_)): n += self.lengthString(self.composite_index_[i].ByteSize())\\n    if (self.has_trusted_): n += 2\\n    if (self.has_force_): n += 2\\n    if (self.has_mark_changes_): n += 2\\n    n += 1 * len(self.snapshot_)\\n    for i in xrange(len(self.snapshot_)): n += self.lengthString(self.snapshot_[i].ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_key()\\n    self.clear_transaction()\\n    self.clear_composite_index()\\n    self.clear_trusted()\\n    self.clear_force()\\n    self.clear_mark_changes()\\n    self.clear_snapshot()', 'def OutputPartial(self, out):\\n    if (self.has_trusted_):\\n      out.putVarInt32(32)\\n      out.putBoolean(self.trusted_)\\n    if (self.has_transaction_):\\n      out.putVarInt32(42)\\n      out.putVarInt32(self.transaction_.ByteSizePartial())\\n      self.transaction_.OutputPartial(out)\\n    for i in xrange(len(self.key_)):\\n      out.putVarInt32(50)\\n      out.putVarInt32(self.key_[i].ByteSizePartial())\\n      self.key_[i].OutputPartial(out)\\n    if (self.has_force_):\\n      out.putVarInt32(56)\\n      out.putBoolean(self.force_)\\n    if (self.has_mark_changes_):\\n      out.putVarInt32(64)\\n      out.putBoolean(self.mark_changes_)\\n    for i in xrange(len(self.snapshot_)):\\n      out.putVarInt32(74)\\n      out.putVarInt32(self.snapshot_[i].ByteSizePartial())\\n      self.snapshot_[i].OutputPartial(out)\\n    if (self.has_header_):\\n      out.putVarInt32(82)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)\\n    for i in xrange(len(self.composite_index_)):\\n      out.putVarInt32(90)\\n      out.putVarInt32(self.composite_index_[i].ByteSizePartial())\\n      self.composite_index_[i].OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.key_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"key%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_transaction_:\\n      res+=prefix+\"transaction <\\\\n\"\\n      res+=self.transaction_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.composite_index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"composite_index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_trusted_: res+=prefix+(\"trusted: %s\\\\n\" % self.DebugFormatBool(self.trusted_))\\n    if self.has_force_: res+=prefix+(\"force: %s\\\\n\" % self.DebugFormatBool(self.force_))\\n    if self.has_mark_changes_: res+=prefix+(\"mark_changes: %s\\\\n\" % self.DebugFormatBool(self.mark_changes_))\\n    cnt=0\\n    for e in self.snapshot_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"snapshot%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    self.version_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_cost(self): self.has_cost_ = 1; return self.cost()', 'def has_cost(self): return self.has_cost_', 'def version_list(self): return self.version_', 'def set_version(self, i, x):\\n    self.version_[i] = x', 'def clear_version(self):\\n    self.version_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_cost_ != x.has_cost_: return 0\\n    if self.has_cost_ and self.cost_ != x.cost_: return 0\\n    if len(self.version_) != len(x.version_): return 0\\n    for e1, e2 in zip(self.version_, x.version_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_cost_): n += 1 + self.lengthString(self.cost_.ByteSize())\\n    n += 1 * len(self.version_)\\n    for i in xrange(len(self.version_)): n += self.lengthVarInt64(self.version_[i])\\n    return n', 'def Clear(self):\\n    self.clear_cost()\\n    self.clear_version()', 'def OutputPartial(self, out):\\n    if (self.has_cost_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.cost_.ByteSizePartial())\\n      self.cost_.OutputPartial(out)\\n    for i in xrange(len(self.version_)):\\n      out.putVarInt32(24)\\n      out.putVarInt64(self.version_[i])', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_cost_:\\n      res+=prefix+\"cost <\\\\n\"\\n      res+=self.cost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.version_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"version%s: %s\\\\n\" % (elm, self.DebugFormatInt64(e)))\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    self.cursor_ = Cursor()\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def mutable_cursor(self): self.has_cursor_ = 1; return self.cursor_', 'def has_cursor(self): return self.has_cursor_', 'def set_count(self, x):\\n    self.has_count_ = 1\\n    self.count_ = x', 'def has_count(self): return self.has_count_', 'def set_offset(self, x):\\n    self.has_offset_ = 1\\n    self.offset_ = x', 'def has_offset(self): return self.has_offset_', 'def set_compile(self, x):\\n    self.has_compile_ = 1\\n    self.compile_ = x', 'def has_compile(self): return self.has_compile_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_cursor_ != x.has_cursor_: return 0\\n    if self.has_cursor_ and self.cursor_ != x.cursor_: return 0\\n    if self.has_count_ != x.has_count_: return 0\\n    if self.has_count_ and self.count_ != x.count_: return 0\\n    if self.has_offset_ != x.has_offset_: return 0\\n    if self.has_offset_ and self.offset_ != x.offset_: return 0\\n    if self.has_compile_ != x.has_compile_: return 0\\n    if self.has_compile_ and self.compile_ != x.compile_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += self.lengthString(self.cursor_.ByteSize())\\n    if (self.has_count_): n += 1 + self.lengthVarInt64(self.count_)\\n    if (self.has_offset_): n += 1 + self.lengthVarInt64(self.offset_)\\n    if (self.has_compile_): n += 2\\n    return n + 1', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_cursor()\\n    self.clear_count()\\n    self.clear_offset()\\n    self.clear_compile()', 'def OutputPartial(self, out):\\n    if (self.has_cursor_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.cursor_.ByteSizePartial())\\n      self.cursor_.OutputPartial(out)\\n    if (self.has_count_):\\n      out.putVarInt32(16)\\n      out.putVarInt32(self.count_)\\n    if (self.has_compile_):\\n      out.putVarInt32(24)\\n      out.putBoolean(self.compile_)\\n    if (self.has_offset_):\\n      out.putVarInt32(32)\\n      out.putVarInt32(self.offset_)\\n    if (self.has_header_):\\n      out.putVarInt32(42)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_cursor_:\\n      res+=prefix+\"cursor <\\\\n\"\\n      res+=self.cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_count_: res+=prefix+(\"count: %s\\\\n\" % self.DebugFormatInt32(self.count_))\\n    if self.has_offset_: res+=prefix+(\"offset: %s\\\\n\" % self.DebugFormatInt32(self.offset_))\\n    if self.has_compile_: res+=prefix+(\"compile: %s\\\\n\" % self.DebugFormatBool(self.compile_))\\n    return res', 'def __init__(self, contents=None):\\n    self.result_ = []\\n    self.index_ = []\\n    self.version_ = []\\n    self.result_compiled_cursor_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_cursor(self): self.has_cursor_ = 1; return self.cursor()', 'def has_cursor(self): return self.has_cursor_', 'def result_list(self): return self.result_', 'def mutable_result(self, i):\\n    return self.result_[i]', 'def clear_result(self):\\n    self.result_ = []', 'def set_skipped_results(self, x):\\n    self.has_skipped_results_ = 1\\n    self.skipped_results_ = x', 'def has_skipped_results(self): return self.has_skipped_results_', 'def set_more_results(self, x):\\n    self.has_more_results_ = 1\\n    self.more_results_ = x', 'def has_more_results(self): return self.has_more_results_', 'def set_keys_only(self, x):\\n    self.has_keys_only_ = 1\\n    self.keys_only_ = x', 'def has_keys_only(self): return self.has_keys_only_', 'def set_index_only(self, x):\\n    self.has_index_only_ = 1\\n    self.index_only_ = x', 'def has_index_only(self): return self.has_index_only_', 'def set_small_ops(self, x):\\n    self.has_small_ops_ = 1\\n    self.small_ops_ = x', 'def has_small_ops(self): return self.has_small_ops_', 'def mutable_compiled_query(self): self.has_compiled_query_ = 1; return self.compiled_query()', 'def has_compiled_query(self): return self.has_compiled_query_', 'def mutable_compiled_cursor(self): self.has_compiled_cursor_ = 1; return self.compiled_cursor()', 'def has_compiled_cursor(self): return self.has_compiled_cursor_', 'def index_list(self): return self.index_', 'def mutable_index(self, i):\\n    return self.index_[i]', 'def clear_index(self):\\n    self.index_ = []', 'def version_list(self): return self.version_', 'def set_version(self, i, x):\\n    self.version_[i] = x', 'def clear_version(self):\\n    self.version_ = []', 'def result_compiled_cursor_list(self): return self.result_compiled_cursor_', 'def mutable_result_compiled_cursor(self, i):\\n    return self.result_compiled_cursor_[i]', 'def clear_result_compiled_cursor(self):\\n    self.result_compiled_cursor_ = []', 'def mutable_skipped_results_compiled_cursor(self): self.has_skipped_results_compiled_cursor_ = 1; return self.skipped_results_compiled_cursor()', 'def has_skipped_results_compiled_cursor(self): return self.has_skipped_results_compiled_cursor_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_cursor_ != x.has_cursor_: return 0\\n    if self.has_cursor_ and self.cursor_ != x.cursor_: return 0\\n    if len(self.result_) != len(x.result_): return 0\\n    for e1, e2 in zip(self.result_, x.result_):\\n      if e1 != e2: return 0\\n    if self.has_skipped_results_ != x.has_skipped_results_: return 0\\n    if self.has_skipped_results_ and self.skipped_results_ != x.skipped_results_: return 0\\n    if self.has_more_results_ != x.has_more_results_: return 0\\n    if self.has_more_results_ and self.more_results_ != x.more_results_: return 0\\n    if self.has_keys_only_ != x.has_keys_only_: return 0\\n    if self.has_keys_only_ and self.keys_only_ != x.keys_only_: return 0\\n    if self.has_index_only_ != x.has_index_only_: return 0\\n    if self.has_index_only_ and self.index_only_ != x.index_only_: return 0\\n    if self.has_small_ops_ != x.has_small_ops_: return 0\\n    if self.has_small_ops_ and self.small_ops_ != x.small_ops_: return 0\\n    if self.has_compiled_query_ != x.has_compiled_query_: return 0\\n    if self.has_compiled_query_ and self.compiled_query_ != x.compiled_query_: return 0\\n    if self.has_compiled_cursor_ != x.has_compiled_cursor_: return 0\\n    if self.has_compiled_cursor_ and self.compiled_cursor_ != x.compiled_cursor_: return 0\\n    if len(self.index_) != len(x.index_): return 0\\n    for e1, e2 in zip(self.index_, x.index_):\\n      if e1 != e2: return 0\\n    if len(self.version_) != len(x.version_): return 0\\n    for e1, e2 in zip(self.version_, x.version_):\\n      if e1 != e2: return 0\\n    if len(self.result_compiled_cursor_) != len(x.result_compiled_cursor_): return 0\\n    for e1, e2 in zip(self.result_compiled_cursor_, x.result_compiled_cursor_):\\n      if e1 != e2: return 0\\n    if self.has_skipped_results_compiled_cursor_ != x.has_skipped_results_compiled_cursor_: return 0\\n    if self.has_skipped_results_compiled_cursor_ and self.skipped_results_compiled_cursor_ != x.skipped_results_compiled_cursor_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_cursor_): n += 1 + self.lengthString(self.cursor_.ByteSize())\\n    n += 1 * len(self.result_)\\n    for i in xrange(len(self.result_)): n += self.lengthString(self.result_[i].ByteSize())\\n    if (self.has_skipped_results_): n += 1 + self.lengthVarInt64(self.skipped_results_)\\n    if (self.has_keys_only_): n += 2\\n    if (self.has_index_only_): n += 2\\n    if (self.has_small_ops_): n += 2\\n    if (self.has_compiled_query_): n += 1 + self.lengthString(self.compiled_query_.ByteSize())\\n    if (self.has_compiled_cursor_): n += 1 + self.lengthString(self.compiled_cursor_.ByteSize())\\n    n += 1 * len(self.index_)\\n    for i in xrange(len(self.index_)): n += self.lengthString(self.index_[i].ByteSize())\\n    n += 1 * len(self.version_)\\n    for i in xrange(len(self.version_)): n += self.lengthVarInt64(self.version_[i])\\n    n += 1 * len(self.result_compiled_cursor_)\\n    for i in xrange(len(self.result_compiled_cursor_)): n += self.lengthString(self.result_compiled_cursor_[i].ByteSize())\\n    if (self.has_skipped_results_compiled_cursor_): n += 1 + self.lengthString(self.skipped_results_compiled_cursor_.ByteSize())\\n    return n + 2', 'def Clear(self):\\n    self.clear_cursor()\\n    self.clear_result()\\n    self.clear_skipped_results()\\n    self.clear_more_results()\\n    self.clear_keys_only()\\n    self.clear_index_only()\\n    self.clear_small_ops()\\n    self.clear_compiled_query()\\n    self.clear_compiled_cursor()\\n    self.clear_index()\\n    self.clear_version()\\n    self.clear_result_compiled_cursor()\\n    self.clear_skipped_results_compiled_cursor()', 'def OutputPartial(self, out):\\n    if (self.has_cursor_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.cursor_.ByteSizePartial())\\n      self.cursor_.OutputPartial(out)\\n    for i in xrange(len(self.result_)):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.result_[i].ByteSizePartial())\\n      self.result_[i].OutputPartial(out)\\n    if (self.has_more_results_):\\n      out.putVarInt32(24)\\n      out.putBoolean(self.more_results_)\\n    if (self.has_keys_only_):\\n      out.putVarInt32(32)\\n      out.putBoolean(self.keys_only_)\\n    if (self.has_compiled_query_):\\n      out.putVarInt32(42)\\n      out.putVarInt32(self.compiled_query_.ByteSizePartial())\\n      self.compiled_query_.OutputPartial(out)\\n    if (self.has_compiled_cursor_):\\n      out.putVarInt32(50)\\n      out.putVarInt32(self.compiled_cursor_.ByteSizePartial())\\n      self.compiled_cursor_.OutputPartial(out)\\n    if (self.has_skipped_results_):\\n      out.putVarInt32(56)\\n      out.putVarInt32(self.skipped_results_)\\n    for i in xrange(len(self.index_)):\\n      out.putVarInt32(66)\\n      out.putVarInt32(self.index_[i].ByteSizePartial())\\n      self.index_[i].OutputPartial(out)\\n    if (self.has_index_only_):\\n      out.putVarInt32(72)\\n      out.putBoolean(self.index_only_)\\n    if (self.has_small_ops_):\\n      out.putVarInt32(80)\\n      out.putBoolean(self.small_ops_)\\n    for i in xrange(len(self.version_)):\\n      out.putVarInt32(88)\\n      out.putVarInt64(self.version_[i])\\n    for i in xrange(len(self.result_compiled_cursor_)):\\n      out.putVarInt32(98)\\n      out.putVarInt32(self.result_compiled_cursor_[i].ByteSizePartial())\\n      self.result_compiled_cursor_[i].OutputPartial(out)\\n    if (self.has_skipped_results_compiled_cursor_):\\n      out.putVarInt32(106)\\n      out.putVarInt32(self.skipped_results_compiled_cursor_.ByteSizePartial())\\n      self.skipped_results_compiled_cursor_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_cursor_:\\n      res+=prefix+\"cursor <\\\\n\"\\n      res+=self.cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.result_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"result%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_skipped_results_: res+=prefix+(\"skipped_results: %s\\\\n\" % self.DebugFormatInt32(self.skipped_results_))\\n    if self.has_more_results_: res+=prefix+(\"more_results: %s\\\\n\" % self.DebugFormatBool(self.more_results_))\\n    if self.has_keys_only_: res+=prefix+(\"keys_only: %s\\\\n\" % self.DebugFormatBool(self.keys_only_))\\n    if self.has_index_only_: res+=prefix+(\"index_only: %s\\\\n\" % self.DebugFormatBool(self.index_only_))\\n    if self.has_small_ops_: res+=prefix+(\"small_ops: %s\\\\n\" % self.DebugFormatBool(self.small_ops_))\\n    if self.has_compiled_query_:\\n      res+=prefix+\"compiled_query <\\\\n\"\\n      res+=self.compiled_query_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_compiled_cursor_:\\n      res+=prefix+\"compiled_cursor <\\\\n\"\\n      res+=self.compiled_cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    cnt=0\\n    for e in self.version_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"version%s: %s\\\\n\" % (elm, self.DebugFormatInt64(e)))\\n      cnt+=1\\n    cnt=0\\n    for e in self.result_compiled_cursor_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"result_compiled_cursor%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_skipped_results_compiled_cursor_:\\n      res+=prefix+\"skipped_results_compiled_cursor <\\\\n\"\\n      res+=self.skipped_results_compiled_cursor_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    self.reserve_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def mutable_model_key(self): self.has_model_key_ = 1; return self.model_key()', 'def has_model_key(self): return self.has_model_key_', 'def set_size(self, x):\\n    self.has_size_ = 1\\n    self.size_ = x', 'def has_size(self): return self.has_size_', 'def set_max(self, x):\\n    self.has_max_ = 1\\n    self.max_ = x', 'def has_max(self): return self.has_max_', 'def reserve_list(self): return self.reserve_', 'def mutable_reserve(self, i):\\n    return self.reserve_[i]', 'def clear_reserve(self):\\n    self.reserve_ = []', 'def set_trusted(self, x):\\n    self.has_trusted_ = 1\\n    self.trusted_ = x', 'def has_trusted(self): return self.has_trusted_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_model_key_ != x.has_model_key_: return 0\\n    if self.has_model_key_ and self.model_key_ != x.model_key_: return 0\\n    if self.has_size_ != x.has_size_: return 0\\n    if self.has_size_ and self.size_ != x.size_: return 0\\n    if self.has_max_ != x.has_max_: return 0\\n    if self.has_max_ and self.max_ != x.max_: return 0\\n    if len(self.reserve_) != len(x.reserve_): return 0\\n    for e1, e2 in zip(self.reserve_, x.reserve_):\\n      if e1 != e2: return 0\\n    if self.has_trusted_ != x.has_trusted_: return 0\\n    if self.has_trusted_ and self.trusted_ != x.trusted_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    if (self.has_model_key_): n += 1 + self.lengthString(self.model_key_.ByteSize())\\n    if (self.has_size_): n += 1 + self.lengthVarInt64(self.size_)\\n    if (self.has_max_): n += 1 + self.lengthVarInt64(self.max_)\\n    n += 1 * len(self.reserve_)\\n    for i in xrange(len(self.reserve_)): n += self.lengthString(self.reserve_[i].ByteSize())\\n    if (self.has_trusted_): n += 2\\n    return n', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_model_key()\\n    self.clear_size()\\n    self.clear_max()\\n    self.clear_reserve()\\n    self.clear_trusted()', 'def OutputPartial(self, out):\\n    if (self.has_model_key_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.model_key_.ByteSizePartial())\\n      self.model_key_.OutputPartial(out)\\n    if (self.has_size_):\\n      out.putVarInt32(16)\\n      out.putVarInt64(self.size_)\\n    if (self.has_max_):\\n      out.putVarInt32(24)\\n      out.putVarInt64(self.max_)\\n    if (self.has_header_):\\n      out.putVarInt32(34)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)\\n    for i in xrange(len(self.reserve_)):\\n      out.putVarInt32(42)\\n      out.putVarInt32(self.reserve_[i].ByteSizePartial())\\n      self.reserve_[i].OutputPartial(out)\\n    if (self.has_trusted_):\\n      out.putVarInt32(48)\\n      out.putBoolean(self.trusted_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_model_key_:\\n      res+=prefix+\"model_key <\\\\n\"\\n      res+=self.model_key_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_size_: res+=prefix+(\"size: %s\\\\n\" % self.DebugFormatInt64(self.size_))\\n    if self.has_max_: res+=prefix+(\"max: %s\\\\n\" % self.DebugFormatInt64(self.max_))\\n    cnt=0\\n    for e in self.reserve_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"reserve%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    if self.has_trusted_: res+=prefix+(\"trusted: %s\\\\n\" % self.DebugFormatBool(self.trusted_))\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def set_start(self, x):\\n    self.has_start_ = 1\\n    self.start_ = x', 'def has_start(self): return self.has_start_', 'def set_end(self, x):\\n    self.has_end_ = 1\\n    self.end_ = x', 'def has_end(self): return self.has_end_', 'def mutable_cost(self): self.has_cost_ = 1; return self.cost()', 'def has_cost(self): return self.has_cost_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_start_ != x.has_start_: return 0\\n    if self.has_start_ and self.start_ != x.start_: return 0\\n    if self.has_end_ != x.has_end_: return 0\\n    if self.has_end_ and self.end_ != x.end_: return 0\\n    if self.has_cost_ != x.has_cost_: return 0\\n    if self.has_cost_ and self.cost_ != x.cost_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.lengthVarInt64(self.start_)\\n    n += self.lengthVarInt64(self.end_)\\n    if (self.has_cost_): n += 1 + self.lengthString(self.cost_.ByteSize())\\n    return n + 2', 'def Clear(self):\\n    self.clear_start()\\n    self.clear_end()\\n    self.clear_cost()', 'def OutputPartial(self, out):\\n    if (self.has_start_):\\n      out.putVarInt32(8)\\n      out.putVarInt64(self.start_)\\n    if (self.has_end_):\\n      out.putVarInt32(16)\\n      out.putVarInt64(self.end_)\\n    if (self.has_cost_):\\n      out.putVarInt32(26)\\n      out.putVarInt32(self.cost_.ByteSizePartial())\\n      self.cost_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_start_: res+=prefix+(\"start: %s\\\\n\" % self.DebugFormatInt64(self.start_))\\n    if self.has_end_: res+=prefix+(\"end: %s\\\\n\" % self.DebugFormatInt64(self.end_))\\n    if self.has_cost_:\\n      res+=prefix+\"cost <\\\\n\"\\n      res+=self.cost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    return res', 'def __init__(self, contents=None):\\n    self.index_ = []\\n    if contents is not None: self.MergeFromString(contents)', 'def index_list(self): return self.index_', 'def mutable_index(self, i):\\n    return self.index_[i]', 'def clear_index(self):\\n    self.index_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if len(self.index_) != len(x.index_): return 0\\n    for e1, e2 in zip(self.index_, x.index_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += 1 * len(self.index_)\\n    for i in xrange(len(self.index_)): n += self.lengthString(self.index_[i].ByteSize())\\n    return n', 'def Clear(self):\\n    self.clear_index()', 'def OutputPartial(self, out):\\n    for i in xrange(len(self.index_)):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.index_[i].ByteSizePartial())\\n      self.index_[i].OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    cnt=0\\n    for e in self.index_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"index%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    self.transaction_ = Transaction()\\n    self.action_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def mutable_transaction(self): self.has_transaction_ = 1; return self.transaction_', 'def has_transaction(self): return self.has_transaction_', 'def action_list(self): return self.action_', 'def mutable_action(self, i):\\n    return self.action_[i]', 'def clear_action(self):\\n    self.action_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_transaction_ != x.has_transaction_: return 0\\n    if self.has_transaction_ and self.transaction_ != x.transaction_: return 0\\n    if len(self.action_) != len(x.action_): return 0\\n    for e1, e2 in zip(self.action_, x.action_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += self.lengthString(self.transaction_.ByteSize())\\n    n += 1 * len(self.action_)\\n    for i in xrange(len(self.action_)): n += self.lengthString(self.action_[i].ByteSize())\\n    return n + 1', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_transaction()\\n    self.clear_action()', 'def OutputPartial(self, out):\\n    if (self.has_transaction_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.transaction_.ByteSizePartial())\\n      self.transaction_.OutputPartial(out)\\n    for i in xrange(len(self.action_)):\\n      out.putVarInt32(18)\\n      out.putVarInt32(self.action_[i].ByteSizePartial())\\n      self.action_[i].OutputPartial(out)\\n    if (self.has_header_):\\n      out.putVarInt32(26)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_transaction_:\\n      res+=prefix+\"transaction <\\\\n\"\\n      res+=self.transaction_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.action_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"action%s <\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n      cnt+=1\\n    return res', 'def __init__(self, contents=None):\\n    pass\\n    if contents is not None: self.MergeFromString(contents)', 'def Equals(self, x):\\n    if x is self: return 1\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    return n', 'def Clear(self):\\n    pass', 'def OutputPartial(self, out):\\n    pass', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    return res', 'def __init__(self, contents=None):\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_header(self): self.has_header_ = 1; return self.header()', 'def has_header(self): return self.has_header_', 'def set_app(self, x):\\n    self.has_app_ = 1\\n    self.app_ = x', 'def has_app(self): return self.has_app_', 'def set_allow_multiple_eg(self, x):\\n    self.has_allow_multiple_eg_ = 1\\n    self.allow_multiple_eg_ = x', 'def has_allow_multiple_eg(self): return self.has_allow_multiple_eg_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_header_ != x.has_header_: return 0\\n    if self.has_header_ and self.header_ != x.header_: return 0\\n    if self.has_app_ != x.has_app_: return 0\\n    if self.has_app_ and self.app_ != x.app_: return 0\\n    if self.has_allow_multiple_eg_ != x.has_allow_multiple_eg_: return 0\\n    if self.has_allow_multiple_eg_ and self.allow_multiple_eg_ != x.allow_multiple_eg_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_header_): n += 1 + self.lengthString(self.header_.ByteSize())\\n    n += self.lengthString(len(self.app_))\\n    if (self.has_allow_multiple_eg_): n += 2\\n    return n + 1', 'def Clear(self):\\n    self.clear_header()\\n    self.clear_app()\\n    self.clear_allow_multiple_eg()', 'def OutputPartial(self, out):\\n    if (self.has_app_):\\n      out.putVarInt32(10)\\n      out.putPrefixedString(self.app_)\\n    if (self.has_allow_multiple_eg_):\\n      out.putVarInt32(16)\\n      out.putBoolean(self.allow_multiple_eg_)\\n    if (self.has_header_):\\n      out.putVarInt32(26)\\n      out.putVarInt32(self.header_.ByteSizePartial())\\n      self.header_.OutputPartial(out)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_header_:\\n      res+=prefix+\"header <\\\\n\"\\n      res+=self.header_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_app_: res+=prefix+(\"app: %s\\\\n\" % self.DebugFormatString(self.app_))\\n    if self.has_allow_multiple_eg_: res+=prefix+(\"allow_multiple_eg: %s\\\\n\" % self.DebugFormatBool(self.allow_multiple_eg_))\\n    return res', 'def __init__(self, contents=None):\\n    self.root_entity_key_ = Reference()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_root_entity_key(self): self.has_root_entity_key_ = 1; return self.root_entity_key_', 'def has_root_entity_key(self): return self.has_root_entity_key_', 'def set_version(self, x):\\n    self.has_version_ = 1\\n    self.version_ = x', 'def has_version(self): return self.has_version_', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_root_entity_key_ != x.has_root_entity_key_: return 0\\n    if self.has_root_entity_key_ and self.root_entity_key_ != x.root_entity_key_: return 0\\n    if self.has_version_ != x.has_version_: return 0\\n    if self.has_version_ and self.version_ != x.version_: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    n += self.lengthString(self.root_entity_key_.ByteSize())\\n    n += self.lengthVarInt64(self.version_)\\n    return n + 2', 'def Clear(self):\\n    self.clear_root_entity_key()\\n    self.clear_version()', 'def OutputPartial(self, out):\\n    if (self.has_root_entity_key_):\\n      out.putVarInt32(34)\\n      out.putVarInt32(self.root_entity_key_.ByteSizePartial())\\n      self.root_entity_key_.OutputPartial(out)\\n    if (self.has_version_):\\n      out.putVarInt32(40)\\n      out.putVarInt64(self.version_)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_root_entity_key_:\\n      res+=prefix+\"root_entity_key <\\\\n\"\\n      res+=self.root_entity_key_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    if self.has_version_: res+=prefix+(\"version: %s\\\\n\" % self.DebugFormatInt64(self.version_))\\n    return res', 'def __init__(self, contents=None):\\n    self.version_ = []\\n    self.lazy_init_lock_ = thread.allocate_lock()\\n    if contents is not None: self.MergeFromString(contents)', 'def mutable_cost(self): self.has_cost_ = 1; return self.cost()', 'def has_cost(self): return self.has_cost_', 'def version_list(self): return self.version_', 'def mutable_version(self, i):\\n    return self.version_[i]', 'def clear_version(self):\\n    self.version_ = []', 'def Equals(self, x):\\n    if x is self: return 1\\n    if self.has_cost_ != x.has_cost_: return 0\\n    if self.has_cost_ and self.cost_ != x.cost_: return 0\\n    if len(self.version_) != len(x.version_): return 0\\n    for e1, e2 in zip(self.version_, x.version_):\\n      if e1 != e2: return 0\\n    return 1', 'def ByteSize(self):\\n    n = 0\\n    if (self.has_cost_): n += 1 + self.lengthString(self.cost_.ByteSize())\\n    n += 2 * len(self.version_)\\n    for i in xrange(len(self.version_)): n += self.version_[i].ByteSize()\\n    return n', 'def Clear(self):\\n    self.clear_cost()\\n    self.clear_version()', 'def OutputPartial(self, out):\\n    if (self.has_cost_):\\n      out.putVarInt32(10)\\n      out.putVarInt32(self.cost_.ByteSizePartial())\\n      self.cost_.OutputPartial(out)\\n    for i in xrange(len(self.version_)):\\n      out.putVarInt32(27)\\n      self.version_[i].OutputPartial(out)\\n      out.putVarInt32(28)', 'def __str__(self, prefix=\"\", printElemNumber=0):\\n    res=\"\"\\n    if self.has_cost_:\\n      res+=prefix+\"cost <\\\\n\"\\n      res+=self.cost_.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\">\\\\n\"\\n    cnt=0\\n    for e in self.version_:\\n      elm=\"\"\\n      if printElemNumber: elm=\"(%d)\" % cnt\\n      res+=prefix+(\"Version%s {\\\\n\" % elm)\\n      res+=e.__str__(prefix + \"  \", printElemNumber)\\n      res+=prefix+\"}\\\\n\"\\n      cnt+=1\\n    return res']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(self, width, height):\\r\\n        \"\"\"\\r\\n        Konstruktor planszy do gry. Przygotowuje okienko gry.', 'def draw(self, *args):\\r\\n        \"\"\"\\r\\n        Rysuje okno gry', 'def __init__(self, width, height, cell_size=10):\\r\\n        \"\"\"\\r\\n        Przygotowanie ustawień gry\\r\\n        :param width: szerokość planszy mierzona liczbą komórek\\r\\n        :param height: wysokość planszy mierzona liczbą komórek\\r\\n        :param cell_size: bok komórki w pikselach\\r\\n        \"\"\"\\r\\n        pygame.init()\\r\\n        self.board = Board(width * cell_size, height * cell_size)\\r\\n        # zegar którego użyjemy do kontrolowania szybkości rysowania\\r\\n        # kolejnych klatek gry\\r\\n        self.fps_clock = pygame.time.Clock()', 'def run(self):\\r\\n        \"\"\"\\r\\n        Główna pętla gry\\r\\n        \"\"\"\\r\\n        while not self.handle_events():\\r\\n            # działaj w pętli do momentu otrzymania sygnału do wyjścia\\r\\n            self.board.draw()\\r\\n            self.fps_clock.tick(15)', 'def handle_events(self):\\r\\n        \"\"\"\\r\\n        Obsługa zdarzeń systemowych, tutaj zinterpretujemy np. ruchy myszką', 'def __init__(self, width, height, cell_size=10):\\r\\n        \"\"\"\\r\\n        Przygotowuje ustawienia populacji', 'def reset_generation(self):\\r\\n        \"\"\"\\r\\n        Tworzy i zwraca macierz pustej populacji\\r\\n        \"\"\"\\r\\n        # w pętli wypełnij listę kolumnami\\r\\n        # które także w pętli zostają wypełnione wartością 0 (DEAD)\\r\\n        return [[DEAD for y in xrange(self.height)] for x in xrange(self.width)]', 'def handle_mouse(self):\\r\\n    # pobierz stan guzików myszki z wykorzystaniem funcji pygame\\r\\n    buttons = pygame.mouse.get_pressed()\\r\\n    if not any(buttons):\\r\\n        # ignoruj zdarzenie jeśli żaden z guzików nie jest wciśnięty\\r\\n        return', 'def draw_on(self, surface):\\r\\n    \"\"\"\\r\\n    Rysuje komórki na planszy\\r\\n    \"\"\"\\r\\n    for x, y in self.alive_cells():\\r\\n        size = (self.box_size, self.box_size)\\r\\n        position = (x * self.box_size, y * self.box_size)\\r\\n        color = (255, 255, 255)\\r\\n        thickness = 1\\r\\n        pygame.draw.rect(surface, color, pygame.locals.Rect(position, size), thickness)', 'def alive_cells(self):\\r\\n    \"\"\"\\r\\n    Generator zwracający współrzędne żywych komórek.\\r\\n    \"\"\"\\r\\n    for x in range(len(self.generation)):\\r\\n        column = self.generation[x]\\r\\n        for y in range(len(column)):\\r\\n            if column[y] == ALIVE:\\r\\n                # jeśli komórka jest żywa zwrócimy jej współrzędne\\r\\n                yield x, y']}, {'features': [], 'snippets': ['def _root_dir_default(self):\\n        try:\\n            return self.parent.root_dir\\n        except AttributeError:\\n            return getcwd()', 'def create_checkpoint(self, contents_mgr, path):\\n        \"\"\"Create a checkpoint.\"\"\"\\n        checkpoint_id = u\\'checkpoint\\'\\n        src_path = contents_mgr._get_os_path(path)\\n        dest_path = self.checkpoint_path(checkpoint_id, path)\\n        self._copy(src_path, dest_path)\\n        return self.checkpoint_model(checkpoint_id, dest_path)', 'def rename_checkpoint(self, checkpoint_id, old_path, new_path):\\n        \"\"\"Rename a checkpoint from old_path to new_path.\"\"\"\\n        old_cp_path = self.checkpoint_path(checkpoint_id, old_path)\\n        new_cp_path = self.checkpoint_path(checkpoint_id, new_path)\\n        if os.path.isfile(old_cp_path):\\n            self.log.debug(\\n                \"Renaming checkpoint %s -> %s\",\\n                old_cp_path,\\n                new_cp_path,\\n            )\\n            with self.perm_to_403():\\n                shutil.move(old_cp_path, new_cp_path)', 'def list_checkpoints(self, path):\\n        \"\"\"list the checkpoints for a given file\\n\\n        This contents manager currently only supports one checkpoint per file.\\n        \"\"\"\\n        path = path.strip(\\'/\\')\\n        checkpoint_id = \"checkpoint\"\\n        os_path = self.checkpoint_path(checkpoint_id, path)\\n        if not os.path.isfile(os_path):\\n            return []\\n        else:\\n            return [self.checkpoint_model(checkpoint_id, os_path)]', 'def checkpoint_path(self, checkpoint_id, path):\\n        \"\"\"find the path to a checkpoint\"\"\"\\n        path = path.strip(\\'/\\')\\n        parent, name = (\\'/\\' + path).rsplit(\\'/\\', 1)\\n        parent = parent.strip(\\'/\\')\\n        basename, ext = os.path.splitext(name)\\n        filename = u\"{name}-{checkpoint_id}{ext}\".format(\\n            name=basename,\\n            checkpoint_id=checkpoint_id,\\n            ext=ext,\\n        )\\n        os_path = self._get_os_path(path=parent)\\n        cp_dir = os.path.join(os_path, self.checkpoint_dir)\\n        with self.perm_to_403():\\n            ensure_dir_exists(cp_dir)\\n        cp_path = os.path.join(cp_dir, filename)\\n        return cp_path', \"def no_such_checkpoint(self, path, checkpoint_id):\\n        raise HTTPError(\\n            404,\\n            u'Checkpoint does not exist: %s@%s' % (path, checkpoint_id)\\n        )\", 'def create_file_checkpoint(self, content, format, path):\\n        \"\"\"Create a checkpoint from the current content of a file.\"\"\"\\n        path = path.strip(\\'/\\')\\n        # only the one checkpoint ID:\\n        checkpoint_id = u\"checkpoint\"\\n        os_checkpoint_path = self.checkpoint_path(checkpoint_id, path)\\n        self.log.debug(\"creating checkpoint for %s\", path)\\n        with self.perm_to_403():\\n            self._save_file(os_checkpoint_path, content, format=format)\\n\\n        # return the checkpoint info\\n        return self.checkpoint_model(checkpoint_id, os_checkpoint_path)', 'def get_notebook_checkpoint(self, checkpoint_id, path):\\n        \"\"\"Get a checkpoint for a notebook.\"\"\"\\n        path = path.strip(\\'/\\')\\n        self.log.info(\"restoring %s from checkpoint %s\", path, checkpoint_id)\\n        os_checkpoint_path = self.checkpoint_path(checkpoint_id, path)\\n\\n        if not os.path.isfile(os_checkpoint_path):\\n            self.no_such_checkpoint(path, checkpoint_id)\\n\\n        return {\\n            \\'type\\': \\'notebook\\',\\n            \\'content\\': self._read_notebook(\\n                os_checkpoint_path,\\n                as_version=4,\\n            ),\\n        }']}, {'features': [], 'snippets': ['def get_id(sensorid, feedtag, feedname, feedid, feeduserid):\\n    \"\"\"Return unique identifier for feed / sensor.\"\"\"\\n    return f\"emoncms{sensorid}_{feedtag}_{feedname}_{feedid}_{feeduserid}\"', 'def __init__(\\n        self, hass, data, name, value_template, unit_of_measurement, sensorid, elem', 'def name(self):\\n        \"\"\"Return the name of the sensor.\"\"\"\\n        return self._name', 'def native_unit_of_measurement(self):\\n        \"\"\"Return the unit of measurement of this entity, if any.\"\"\"\\n        return self._unit_of_measurement', 'def native_value(self):\\n        \"\"\"Return the state of the device.\"\"\"\\n        return self._state', 'def extra_state_attributes(self):\\n        \"\"\"Return the attributes of the sensor.\"\"\"\\n        return {\\n            ATTR_FEEDID: self._elem[\"id\"],\\n            ATTR_TAG: self._elem[\"tag\"],\\n            ATTR_FEEDNAME: self._elem[\"name\"],\\n            ATTR_SIZE: self._elem[\"size\"],\\n            ATTR_USERID: self._elem[\"userid\"],\\n            ATTR_LASTUPDATETIME: self._elem[\"time\"],\\n            ATTR_LASTUPDATETIMESTR: template.timestamp_local(float(self._elem[\"time\"])),\\n        }', 'def __init__(self, hass, url, apikey, interval):\\n        \"\"\"Initialize the data object.\"\"\"\\n        self._apikey = apikey\\n        self._url = f\"{url}/feed/list.json\"\\n        self._interval = interval\\n        self._hass = hass\\n        self.data = None']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def __init__(\\n            self,\\n            prefix_dir,\\n            popen=subprocess.Popen,\\n            makedirs=os.makedirs', 'def _create_path_if_not_exists(self):\\n        if not os.path.exists(self.prefix_dir):\\n            self.__makedirs(self.prefix_dir)', 'def path(self, *parts):\\n        path = os.path.join(self.prefix_dir, *parts)\\n        return os.path.normpath(path)']}, {'features': [], 'snippets': ['def setup():\\n    \"\"\"\\n    Install python develop tools\\n    \"\"\"\\n    install()']}, {'features': [], 'snippets': []}, {'features': [], 'snippets': []}, {'features': [], 'snippets': ['def get_words( text ):\\n\\ttext = text.replace( \"\\'\", \"\" )\\n\\ttext = re.sub( r\\'\\\\W+\\', \\' \\', text )\\n\\ttext = text.lower()']}, {'features': [], 'snippets': [\"def __init__(self, fileName='schott_glasses.csv'):\\n        path = os.path.dirname(__file__)\\n        fh = gzip.open(os.path.join(path, 'schott_glasses.csv.gz'), 'rb')\\n        r = csv.reader(map(str, fh.readlines()))\\n        lines = [x for x in r]\\n        self.data = {}\\n        header = lines[0]\\n        for l in lines[1:]:\\n            info = {}\\n            for i in range(1, len(l)):\\n                info[header[i]] = l[i]\\n            self.data[l[0]] = info\\n        self.data['Corning7980'] = {   ## Thorlabs UV fused silica--not in schott catalog.\\n            'B1': 0.68374049400,\\n            'B2': 0.42032361300,\\n            'B3': 0.58502748000,\\n            'C1': 0.00460352869,\\n            'C2': 0.01339688560,\\n            'C3': 64.49327320000,\\n            'TAUI25/250': 0.95,    ## transmission data is fabricated, but close.\\n            'TAUI25/1400': 0.98,\\n        }\", 'def ior(self, glass, wl):\\n        \"\"\"\\n        Return the index of refraction for *glass* at wavelength *wl*.', \"def transmissionCurve(self, glass):\\n        data = self.data[glass]\\n        keys = [int(x[7:]) for x in data.keys() if 'TAUI25' in x]\\n        keys.sort()\\n        curve = np.empty((2,len(keys)))\\n        for i in range(len(keys)):\\n            curve[0][i] = keys[i]\\n            key = 'TAUI25/%d' % keys[i]\\n            val = data[key]\\n            if val == '':\\n                val = 0\\n            else:\\n                val = float(val)\\n            curve[1][i] = val\\n        return curve\", 'def wlPen(wl):\\n    \"\"\"Return a pen representing the given wavelength\"\"\"\\n    l1 = 400\\n    l2 = 700\\n    hue = np.clip(((l2-l1) - (wl-l1)) * 0.8 / (l2-l1), 0, 0.8)\\n    val = 1.0\\n    if wl > 700:\\n        val = 1.0 * (((700-wl)/700.) + 1)\\n    elif wl < 400:\\n        val = wl * 1.0/400.\\n    #print hue, val\\n    color = pg.hsvColor(hue, 1.0, val)\\n    pen = pg.mkPen(color)\\n    return pen', 'def __init__(self):\\n        self.__params = {}', 'def __setitem__(self, item, val):\\n        self.setParam(item, val)', 'def setParam(self, param, val):\\n        self.setParams(**{param:val})', 'def setParams(self, **params):\\n        \"\"\"Set parameters for this optic. This is a good function to override for subclasses.\"\"\"\\n        self.__params.update(params)\\n        self.paramStateChanged()', 'def __getitem__(self, item):\\n        # bug in pyside 1.2.2 causes getitem to be called inside QGraphicsObject.parentItem:\\n        return self.getParam(item)  # PySide bug: https://bugreports.qt.io/browse/PYSIDE-441', 'def __init__(self, gitem, **params):\\n        ParamObj.__init__(self)\\n        pg.GraphicsObject.__init__(self) #, [0,0], [1,1])\\n\\n        self.gitem = gitem\\n        self.surfaces = gitem.surfaces\\n        gitem.setParentItem(self)', \"def updateTransform(self):\\n        self.resetTransform()\\n        self.setPos(0, 0)\\n        self.translate(Point(self['pos']))\\n        self.rotate(self['angle'])\", 'def setParam(self, param, val):\\n        ParamObj.setParam(self, param, val)', 'def roiChanged(self, *args):\\n        pos = self.roi.pos()\\n        # rotate gitem temporarily so we can decide where it will need to move\\n        self.gitem.resetTransform()\\n        self.gitem.rotate(self.roi.angle())\\n        br = self.gitem.boundingRect()\\n        o1 = self.gitem.mapToParent(br.topLeft())\\n        self.setParams(angle=self.roi.angle(), pos=pos + (self.gitem.pos() - o1))', 'def boundingRect(self):\\n        return QtCore.QRectF()', 'def paint(self, p, *args):\\n        pass', \"def __init__(self, **params):\\n        defaults = {\\n            'dia': 25.4,  ## diameter of lens\\n            'r1': 50.,    ## positive means convex, use 0 for planar\\n            'r2': 0,   ## negative means convex\\n            'd': 4.0,\\n            'glass': 'N-BK7',\\n            'reflect': False,\\n        }\\n        defaults.update(params)\\n        d = defaults.pop('d')\\n        defaults['x1'] = -d/2.\\n        defaults['x2'] = d/2.\", 'def propagateRay(self, ray):\\n        \"\"\"Refract, reflect, absorb, and/or scatter ray. This function may create and return new rays\"\"\"\\n\\n        \"\"\"\\n        NOTE:: We can probably use this to compute refractions faster: (from GLSL 120 docs)\\n\\n        For the incident vector I and surface normal N, and the\\n        ratio of indices of refraction eta, return the refraction\\n        vector. The result is computed by\\n        k = 1.0 - eta * eta * (1.0 - dot(N, I) * dot(N, I))\\n        if (k < 0.0)\\n            return genType(0.0)\\n        else\\n            return eta * I - (eta * dot(N, I) + sqrt(k)) * N\\n        The input parameters for the incident vector I and the\\n        surface normal N must already be normalized to get the\\n        desired results. eta == ratio of IORs\\n\\n\\n        For reflection:\\n        For the incident vector I and surface orientation N,\\n        returns the reflection direction:\\n        I – 2 ∗ dot(N, I) ∗ N\\n        N must already be normalized in order to achieve the\\n        desired result.\\n        \"\"\"\\n\\n\\n\\n        iors = [self.ior(ray[\\'wl\\']), 1.0]\\n        for i in [0,1]:\\n            surface = self.surfaces[i]\\n            ior = iors[i]\\n            p1, ai = surface.intersectRay(ray)\\n            #print \"surface intersection:\", p1, ai*180/3.14159\\n            #trans = self.sceneTransform().inverted()[0] * surface.sceneTransform()\\n            #p1 = trans.map(p1)\\n            if p1 is None:\\n                ray.setEnd(None)\\n                break\\n            p1 = surface.mapToItem(ray, p1)', \"def __init__(self, **params):\\n        defaults = {\\n            'r1': 0,\\n            'r2': 0,\\n            'd': 0.01,\\n        }\\n        defaults.update(params)\\n        d = defaults.pop('d')\\n        defaults['x1'] = -d/2.\\n        defaults['x2'] = d/2.\\n        gitem = CircularSolid(brush=(100,100,100,255), **defaults)\\n        Optic.__init__(self, gitem, **defaults)\", 'def propagateRay(self, ray):\\n        \"\"\"Refract, reflect, absorb, and/or scatter ray. This function may create and return new rays\"\"\"', 'def __init__(self, pen=None, brush=None, **opts):\\n        \"\"\"\\n        Arguments for each surface are:\\n           x1,x2 - position of center of _physical surface_\\n           r1,r2 - radius of curvature\\n           d1,d2 - diameter of optic\\n        \"\"\"\\n        defaults = dict(x1=-2, r1=100, d1=25.4, x2=2, r2=100, d2=25.4)\\n        defaults.update(opts)\\n        ParamObj.__init__(self)\\n        self.surfaces = [CircleSurface(defaults[\\'r1\\'], defaults[\\'d1\\']), CircleSurface(-defaults[\\'r2\\'], defaults[\\'d2\\'])]\\n        pg.GraphicsObject.__init__(self)\\n        for s in self.surfaces:\\n            s.setParentItem(self)', 'def paramStateChanged(self):\\n        self.updateSurfaces()', 'def boundingRect(self):\\n        return self.path.boundingRect()', 'def shape(self):\\n        return self.path', 'def paint(self, p, *args):\\n        p.setRenderHints(p.renderHints() | p.Antialiasing)\\n        p.setPen(self.pen)\\n        p.fillPath(self.path, self.brush)\\n        p.drawPath(self.path)', 'def __init__(self, radius=None, diameter=None):\\n        \"\"\"center of physical surface is at 0,0\\n        radius is the radius of the surface. If radius is None, the surface is flat. \\n        diameter is of the optic\\'s edge.\"\"\"\\n        pg.GraphicsObject.__init__(self)', 'def setParams(self, r, d):\\n        self.r = r\\n        self.d = d\\n        self.mkPath()', \"def mkPath(self):\\n        self.prepareGeometryChange()\\n        r = self.r\\n        d = self.d\\n        h2 = d/2.\\n        self.path = QtGui.QPainterPath()\\n        if r == 0:  ## flat surface\\n            self.path.moveTo(0, h2)\\n            self.path.lineTo(0, -h2)\\n        else:\\n            ## half-height of surface can't be larger than radius\\n            h2 = min(h2, abs(r))\", 'def boundingRect(self):\\n        return self.path.boundingRect()', \"def paint(self, p, *args):\\n        return  ## usually we let the optic draw.\\n        #p.setPen(pg.mkPen('r'))\\n        #p.drawPath(self.path)\", 'def intersectRay(self, ray):\\n        ## return the point of intersection and the angle of incidence\\n        #print \"intersect ray\"\\n        h = self.h2\\n        r = self.r\\n        p, dir = ray.currentState(relativeTo=self)  # position and angle of ray in local coords.\\n        #print \"  ray: \", p, dir\\n        p = p - Point(r, 0)  ## move position so center of circle is at 0,0\\n        #print \"  adj: \", p, r', \"def __init__(self, **params):\\n        ParamObj.__init__(self)\\n        defaults = {\\n            'ior': 1.0,\\n            'wl': 500,\\n            'end': None,\\n            'dir': Point(1,0),\\n        }\\n        self.params = {}\\n        pg.GraphicsObject.__init__(self)\\n        self.children = []\\n        parent = params.get('parent', None)\\n        if parent is not None:\\n            defaults['start'] = parent['end']\\n            defaults['wl'] = parent['wl']\\n            self['ior'] = parent['ior']\\n            self['dir'] = parent['dir']\\n            parent.addChild(self)\", 'def clearChildren(self):\\n        for c in self.children:\\n            c.clearChildren()\\n            c.setParentItem(None)\\n            self.scene().removeItem(c)\\n        self.children = []', 'def paramStateChanged(self):\\n        pass', 'def addChild(self, ch):\\n        self.children.append(ch)\\n        ch.setParentItem(self)', \"def currentState(self, relativeTo=None):\\n        pos = self['start']\\n        dir = self['dir']\\n        if relativeTo is None:\\n            return pos, dir\\n        else:\\n            trans = self.itemTransform(relativeTo)[0]\\n            p1 = trans.map(pos)\\n            p2 = trans.map(pos + dir)\\n            return Point(p1), Point(p2-p1)\", \"def setEnd(self, end):\\n        self['end'] = end\\n        self.mkPath()\", \"def paint(self, p, *args):\\n        #p.setPen(pg.mkPen((255,0,0, 150)))\\n        p.setRenderHints(p.renderHints() | p.Antialiasing)\\n        p.setCompositionMode(p.CompositionMode_Plus)\\n        p.setPen(wlPen(self['wl']))\\n        p.drawPath(self.path)\", \"def mkPath(self):\\n        self.prepareGeometryChange()\\n        self.path = QtGui.QPainterPath()\\n        self.path.moveTo(self['start'])\\n        if self['end'] is not None:\\n            self.path.lineTo(self['end'])\\n        else:\\n            self.path.lineTo(self['start']+500*self['dir'])\", 'def __init__(self, rays, optics):\\n        QtCore.QObject.__init__(self)\\n        self.optics = optics\\n        self.rays = rays\\n        for o in self.optics:\\n            o.sigStateChanged.connect(self.trace)\\n        self.trace()']}, {'features': [], 'snippets': ['def __init__(self, message, response=None):\\n        super(RequestError, self).__init__(message)\\n        self.response = response', 'def __init__(self, content):\\n        etree = lxml.etree.fromstring(content)  # noqa: S320\\n        self.init_response_attributes(etree)', \"def execute(cls, **kwargs):\\n        url = cls._get_url()\\n        headers = {\\n            'content-type': 'text/xml',\\n            'SOAPAction': url,\\n        }\\n        data = cls.template.strip().format(\\n            AP_ID=cls.settings['AP_ID'],\\n            AP_PWD=cls.settings['AP_PWD'],\\n            Instant=cls._format_datetime(timezone.now()),\\n            DNSName=cls.settings['DNSName'],\\n            **kwargs\\n        )\\n        cert = (cls.settings['cert_path'], cls.settings['key_path'])\\n        # TODO: add verification\\n        logger.debug(\\n            'Executing POST request to %s with data:\\\\n %s \\\\nheaders: %s',\\n            url,\\n            data,\\n            headers,\\n        )\\n        response = requests.post(\\n            url,\\n            data=data,\\n            headers=headers,\\n            cert=cert,\\n            verify=cls.settings['verify_ssl'],\\n        )\\n        if response.ok:\\n            return cls.response_class(response.content)\\n        else:\\n            message = (\\n                'Failed to execute POST request against %s endpoint. Response [%s]: %s'\\n                % (url, response.status_code, response.content)\\n            )\\n            raise RequestError(message, response)\", \"def _format_datetime(cls, d):\\n        return d.strftime('%Y-%m-%dT%H:%M:%S.000Z')\", \"def _format_transaction_id(cls, transaction_id):\\n        return ('_' + transaction_id)[:32]  # such formation is required by server.\", \"def _get_url(cls):\\n        return urljoin(cls.settings['URL'], cls.url)\", \"def init_response_attributes(self, etree):\\n        try:\\n            self.backend_transaction_id = etree.xpath('//MSS_SignatureResp')[0].attrib[\\n                'MSSP_TransID'\\n            ]\\n            self.status = etree.xpath(\\n                '//ns6:StatusCode', namespaces={'ns6': self.ns_namespace}\\n            )[0].attrib['Value']\\n        except (IndexError, KeyError, lxml.etree.XMLSchemaError) as e:\\n            raise ResponseParseError(\\n                'Cannot parse signature response: %s. Response content: %s'\\n                % (e, lxml.etree.tostring(etree))\\n            )\", \"def execute(cls, transaction_id, phone, message):\\n        kwargs = {\\n            'MessagingMode': 'asynchClientServer',\\n            'AP_TransID': cls._format_transaction_id(transaction_id),\\n            'MSISDN': phone,\\n            'DataToBeSigned': '%s %s' % (cls.settings['message_prefix'], message),\\n            'SignatureProfile': cls.settings['SignatureProfile'],\\n        }\\n        return super(SignatureRequest, cls).execute(**kwargs)\", \"def map(cls, status_code):\\n        if status_code == '502':\\n            return cls.OK\\n        elif status_code == '504':\\n            return cls.PROCESSING\\n        else:\\n            raise UnknownStatusError(\\n                'Received unsupported status in response: %s' % status_code\\n            )\", \"def init_response_attributes(self, etree):\\n        try:\\n            status_code = etree.xpath(\\n                '//ns5:StatusCode', namespaces={'ns5': self.ns_namespace}\\n            )[0].attrib['Value']\\n        except (IndexError, KeyError, lxml.etree.XMLSchemaError) as e:\\n            raise ResponseParseError(\\n                'Cannot parse status response: %s. Response content: %s'\\n                % (e, lxml.etree.tostring(etree))\\n            )\\n        self.status = Statuses.map(status_code)\\n\\n        try:\\n            civil_number_tag = etree.xpath(\\n                '//ns4:UserIdentifier', namespaces={'ns4': self.ns_namespace}\\n            )[0]\\n        except IndexError:\\n            # civil number tag does not exist - this is possible if request is still processing\\n            return\\n        else:\\n            try:\\n                self.civil_number = civil_number_tag.text.split('=')[1]\\n            except IndexError:\\n                raise ResponseParseError(\\n                    'Cannot get civil_number from tag text: %s' % civil_number_tag.text\\n                )\", \"def init_response_attributes(self, etree):\\n        self.status = Statuses.ERRED\\n        try:\\n            self.details = etree.xpath(\\n                '//soapenv:Text', namespaces={'soapenv': self.soapenv_namespace}\\n            )[0].text\\n        except (IndexError, lxml.etree.XMLSchemaError) as e:\\n            raise ResponseParseError(\\n                'Cannot parse error status response: %s. Response content: %s'\\n                % (e, lxml.etree.tostring(etree))\\n            )\"]}, {'features': [], 'snippets': ['def collect(frame):\\n    global frames\\n    frames.append(frame)', 'def measure():\\n    if len(frames) == 0:\\n        QtCore.QTimer.singleShot(100, measure)\\n        return\\n    global run\\n    if run:\\n        global frames\\n        frame = frames[-1]\\n        frames = []\\n        img = frame.data()\\n        w,h = img.shape\\n        img = img[2*w/5:3*w/5, 2*h/5:3*h/5]\\n        w,h = img.shape']}, {'features': [], 'snippets': ['def upgrade():\\n    \"\"\"Upgrade database.\"\"\"\\n    op.create_table(\\n        \\'oauthclient_remoteaccount\\',\\n        sa.Column(\\'id\\', sa.Integer(), nullable=False),\\n        sa.Column(\\'user_id\\', sa.Integer(), nullable=False),\\n        sa.Column(\\'client_id\\', sa.String(length=255), nullable=False),\\n        sa.Column(\\n            \\'extra_data\\',\\n            sqlalchemy_utils.JSONType(),\\n            nullable=False),\\n        sa.ForeignKeyConstraint([\\'user_id\\'], [u\\'accounts_user.id\\'], ),\\n        sa.PrimaryKeyConstraint(\\'id\\'),\\n        sa.UniqueConstraint(\\'user_id\\', \\'client_id\\')\\n    )\\n    op.create_table(\\n        \\'oauthclient_useridentity\\',\\n        sa.Column(\\'id\\', sa.String(length=255), nullable=False),\\n        sa.Column(\\'method\\', sa.String(length=255), nullable=False),\\n        sa.Column(\\'id_user\\', sa.Integer(), nullable=False),\\n        sa.ForeignKeyConstraint([\\'id_user\\'], [u\\'accounts_user.id\\'], ),\\n        sa.PrimaryKeyConstraint(\\'id\\', \\'method\\')\\n    )\\n    op.create_index(\\n        \\'useridentity_id_user_method\\', \\'oauthclient_useridentity\\',\\n        [\\'id_user\\', \\'method\\'], unique=True\\n    )\\n    op.create_table(\\n        \\'oauthclient_remotetoken\\',\\n        sa.Column(\\'id_remote_account\\', sa.Integer(), nullable=False),\\n        sa.Column(\\'token_type\\', sa.String(length=40), nullable=False),\\n        sa.Column(\\n            \\'access_token\\',\\n            sqlalchemy_utils.EncryptedType(),\\n            nullable=False),\\n        sa.Column(\\'secret\\', sa.Text(), nullable=False),\\n        sa.ForeignKeyConstraint(\\n            [\\'id_remote_account\\'], [u\\'oauthclient_remoteaccount.id\\'],\\n            name=\\'fk_oauthclient_remote_token_remote_account\\'\\n        ),\\n        sa.PrimaryKeyConstraint(\\'id_remote_account\\', \\'token_type\\')\\n    )']}, {'features': [], 'snippets': [\"def _normalize_url(url):\\n    url = http_loader.quote_url(unquote(url))\\n    if url.startswith('http:'):\\n        url = url.replace('http:', 'https:', 1)\\n\\n    return url if url.startswith('https://') else 'https://%s' % url\", 'def return_contents(response, url, callback, context):\\n    return http_loader.return_contents(response, url, callback, context)', 'def load(context, url, callback):\\n    return http_loader.load_sync(context, url, callback, normalize_url_func=_normalize_url)']}]\n"
     ]
    }
   ],
   "source": [
    "print(augmented_code_entry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "224n_final_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fc55353a1bb80ecd70b1e2768f9d432cf9eb903a27b85be7b0971feae070db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
